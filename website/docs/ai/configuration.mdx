---
title: AI Configuration
sidebar_label: Configuration
sidebar_position: 2
description: Configure AI providers for the Atmos AI Assistant
---
import Terminal from '@site/src/components/Terminal'
import File from '@site/src/components/File'

# AI Assistant Configuration

Configure the Atmos AI Assistant to use different AI providers and customize their settings.

## Quick Start

Enable AI features with a single provider:

<File title="atmos.yaml">
```yaml
settings:
  ai:
    enabled: true                         # Enable AI features (required)
    default_provider: "anthropic"         # Default AI provider

    # Configure your chosen provider
    providers:
      anthropic:
        model: "claude-3-5-sonnet-20241022"
        api_key_env: "ANTHROPIC_API_KEY"
        max_tokens: 4096
```
</File>

Set your API key:

<Terminal title="shell">
```bash
export ANTHROPIC_API_KEY="your-api-key-here"
```
</Terminal>

Start using AI:

<Terminal title="atmos ai chat">
```bash
atmos ai chat
```
</Terminal>

## Multi-Provider Configuration

Atmos supports 7 AI providers. You can configure multiple providers and switch between them:

<File title="atmos.yaml">
```yaml
settings:
  ai:
    enabled: true
    default_provider: "anthropic"         # Used by default for CLI commands

    # Configure multiple providers
    providers:
      # Cloud Providers
      anthropic:
        model: "claude-3-5-sonnet-20241022"
        api_key_env: "ANTHROPIC_API_KEY"
        max_tokens: 4096

      openai:
        model: "gpt-4o"
        api_key_env: "OPENAI_API_KEY"
        max_tokens: 4096

      gemini:
        model: "gemini-2.0-flash-exp"
        api_key_env: "GEMINI_API_KEY"
        max_tokens: 8192

      grok:
        model: "grok-beta"
        api_key_env: "XAI_API_KEY"
        max_tokens: 4096
        base_url: "https://api.x.ai/v1"

      # Local Provider (no API key needed)
      ollama:
        model: "llama3.3:70b"
        base_url: "http://localhost:11434/v1"

      # Enterprise Providers
      bedrock:
        model: "anthropic.claude-3-5-sonnet-20241022-v2:0"
        base_url: "us-east-1"              # AWS region
        # Uses AWS credentials (no api_key_env needed)

      azureopenai:
        model: "gpt-4o"                    # Your Azure deployment name
        api_key_env: "AZURE_OPENAI_API_KEY"
        base_url: "https://your-resource.openai.azure.com"
        api_version: "2024-02-15-preview"
```
</File>

**Provider Switching:**
- **In CLI**: Set `default_provider` for `atmos ai ask` commands
- **In TUI**: Press `Ctrl+P` to switch providers mid-conversation
- **Per Session**: Select provider when creating sessions with `Ctrl+N`

:::info Complete Provider Documentation
For detailed setup instructions, hardware requirements, security considerations, and pricing for each provider, see [AI Providers](/ai/providers).
:::

## Migration from Old Configuration

If you have an existing single-provider configuration:

<File title="Old Format (still works, but limited)">
```yaml
settings:
  ai:
    enabled: true
    provider: "anthropic"                 # Old pattern
    model: "claude-3-5-sonnet-20241022"
    api_key_env: "ANTHROPIC_API_KEY"
```
</File>

**Migrate to new format** for multi-provider support:

<File title="New Format (recommended)">
```yaml
settings:
  ai:
    enabled: true
    default_provider: "anthropic"         # New pattern
    providers:
      anthropic:
        model: "claude-3-5-sonnet-20241022"
        api_key_env: "ANTHROPIC_API_KEY"
```
</File>

The old format still works for backward compatibility but limits you to a single provider.

## Basic Configuration Options

Global AI settings:

<File title="atmos.yaml">
```yaml
settings:
  ai:
    enabled: true                         # Enable AI features (required)
    default_provider: "anthropic"         # Default provider for CLI commands
    send_context: false                   # Send stack configs to AI (default: false)
    prompt_on_send: true                  # Prompt before sending context (default: true)
    timeout_seconds: 60                   # Request timeout in seconds (default: 60)
    max_context_files: 10                 # Maximum stack files to send (default: 10)
    max_context_lines: 500                # Maximum lines per file (default: 500)
```
</File>

## Provider-Specific Configuration

Configure each provider under the `providers` section. You can configure one or multiple providers.

:::tip
All providers shown below can be configured simultaneously in the `providers` section. Set `default_provider` to choose which one is used by default.
:::

### Anthropic (Claude)

Claude offers advanced reasoning and detailed explanations. Best for complex analysis and code generation.

<File title="atmos.yaml">
```yaml
settings:
  ai:
    enabled: true
    default_provider: "anthropic"
    providers:
      anthropic:
        model: "claude-3-5-sonnet-20241022"   # Default model
        api_key_env: "ANTHROPIC_API_KEY"      # Default: ANTHROPIC_API_KEY
        max_tokens: 4096                      # Default: 4096
```
</File>

**Available Models:**
- `claude-3-5-sonnet-20241022` - Best balance of speed and capability
- `claude-3-opus-20240229` - Most capable, slower
- `claude-3-haiku-20240307` - Fastest, lighter tasks

**Setup:**

1. Get an API key from [Anthropic Console](https://console.anthropic.com/)
2. Set the environment variable:

<Terminal title="shell">
```bash
export ANTHROPIC_API_KEY="your-api-key-here"
```
</Terminal>

### OpenAI (GPT)

OpenAI provides widely available models with strong general capabilities. Best for general-purpose tasks.

<File title="atmos.yaml">
```yaml
settings:
  ai:
    enabled: true
    default_provider: "openai"
    providers:
      openai:
        model: "gpt-4o"                  # Default: gpt-4o
        api_key_env: "OPENAI_API_KEY"    # Default: OPENAI_API_KEY
        max_tokens: 4096                 # Default: 4096
```
</File>

**Available Models:**
- `gpt-4o` (recommended) - Latest multimodal model
- `gpt-4-turbo` - Fast GPT-4 variant
- `gpt-3.5-turbo` - Faster, more economical

**Setup:**

1. Get an API key from [OpenAI Platform](https://platform.openai.com/)
2. Set the environment variable:

<Terminal title="shell">
```bash
export OPENAI_API_KEY="your-api-key-here"
```
</Terminal>

### Google (Gemini)

Gemini offers fast responses with a larger context window. Best for fast queries and large documents.

<File title="atmos.yaml">
```yaml
settings:
  ai:
    enabled: true
    default_provider: "gemini"
    providers:
      gemini:
        model: "gemini-2.0-flash-exp"      # Default: gemini-2.0-flash-exp
        api_key_env: "GEMINI_API_KEY"      # Default: GEMINI_API_KEY
        max_tokens: 8192                   # Default: 8192
```
</File>

**Available Models:**
- `gemini-2.0-flash-exp` (recommended) - Latest fast model
- `gemini-1.5-pro` - Most capable
- `gemini-1.5-flash` - Fastest

**Setup:**

1. Get an API key from [Google AI Studio](https://aistudio.google.com/)
2. Set the environment variable:

<Terminal title="shell">
```bash
export GEMINI_API_KEY="your-api-key-here"
```
</Terminal>

### xAI (Grok)

Grok provides OpenAI-compatible access with real-time knowledge capabilities. Best for current events and real-time information.

<File title="atmos.yaml">
```yaml
settings:
  ai:
    enabled: true
    default_provider: "grok"
    providers:
      grok:
        model: "grok-beta"                # Default: grok-beta
        api_key_env: "XAI_API_KEY"        # Default: XAI_API_KEY
        max_tokens: 4096                  # Default: 4096
        base_url: "https://api.x.ai/v1"   # Default: https://api.x.ai/v1
```
</File>

**Available Models:**
- `grok-beta` - Latest Grok model
- `grok-2` - Previous version

**Setup:**

1. Get an API key from [xAI API](https://x.ai/api)
2. Set the environment variable:

<Terminal title="shell">
```bash
export XAI_API_KEY="your-api-key-here"
```
</Terminal>

### Ollama (Local)

Ollama runs AI models entirely on your machine. No data leaves your computer. Best for privacy, offline use, and zero API costs.

<File title="atmos.yaml">
```yaml
settings:
  ai:
    enabled: true
    default_provider: "ollama"
    providers:
      ollama:
        model: "llama3.3:70b"                     # Default: llama3.3:70b
        base_url: "http://localhost:11434/v1"     # Default: http://localhost:11434/v1
        timeout_seconds: 120                       # Longer timeout for large models
        # No api_key_env needed for local instances
```
</File>

**Available Models:**
- `llama3.3:70b` (recommended, 64GB+ RAM) - Production quality
- `llama3.1:8b` (8GB+ RAM) - Quick queries, limited hardware
- `codellama:13b` (16GB+ RAM) - Code-focused tasks
- `mistral:7b` (8GB+ RAM) - Fast responses

:::info Hardware Requirements & Setup
Ollama requires installation and adequate RAM. See [Ollama Documentation](/ai/providers#ollama) for installation instructions, model selection, and hardware requirements.
:::

**Setup:**

<Terminal title="shell">
```bash
# Install Ollama (macOS)
brew install ollama

# Start Ollama service
ollama serve

# Pull a model
ollama pull llama3.3:70b
```
</Terminal>

### AWS Bedrock (Enterprise)

Run Claude models via AWS Bedrock for enterprise security and compliance. Best for AWS-native organizations.

<File title="atmos.yaml">
```yaml
settings:
  ai:
    enabled: true
    default_provider: "bedrock"
    providers:
      bedrock:
        model: "anthropic.claude-3-5-sonnet-20241022-v2:0"  # Full Bedrock model ID
        base_url: "us-east-1"                                # AWS region
        max_tokens: 4096
        # Uses AWS SDK credential chain (no api_key_env needed)
```
</File>

**Available Models:**
- `anthropic.claude-3-5-sonnet-20241022-v2:0` (recommended)
- `anthropic.claude-3-opus-20240229-v1:0`
- `anthropic.claude-3-haiku-20240307-v1:0`

**Security Benefits:**
- Data never leaves AWS infrastructure
- IAM-based access control
- Complete audit trail via CloudTrail
- VPC endpoints and PrivateLink support
- Compliance certifications (SOC2, HIPAA, ISO)

**Setup:**

<Terminal title="shell">
```bash
# Configure AWS credentials (one of):

# Option 1: Environment variables
export AWS_ACCESS_KEY_ID="AKIA..."
export AWS_SECRET_ACCESS_KEY="..."

# Option 2: AWS Profile
export AWS_PROFILE="bedrock-profile"

# Option 3: IAM role (automatic in EC2/ECS/EKS)
```
</Terminal>

:::info Complete Bedrock Documentation
See [AWS Bedrock Setup](/ai/providers#aws-bedrock) for IAM permissions, model access, and detailed configuration.
:::

### Azure OpenAI (Enterprise)

Run GPT models via Azure OpenAI for enterprise compliance. Best for Azure-native organizations.

<File title="atmos.yaml">
```yaml
settings:
  ai:
    enabled: true
    default_provider: "azureopenai"
    providers:
      azureopenai:
        model: "gpt-4o"                                      # Your Azure deployment name
        api_key_env: "AZURE_OPENAI_API_KEY"
        base_url: "https://your-resource.openai.azure.com"   # Your Azure resource URL
        api_version: "2024-02-15-preview"                    # API version
        max_tokens: 4096
```
</File>

**Important:** Use your **deployment name** from Azure Portal, not the OpenAI model name.

**Security Benefits:**
- Data residency control
- Azure AD integration with managed identities
- Compliance certifications (SOC2, HIPAA, ISO 27001)
- Private endpoints via Azure Private Link
- Customer-managed encryption keys (BYOK)

**Setup:**

<Terminal title="shell">
```bash
# Get API key from Azure Portal
export AZURE_OPENAI_API_KEY="..."

# Or use Azure AD managed identity (in production)
```
</Terminal>

:::info Complete Azure OpenAI Documentation
See [Azure OpenAI Setup](/ai/providers#azure-openai) for resource configuration, deployment names, and authentication options.
:::

## Configuration Options

### Required Settings

<dl>
  <dt>`enabled`</dt>
  <dd>Enable or disable AI features (default: `false`)</dd>

  <dt>`default_provider`</dt>
  <dd>Default AI provider for CLI commands: `anthropic`, `openai`, `gemini`, `grok`, `ollama`, `bedrock`, or `azureopenai` (default: `anthropic`)</dd>

  <dt>`providers`</dt>
  <dd>Map of provider configurations. Each provider has its own `model`, `api_key_env`, and other provider-specific settings.</dd>
</dl>

### Provider-Specific Settings

Each provider in the `providers` map supports:

<dl>
  <dt>`model`</dt>
  <dd>Specific model to use (defaults vary by provider)</dd>

  <dt>`api_key_env`</dt>
  <dd>Environment variable name containing the API key (not needed for Ollama local instances or AWS Bedrock with IAM)</dd>

  <dt>`max_tokens`</dt>
  <dd>Maximum tokens per response (defaults vary by provider)</dd>

  <dt>`base_url`</dt>
  <dd>Custom API endpoint URL (required for Grok, Azure OpenAI; optional for Ollama remote instances; AWS region for Bedrock)</dd>

  <dt>`timeout_seconds`</dt>
  <dd>Request timeout in seconds (default: 60, increase for large local models)</dd>
</dl>

### Global Settings

<dl>
  <dt>`send_context`</dt>
  <dd>Automatically send stack configurations to AI for context-aware answers (default: `false`)</dd>

  <dt>`prompt_on_send`</dt>
  <dd>Prompt user for consent before sending context, even when `send_context` is `true` (default: `true`)</dd>
</dl>

## Session Management

Atmos AI supports persistent conversation sessions that maintain message history across CLI invocations. Sessions are stored locally in SQLite and can be resumed anytime.

### Configuration

<File title="atmos.yaml">
```yaml
settings:
  ai:
    enabled: true
    default_provider: "anthropic"
    providers:
      anthropic:
        model: "claude-3-5-sonnet-20241022"
        api_key_env: "ANTHROPIC_API_KEY"

    # Session management settings
    sessions:
      enabled: true                    # Enable persistent sessions (default: false)
      storage: "sqlite"                # Storage backend: sqlite (default: sqlite)
      path: ".atmos/sessions"          # Storage location (default: .atmos/sessions)
      max_sessions: 10                 # Maximum sessions to keep (default: 10)
      auto_save: true                  # Auto-save messages (default: true)
      retention_days: 30               # Delete sessions older than N days (default: 30)
```
</File>

### Session Settings

<dl>
  <dt>`sessions.enabled`</dt>
  <dd>Enable persistent conversation sessions (default: `false`)</dd>

  <dt>`sessions.storage`</dt>
  <dd>Storage backend for sessions. Currently only `sqlite` is supported (default: `sqlite`)</dd>

  <dt>`sessions.path`</dt>
  <dd>Directory path where session database is stored (default: `.atmos/sessions`)</dd>

  <dt>`sessions.max_sessions`</dt>
  <dd>Maximum number of sessions to keep per project. Oldest sessions are cleaned up automatically (default: `10`)</dd>

  <dt>`sessions.auto_save`</dt>
  <dd>Automatically save messages to the session after each exchange (default: `true`)</dd>

  <dt>`sessions.retention_days`</dt>
  <dd>Number of days to retain sessions before cleanup. Use `0` to keep sessions indefinitely (default: `30`)</dd>
</dl>

### Using Sessions

When sessions are enabled, you can:

<Terminal title="shell">
```bash
# Start a named session
atmos ai chat --session vpc-refactor

# List all sessions
atmos ai sessions

# Resume an existing session
atmos ai chat --session vpc-refactor

# Clean old sessions
atmos ai sessions clean --older-than 30d
```
</Terminal>

Sessions store:
- Complete message history (user and assistant messages)
- Session metadata (creation time, last update, model used)
- Context items (stack files, component references)

### Storage Location

Sessions are stored in a SQLite database at:
```
<project-root>/.atmos/sessions/sessions.db
```

The database includes three tables:
- `sessions` - Session metadata
- `messages` - Message history
- `session_context` - Context items

### Privacy Note

Sessions are stored **locally on your machine**. No session data is sent to AI providers except during active conversations.

## Tool Execution

Atmos AI can execute read-only tools to query your infrastructure configuration. Tools allow the AI to inspect stacks, components, and validate configurations without manual intervention.

### Configuration

<File title="atmos.yaml">
```yaml
settings:
  ai:
    enabled: true
    default_provider: "anthropic"
    providers:
      anthropic:
        model: "claude-3-5-sonnet-20241022"
        api_key_env: "ANTHROPIC_API_KEY"

    # Tool execution settings
    tools:
      enabled: true                        # Enable tool execution (default: false)
      require_confirmation: true           # Prompt before executing tools (default: true)

      # Allowed tools (no confirmation needed)
      allowed_tools:
        - atmos_describe_component         # Allow without prompting
        - atmos_list_stacks
        - atmos_validate_stacks
        - atmos_describe_*                 # Wildcard patterns supported

      # Restricted tools (always prompt)
      restricted_tools:
        - file_edit                        # Always require confirmation
        - atmos_terraform_plan

      # Blocked tools (never allow)
      blocked_tools:
        - atmos_terraform_destroy          # Completely blocked
        - atmos_terraform_apply

      yolo_mode: false                     # Skip all confirmations (DANGEROUS!)
```
</File>

### Tool Settings

<dl>
  <dt>`tools.enabled`</dt>
  <dd>Enable AI tool execution capabilities (default: `false`)</dd>

  <dt>`tools.require_confirmation`</dt>
  <dd>Prompt user before executing any tool not in `allowed_tools` (default: `true`)</dd>

  <dt>`tools.allowed_tools`</dt>
  <dd>List of tools that can execute without user confirmation. Supports wildcard patterns like `atmos_*` (default: `[]`)</dd>

  <dt>`tools.restricted_tools`</dt>
  <dd>List of tools that always require user confirmation, even if `require_confirmation` is `false` (default: `[]`)</dd>

  <dt>`tools.blocked_tools`</dt>
  <dd>List of tools that are completely blocked from execution (default: `[]`)</dd>

  <dt>`tools.yolo_mode`</dt>
  <dd>Skip all permission checks and execute all tools automatically. **DANGEROUS!** Only use in trusted CI/CD environments (default: `false`)</dd>
</dl>

### Available Tools

Atmos AI includes these read-only tools:

| Tool | Description | Permission Required |
|------|-------------|---------------------|
| `atmos_describe_component` | Describe component configuration in a stack | No |
| `atmos_list_stacks` | List all available stacks | No |
| `atmos_validate_stacks` | Validate stack configurations | No |

### Permission Flow

When the AI requests tool execution:

1. **YOLO Mode Enabled?** → Execute immediately (skip all checks)
2. **Tool Blocked?** → Deny execution
3. **Tool in Allowed List?** → Execute without prompt
4. **Tool in Restricted List?** → Always prompt user
5. **Default Behavior** → Prompt user if `require_confirmation` is `true`

### Permission Prompt Example

When a tool requires confirmation:

<Terminal title="atmos ai chat">
```bash
🔧 Tool Execution Request
Tool: atmos_describe_component
Description: Describe an Atmos component configuration in a specific stack

Parameters:
  component: vpc
  stack: prod-use1

Allow execution? (y/N): y

✅ Executing tool...
```
</Terminal>

### Wildcard Patterns

You can use wildcards in tool lists:

```yaml
tools:
  allowed_tools:
    - atmos_describe_*    # Allow all describe commands
    - atmos_list_*        # Allow all list commands
    - file_*              # Allow all file operations
```

### Security Best Practices

1. **Start Conservative**: Begin with `require_confirmation: true` and add trusted tools to `allowed_tools` over time
2. **Block Destructive Operations**: Always add dangerous tools like `terraform_destroy` to `blocked_tools`
3. **Use Restricted List**: For tools that modify state, add them to `restricted_tools` to force confirmation
4. **Avoid YOLO Mode**: Only enable `yolo_mode` in isolated CI/CD environments with strict access controls
5. **Review Permissions Regularly**: Audit your tool configuration as new tools are added

### Tool Execution Timeouts

Tools have a default timeout of 30 seconds. Long-running operations will be cancelled automatically to prevent hanging.

## Language Server Protocol (LSP) Integration

Atmos AI can integrate with Language Server Protocol (LSP) servers to validate YAML, Terraform, and HCL files with precise error diagnostics.

:::info
LSP is an independent Atmos feature that can optionally be used by AI. For comprehensive LSP documentation, see [LSP Integration](/lsp).
:::

### Quick Setup

<File title="atmos.yaml">
```yaml
settings:
  # Configure LSP (independent feature)
  lsp:
    enabled: true
    servers:
      yaml-ls:
        command: "yaml-language-server"
        args: ["--stdio"]
        filetypes: ["yaml", "yml"]

  # Enable AI to use LSP
  ai:
    enabled: true
    default_provider: "anthropic"
    providers:
      anthropic:
        model: "claude-3-5-sonnet-20241022"
        api_key_env: "ANTHROPIC_API_KEY"
    use_lsp: true                      # Allow AI to use LSP (default: false)
    tools:
      enabled: true                    # Required for AI to use tools
```
</File>

### AI LSP Tool

When enabled, AI can use the `validate_file_lsp` tool to validate files:

<Terminal title="atmos ai chat">
```bash
$ atmos ai chat

You: Validate stacks/prod/vpc.yaml

AI: [Uses validate_file_lsp tool]

Found 2 issue(s):
ERRORS (1):
1. Line 15: Unknown property 'vpc_ciddr'

WARNINGS (1):
1. Line 23: Deprecated property 'availability_zones'
```
</Terminal>

**Learn more:**
- [LSP Integration](/lsp) - Complete LSP documentation
- [LSP Configuration](/lsp#configuration-options) - Server setup
- [LSP Troubleshooting](/lsp#troubleshooting) - Common issues

## Project Memory

Atmos AI can remember project-specific context, patterns, and conventions using a persistent memory file (`ATMOS.md`). This allows the AI to provide consistent, context-aware responses without you having to repeatedly explain your project setup.

### What is Project Memory?

Project Memory is a markdown file (`ATMOS.md`) stored in your project root that contains:
- **Project Context**: Organization name, Atmos version, regions, environments
- **Common Commands**: Frequently used Atmos commands for your project
- **Stack Patterns**: Naming conventions and YAML structure patterns
- **Infrastructure Patterns**: Multi-region setups, component dependencies
- **Component Catalog**: Your component organization and structure
- **Team Conventions**: Development practices and documentation standards
- **Recent Learnings**: AI-discovered insights about your project

This memory is automatically included with every AI request when enabled, providing consistent context across all conversations.

### Configuration

<File title="atmos.yaml">
```yaml
settings:
  ai:
    enabled: true
    default_provider: "anthropic"
    providers:
      anthropic:
        model: "claude-3-5-sonnet-20241022"
        api_key_env: "ANTHROPIC_API_KEY"

    # Project memory settings
    memory:
      enabled: true                        # Enable project memory (default: false)
      file_path: "ATMOS.md"                # Memory file location (default: ATMOS.md)
      auto_update: false                   # Let AI update memory (default: false)
      create_if_missing: true              # Auto-create template (default: true)

      # Sections to include in AI context
      sections:
        - project_context                  # Basic project info
        - common_commands                  # Frequently used commands
        - stack_patterns                   # Stack naming and structure
        - frequent_issues                  # Known issues and solutions
        - infrastructure_patterns          # Multi-region, dependencies
        - component_catalog_structure      # Component organization
        - team_conventions                 # Development practices
        - recent_learnings                 # AI-discovered insights
```
</File>

### Memory Settings

<dl>
  <dt>`memory.enabled`</dt>
  <dd>Enable project memory features (default: `false`)</dd>

  <dt>`memory.file_path`</dt>
  <dd>Path to the memory file, relative to project root or absolute (default: `ATMOS.md`)</dd>

  <dt>`memory.auto_update`</dt>
  <dd>Allow AI to automatically update memory with learned insights (default: `false`). When enabled, AI can add new learnings to the Recent Learnings section.</dd>

  <dt>`memory.create_if_missing`</dt>
  <dd>Automatically create a template memory file if it doesn't exist (default: `true`)</dd>

  <dt>`memory.sections`</dt>
  <dd>List of memory sections to include in AI context. Order matters - sections appear in the specified order (default: all sections enabled)</dd>
</dl>

### Creating Your Memory File

When you enable project memory with `create_if_missing: true`, Atmos automatically creates a template `ATMOS.md` file on first use:

<Terminal title="atmos ai chat">
```bash
$ atmos ai chat

# If ATMOS.md doesn't exist, it will be created automatically
✅ Created default ATMOS.md template

# The AI will now include project memory in all requests
```
</Terminal>

Alternatively, you can create it manually:

<Terminal title="shell">
```bash
$ atmos ai memory init

✅ Created ATMOS.md template at /path/to/your/project/ATMOS.md

Edit this file to customize your project memory.
```
</Terminal>

### Memory File Structure

The `ATMOS.md` file uses markdown with specific section headers:

<File title="ATMOS.md">
```markdown
# Atmos Project Memory

> This file is automatically maintained by Atmos AI to remember project-specific
> context, patterns, and conventions. Edit freely - AI will preserve manual changes.

## Project Context

**Organization:** acme-corp
**Atmos Version:** 1.89.0
**Primary Regions:** us-east-1, us-west-2
**Environments:** dev, staging, prod

**Stack Naming Pattern:**
\```
{org}-{tenant}-{environment}-{region}-{stage}
Example: acme-core-prod-use1-network
\```

## Common Commands

### Describe and Validate
\```bash
# Describe a component
atmos describe component <component> -s <stack>

# List all stacks
atmos list stacks

# Validate stacks
atmos validate stacks
\```

### Terraform Operations
\```bash
# Plan a component
atmos terraform plan <component> -s <stack>

# Apply a component
atmos terraform apply <component> -s <stack>
\```

## Stack Patterns

### Stack Structure
\```yaml
# Common import pattern
import:
  - catalog/stacks/baseline
  - catalog/stacks/network
\```

### CIDR Blocks
- dev: 10.0.0.0/16
- staging: 10.1.0.0/16
- prod: 10.2.0.0/16

## Frequent Issues & Solutions

### Stack not found error
**Problem:** `Error: stack 'my-stack' not found`
**Solution:** Check stack naming and verify config exists in stacks/ directory

### YAML validation fails
**Problem:** `invalid YAML: mapping values are not allowed`
**Solution:** Check indentation - YAML requires consistent 2-space indents

## Infrastructure Patterns

### Multi-Region Setup
- Primary region for production workloads
- DR region with replication
- Cross-region networking configured

### Component Dependencies
\```
vpc → subnets → security-groups → rds → eks
\```

## Component Catalog Structure

\```
components/
  terraform/
    vpc/           # VPC and networking
    rds/           # RDS databases
    eks/           # EKS clusters
    s3/            # S3 buckets
\```

## Team Conventions

- All infrastructure changes require PR review
- Use `atmos validate` before committing
- Tag all resources appropriately
- Document component changes

## Recent Learnings

*AI will add notes here as it learns about the project*
```
</File>

### Customizing Memory Sections

You can control which sections are included in AI context:

<File title="atmos.yaml">
```yaml
settings:
  ai:
    memory:
      enabled: true
      # Only include essential sections for faster performance
      sections:
        - project_context
        - common_commands
        - frequent_issues
```
</File>

This reduces token usage while keeping the most relevant context.

### Manual vs Auto-Update

**Manual Updates (Recommended):**
```yaml
memory:
  enabled: true
  auto_update: false  # You control all changes
```

You edit `ATMOS.md` directly with your preferred text editor. The AI reads it but never modifies it. This gives you full control over project memory.

**Auto-Update Mode (Experimental):**
```yaml
memory:
  enabled: true
  auto_update: true  # AI can add learnings
```

The AI can append insights to the "Recent Learnings" section as it discovers patterns in your project. Other sections remain read-only.

### Memory vs Context Sharing

Project Memory and Context Sharing serve different purposes:

| Feature | Project Memory | Context Sharing |
|---------|----------------|-----------------|
| **Content** | High-level patterns, conventions, common commands | Actual stack YAML files and configurations |
| **Update frequency** | Rarely (manual or learned insights) | Every request (real-time) |
| **Size** | Small (few KB) | Large (multiple files) |
| **Privacy** | Low risk (you control content) | Higher risk (may contain secrets) |
| **Use case** | Consistent AI behavior across chats | Deep analysis of specific configs |

**Best Practice:** Enable both for optimal results:
- Memory provides consistent baseline knowledge
- Context sharing adds real-time specifics when needed

### Viewing Memory

Check what memory the AI is using:

<Terminal title="atmos ai memory show">
```bash
$ atmos ai memory show

# Displays formatted memory sections
# Shows exactly what's being sent to AI

# Project Memory (ATMOS.md)

## Project Context

**Organization:** acme-corp
**Atmos Version:** 1.89.0
...
```
</Terminal>

### Editing Memory

Edit `ATMOS.md` with any text editor:

<Terminal title="shell">
```bash
$ code ATMOS.md           # VS Code
$ vim ATMOS.md            # Vim
$ nano ATMOS.md           # Nano
```
</Terminal>

Changes take effect immediately on the next AI request.

### Privacy & Security

Project memory is **never sent to external servers** except as part of your AI requests:

- Memory file is stored locally in your project
- Only included in AI API requests when memory is enabled
- You control all content (with `auto_update: false`)
- Review `ATMOS.md` before committing to version control
- Consider adding sensitive projects to `.gitignore` if needed

### Memory File in Version Control

**Should you commit `ATMOS.md` to git?**

✅ **Yes, if:**
- Memory contains general patterns and conventions
- Team members would benefit from shared context
- No sensitive information is included

❌ **No, if:**
- Memory contains account IDs, secrets, or sensitive data
- Project is internal and context shouldn't be shared

Add to `.gitignore` if sensitive:
```
# Don't commit project memory
ATMOS.md
```

### Performance Considerations

Project memory adds tokens to every AI request:

- **Small memory** (1-2 KB): Negligible impact
- **Medium memory** (5-10 KB): ~500-1000 tokens per request
- **Large memory** (20+ KB): ~2000+ tokens per request

**Optimization tips:**
1. Only enable sections you actively use
2. Keep descriptions concise
3. Remove outdated information
4. Use `auto_update: false` to prevent growth

### Troubleshooting

**Memory not loading:**
```bash
# Check if file exists
ls -la ATMOS.md

# Verify configuration
grep -A5 "memory:" atmos.yaml

# Test with explicit enable
ATMOS_AI_MEMORY_ENABLED=true atmos ai chat
```

**AI not using memory context:**
- Verify `memory.enabled: true` in atmos.yaml
- Check that `ATMOS.md` exists in project root
- Ensure sections are properly formatted with `##` headers

**Memory file not created:**
- Set `create_if_missing: true`
- Check file permissions in project directory
- Verify Atmos has write access to project root

### Example Workflow

1. **Initialize memory:**
   ```bash
   atmos ai chat  # Auto-creates ATMOS.md if missing
   ```

2. **Customize for your project:**
   ```bash
   vim ATMOS.md  # Add your org name, patterns, conventions
   ```

3. **Use AI with memory:**
   ```bash
   atmos ai chat
   # AI now remembers your project context across all conversations
   ```

4. **Update as project evolves:**
   ```bash
   vim ATMOS.md  # Update stack patterns, add new learnings
   ```

5. **Share with team (optional):**
   ```bash
   git add ATMOS.md
   git commit -m "docs: Add Atmos AI project memory"
   git push
   ```

## Context-Aware AI Queries

The AI assistant can analyze your actual stack configurations to provide specific, context-aware answers.

### How It Works

When you ask questions about your repository (e.g., "Describe the stacks in this repo"), the AI can:
- Read your stack YAML files
- Analyze component configurations
- Provide specific answers based on your actual setup

### Privacy & Security

**Important**: Enabling context sharing sends your stack files to the AI provider's servers.

- Stack configs may contain sensitive information (AWS account IDs, regions, etc.)
- Data is sent to your chosen AI provider (Anthropic, OpenAI, Google, or xAI)
- No data is stored by Atmos - it's only included in the API request
- You control when context is sent through configuration and prompts

### Configuration Options

There are three ways to control context sharing:

#### Option 1: Environment Variable (Highest Priority)

<Terminal title="shell">
```bash
# Always send context without prompting
export ATMOS_AI_SEND_CONTEXT=true
atmos ai ask "Describe the stacks in this repo"

# Never send context
export ATMOS_AI_SEND_CONTEXT=false
atmos ai ask "What components are available?"
```
</Terminal>

#### Option 2: Configuration File

<File title="atmos.yaml">
```yaml
settings:
  ai:
    enabled: true
    default_provider: "anthropic"
    providers:
      anthropic:
        model: "claude-3-5-sonnet-20241022"
        api_key_env: "ANTHROPIC_API_KEY"
    # Always send context, but prompt each time for confirmation
    send_context: true
    prompt_on_send: true
```
</File>

Or automatically send without prompting:

<File title="atmos.yaml">
```yaml
settings:
  ai:
    enabled: true
    default_provider: "anthropic"
    providers:
      anthropic:
        model: "claude-3-5-sonnet-20241022"
        api_key_env: "ANTHROPIC_API_KEY"
    # Always send context without prompting (not recommended)
    send_context: true
    prompt_on_send: false
```
</File>

#### Option 3: Interactive Prompt (Default)

By default, if you ask a question that seems to need context, you'll be prompted:

<Terminal title="atmos ai ask">
```bash
$ atmos ai ask "Describe the stacks in this repo"

⚠️  This question may benefit from your stack configurations.
📤 Send stack files to AI provider for analysis? (y/N): y
📖 Reading stack configurations...
👽 Thinking...

Based on your stack configurations, you have the following stacks:
...
```
</Terminal>

### Questions That Trigger Context

The AI automatically detects when context might be helpful based on keywords:

- "this repo" / "my repo"
- "my stack" / "these stacks"
- "my component" / "these components"
- "my configuration" / "my config"
- "what stacks" / "list stacks"
- "describe the" / "analyze"

### Disabling Context

To never send context:

<File title="atmos.yaml">
```yaml
settings:
  ai:
    enabled: true
    default_provider: "anthropic"
    providers:
      anthropic:
        model: "claude-3-5-sonnet-20241022"
        api_key_env: "ANTHROPIC_API_KEY"
    send_context: false  # Default - never send context
```
</File>

Or use the environment variable:

<Terminal title="shell">
```bash
export ATMOS_AI_SEND_CONTEXT=false
```
</Terminal>

## Environment Variables

You can use Atmos-prefixed environment variables as alternatives:

<Terminal title="shell">
```bash
# API Keys
export ATMOS_ANTHROPIC_API_KEY="your-key"  # Anthropic
export ATMOS_OPENAI_API_KEY="your-key"     # OpenAI
export ATMOS_GEMINI_API_KEY="your-key"     # Gemini
export ATMOS_XAI_API_KEY="your-key"        # Grok

# Context Control
export ATMOS_AI_SEND_CONTEXT=true   # Send stack configs to AI
export ATMOS_AI_SEND_CONTEXT=false  # Never send context (default)
```
</Terminal>

## Verify Configuration

Test your AI configuration:

<Terminal title="atmos ai ask">
```bash
$ atmos ai ask "Hello, are you working?"

👽 Thinking...

Yes, I'm working! The AI assistant is properly configured and ready
to help you with your Atmos infrastructure management tasks.
```
</Terminal>

## Switching Providers

With multi-provider configuration, you can switch between providers in multiple ways:

### Runtime Switching (TUI Only)

In the interactive TUI mode (`atmos ai chat`), press **Ctrl+P** to switch providers mid-conversation:

<Terminal title="atmos ai chat">
```bash
$ atmos ai chat

You: Explain VPC CIDR blocks

AI: [Response from Anthropic Claude...]

# Press Ctrl+P during conversation
Provider Selection:
  1. Anthropic (Claude) - claude-3-5-sonnet-20241022
  2. OpenAI (GPT) - gpt-4o
  3. Google (Gemini) - gemini-2.0-flash-exp
  4. Ollama (Local) - llama3.3:70b

Select provider (1-4): 2

Switched to OpenAI (GPT)

You: Continue with the same question

AI: [Response from OpenAI GPT...]
```
</Terminal>

### Change Default Provider

For CLI commands (`atmos ai ask`), change the `default_provider` setting:

<File title="atmos.yaml">
```yaml
settings:
  ai:
    enabled: true
    default_provider: "gemini"  # Changed from anthropic to gemini
    providers:
      anthropic:
        model: "claude-3-5-sonnet-20241022"
        api_key_env: "ANTHROPIC_API_KEY"
      gemini:
        model: "gemini-2.0-flash-exp"
        api_key_env: "GEMINI_API_KEY"
```
</File>

### Create Provider-Specific Sessions

When creating a new session with Ctrl+N in TUI, you can select which provider to use for that session. The session remembers the provider choice.

### Provider Strengths

Choose based on your needs:

- **Claude (Anthropic)**: Best for detailed explanations, code analysis, and complex reasoning
- **GPT (OpenAI)**: Strong general capabilities, widely available, well-documented
- **Gemini (Google)**: Fast responses, excellent for large contexts, cost-effective
- **Grok (xAI)**: Real-time knowledge, OpenAI-compatible API
- **Ollama (Local)**: Complete privacy, offline capable, no API costs
- **Bedrock (AWS)**: Enterprise security, AWS integration, compliance
- **Azure OpenAI**: Enterprise compliance, Azure integration, data residency

## Performance Tuning

You can adjust timeout and context limits to optimize performance for your needs:

<File title="atmos.yaml">
```yaml
settings:
  ai:
    enabled: true
    default_provider: "anthropic"
    providers:
      anthropic:
        model: "claude-3-5-sonnet-20241022"
        api_key_env: "ANTHROPIC_API_KEY"
        timeout_seconds: 120     # Increase for complex questions (default: 60)

    # Context limits (global settings)
    max_context_files: 20        # Send more files for deeper analysis (default: 10)
    max_context_lines: 1000      # Increase for larger files (default: 500)
```
</File>

### Configuration Options

| Option | Default | Description |
|--------|---------|-------------|
| `timeout_seconds` | 60 | Maximum time to wait for AI response |
| `max_context_files` | 10 | Maximum number of stack files to send |
| `max_context_lines` | 500 | Maximum lines per file to include |

### Recommendations

- **For large repositories**: Increase `max_context_files` and `max_context_lines`
- **For slow connections**: Increase `timeout_seconds` to 120 or more
- **For faster responses**: Reduce context limits to 5 files and 250 lines
- **For cost optimization**: Lower context limits to reduce token usage

## Security Best Practices

1. **Never commit API keys** to version control
2. **Use environment variables** for all API keys
3. **Rotate keys regularly** per provider recommendations
4. **Set spending limits** in provider dashboards
5. **Monitor usage** to detect anomalies

## Next Steps

- [AI Commands](/cli/commands/ai/usage) - Learn how to use AI features
- [Troubleshooting](/ai/troubleshooting) - Common configuration issues
