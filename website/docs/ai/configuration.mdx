---
title: AI Configuration
sidebar_label: Configuration
sidebar_position: 2
description: Configure AI providers for the Atmos AI Assistant
---
import Terminal from '@site/src/components/Terminal'
import File from '@site/src/components/File'

# AI Assistant Configuration

Configure the Atmos AI Assistant to use different AI providers and customize their settings.

## Basic Configuration

Enable AI features in your `atmos.yaml` configuration file:

<File title="atmos.yaml">
```yaml
settings:
  ai:
    enabled: true                         # Enable AI features (required)
    provider: "anthropic"                 # AI provider (default: anthropic)
    model: "claude-3-5-sonnet-20241022"   # Model to use
    api_key_env: "ANTHROPIC_API_KEY"      # Environment variable for API key
    max_tokens: 4096                      # Maximum tokens per response
    send_context: false                   # Send stack configs to AI (default: false)
    prompt_on_send: true                  # Prompt before sending context (default: true)
    timeout_seconds: 60                   # Request timeout in seconds (default: 60)
    max_context_files: 10                 # Maximum stack files to send (default: 10)
    max_context_lines: 500                # Maximum lines per file (default: 500)
```
</File>

## Provider-Specific Configuration

### Anthropic (Claude)

Claude is the default provider, offering advanced reasoning and detailed explanations.

<File title="atmos.yaml">
```yaml
settings:
  ai:
    enabled: true
    provider: "anthropic"
    model: "claude-3-5-sonnet-20241022"   # Default model
    api_key_env: "ANTHROPIC_API_KEY"      # Default: ANTHROPIC_API_KEY
    max_tokens: 4096                      # Default: 4096
```
</File>

**Available Models:**
- `claude-3-5-sonnet-20241022` - Best balance of speed and capability
- `claude-3-opus-20240229` - Most capable, slower
- `claude-3-haiku-20240307` - Fastest, lighter tasks

**Setup:**

1. Get an API key from [Anthropic Console](https://console.anthropic.com/)
2. Set the environment variable:

<Terminal title="shell">
```bash
export ANTHROPIC_API_KEY="your-api-key-here"
```
</Terminal>

### OpenAI (GPT)

OpenAI provides widely available models with strong general capabilities.

<File title="atmos.yaml">
```yaml
settings:
  ai:
    enabled: true
    provider: "openai"
    model: "gpt-4o"                  # Default: gpt-4o
    api_key_env: "OPENAI_API_KEY"    # Default: OPENAI_API_KEY
    max_tokens: 4096                 # Default: 4096
```
</File>

**Available Models:**
- `gpt-4o` (recommended) - Latest multimodal model
- `gpt-4-turbo` - Fast GPT-4 variant
- `gpt-3.5-turbo` - Faster, more economical

**Setup:**

1. Get an API key from [OpenAI Platform](https://platform.openai.com/)
2. Set the environment variable:

<Terminal title="shell">
```bash
export OPENAI_API_KEY="your-api-key-here"
```
</Terminal>

### Google (Gemini)

Gemini offers fast responses with a larger context window.

<File title="atmos.yaml">
```yaml
settings:
  ai:
    enabled: true
    provider: "gemini"
    model: "gemini-2.0-flash-exp"      # Default: gemini-2.0-flash-exp
    api_key_env: "GEMINI_API_KEY"      # Default: GEMINI_API_KEY
    max_tokens: 8192                   # Default: 8192
```
</File>

**Available Models:**
- `gemini-2.0-flash-exp` (recommended) - Latest fast model
- `gemini-1.5-pro` - Most capable
- `gemini-1.5-flash` - Fastest

**Setup:**

1. Get an API key from [Google AI Studio](https://aistudio.google.com/)
2. Set the environment variable:

<Terminal title="shell">
```bash
export GEMINI_API_KEY="your-api-key-here"
```
</Terminal>

### xAI (Grok)

Grok provides OpenAI-compatible access with real-time knowledge capabilities.

<File title="atmos.yaml">
```yaml
settings:
  ai:
    enabled: true
    provider: "grok"
    model: "grok-beta"                # Default: grok-beta
    api_key_env: "XAI_API_KEY"        # Default: XAI_API_KEY
    max_tokens: 4096                  # Default: 4096
    base_url: "https://api.x.ai/v1"   # Default: https://api.x.ai/v1
```
</File>

**Available Models:**
- `grok-beta` - Latest Grok model
- `grok-2` - Previous version

**Setup:**

1. Get an API key from [xAI API](https://x.ai/api)
2. Set the environment variable:

<Terminal title="shell">
```bash
export XAI_API_KEY="your-api-key-here"
```
</Terminal>

## Configuration Options

### Required Settings

<dl>
  <dt>`enabled`</dt>
  <dd>Enable or disable AI features (default: `false`)</dd>

  <dt>`provider`</dt>
  <dd>AI provider to use: `anthropic`, `openai`, `gemini`, or `grok` (default: `anthropic`)</dd>
</dl>

### Optional Settings

<dl>
  <dt>`model`</dt>
  <dd>Specific model to use (defaults vary by provider)</dd>

  <dt>`api_key_env`</dt>
  <dd>Environment variable name containing the API key (defaults vary by provider)</dd>

  <dt>`max_tokens`</dt>
  <dd>Maximum tokens per response (defaults vary by provider)</dd>

  <dt>`base_url`</dt>
  <dd>Custom API endpoint URL (Grok only, optional for other providers)</dd>

  <dt>`send_context`</dt>
  <dd>Automatically send stack configurations to AI for context-aware answers (default: `false`)</dd>

  <dt>`prompt_on_send`</dt>
  <dd>Prompt user for consent before sending context, even when `send_context` is `true` (default: `true`)</dd>
</dl>

## Session Management

Atmos AI supports persistent conversation sessions that maintain message history across CLI invocations. Sessions are stored locally in SQLite and can be resumed anytime.

### Configuration

<File title="atmos.yaml">
```yaml
settings:
  ai:
    enabled: true
    provider: "anthropic"

    # Session management settings
    sessions:
      enabled: true                    # Enable persistent sessions (default: false)
      storage: "sqlite"                # Storage backend: sqlite (default: sqlite)
      path: ".atmos/sessions"          # Storage location (default: .atmos/sessions)
      max_sessions: 10                 # Maximum sessions to keep (default: 10)
      auto_save: true                  # Auto-save messages (default: true)
      retention_days: 30               # Delete sessions older than N days (default: 30)
```
</File>

### Session Settings

<dl>
  <dt>`sessions.enabled`</dt>
  <dd>Enable persistent conversation sessions (default: `false`)</dd>

  <dt>`sessions.storage`</dt>
  <dd>Storage backend for sessions. Currently only `sqlite` is supported (default: `sqlite`)</dd>

  <dt>`sessions.path`</dt>
  <dd>Directory path where session database is stored (default: `.atmos/sessions`)</dd>

  <dt>`sessions.max_sessions`</dt>
  <dd>Maximum number of sessions to keep per project. Oldest sessions are cleaned up automatically (default: `10`)</dd>

  <dt>`sessions.auto_save`</dt>
  <dd>Automatically save messages to the session after each exchange (default: `true`)</dd>

  <dt>`sessions.retention_days`</dt>
  <dd>Number of days to retain sessions before cleanup. Use `0` to keep sessions indefinitely (default: `30`)</dd>
</dl>

### Using Sessions

When sessions are enabled, you can:

<Terminal title="shell">
```bash
# Start a named session
atmos ai chat --session vpc-refactor

# List all sessions
atmos ai sessions

# Resume an existing session
atmos ai chat --session vpc-refactor

# Clean old sessions
atmos ai sessions clean --older-than 30d
```
</Terminal>

Sessions store:
- Complete message history (user and assistant messages)
- Session metadata (creation time, last update, model used)
- Context items (stack files, component references)

### Storage Location

Sessions are stored in a SQLite database at:
```
<project-root>/.atmos/sessions/sessions.db
```

The database includes three tables:
- `sessions` - Session metadata
- `messages` - Message history
- `session_context` - Context items

### Privacy Note

Sessions are stored **locally on your machine**. No session data is sent to AI providers except during active conversations.

## Tool Execution

Atmos AI can execute read-only tools to query your infrastructure configuration. Tools allow the AI to inspect stacks, components, and validate configurations without manual intervention.

### Configuration

<File title="atmos.yaml">
```yaml
settings:
  ai:
    enabled: true
    provider: "anthropic"

    # Tool execution settings
    tools:
      enabled: true                        # Enable tool execution (default: false)
      require_confirmation: true           # Prompt before executing tools (default: true)

      # Allowed tools (no confirmation needed)
      allowed_tools:
        - atmos_describe_component         # Allow without prompting
        - atmos_list_stacks
        - atmos_validate_stacks
        - atmos_describe_*                 # Wildcard patterns supported

      # Restricted tools (always prompt)
      restricted_tools:
        - file_edit                        # Always require confirmation
        - atmos_terraform_plan

      # Blocked tools (never allow)
      blocked_tools:
        - atmos_terraform_destroy          # Completely blocked
        - atmos_terraform_apply

      yolo_mode: false                     # Skip all confirmations (DANGEROUS!)
```
</File>

### Tool Settings

<dl>
  <dt>`tools.enabled`</dt>
  <dd>Enable AI tool execution capabilities (default: `false`)</dd>

  <dt>`tools.require_confirmation`</dt>
  <dd>Prompt user before executing any tool not in `allowed_tools` (default: `true`)</dd>

  <dt>`tools.allowed_tools`</dt>
  <dd>List of tools that can execute without user confirmation. Supports wildcard patterns like `atmos_*` (default: `[]`)</dd>

  <dt>`tools.restricted_tools`</dt>
  <dd>List of tools that always require user confirmation, even if `require_confirmation` is `false` (default: `[]`)</dd>

  <dt>`tools.blocked_tools`</dt>
  <dd>List of tools that are completely blocked from execution (default: `[]`)</dd>

  <dt>`tools.yolo_mode`</dt>
  <dd>Skip all permission checks and execute all tools automatically. **DANGEROUS!** Only use in trusted CI/CD environments (default: `false`)</dd>
</dl>

### Available Tools

Atmos AI includes these read-only tools:

| Tool | Description | Permission Required |
|------|-------------|---------------------|
| `atmos_describe_component` | Describe component configuration in a stack | No |
| `atmos_list_stacks` | List all available stacks | No |
| `atmos_validate_stacks` | Validate stack configurations | No |

### Permission Flow

When the AI requests tool execution:

1. **YOLO Mode Enabled?** → Execute immediately (skip all checks)
2. **Tool Blocked?** → Deny execution
3. **Tool in Allowed List?** → Execute without prompt
4. **Tool in Restricted List?** → Always prompt user
5. **Default Behavior** → Prompt user if `require_confirmation` is `true`

### Permission Prompt Example

When a tool requires confirmation:

<Terminal title="atmos ai chat">
```bash
🔧 Tool Execution Request
Tool: atmos_describe_component
Description: Describe an Atmos component configuration in a specific stack

Parameters:
  component: vpc
  stack: prod-use1

Allow execution? (y/N): y

✅ Executing tool...
```
</Terminal>

### Wildcard Patterns

You can use wildcards in tool lists:

```yaml
tools:
  allowed_tools:
    - atmos_describe_*    # Allow all describe commands
    - atmos_list_*        # Allow all list commands
    - file_*              # Allow all file operations
```

### Security Best Practices

1. **Start Conservative**: Begin with `require_confirmation: true` and add trusted tools to `allowed_tools` over time
2. **Block Destructive Operations**: Always add dangerous tools like `terraform_destroy` to `blocked_tools`
3. **Use Restricted List**: For tools that modify state, add them to `restricted_tools` to force confirmation
4. **Avoid YOLO Mode**: Only enable `yolo_mode` in isolated CI/CD environments with strict access controls
5. **Review Permissions Regularly**: Audit your tool configuration as new tools are added

### Tool Execution Timeouts

Tools have a default timeout of 30 seconds. Long-running operations will be cancelled automatically to prevent hanging.

## Context-Aware AI Queries

The AI assistant can analyze your actual stack configurations to provide specific, context-aware answers.

### How It Works

When you ask questions about your repository (e.g., "Describe the stacks in this repo"), the AI can:
- Read your stack YAML files
- Analyze component configurations
- Provide specific answers based on your actual setup

### Privacy & Security

**Important**: Enabling context sharing sends your stack files to the AI provider's servers.

- Stack configs may contain sensitive information (AWS account IDs, regions, etc.)
- Data is sent to your chosen AI provider (Anthropic, OpenAI, Google, or xAI)
- No data is stored by Atmos - it's only included in the API request
- You control when context is sent through configuration and prompts

### Configuration Options

There are three ways to control context sharing:

#### Option 1: Environment Variable (Highest Priority)

<Terminal title="shell">
```bash
# Always send context without prompting
export ATMOS_AI_SEND_CONTEXT=true
atmos ai ask "Describe the stacks in this repo"

# Never send context
export ATMOS_AI_SEND_CONTEXT=false
atmos ai ask "What components are available?"
```
</Terminal>

#### Option 2: Configuration File

<File title="atmos.yaml">
```yaml
settings:
  ai:
    enabled: true
    provider: "anthropic"
    # Always send context, but prompt each time for confirmation
    send_context: true
    prompt_on_send: true
```
</File>

Or automatically send without prompting:

<File title="atmos.yaml">
```yaml
settings:
  ai:
    enabled: true
    provider: "anthropic"
    # Always send context without prompting (not recommended)
    send_context: true
    prompt_on_send: false
```
</File>

#### Option 3: Interactive Prompt (Default)

By default, if you ask a question that seems to need context, you'll be prompted:

<Terminal title="atmos ai ask">
```bash
$ atmos ai ask "Describe the stacks in this repo"

⚠️  This question may benefit from your stack configurations.
📤 Send stack files to AI provider for analysis? (y/N): y
📖 Reading stack configurations...
👽 Thinking...

Based on your stack configurations, you have the following stacks:
...
```
</Terminal>

### Questions That Trigger Context

The AI automatically detects when context might be helpful based on keywords:

- "this repo" / "my repo"
- "my stack" / "these stacks"
- "my component" / "these components"
- "my configuration" / "my config"
- "what stacks" / "list stacks"
- "describe the" / "analyze"

### Disabling Context

To never send context:

<File title="atmos.yaml">
```yaml
settings:
  ai:
    enabled: true
    provider: "anthropic"
    send_context: false  # Default - never send context
```
</File>

Or use the environment variable:

<Terminal title="shell">
```bash
export ATMOS_AI_SEND_CONTEXT=false
```
</Terminal>

## Environment Variables

You can use Atmos-prefixed environment variables as alternatives:

<Terminal title="shell">
```bash
# API Keys
export ATMOS_ANTHROPIC_API_KEY="your-key"  # Anthropic
export ATMOS_OPENAI_API_KEY="your-key"     # OpenAI
export ATMOS_GEMINI_API_KEY="your-key"     # Gemini
export ATMOS_XAI_API_KEY="your-key"        # Grok

# Context Control
export ATMOS_AI_SEND_CONTEXT=true   # Send stack configs to AI
export ATMOS_AI_SEND_CONTEXT=false  # Never send context (default)
```
</Terminal>

## Verify Configuration

Test your AI configuration:

<Terminal title="atmos ai ask">
```bash
$ atmos ai ask "Hello, are you working?"

👽 Thinking...

Yes, I'm working! The AI assistant is properly configured and ready
to help you with your Atmos infrastructure management tasks.
```
</Terminal>

## Switching Providers

You can easily switch between providers by changing the `provider` field:

<File title="atmos.yaml">
```yaml
settings:
  ai:
    enabled: true
    provider: "gemini"  # Changed from anthropic to gemini
    # Other settings will use Gemini defaults
```
</File>

Each provider has its strengths:

- **Claude (Anthropic)**: Best for detailed explanations and complex reasoning
- **GPT (OpenAI)**: Strong general capabilities, widely available
- **Gemini (Google)**: Fast responses, excellent for large contexts
- **Grok (xAI)**: Real-time knowledge, OpenAI-compatible

## Performance Tuning

You can adjust timeout and context limits to optimize performance for your needs:

<File title="atmos.yaml">
```yaml
settings:
  ai:
    enabled: true
    provider: "anthropic"
    # Timeout configuration
    timeout_seconds: 120         # Increase for complex questions (default: 60)

    # Context limits
    max_context_files: 20        # Send more files for deeper analysis (default: 10)
    max_context_lines: 1000      # Increase for larger files (default: 500)
```
</File>

### Configuration Options

| Option | Default | Description |
|--------|---------|-------------|
| `timeout_seconds` | 60 | Maximum time to wait for AI response |
| `max_context_files` | 10 | Maximum number of stack files to send |
| `max_context_lines` | 500 | Maximum lines per file to include |

### Recommendations

- **For large repositories**: Increase `max_context_files` and `max_context_lines`
- **For slow connections**: Increase `timeout_seconds` to 120 or more
- **For faster responses**: Reduce context limits to 5 files and 250 lines
- **For cost optimization**: Lower context limits to reduce token usage

## Security Best Practices

1. **Never commit API keys** to version control
2. **Use environment variables** for all API keys
3. **Rotate keys regularly** per provider recommendations
4. **Set spending limits** in provider dashboards
5. **Monitor usage** to detect anomalies

## Next Steps

- [AI Commands](/cli/commands/ai/usage) - Learn how to use AI features
- [Troubleshooting](/ai/troubleshooting) - Common configuration issues
