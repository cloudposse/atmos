---
title: AI Configuration
sidebar_label: Configuration
sidebar_position: 2
description: Configure AI providers for the Atmos AI Assistant
---
import Terminal from '@site/src/components/Terminal'
import File from '@site/src/components/File'

# AI Assistant Configuration

Configure the Atmos AI Assistant to use different AI providers and customize their settings.

## Quick Start

Enable AI features with a single provider:

<File title="atmos.yaml">
```yaml
settings:
  ai:
    enabled: true                         # Enable AI features (required)
    default_provider: "anthropic"         # Default AI provider

    # Configure your chosen provider
    providers:
      anthropic:
        model: "claude-sonnet-4-20250514"
        api_key_env: "ANTHROPIC_API_KEY"
        max_tokens: 4096
```
</File>

Set your API key:

<Terminal title="shell">
```bash
export ANTHROPIC_API_KEY="your-api-key-here"
```
</Terminal>

Start using AI:

<Terminal title="atmos ai chat">
```bash
atmos ai chat
```
</Terminal>

## Multi-Provider Configuration

Atmos supports 7 AI providers. You can configure multiple providers and switch between them:

<File title="atmos.yaml">
```yaml
settings:
  ai:
    enabled: true
    default_provider: "anthropic"         # Used by default for CLI commands

    # Configure multiple providers
    providers:
      # Cloud Providers
      anthropic:
        model: "claude-sonnet-4-20250514"
        api_key_env: "ANTHROPIC_API_KEY"
        max_tokens: 4096

      openai:
        model: "gpt-5"
        api_key_env: "OPENAI_API_KEY"
        max_tokens: 4096

      gemini:
        model: "gemini-2.0-flash-exp"
        api_key_env: "GEMINI_API_KEY"
        max_tokens: 8192

      grok:
        model: "grok-4"
        api_key_env: "XAI_API_KEY"
        max_tokens: 4096
        base_url: "https://api.x.ai/v1"

      # Local Provider (no API key needed)
      ollama:
        model: "llama3.3:70b"
        base_url: "http://localhost:11434/v1"

      # Enterprise Providers
      bedrock:
        model: "anthropic.claude-sonnet-4-20250514-v2:0"
        base_url: "us-east-1"              # AWS region
        # Uses AWS credentials (no api_key_env needed)

      azureopenai:
        model: "gpt-4o"                    # Your Azure deployment name
        api_key_env: "AZURE_OPENAI_API_KEY"
        base_url: "https://your-resource.openai.azure.com"
        api_version: "2024-02-15-preview"
```
</File>

**Provider Switching:**
- **In CLI**: Set `default_provider` for `atmos ai ask` commands
- **In TUI**: Press `Ctrl+P` to switch providers mid-conversation
- **Per Session**: Select provider when creating sessions with `Ctrl+N`

:::info Complete Provider Documentation
For detailed setup instructions, hardware requirements, security considerations, and pricing for each provider, see [AI Providers](/ai/providers).
:::

## Migration from Old Configuration

If you have an existing single-provider configuration:

<File title="Old Format (still works, but limited)">
```yaml
settings:
  ai:
    enabled: true
    provider: "anthropic"                 # Old pattern
    model: "claude-sonnet-4-20250514"
    api_key_env: "ANTHROPIC_API_KEY"
```
</File>

**Migrate to new format** for multi-provider support:

<File title="New Format (recommended)">
```yaml
settings:
  ai:
    enabled: true
    default_provider: "anthropic"         # New pattern
    providers:
      anthropic:
        model: "claude-sonnet-4-20250514"
        api_key_env: "ANTHROPIC_API_KEY"
```
</File>

The old format still works for backward compatibility but limits you to a single provider.

## Basic Configuration Options

Global AI settings:

<File title="atmos.yaml">
```yaml
settings:
  ai:
    enabled: true                         # Enable AI features (required)
    default_provider: "anthropic"         # Default provider for CLI commands
    send_context: false                   # Send stack configs to AI (default: false)
    prompt_on_send: true                  # Prompt before sending context (default: true)
    timeout_seconds: 60                   # Request timeout in seconds (default: 60)
    max_context_files: 10                 # Maximum stack files to send (default: 10)
    max_context_lines: 500                # Maximum lines per file (default: 500)
    max_history_messages: 20              # Maximum conversation messages to keep (default: 0 = unlimited)
    max_history_tokens: 8000              # Maximum tokens in conversation history (default: 0 = unlimited)
    default_agent: "general"              # Default AI agent (default: "general")
```
</File>

## AI Agents Configuration

Atmos AI includes specialized agents for different tasks. You can use built-in agents or create custom ones.

### Built-in Agents

Atmos ships with 5 built-in agents:

- **general** (default) - General-purpose assistant for all operations
- **stack-analyzer** - Specialized in analyzing stack configurations and dependencies
- **component-refactor** - Expert in refactoring Terraform/Helmfile components
- **security-auditor** - Focused on security review and vulnerability detection
- **config-validator** - Validates YAML configurations and schema compliance

Switch between agents during a chat session by pressing **Ctrl+A**.

### Custom Agents

Create your own specialized agents:

<File title="atmos.yaml">
```yaml
settings:
  ai:
    enabled: true
    default_provider: "anthropic"
    default_agent: "stack-analyzer"     # Use stack analyzer by default

    # Define custom agents
    agents:
      cost-optimizer:
        display_name: "Cost Optimizer"
        description: "Analyzes infrastructure for cost savings opportunities"
        category: "analysis"
        system_prompt: |
          You are a cost optimization expert for Atmos infrastructure.

          FOCUS AREAS:
          - Identify overprovisioned resources
          - Suggest rightsizing opportunities
          - Find unused or idle resources

          Always use tools to gather data before making recommendations.

        allowed_tools:
          - atmos_describe_component
          - atmos_list_stacks
          - read_stack_file
          - read_file

        restricted_tools: []
```
</File>

### Agent Configuration Schema

```yaml
agents:
  <agent-name>:
    display_name: string          # User-facing name (required)
    description: string           # What this agent does (required)
    system_prompt: string         # Specialized instructions (required)
    category: string              # "analysis", "refactor", "security", etc.
    allowed_tools: []             # Tool names (empty = all tools)
    restricted_tools: []          # Tools requiring confirmation
```

:::tip AI Agents
For complete agent documentation including examples and best practices, see [AI Agents](/ai/agents).
:::

## Provider-Specific Configuration

Configure each provider under the `providers` section. You can configure one or multiple providers.

:::tip
All providers shown below can be configured simultaneously in the `providers` section. Set `default_provider` to choose which one is used by default.
:::

### Anthropic (Claude)

Claude offers advanced reasoning and detailed explanations. Best for complex analysis and code generation.

<File title="atmos.yaml">
```yaml
settings:
  ai:
    enabled: true
    default_provider: "anthropic"
    providers:
      anthropic:
        model: "claude-sonnet-4-20250514"   # Default model
        api_key_env: "ANTHROPIC_API_KEY"      # Default: ANTHROPIC_API_KEY
        max_tokens: 4096                      # Default: 4096
```
</File>

**Available Models:**
- `claude-sonnet-4-20250514` - Best balance of speed and capability
- `claude-3-opus-20240229` - Most capable, slower
- `claude-3-haiku-20240307` - Fastest, lighter tasks

**Setup:**

1. Get an API key from [Anthropic Console](https://console.anthropic.com/)
2. Set the environment variable:

<Terminal title="shell">
```bash
export ANTHROPIC_API_KEY="your-api-key-here"
```
</Terminal>

### OpenAI (GPT)

OpenAI provides widely available models with strong general capabilities. Best for general-purpose tasks.

<File title="atmos.yaml">
```yaml
settings:
  ai:
    enabled: true
    default_provider: "openai"
    providers:
      openai:
        model: "gpt-5"                   # Default: gpt-5
        api_key_env: "OPENAI_API_KEY"    # Default: OPENAI_API_KEY
        max_tokens: 4096                 # Default: 4096
```
</File>

**Available Models:**
- `gpt-4o` (recommended) - Latest multimodal model
- `gpt-4-turbo` - Fast GPT-4 variant
- `gpt-3.5-turbo` - Faster, more economical

**Setup:**

1. Get an API key from [OpenAI Platform](https://platform.openai.com/)
2. Set the environment variable:

<Terminal title="shell">
```bash
export OPENAI_API_KEY="your-api-key-here"
```
</Terminal>

### Google (Gemini)

Gemini offers fast responses with a larger context window. Best for fast queries and large documents.

<File title="atmos.yaml">
```yaml
settings:
  ai:
    enabled: true
    default_provider: "gemini"
    providers:
      gemini:
        model: "gemini-2.0-flash-exp"      # Default: gemini-2.0-flash-exp
        api_key_env: "GEMINI_API_KEY"      # Default: GEMINI_API_KEY
        max_tokens: 8192                   # Default: 8192
```
</File>

**Available Models:**
- `gemini-2.0-flash-exp` (recommended) - Latest fast model
- `gemini-1.5-pro` - Most capable
- `gemini-1.5-flash` - Fastest

**Setup:**

1. Get an API key from [Google AI Studio](https://aistudio.google.com/)
2. Set the environment variable:

<Terminal title="shell">
```bash
export GEMINI_API_KEY="your-api-key-here"
```
</Terminal>

### xAI (Grok)

Grok provides OpenAI-compatible access with real-time knowledge capabilities. Best for current events and real-time information.

<File title="atmos.yaml">
```yaml
settings:
  ai:
    enabled: true
    default_provider: "grok"
    providers:
      grok:
        model: "grok-4"                   # Default: grok-4
        api_key_env: "XAI_API_KEY"        # Default: XAI_API_KEY
        max_tokens: 4096                  # Default: 4096
        base_url: "https://api.x.ai/v1"   # Default: https://api.x.ai/v1
```
</File>

**Available Models:**
- `grok-4` - Latest Grok model
- `grok-2` - Previous version

**Setup:**

1. Get an API key from [xAI API](https://x.ai/api)
2. Set the environment variable:

<Terminal title="shell">
```bash
export XAI_API_KEY="your-api-key-here"
```
</Terminal>

### Ollama (Local)

Ollama runs AI models entirely on your machine. No data leaves your computer. Best for privacy, offline use, and zero API costs.

<File title="atmos.yaml">
```yaml
settings:
  ai:
    enabled: true
    default_provider: "ollama"
    providers:
      ollama:
        model: "llama3.3:70b"                     # Default: llama3.3:70b
        base_url: "http://localhost:11434/v1"     # Default: http://localhost:11434/v1
        timeout_seconds: 120                       # Longer timeout for large models
        # No api_key_env needed for local instances
```
</File>

**Available Models:**
- `llama3.3:70b` (recommended, 64GB+ RAM) - Production quality
- `llama3.1:8b` (8GB+ RAM) - Quick queries, limited hardware
- `codellama:13b` (16GB+ RAM) - Code-focused tasks
- `mistral:7b` (8GB+ RAM) - Fast responses

:::info Hardware Requirements & Setup
Ollama requires installation and adequate RAM. See [Ollama Documentation](/ai/providers#ollama) for installation instructions, model selection, and hardware requirements.
:::

**Setup:**

<Terminal title="shell">
```bash
# Install Ollama (macOS)
brew install ollama

# Start Ollama service
ollama serve

# Pull a model
ollama pull llama3.3:70b
```
</Terminal>

### AWS Bedrock (Enterprise)

Run Claude models via AWS Bedrock for enterprise security and compliance. Best for AWS-native organizations.

<File title="atmos.yaml">
```yaml
settings:
  ai:
    enabled: true
    default_provider: "bedrock"
    providers:
      bedrock:
        model: "anthropic.claude-sonnet-4-20250514-v2:0"  # Full Bedrock model ID
        base_url: "us-east-1"                                # AWS region
        max_tokens: 4096
        # Uses AWS SDK credential chain (no api_key_env needed)
```
</File>

**Available Models:**
- `anthropic.claude-sonnet-4-20250514-v2:0` (recommended)
- `anthropic.claude-3-opus-20240229-v1:0`
- `anthropic.claude-3-haiku-20240307-v1:0`

**Security Benefits:**
- Data never leaves AWS infrastructure
- IAM-based access control
- Complete audit trail via CloudTrail
- VPC endpoints and PrivateLink support
- Compliance certifications (SOC2, HIPAA, ISO)

**Setup:**

<Terminal title="shell">
```bash
# Configure AWS credentials (one of):

# Option 1: Environment variables
export AWS_ACCESS_KEY_ID="AKIA..."
export AWS_SECRET_ACCESS_KEY="..."

# Option 2: AWS Profile
export AWS_PROFILE="bedrock-profile"

# Option 3: IAM role (automatic in EC2/ECS/EKS)
```
</Terminal>

:::info Complete Bedrock Documentation
See [AWS Bedrock Setup](/ai/providers#aws-bedrock) for IAM permissions, model access, and detailed configuration.
:::

### Azure OpenAI (Enterprise)

Run GPT models via Azure OpenAI for enterprise compliance. Best for Azure-native organizations.

<File title="atmos.yaml">
```yaml
settings:
  ai:
    enabled: true
    default_provider: "azureopenai"
    providers:
      azureopenai:
        model: "gpt-4o"                                      # Your Azure deployment name
        api_key_env: "AZURE_OPENAI_API_KEY"
        base_url: "https://your-resource.openai.azure.com"   # Your Azure resource URL
        api_version: "2024-02-15-preview"                    # API version
        max_tokens: 4096
```
</File>

**Important:** Use your **deployment name** from Azure Portal, not the OpenAI model name.

**Security Benefits:**
- Data residency control
- Azure AD integration with managed identities
- Compliance certifications (SOC2, HIPAA, ISO 27001)
- Private endpoints via Azure Private Link
- Customer-managed encryption keys (BYOK)

**Setup:**

<Terminal title="shell">
```bash
# Get API key from Azure Portal
export AZURE_OPENAI_API_KEY="..."

# Or use Azure AD managed identity (in production)
```
</Terminal>

:::info Complete Azure OpenAI Documentation
See [Azure OpenAI Setup](/ai/providers#azure-openai) for resource configuration, deployment names, and authentication options.
:::

## Configuration Options

### Required Settings

<dl>
  <dt>`enabled`</dt>
  <dd>Enable or disable AI features (default: `false`)</dd>

  <dt>`default_provider`</dt>
  <dd>Default AI provider for CLI commands: `anthropic`, `openai`, `gemini`, `grok`, `ollama`, `bedrock`, or `azureopenai` (default: `anthropic`)</dd>

  <dt>`providers`</dt>
  <dd>Map of provider configurations. Each provider has its own `model`, `api_key_env`, and other provider-specific settings.</dd>
</dl>

### Provider-Specific Settings

Each provider in the `providers` map supports:

<dl>
  <dt>`model`</dt>
  <dd>Specific model to use (defaults vary by provider)</dd>

  <dt>`api_key_env`</dt>
  <dd>Environment variable name containing the API key (not needed for Ollama local instances or AWS Bedrock with IAM)</dd>

  <dt>`max_tokens`</dt>
  <dd>Maximum tokens per response (defaults vary by provider)</dd>

  <dt>`base_url`</dt>
  <dd>Custom API endpoint URL (required for Grok, Azure OpenAI; optional for Ollama remote instances; AWS region for Bedrock)</dd>

  <dt>`timeout_seconds`</dt>
  <dd>Request timeout in seconds (default: 60, increase for large local models)</dd>
</dl>

### Understanding max_tokens

The `max_tokens` setting controls the maximum length of the AI's response. A "token" is a piece of text - roughly 3-4 characters for English text, or about 0.75 words on average.

**What it means:**
- **Higher values** = Longer, more detailed responses (costs more, takes longer)
- **Lower values** = Shorter, more concise responses (costs less, faster)
- The AI can respond with fewer tokens than the limit, but never more

**Token-to-text approximations:**
- `1000 tokens` ≈ 750 words ≈ 1-2 paragraphs
- `2048 tokens` ≈ 1,500 words ≈ 1 page of text
- `4096 tokens` ≈ 3,000 words ≈ 2-3 pages of text
- `8192 tokens` ≈ 6,000 words ≈ 5-6 pages of text

**Recommended values for average use:**

<File title="atmos.yaml">
```yaml
providers:
  anthropic:
    max_tokens: 4096      # Good balance for most queries

  openai:
    max_tokens: 4096      # Sufficient for detailed explanations

  gemini:
    max_tokens: 8192      # Gemini handles longer contexts well

  grok:
    max_tokens: 4096      # Standard for most use cases

  ollama:
    max_tokens: 2048      # Lower for local models (faster generation)
```
</File>

**When to adjust:**

- **Increase to 8192+** for:
  - Long code generation tasks
  - Detailed infrastructure analysis
  - Complex multi-step explanations
  - Stack configuration reviews

- **Decrease to 1024-2048** for:
  - Quick questions and answers
  - Simple clarifications
  - Local models (Ollama) for faster responses
  - Cost optimization

**Cost impact:**
Each provider charges per token (both input and output). Higher `max_tokens` means higher maximum cost per request. See your provider's pricing page for exact rates.

:::tip Token Limits by Provider
Different models have different maximum token limits:
- **Claude Sonnet 4**: Up to 200,000 tokens
- **GPT-4/GPT-5**: Up to 128,000 tokens
- **Gemini 2.0**: Up to 1,000,000 tokens
- **Local models (Ollama)**: Typically 2,048-8,192 tokens

Setting `max_tokens` higher than the model's limit will be automatically capped by the provider.
:::

### Global Settings

<dl>
  <dt>`send_context`</dt>
  <dd>Automatically send stack configurations to AI for context-aware answers (default: `false`)</dd>

  <dt>`prompt_on_send`</dt>
  <dd>Prompt user for consent before sending context, even when `send_context` is `true` (default: `true`)</dd>

  <dt>`max_history_messages`</dt>
  <dd>Maximum conversation messages to keep in history (default: `0` = unlimited). Simple message-based limit - easy to configure and understand.</dd>

  <dt>`max_history_tokens`</dt>
  <dd>Maximum tokens in conversation history (default: `0` = unlimited). More precise token-based limit using approximate counting (~±10-20% accuracy). If both limits are set, whichever is hit first is applied.</dd>
</dl>

## Session Management

Atmos AI supports persistent conversation sessions that maintain message history across CLI invocations. Sessions are stored locally in SQLite and can be resumed anytime.

### Configuration

<File title="atmos.yaml">
```yaml
settings:
  ai:
    enabled: true
    default_provider: "anthropic"
    providers:
      anthropic:
        model: "claude-sonnet-4-20250514"
        api_key_env: "ANTHROPIC_API_KEY"

    # Session management settings
    sessions:
      enabled: true                    # Enable persistent sessions (default: false)
      storage: "sqlite"                # Storage backend: sqlite (default: sqlite)
      path: ".atmos/sessions"          # Storage location (default: .atmos/sessions)
      max_sessions: 10                 # Maximum sessions to keep (default: 10)
      auto_save: true                  # Auto-save messages (default: true)
      retention_days: 30               # Delete sessions older than N days (default: 30)
```
</File>

### Session Settings

<dl>
  <dt>`sessions.enabled`</dt>
  <dd>Enable persistent conversation sessions (default: `false`)</dd>

  <dt>`sessions.storage`</dt>
  <dd>Storage backend for sessions. Currently only `sqlite` is supported (default: `sqlite`)</dd>

  <dt>`sessions.path`</dt>
  <dd>Directory path where session database is stored (default: `.atmos/sessions`)</dd>

  <dt>`sessions.max_sessions`</dt>
  <dd>Maximum number of sessions to keep per project. Oldest sessions are cleaned up automatically (default: `10`)</dd>

  <dt>`sessions.auto_save`</dt>
  <dd>Automatically save messages to the session after each exchange (default: `true`)</dd>

  <dt>`sessions.retention_days`</dt>
  <dd>Number of days to retain sessions before cleanup. Use `0` to keep sessions indefinitely (default: `30`)</dd>
</dl>

### Using Sessions

When sessions are enabled, you can:

<Terminal title="shell">
```bash
# Start a named session
atmos ai chat --session vpc-refactor

# List all sessions
atmos ai sessions

# Resume an existing session
atmos ai chat --session vpc-refactor

# Clean old sessions
atmos ai sessions clean --older-than 30d
```
</Terminal>

Sessions store:
- Complete message history (user and assistant messages)
- Session metadata (creation time, last update, model used)
- Context items (stack files, component references)

### Storage Location

Sessions are stored in a SQLite database at:
```
<project-root>/.atmos/sessions/sessions.db
```

The database includes three tables:
- `sessions` - Session metadata
- `messages` - Message history
- `session_context` - Context items

### Privacy Note

Sessions are stored **locally on your machine**. No session data is sent to AI providers except during active conversations.

## Conversation History Management

Control how much conversation history is sent to AI providers with each request. This helps manage costs and prevent rate limiting in long conversations.

### Two Approaches: Messages vs. Tokens

By default, Atmos sends the **full conversation history** to the AI with each request. For long conversations, this can lead to:
- **Rate limiting**: Exceeding tokens-per-minute limits
- **Higher costs**: More tokens = higher API charges
- **Slower responses**: More data to process

Atmos offers two ways to limit conversation history:

#### Option 1: Message-Based Limit (Simple)

Use `max_history_messages` to keep only the most recent N messages. **Recommended for most users** - easy to understand and configure.

<File title="atmos.yaml">
```yaml
settings:
  ai:
    enabled: true
    default_provider: "anthropic"
    max_history_messages: 20  # Keep only last 20 messages

    providers:
      anthropic:
        model: "claude-sonnet-4-20250514"
        api_key_env: "ANTHROPIC_API_KEY"
```
</File>

#### Option 2: Token-Based Limit (Precise)

Use `max_history_tokens` for more precise control based on estimated token count. Uses approximate counting (words × 1.3) with ~±10-20% accuracy.

<File title="atmos.yaml">
```yaml
settings:
  ai:
    enabled: true
    default_provider: "anthropic"
    max_history_tokens: 8000  # Keep ~8000 tokens of history

    providers:
      anthropic:
        model: "claude-sonnet-4-20250514"
        api_key_env: "ANTHROPIC_API_KEY"
```
</File>

#### Option 3: Combine Both Limits

Set both limits - whichever is hit first will be applied. This gives you both control mechanisms.

<File title="atmos.yaml">
```yaml
settings:
  ai:
    enabled: true
    default_provider: "anthropic"
    max_history_messages: 50  # Maximum 50 messages
    max_history_tokens: 8000  # OR maximum 8000 tokens (whichever comes first)

    providers:
      anthropic:
        model: "claude-sonnet-4-20250514"
        api_key_env: "ANTHROPIC_API_KEY"
```
</File>

### How It Works

When history limits are configured, Atmos applies them in this order:

1. **Provider filtering applied first**: Only messages from the current provider session are included
2. **Message-based limit applied (if set)**: If history exceeds `max_history_messages`, keep only the most recent N messages
3. **Token-based limit applied (if set)**: Count backwards from most recent, keep messages up to `max_history_tokens`
4. **Whichever is more restrictive wins**: If both limits are set, the one that results in fewer messages is used
5. **System messages excluded**: UI notifications don't count toward limits
6. **Current message added**: Your new message is always included (not counted in the limit)

**Example**: With `max_history_messages: 4` and 6 historical messages:
- **Before**: Message 1, Response 1, Message 2, Response 2, Message 3, Response 3
- **After**: Message 2, Response 2, Message 3, Response 3 (oldest 2 messages pruned)
- **Sent to API**: Message 2, Response 2, Message 3, Response 3, + New Message = 5 total

### Recommendations

| Use Case | Message-Based (`max_history_messages`) | Token-Based (`max_history_tokens`) | Best Approach |
|----------|----------------------------------------|-------------------------------------|---------------|
| Short questions | 0 (unlimited) | 0 (unlimited) | No limit needed |
| Long conversations | 20-30 messages | 6000-8000 tokens | Either works well |
| Complex analysis | 40-50 messages | 10000-15000 tokens | Token-based for precision |
| Cost optimization | 10-20 messages | 4000-6000 tokens | Token-based for exact control |
| Rate limit issues | 10-20 messages | 6000-8000 tokens | Token-based (matches provider limits) |
| Combined protection | 50 messages | 8000 tokens | Use both limits together |

:::tip
Set to `0` (or omit) for **unlimited history** (default behavior). This works well for most conversations but may cause rate limiting in extended sessions.
:::

:::warning
Setting too low (e.g., 4-6 messages) may cause the AI to lose important context from earlier in the conversation. Start with 20-30 for a good balance.
:::

## Tool Execution

Atmos AI can execute read-only tools to query your infrastructure configuration. Tools allow the AI to inspect stacks, components, and validate configurations without manual intervention.

### Configuration

<File title="atmos.yaml">
```yaml
settings:
  ai:
    enabled: true
    default_provider: "anthropic"
    providers:
      anthropic:
        model: "claude-sonnet-4-20250514"
        api_key_env: "ANTHROPIC_API_KEY"

    # Tool execution settings
    tools:
      enabled: true                        # Enable tool execution (default: false)
      require_confirmation: true           # Prompt before executing tools (default: true)

      # Allowed tools (no confirmation needed)
      allowed_tools:
        - atmos_describe_component         # Allow without prompting
        - atmos_list_stacks
        - atmos_validate_stacks
        - atmos_describe_*                 # Wildcard patterns supported

      # Restricted tools (always prompt)
      restricted_tools:
        - file_edit                        # Always require confirmation
        - atmos_terraform_plan

      # Blocked tools (never allow)
      blocked_tools:
        - atmos_terraform_destroy          # Completely blocked
        - atmos_terraform_apply

      yolo_mode: false                     # Skip all confirmations (DANGEROUS!)
```
</File>

### Tool Settings

<dl>
  <dt>`tools.enabled`</dt>
  <dd>Enable AI tool execution capabilities (default: `false`)</dd>

  <dt>`tools.require_confirmation`</dt>
  <dd>Prompt user before executing any tool not in `allowed_tools` (default: `true`)</dd>

  <dt>`tools.allowed_tools`</dt>
  <dd>List of tools that can execute without user confirmation. Supports wildcard patterns like `atmos_*` (default: `[]`)</dd>

  <dt>`tools.restricted_tools`</dt>
  <dd>List of tools that always require user confirmation, even if `require_confirmation` is `false` (default: `[]`)</dd>

  <dt>`tools.blocked_tools`</dt>
  <dd>List of tools that are completely blocked from execution (default: `[]`)</dd>

  <dt>`tools.yolo_mode`</dt>
  <dd>Skip all permission checks and execute all tools automatically. **DANGEROUS!** Only use in trusted CI/CD environments (default: `false`)</dd>
</dl>

:::tip How Tool Calling Works
For a detailed explanation of the tool calling process, what data is sent to AI providers, and security implications, see [How Tool Calling Works](/ai/tools#how-tool-calling-works) with interactive diagrams.
:::

### Available Tools

Atmos AI includes these read-only tools:

| Tool | Description | Permission Required |
|------|-------------|---------------------|
| `atmos_describe_component` | Describe component configuration in a stack | No |
| `atmos_list_stacks` | List all available stacks | No |
| `atmos_validate_stacks` | Validate stack configurations | No |

### Permission Flow

When the AI requests tool execution:

1. **YOLO Mode Enabled?** → Execute immediately (skip all checks)
2. **Tool Blocked?** → Deny execution
3. **Tool in Allowed List?** → Execute without prompt
4. **Tool in Restricted List?** → Always prompt user
5. **Default Behavior** → Prompt user if `require_confirmation` is `true`

### Permission Prompt Example

When a tool requires confirmation:

<Terminal title="atmos ai chat">
```bash
🔧 Tool Execution Request
Tool: atmos_describe_component
Description: Describe an Atmos component configuration in a specific stack

Parameters:
  component: vpc
  stack: prod-use1

Allow execution? (y/N): y

✅ Executing tool...
```
</Terminal>

### Wildcard Patterns

You can use wildcards in tool lists:

```yaml
tools:
  allowed_tools:
    - atmos_describe_*    # Allow all describe commands
    - atmos_list_*        # Allow all list commands
    - file_*              # Allow all file operations
```

### Security Best Practices

1. **Start Conservative**: Begin with `require_confirmation: true` and add trusted tools to `allowed_tools` over time
2. **Block Destructive Operations**: Always add dangerous tools like `terraform_destroy` to `blocked_tools`
3. **Use Restricted List**: For tools that modify state, add them to `restricted_tools` to force confirmation
4. **Avoid YOLO Mode**: Only enable `yolo_mode` in isolated CI/CD environments with strict access controls
5. **Review Permissions Regularly**: Audit your tool configuration as new tools are added

### Tool Execution Timeouts

Tools have a default timeout of 30 seconds. Long-running operations will be cancelled automatically to prevent hanging.

## Language Server Protocol (LSP) Integration

Atmos AI can integrate with Language Server Protocol (LSP) servers to validate YAML, Terraform, and HCL files with precise error diagnostics.

:::info
LSP is an independent Atmos feature that can optionally be used by AI. For comprehensive LSP documentation, see [LSP Integration](/lsp/lsp-client).
:::

### Quick Setup

<File title="atmos.yaml">
```yaml
settings:
  # Configure LSP (independent feature)
  lsp:
    enabled: true
    servers:
      yaml-ls:
        command: "yaml-language-server"
        args: ["--stdio"]
        filetypes: ["yaml", "yml"]

  # Enable AI to use LSP
  ai:
    enabled: true
    default_provider: "anthropic"
    providers:
      anthropic:
        model: "claude-sonnet-4-20250514"
        api_key_env: "ANTHROPIC_API_KEY"
    use_lsp: true                      # Allow AI to use LSP (default: false)
    tools:
      enabled: true                    # Required for AI to use tools
```
</File>

### AI LSP Tool

When enabled, AI can use the `validate_file_lsp` tool to validate files:

<Terminal title="atmos ai chat">
```bash
$ atmos ai chat

You: Validate stacks/prod/vpc.yaml

AI: [Uses validate_file_lsp tool]

Found 2 issue(s):
ERRORS (1):
1. Line 15: Unknown property 'vpc_ciddr'

WARNINGS (1):
1. Line 23: Deprecated property 'availability_zones'
```
</Terminal>

**Learn more:**
- [LSP Integration](/lsp/lsp-client) - Complete LSP documentation
- [LSP Configuration](/lsp/lsp-client#configuration-options) - Server setup
- [LSP Troubleshooting](/lsp/lsp-client#troubleshooting) - Common issues

## Project Memory

Atmos AI can remember project-specific context, patterns, and conventions using a persistent memory file (`ATMOS.md`). This provides consistent, context-aware responses without repeatedly explaining your project setup.

**Quick Configuration:**

<File title="atmos.yaml">
```yaml
settings:
  ai:
    memory:
      enabled: true                        # Enable project memory
      file_path: "ATMOS.md"                # Memory file location
      auto_update: false                   # Manual updates only
      create_if_missing: true              # Auto-create template
```
</File>

**Learn more:** [Project Memory](/ai/memory) - Complete memory documentation including:
- Memory file structure and sections
- Configuration options and best practices
- Privacy, security, and version control
- Performance optimization
- Troubleshooting guide

## Context-Aware AI Queries

The AI assistant can analyze your actual stack configurations to provide specific, context-aware answers.

### How It Works

When you ask questions about your repository (e.g., "Describe the stacks in this repo"), the AI can:
- Read your stack YAML files
- Analyze component configurations
- Provide specific answers based on your actual setup

### Privacy & Security

**Important**: Enabling context sharing sends your stack files to the AI provider's servers.

- Stack configs may contain sensitive information (AWS account IDs, regions, etc.)
- Data is sent to your chosen AI provider (Anthropic, OpenAI, Google, or xAI)
- No data is stored by Atmos - it's only included in the API request
- You control when context is sent through configuration and prompts

### Configuration Options

There are three ways to control context sharing:

#### Option 1: Environment Variable (Highest Priority)

<Terminal title="shell">
```bash
# Always send context without prompting
export ATMOS_AI_SEND_CONTEXT=true
atmos ai ask "Describe the stacks in this repo"

# Never send context
export ATMOS_AI_SEND_CONTEXT=false
atmos ai ask "What components are available?"
```
</Terminal>

#### Option 2: Configuration File

<File title="atmos.yaml">
```yaml
settings:
  ai:
    enabled: true
    default_provider: "anthropic"
    providers:
      anthropic:
        model: "claude-sonnet-4-20250514"
        api_key_env: "ANTHROPIC_API_KEY"
    # Always send context, but prompt each time for confirmation
    send_context: true
    prompt_on_send: true
```
</File>

Or automatically send without prompting:

<File title="atmos.yaml">
```yaml
settings:
  ai:
    enabled: true
    default_provider: "anthropic"
    providers:
      anthropic:
        model: "claude-sonnet-4-20250514"
        api_key_env: "ANTHROPIC_API_KEY"
    # Always send context without prompting (not recommended)
    send_context: true
    prompt_on_send: false
```
</File>

#### Option 3: Interactive Prompt (Default)

By default, if you ask a question that seems to need context, you'll be prompted:

<Terminal title="atmos ai ask">
```bash
$ atmos ai ask "Describe the stacks in this repo"

⚠️  This question may benefit from your stack configurations.
📤 Send stack files to AI provider for analysis? (y/N): y
📖 Reading stack configurations...
👽 Thinking...

Based on your stack configurations, you have the following stacks:
...
```
</Terminal>

### Questions That Trigger Context

The AI automatically detects when context might be helpful based on keywords:

- "this repo" / "my repo"
- "my stack" / "these stacks"
- "my component" / "these components"
- "my configuration" / "my config"
- "what stacks" / "list stacks"
- "describe the" / "analyze"

### Disabling Context

To never send context:

<File title="atmos.yaml">
```yaml
settings:
  ai:
    enabled: true
    default_provider: "anthropic"
    providers:
      anthropic:
        model: "claude-sonnet-4-20250514"
        api_key_env: "ANTHROPIC_API_KEY"
    send_context: false  # Default - never send context
```
</File>

Or use the environment variable:

<Terminal title="shell">
```bash
export ATMOS_AI_SEND_CONTEXT=false
```
</Terminal>

## Environment Variables

You can use Atmos-prefixed environment variables as alternatives:

<Terminal title="shell">
```bash
# API Keys
export ATMOS_ANTHROPIC_API_KEY="your-key"  # Anthropic
export ATMOS_OPENAI_API_KEY="your-key"     # OpenAI
export ATMOS_GEMINI_API_KEY="your-key"     # Gemini
export ATMOS_XAI_API_KEY="your-key"        # Grok

# Context Control
export ATMOS_AI_SEND_CONTEXT=true   # Send stack configs to AI
export ATMOS_AI_SEND_CONTEXT=false  # Never send context (default)
```
</Terminal>

## Verify Configuration

Test your AI configuration:

<Terminal title="atmos ai ask">
```bash
$ atmos ai ask "Hello, are you working?"

👽 Thinking...

Yes, I'm working! The AI assistant is properly configured and ready
to help you with your Atmos infrastructure management tasks.
```
</Terminal>

## Switching Providers

With multi-provider configuration, you can switch between providers in multiple ways:

### Runtime Switching (TUI Only)

In the interactive TUI mode (`atmos ai chat`), press **Ctrl+P** to switch providers mid-conversation:

<Terminal title="atmos ai chat">
```bash
$ atmos ai chat

You: Explain VPC CIDR blocks

AI: [Response from Anthropic Claude...]

# Press Ctrl+P during conversation
Provider Selection:
  1. Anthropic (Claude) - claude-sonnet-4-20250514
  2. OpenAI (GPT) - gpt-4o
  3. Google (Gemini) - gemini-2.0-flash-exp
  4. Ollama (Local) - llama3.3:70b

Select provider (1-4): 2

Switched to OpenAI (GPT)

You: Continue with the same question

AI: [Response from OpenAI GPT...]
```
</Terminal>

### Change Default Provider

For CLI commands (`atmos ai ask`), change the `default_provider` setting:

<File title="atmos.yaml">
```yaml
settings:
  ai:
    enabled: true
    default_provider: "gemini"  # Changed from anthropic to gemini
    providers:
      anthropic:
        model: "claude-sonnet-4-20250514"
        api_key_env: "ANTHROPIC_API_KEY"
      gemini:
        model: "gemini-2.0-flash-exp"
        api_key_env: "GEMINI_API_KEY"
```
</File>

### Create Provider-Specific Sessions

When creating a new session with Ctrl+N in TUI, you can select which provider to use for that session. The session remembers the provider choice.

### Provider Strengths

Choose based on your needs:

- **Claude (Anthropic)**: Best for detailed explanations, code analysis, and complex reasoning
- **GPT (OpenAI)**: Strong general capabilities, widely available, well-documented
- **Gemini (Google)**: Fast responses, excellent for large contexts, cost-effective
- **Grok (xAI)**: Real-time knowledge, OpenAI-compatible API
- **Ollama (Local)**: Complete privacy, offline capable, no API costs
- **Bedrock (AWS)**: Enterprise security, AWS integration, compliance
- **Azure OpenAI**: Enterprise compliance, Azure integration, data residency

## Performance Tuning

You can adjust timeout and context limits to optimize performance for your needs:

<File title="atmos.yaml">
```yaml
settings:
  ai:
    enabled: true
    default_provider: "anthropic"
    providers:
      anthropic:
        model: "claude-sonnet-4-20250514"
        api_key_env: "ANTHROPIC_API_KEY"
        timeout_seconds: 120     # Increase for complex questions (default: 60)

    # Context limits (global settings)
    max_context_files: 20        # Send more files for deeper analysis (default: 10)
    max_context_lines: 1000      # Increase for larger files (default: 500)
```
</File>

### Configuration Options

| Option | Default | Description |
|--------|---------|-------------|
| `timeout_seconds` | 60 | Maximum time to wait for AI response |
| `max_context_files` | 10 | Maximum number of stack files to send |
| `max_context_lines` | 500 | Maximum lines per file to include |
| `max_history_messages` | 0 (unlimited) | Maximum conversation messages to keep in history (message-based limit) |
| `max_history_tokens` | 0 (unlimited) | Maximum tokens in conversation history (token-based limit, ~±10-20% accuracy). If both limits are set, whichever is hit first is applied |

### Recommendations

- **For large repositories**: Increase `max_context_files` and `max_context_lines`
- **For slow connections**: Increase `timeout_seconds` to 120 or more
- **For faster responses**: Reduce context limits to 5 files and 250 lines
- **For cost optimization**: Lower context limits to reduce token usage
- **For long conversations (message-based)**: Set `max_history_messages` to 20-50 to prevent rate limiting
- **For long conversations (token-based)**: Set `max_history_tokens` to 6000-10000 for precise rate limit control
- **For tight rate limits**: Use `max_history_tokens` with provider's per-minute token limit in mind (e.g., 40,000 TPM = set to 8000-10000)
- **For combined control**: Set both limits - e.g., `max_history_messages: 50` and `max_history_tokens: 8000` for dual protection

## Security Best Practices

1. **Never commit API keys** to version control
2. **Use environment variables** for all API keys
3. **Rotate keys regularly** per provider recommendations
4. **Set spending limits** in provider dashboards
5. **Monitor usage** to detect anomalies

## Next Steps

- [AI Commands](/cli/commands/ai/usage) - Learn how to use AI features
- [Troubleshooting](/ai/troubleshooting) - Common configuration issues
