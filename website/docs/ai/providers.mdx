---
title: AI Providers
sidebar_position: 2
sidebar_label: Providers
description: Configure and use multiple AI providers with Atmos
id: providers
---
import Terminal from '@site/src/components/Terminal'
import File from '@site/src/components/File'

# AI Providers

Atmos supports 7 AI providers, giving you flexibility to choose based on your needs: cloud services for convenience, local models for privacy, or enterprise solutions for compliance.

## Supported Providers

| Provider | Default Model | Authentication | Best For |
|----------|---------------|----------------|----------|
| **Anthropic (Claude)** | `claude-sonnet-4-20250514` | API Key | Advanced reasoning, default choice |
| **OpenAI (GPT)** | `gpt-4o` | API Key | Widely available, strong general capabilities |
| **Google (Gemini)** | `gemini-2.0-flash-exp` | API Key | Fast responses, larger context window |
| **xAI (Grok)** | `grok-beta` | API Key | OpenAI-compatible, real-time knowledge |
| **Ollama (Local)** | `llama3.3:70b` | None (local) | Privacy, offline use, zero API costs |
| **AWS Bedrock** | `anthropic.claude-sonnet-4-20250514-v2:0` | AWS Credentials | Enterprise security, AWS integration |
| **Azure OpenAI** | `gpt-4o` | API Key / Azure AD | Enterprise compliance, Azure integration |

## Feature Comparison

Choose the right provider based on your requirements:

| Feature | Cloud Providers<br/>(Claude, GPT, Gemini, Grok) | Ollama<br/>(Local) | Enterprise<br/>(Bedrock, Azure) |
|---------|------------------------------------------|-------------------|------------------------------|
| **Authentication** | API Key | None | AWS/Azure Credentials |
| **Internet Required** | ✅ Yes | ❌ No | ✅ Yes |
| **Cost** | Per-token pricing | Free | Per-token + cloud costs |
| **Data Privacy** | Sent to provider | 100% local | Stays in your cloud |
| **Rate Limits** | Yes | No | Yes (configurable) |
| **Setup Complexity** | Easy | Medium | Complex |
| **Offline Capable** | ❌ No | ✅ Yes | ❌ No |
| **Compliance Certs** | Provider-dependent | N/A | SOC2, HIPAA, ISO |
| **Private Network** | ❌ No | ✅ Yes | ✅ Yes (VPC/VNet) |
| **Hardware Requirements** | None | 8-64GB+ RAM | None (cloud) |

**Use Cases:**
- **Cloud Providers**: Best for most users, easy setup, pay-as-you-go
- **Ollama**: Ideal for sensitive data, offline work, unlimited queries
- **Enterprise**: Required for compliance, data residency, private networks

## Quick Configuration

<File title="atmos.yaml">
```yaml
settings:
  ai:
    enabled: true
    default_provider: "anthropic"  # Provider for CLI commands

    # Configure one or more providers
    providers:
      anthropic:
        model: "claude-sonnet-4-20250514"
        api_key_env: "ANTHROPIC_API_KEY"
        max_tokens: 4096
```
</File>

## Cloud Providers

### Anthropic (Claude)

**Best for**: Advanced reasoning, code generation, complex analysis

<File title="atmos.yaml">
```yaml
providers:
  anthropic:
    model: "claude-sonnet-4-20250514"  # Default
    # Alternatives:
    # model: "claude-3-opus-20240229"     # Most capable
    # model: "claude-3-haiku-20240307"    # Fastest, cheapest
    api_key_env: "ANTHROPIC_API_KEY"
    max_tokens: 4096
```
</File>

**Get API Key**: [console.anthropic.com](https://console.anthropic.com/)

<Terminal title="shell">
```bash
export ANTHROPIC_API_KEY="sk-ant-..."
```
</Terminal>

### OpenAI (GPT)

**Best for**: General-purpose tasks, wide ecosystem support

<File title="atmos.yaml">
```yaml
providers:
  openai:
    model: "gpt-4o"  # Default
    # Alternatives:
    # model: "gpt-4-turbo"      # More capable
    # model: "gpt-3.5-turbo"    # Faster, cheaper
    api_key_env: "OPENAI_API_KEY"
    max_tokens: 4096
```
</File>

**Get API Key**: [platform.openai.com/api-keys](https://platform.openai.com/api-keys)

<Terminal title="shell">
```bash
export OPENAI_API_KEY="sk-..."
```
</Terminal>

### Google (Gemini)

**Best for**: Fast responses, large context windows

<File title="atmos.yaml">
```yaml
providers:
  gemini:
    model: "gemini-2.0-flash-exp"  # Default
    # Alternatives:
    # model: "gemini-1.5-pro"         # Most capable
    # model: "gemini-1.5-flash"       # Faster
    api_key_env: "GEMINI_API_KEY"
    max_tokens: 8192
```
</File>

**Get API Key**: [aistudio.google.com/app/apikey](https://aistudio.google.com/app/apikey)

<Terminal title="shell">
```bash
export GEMINI_API_KEY="AI..."
```
</Terminal>

### xAI (Grok)

**Best for**: OpenAI-compatible alternative, real-time information

<File title="atmos.yaml">
```yaml
providers:
  grok:
    model: "grok-beta"
    api_key_env: "XAI_API_KEY"
    max_tokens: 4096
    base_url: "https://api.x.ai/v1"
```
</File>

**Get API Key**: [x.ai](https://x.ai/)

<Terminal title="shell">
```bash
export XAI_API_KEY="xai-..."
```
</Terminal>

## Local Provider

### Ollama

**Best for**: Privacy, offline use, zero API costs, full data control

Ollama runs AI models entirely on your machine - no data leaves your computer.

**Benefits:**
- **Complete Privacy**: All processing happens locally
- **Offline Capable**: Works without internet connection
- **Zero API Costs**: No usage fees after initial setup
- **No Rate Limits**: Query as much as your hardware allows
- **Customizable**: Use any Ollama-supported model

#### Installation

**macOS:**

<Terminal title="shell">
```bash
# Download and install from ollama.com, or use Homebrew:
brew install ollama

# Start Ollama service
ollama serve
```
</Terminal>

**Linux:**

<Terminal title="shell">
```bash
# Install Ollama
curl -fsSL https://ollama.com/install.sh | sh

# Ollama starts automatically as a service
# Check status: systemctl status ollama
```
</Terminal>

**Windows:**

Download the installer from [ollama.com/download](https://ollama.com/download) and run it. Ollama starts automatically.

**Docker:**

<Terminal title="shell">
```bash
# Run Ollama in Docker
docker run -d \
  -v ollama:/root/.ollama \
  -p 11434:11434 \
  --name ollama \
  ollama/ollama

# For GPU support (NVIDIA):
docker run -d \
  --gpus=all \
  -v ollama:/root/.ollama \
  -p 11434:11434 \
  --name ollama \
  ollama/ollama
```
</Terminal>

#### Model Selection

Choose based on your hardware and quality needs:

| Model | Size | RAM Required | Quality | Best For |
|-------|------|--------------|---------|----------|
| `llama3.3:70b` | ~40GB | 64GB+ | ⭐⭐⭐⭐⭐ Excellent | Production use (default) |
| `llama3.1:8b` | ~5GB | 8GB+ | ⭐⭐⭐ Good | Quick queries, limited hardware |
| `codellama:13b` | ~8GB | 16GB+ | ⭐⭐⭐⭐ Very Good | Code-focused tasks |
| `mistral:7b` | ~4GB | 8GB+ | ⭐⭐⭐ Good | Fast responses |
| `mixtral:8x7b` | ~26GB | 32GB+ | ⭐⭐⭐⭐ Very Good | Complex reasoning |
| `llama3.2:3b` | ~2GB | 4GB+ | ⭐⭐ Fair | Simple tasks only |

**Download a model:**

<Terminal title="shell">
```bash
# Pull the recommended model
ollama pull llama3.3:70b

# Or choose a smaller model for limited hardware
ollama pull llama3.1:8b

# List available models
ollama list

# Remove a model
ollama rm llama3.2:3b
```
</Terminal>

:::tip Model Download Size
Large models like `llama3.3:70b` are ~40GB downloads. Ensure you have sufficient disk space (~50GB free) and adequate RAM.
:::

#### Configuration

**Basic Local Setup (No API Key):**

<File title="atmos.yaml">
```yaml
providers:
  ollama:
    model: "llama3.3:70b"
    # base_url defaults to http://localhost:11434/v1
    # No api_key_env needed for local instances
```
</File>

**Custom Port:**

<File title="atmos.yaml">
```yaml
providers:
  ollama:
    model: "llama3.3:70b"
    base_url: "http://localhost:8080/v1"
```
</File>

**Remote Ollama Server:**

<File title="atmos.yaml">
```yaml
providers:
  ollama:
    model: "llama3.3:70b"
    base_url: "https://ollama.company.com/v1"
    api_key_env: "OLLAMA_API_KEY"  # If authentication enabled
```
</File>

#### Performance Tips

**1. Hardware Recommendations:**

- **Mac M1/M2/M3**: Can handle `llama3.3:70b` well with unified memory
- **16GB RAM**: Use `llama3.1:8b` or `mistral:7b`
- **32GB+ RAM**: Use `llama3.3:70b` or `mixtral:8x7b`
- **Limited RAM**: Use `llama3.2:3b`

**2. GPU Acceleration:**

Ollama automatically uses GPU if available (NVIDIA, AMD, Apple Silicon).

<Terminal title="shell">
```bash
# Check GPU (NVIDIA)
nvidia-smi

# Check GPU (AMD)
rocm-smi

# Ollama will show GPU usage in logs
```
</Terminal>

**3. Optimize Settings:**

<File title="atmos.yaml">
```yaml
providers:
  ollama:
    model: "llama3.3:70b"
    max_tokens: 4096  # Smaller = faster
    timeout_seconds: 120  # Longer timeout for large models
```
</File>

#### Troubleshooting

**Connection Refused:**

<Terminal title="shell">
```bash
# Check if Ollama is running
curl http://localhost:11434/api/version

# If not running, start it:
ollama serve  # macOS/Linux

# Verify port in atmos.yaml matches
```
</Terminal>

**Model Not Found:**

<Terminal title="shell">
```bash
# Pull the model first
ollama pull llama3.3:70b

# List available models
ollama list

# Verify model name matches exactly in atmos.yaml
```
</Terminal>

**Slow Responses:**

- Switch to smaller model (`llama3.1:8b`)
- Reduce `max_tokens` in configuration
- Check if GPU acceleration is working
- Ensure no other heavy processes running

**Out of Memory:**

<Terminal title="shell">
```bash
# Use a smaller model
ollama pull llama3.1:8b

# Update atmos.yaml with smaller model
```
</Terminal>

## Enterprise Providers

For organizations with strict security, compliance, and data governance requirements.

### AWS Bedrock

Run Claude models via AWS Bedrock for enterprise security and compliance.

**Security & Compliance Benefits:**
- **Data Privacy**: Your data never leaves AWS infrastructure
- **IAM-Based Access**: No API keys to manage - uses standard AWS credentials
- **Audit Logging**: Complete audit trail via AWS CloudTrail
- **Network Isolation**: Supports VPC endpoints and AWS PrivateLink for private connectivity
- **Compliance**: Inherits AWS compliance certifications (SOC2, HIPAA, ISO, etc.)

**Configuration:**

<File title="atmos.yaml">
```yaml
providers:
  bedrock:
    model: "anthropic.claude-sonnet-4-20250514-v2:0"
    # Alternatives:
    # model: "anthropic.claude-3-opus-20240229-v1:0"
    # model: "anthropic.claude-3-haiku-20240307-v1:0"
    max_tokens: 4096
    base_url: "us-east-1"  # AWS region
```
</File>

**Authentication**: Uses standard AWS SDK credential chain (IAM roles, profiles, environment variables)

<Terminal title="shell">
```bash
# Configure AWS credentials (one of):

# Option 1: Environment variables
export AWS_ACCESS_KEY_ID="AKIA..."
export AWS_SECRET_ACCESS_KEY="..."

# Option 2: AWS Profile
export AWS_PROFILE="bedrock-profile"

# Option 3: IAM role (automatic in EC2/ECS/EKS)
```
</Terminal>

**Use Cases:**
- AWS-native organizations
- Compliance requirements (HIPAA, SOC2, ISO)
- Data residency requirements
- VPC isolation needs
- Centralized AWS billing

### Azure OpenAI

Run GPT models via Azure OpenAI for enterprise integration.

**Security & Compliance Benefits:**
- **Data Residency**: Control exactly where your data is stored and processed
- **Azure AD Integration**: Use managed identities instead of API keys
- **Compliance Certifications**: SOC2, HIPAA, ISO 27001, and more
- **Private Endpoints**: Azure Private Link for secure, private connections
- **Customer-Managed Keys**: Bring your own encryption keys (BYOK)

**Configuration:**

<File title="atmos.yaml">
```yaml
providers:
  azureopenai:
    model: "gpt-4o"  # Your Azure deployment name
    # Use your actual deployment name from Azure portal
    api_key_env: "AZURE_OPENAI_API_KEY"
    max_tokens: 4096
    base_url: "https://<your-resource>.openai.azure.com"
    # Replace <your-resource> with your Azure resource name
```
</File>

**Authentication:**

<Terminal title="shell">
```bash
# API Key (from Azure Portal)
export AZURE_OPENAI_API_KEY="..."

# Or use Azure AD managed identity (in production)
```
</Terminal>

**Use Cases:**
- Azure-native organizations
- Compliance requirements (healthcare, finance, government)
- European data residency requirements
- Azure AD/Entra integration
- Centralized Azure billing

**When to Use Enterprise Providers:**
- Your organization has existing cloud infrastructure commitments
- Compliance requirements mandate data residency controls
- Need private network connectivity (no public internet)
- Want centralized billing and governance
- Require enhanced audit and logging capabilities

## Multi-Provider Configuration

Configure multiple providers and switch between them:

<File title="atmos.yaml">
```yaml
settings:
  ai:
    enabled: true
    default_provider: "anthropic"  # For CLI commands

    providers:
      # Cloud providers
      anthropic:
        model: "claude-sonnet-4-20250514"
        api_key_env: "ANTHROPIC_API_KEY"
        max_tokens: 4096

      openai:
        model: "gpt-4o"
        api_key_env: "OPENAI_API_KEY"
        max_tokens: 4096

      # Local provider
      ollama:
        model: "llama3.3:70b"
        # No API key needed

      # Enterprise providers
      bedrock:
        model: "anthropic.claude-sonnet-4-20250514-v2:0"
        base_url: "us-east-1"
        # Uses AWS credentials

      azureopenai:
        model: "gpt-4o"
        api_key_env: "AZURE_OPENAI_API_KEY"
        base_url: "https://mycompany.openai.azure.com"
```
</File>

**Switch providers in TUI:**

Press `Ctrl+P` during a chat session to switch providers mid-conversation.

**Use specific provider for CLI:**

Change `default_provider` to your preferred provider for `atmos ai ask` commands.

## Security & Privacy by Provider

### Cloud Providers (Anthropic, OpenAI, Google, xAI)

**Data Processing:**
- Data sent to provider's API servers
- Subject to provider's privacy policy
- Processed in provider's data centers

**Best Practices:**
- Use environment variables for API keys (never commit to git)
- Rotate API keys regularly
- Monitor API usage and costs
- Review provider's data retention policies

### Local Provider (Ollama)

**Data Processing:**
- 100% local processing
- No data leaves your machine
- Works offline

**Best Practices:**
- Keep Ollama updated
- Secure your machine appropriately
- Use disk encryption if handling sensitive data

### Enterprise Providers (Bedrock, Azure OpenAI)

**Data Processing:**
- Stays within cloud provider's infrastructure
- Subject to enterprise agreements
- Enhanced compliance certifications

**Best Practices:**
- Use IAM roles/managed identities instead of keys
- Enable audit logging (CloudTrail/Azure Monitor)
- Configure private endpoints for production
- Implement network security groups
- Use customer-managed encryption keys

## Cost Considerations

### Cloud Providers

**Approximate Pricing** (check provider's website for current rates):

- **Anthropic Claude**: ~$3/1M input tokens, ~$15/1M output tokens
- **OpenAI GPT-4o**: ~$2.50/1M input tokens, ~$10/1M output tokens
- **Google Gemini**: Varies by model and region
- **xAI Grok**: Similar to OpenAI pricing

**Cost Optimization:**
- Use smaller models for simple queries (Haiku, GPT-3.5)
- Configure `max_tokens` to limit response length
- Monitor usage via provider dashboards
- Set up billing alerts

### Local Provider

**Ollama**: Free after initial hardware investment
- No ongoing API costs
- Requires adequate RAM and storage
- Optional: GPU for better performance

### Enterprise Providers

**AWS Bedrock & Azure OpenAI**:
- Can leverage existing cloud commitments
- Reserved capacity available for high volume
- Pricing varies by region and model
- Can be included in Enterprise Agreements

## Related Documentation

- [AI Assistant Overview](/ai/) - General AI features and capabilities
- [Configuration](/ai/configuration) - Detailed configuration options
- [Sessions](/ai/sessions) - Persistent conversation management
- [Tools](/ai/tools) - AI tool execution system
- [MCP Server](/ai/mcp-server) - Universal MCP integration
- [Subagents](/ai/subagents) - Claude Code IDE integration
