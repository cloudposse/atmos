---
title: Performance Profiling
sidebar_position: 2
sidebar_label: Performance Profiling
description: Profile and optimize Atmos performance using built-in heatmap visualization and pprof analysis tools.
---

import Terminal from '@site/src/components/Terminal'
import File from '@site/src/components/File'
import Intro from '@site/src/components/Intro'

<Intro>
    Atmos provides comprehensive performance profiling capabilities through two complementary approaches: a lightweight
    interactive heatmap for quick performance insights, and deep `pprof` integration for detailed runtime analysis.
</Intro>

## Profiling Approaches

Atmos offers two methods for performance analysis, each suited to different use cases:

### Performance Heatmap (Quick Analysis)

A lightweight, built-in visualization tool that provides real-time performance metrics with minimal overhead:

- **Function-level metrics**: Call counts, execution times, and percentile statistics
- **Interactive visualization**: Multiple display modes (bar charts, sparklines, tables)
- **Zero setup**: Single CLI flag (`--heatmap`) enables tracking
- **Microsecond precision**: Captures fast function executions
- **Best for**: Quick performance checks, development workflow, identifying hot paths

### pprof Profiling (Deep Analysis)

Go's comprehensive profiling toolkit for detailed runtime performance analysis:

- **Multiple profile types**: CPU, memory (heap/allocs), goroutines, blocking, mutex contention
- **Line-level profiling**: Pinpoint exact code locations
- **Flame graphs**: Visual call stack analysis
- **Persistent data**: Export profiles for historical comparison
- **Best for**: Deep performance investigation, memory leak detection, production debugging

## Performance Heatmap

### Quick Start

Display the performance heatmap after any Atmos command:

```shell
# Run any Atmos command with the --heatmap flag
atmos describe stacks --heatmap
```

This launches an interactive TUI (Terminal User Interface) where you can switch between visualization modes by pressing 1-3.

### Real-World Example

Here's actual output from `atmos describe stacks --heatmap`:

**Interactive Mode** (when running in a terminal with TTY):

![Performance Heatmap - Bar Chart](/img/cli/perf/atmos-describe-stacks-heatmap-bar-chart.png)

The interactive TUI shows the top 25 functions with color-coded bars representing total CPU time. Each function displays its **average execution time per call** alongside the **total number of calls**, making it easy to identify both slow functions and high-frequency functions at a glance.

**Example Interactive Display:**
```
ðŸ”¥ Performance Heatmap - Bar Chart

utils.processCustomTags           â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ avg: 0.37ms | calls: 447586
exec.ProcessYAMLConfigFile        â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ avg: 686Âµs | calls: 199
utils.UnmarshalYAMLFromFile       â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ avg: 82Âµs | calls: 53429
```

- **Bar length**: Proportional to total CPU time (shows overall impact on performance)
- **avg**: Average self-time per call (shows typical function performance)
- **calls**: Number of times the function was invoked

**Interactive TUI Legend:**

The TUI displays a comprehensive legend at the top showing performance summary and metric descriptions:

```
Parallelism: ~0.9x | Elapsed: 279.002ms | CPU Time: 255.378ms
Count: # calls (incl. recursion) | CPU Time: sum of self-time (excludes children)
Avg: avg self-time | Max: max self-time | P95: 95th percentile self-time
```

- **Line 1**: Live performance metrics for this execution (Parallelism, Elapsed time, Total CPU time)
- **Line 2**: Explanation of Count and CPU Time columns
- **Line 3**: Explanation of statistical timing columns (Avg, Max, P95)

**Non-Interactive Mode** (CI/CD, scripts, or redirected output):

<Terminal title="Performance heatmap output">
    ```
    === Atmos Performance Summary ===
    Elapsed: 54.16ms | CPU Time: 37.21ms | Parallelism: ~0.7x
    Functions: 42 | Total Calls: 5980

    Function                                            Count    CPU Time        Avg        Max        P95
    exec.ProcessYAMLConfigFileWithContext                  52     7.488ms      144Âµs      760Âµs      662Âµs
    exec.Execute                                            1     6.233ms    6.233ms    6.233ms    6.233ms
    exec.ValidateStacks                                     1     5.499ms    5.499ms    5.499ms    5.499ms
    merge.MergeWithOptions                                746     4.561ms        6Âµs      654Âµs       21Âµs
    utils.processCustomTags                              1024      4.27ms        4Âµs      146Âµs       15Âµs
    utils.GetHighlightedYAML                                1      3.86ms     3.86ms     3.86ms     3.86ms
    merge.MergeWithContext                                356     3.123ms        8Âµs      655Âµs       29Âµs
    utils.ConvertToYAML                                   177       2.5ms       14Âµs      309Âµs       56Âµs
    exec.ProcessStackConfig                                12     1.926ms      160Âµs      367Âµs      186Âµs
    utils.GetGlobMatches                                   48     1.859ms       38Âµs      418Âµs      226Âµs
    ```
</Terminal>

### Visualization Modes

The heatmap supports three visualization modes. In interactive mode, press 1-3 to switch:

#### 1. Bar Chart (Default)

Horizontal bars with color gradient showing relative total CPU time:

```shell
atmos describe stacks --heatmap --heatmap-mode=bar
```

- **Bar length**: Proportional to total CPU time (overall performance impact)
- **Display format**: Shows `avg: Xms | calls: N` for each function
- **Color gradient**: Red (highest impact) â†’ Green (lowest impact)
- **Best for**: Quick identification of both slow functions and high-impact functions
- **Example**: `utils.processCustomTags  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ avg: 0.37ms | calls: 447586`

#### 2. Sparkline Mode

Compact sparkline charts showing relative average execution times:

```shell
atmos describe stacks --heatmap --heatmap-mode=sparkline
```

- **Sparkline height**: Proportional to average self-time per call
- **Display format**: Shows `avg: Xms | calls: N` for each function
- **Compact view**: See more functions in less vertical space
- **Best for**: Quick pattern recognition and comparing many functions at once
- **Example**: `utils.processCustomTags  â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡ avg: 0.37ms | calls: 447586`

#### 3. Table Mode

Detailed tabular view with all metric columns (top 50 rows):

```shell
atmos describe stacks --heatmap --heatmap-mode=table
```

- All metric columns (Count/CPU Time/Avg/Max/P95)
- Shows top 50 functions by CPU time
- Best for detailed analysis

### Understanding Metrics

**Performance Summary:**
```
=== Atmos Performance Summary ===
Elapsed: 234.56ms | CPU Time: 185.32ms | Parallelism: ~0.8x
Functions: 12 | Total Calls: 156
```

- **Elapsed**: Total wall-clock execution time for the command
- **CPU Time**: Sum of all self-times (actual CPU work done, excludes time spent in child functions)
- **Parallelism**: CPU Time Ã· Elapsed time ratio (greater than 1.0 = parallel execution, less than 1.0 = single-threaded)
- **Functions**: Number of unique functions tracked
- **Total Calls**: Total number of function calls tracked

**Function Metrics:**

<dl>
    <dt>`Function`</dt>
    <dd>Name of the tracked function</dd>

    <dt>`Count`</dt>
    <dd>Number of times the function was called (includes all recursive calls)</dd>

    <dt>`CPU Time`</dt>
    <dd>Sum of self-time across all calls - total CPU work done by the function itself, excluding time spent in child function calls. This avoids double-counting in nested/recursive calls and accurately represents the total CPU time spent executing the function's own code.</dd>

    <dt>`Avg`</dt>
    <dd>Average self-time per call - actual work done in the function, excluding time spent in child function calls (SelfTime Ã· Count). This metric accurately represents the function's own execution time and is the best indicator for optimization opportunities.</dd>

    <dt>`Max`</dt>
    <dd>Maximum self-time for a single function call among ALL executions (excludes time spent in children). Critical for identifying performance outliers and worst-case scenarios. If a function is called 100 times with most calls taking ~10ms but one call took 500ms, Max will show 500ms - revealing intermittent issues like excessive work, algorithmic edge cases, or system resource contention affecting that specific call.</dd>

    <dt>`P95`</dt>
    <dd>95th percentile of self-time latency - represents the self-time below which 95% of function calls complete. A consistent indicator of the function's own work performance, excluding time spent in child functions.</dd>
</dl>

:::tip Understanding CPU Time and Self-Time Metrics

**All timing columns use Self-Time**: CPU Time, Avg, Max, and P95 all exclude time spent in child function calls, showing only the actual work done in the function itself. This avoids double-counting in nested/recursive calls.

**CPU Time**: Sum of self-time across all calls - represents total CPU work done by the function
**Avg**: Average self-time per call (CPU Time Ã· Count)
**Max**: Maximum self-time for any single call
**P95**: 95th percentile of self-time across all calls

**Example**: If `ProcessConfig` is called 10 times and each call does 20ms of its own work (excluding 80ms spent in child functions):
- **CPU Time**: 200ms (10 calls Ã— 20ms per call)
- **Avg**: 20ms (200ms Ã· 10 calls)
- **Wall-Clock Time**: Would be ~1000ms (10 Ã— 100ms including children) - but this would double-count child execution time

**Why CPU Time vs Wall-Clock?** When functions call each other, summing wall-clock times counts the same work multiple times. CPU Time (sum of self-times) accurately represents total work without double-counting, making it possible to meaningfully compare the sum of all CPU Times to the command's elapsed time.

**Parallelism Factor**: The ratio of CPU Time to Elapsed time shows execution characteristics:
- **~0.8x**: Single-threaded with some overhead
- **~1.0x**: Perfect single-threaded execution
- **Greater than 1.0x**: Parallel execution across multiple cores (e.g., 4.0x = ~4 cores utilized)
:::

:::warning Detecting Performance Outliers with Max

**Max is your outlier detector!** It shows the slowest single execution among all calls to a function (excluding time spent in children).

**Example - Spotting Intermittent Issues:**
```
Function                Count    CPU Time  Avg       Max       P95
network.FetchRemote     100      1.2s      12ms      850ms     15ms
```

**Analysis:**
- **Count**: 100 calls total
- **CPU Time**: 1.2s total (sum of all self-times)
- **Avg**: 12ms average (typical performance)
- **Max**: 850ms (one call's own work took 71x longer!)
- **P95**: 15ms (95% of calls complete within 15ms)

**Diagnosis**: The large gap between Max (850ms) and P95 (15ms) reveals an intermittent performance issue affecting ~5% of calls. Since Max tracks self-time (excluding children), this indicates the function's own work was slow. Likely causes:
- **Network timeout** on one request (if function makes network calls)
- **Disk I/O spike** during that specific call
- **CPU contention** or system resource starvation
- **GC pause** occurred during execution
- **Algorithmic edge case** (e.g., processing unusually large input)

**Action**: Investigate why that specific call was 71x slower than average. Use `--profile-file` with pprof for deeper analysis.
:::

### Interactive Controls

**Keyboard Shortcuts:**
- **â†‘/â†“**: Move selection up/down (also **k/j**)
- **1**: Switch to Bar Chart mode
- **2**: Switch to Sparkline mode
- **3**: Switch to Table mode
- **q/esc**: Quit and return to terminal

### CLI Flags

<dl>
    <dt>`--heatmap`</dt>
    <dd>Show performance heatmap visualization after command execution (includes P95 latency) (default: `false`)</dd>

    <dt>`--heatmap-mode`</dt>
    <dd>Heatmap visualization mode: `bar`, `sparkline`, `table` (press 1-3 to switch in TUI) (default: `bar`)</dd>
</dl>

### Common Use Cases

**Identifying Performance Bottlenecks:**
```bash
# Track which functions consume the most time
atmos terraform plan large-component -s prod --heatmap

# Focus on functions with high CPU Time
```

**Before/After Comparison:**
```bash
# Baseline measurement
atmos describe stacks --heatmap 2>baseline.txt

# After optimization
atmos describe stacks --heatmap 2>optimized.txt

# Compare results
diff baseline.txt optimized.txt
```

**CI/CD Integration:**
```bash
# Capture performance in automated pipelines
atmos terraform plan vpc -s prod --heatmap 2>&1 | tee performance.log

# Parse for regression detection
```

**Development Workflow:**
```bash
# Create an alias for convenient usage
alias atmos-perf='atmos --heatmap'

# Use during development
atmos-perf validate stacks
atmos-perf describe component vpc -s dev
```

## pprof Profiling

### Overview

[pprof](https://pkg.go.dev/net/http/pprof) is Go's standard profiling tool that captures detailed runtime performance data.

**Profile Types:**
- **CPU Profile**: Where your program spends CPU time
- **Heap Profile**: Current heap memory allocation patterns
- **Allocs Profile**: All memory allocations since program start
- **Goroutine Profile**: Active goroutines and call stacks
- **Block Profile**: Operations blocking on synchronization
- **Mutex Profile**: Lock contention patterns
- **Thread Create Profile**: Stack traces leading to thread creation
- **Trace Profile**: Detailed execution traces for analysis

### File-Based Profiling

Capture profiles directly to a file - ideal for CLI tools:

```shell
# CPU profiling (default)
atmos terraform plan vpc -s plat-ue2-dev --profile-file=cpu.prof

# Memory heap profiling
atmos terraform plan vpc -s plat-ue2-dev --profile-file=heap.prof --profile-type=heap

# Execution trace profiling
atmos terraform plan vpc -s plat-ue2-dev --profile-file=trace.out --profile-type=trace

# Goroutine profiling
atmos terraform plan vpc -s plat-ue2-dev --profile-file=goroutine.prof --profile-type=goroutine
```

<Terminal title="File-based profiling output">
    ```
    INFO Profiling started type=cpu file=cpu.prof
    INFO Profiling completed type=cpu file=cpu.prof
    ```
</Terminal>

### Server-Based Profiling

Start an HTTP server for interactive profiling:

```shell
atmos terraform plan vpc -s plat-ue2-dev --profiler-enabled
```

<Terminal title="Server-based profiling output">
    ```
    INFO Profiler server available at: url=http://localhost:6060/debug/pprof/
    ```
</Terminal>

Access different profiles through HTTP endpoints:

```shell
# CPU profile (30-second sample)
go tool pprof http://localhost:6060/debug/pprof/profile

# Memory allocation profile
go tool pprof http://localhost:6060/debug/pprof/heap

# Goroutine profile
go tool pprof http://localhost:6060/debug/pprof/goroutine

# Web interface
open http://localhost:6060/debug/pprof/
```

### Configuration

**CLI Flags:**

<dl>
    <dt>`--profile-file`</dt>
    <dd>Write profiling data to file instead of starting server</dd>

    <dt>`--profile-type`</dt>
    <dd>Type of profile to collect when using `--profile-file`. Options: `cpu`, `heap`, `allocs`, `goroutine`, `block`, `mutex`, `threadcreate`, `trace` (default: `cpu`)</dd>

    <dt>`--profiler-enabled`</dt>
    <dd>Enable pprof profiling server (default: `false`)</dd>

    <dt>`--profiler-host`</dt>
    <dd>Host for pprof profiling server (default: `localhost`)</dd>

    <dt>`--profiler-port`</dt>
    <dd>Port for pprof profiling server (default: `6060`)</dd>
</dl>

**Environment Variables:**

<dl>
    <dt>`ATMOS_PROFILER_ENABLED`</dt>
    <dd>Enable pprof profiling server</dd>

    <dt>`ATMOS_PROFILER_HOST`</dt>
    <dd>Host address for profiling server</dd>

    <dt>`ATMOS_PROFILER_PORT`</dt>
    <dd>Port for profiling server</dd>

    <dt>`ATMOS_PROFILE_FILE`</dt>
    <dd>File path for file-based profiling</dd>

    <dt>`ATMOS_PROFILE_TYPE`</dt>
    <dd>Profile type for file-based profiling</dd>
</dl>

**Configuration File:**

<File title="atmos.yaml">
    ```yaml
    profiler:
      enabled: true
      host: "localhost"
      port: 6060
      file: "profile.out"           # Optional: file-based profiling
      profile_type: "cpu"           # Optional: profile type
    ```
</File>

**Configuration Precedence:**
1. Command-line flags (highest priority)
2. Environment variables
3. Configuration file (`atmos.yaml`)
4. Default values (lowest priority)

### Analyzing Profiles

**CPU and Memory Profiles:**
```shell
# Interactive text mode
go tool pprof cpu.prof
go tool pprof heap.prof

# Web interface (requires Graphviz: brew install graphviz)
go tool pprof -http=:8080 cpu.prof
go tool pprof -http=:8080 heap.prof

# Direct text output
go tool pprof -top cpu.prof
go tool pprof -top heap.prof
```

**Trace Profiles:**
```shell
# Use go tool trace for execution traces
go tool trace trace.out

# Opens web interface showing:
# - Timeline view of goroutines
# - Network blocking profile
# - Synchronization blocking profile
# - System call blocking profile
```

<Terminal title="Sample pprof output">
    ```
    (pprof) top
    Showing nodes accounting for 230ms, 95.83% of 240ms total
    Dropped 15 nodes (cum <= 1.20ms)
    flat flat% sum% cum cum%
    80ms 33.33% 33.33% 80ms 33.33% github.com/cloudposse/atmos/internal/exec.processStackConfig
    60ms 25.00% 58.33% 60ms 25.00% gopkg.in/yaml.v3.(*Decoder).Decode
    40ms 16.67% 75.00% 40ms 16.67% github.com/cloudposse/atmos/pkg/utils.ProcessTmplWithDatasources
    30ms 12.50% 87.50% 30ms 12.50% encoding/json.(*Decoder).Decode
    20ms 8.33% 95.83% 20ms 8.33% github.com/cloudposse/atmos/pkg/stack.ProcessStackConfig
    ```
</Terminal>

**Understanding pprof Data:**
- **flat**: Time spent in the function itself
- **cum**: Cumulative time (function + callees)
- **flat%**: Percentage of total execution time
- **sum%**: Cumulative percentage up to this function

Focus optimization on functions with high **flat** time and **flat%**.

### Common Scenarios

**Performance Optimization:**
```shell
# Profile CPU usage in slow operations
atmos terraform plan large-component -s prod --profile-file=slow-plan.prof --profile-type=cpu

# Profile memory usage
atmos terraform plan large-component -s prod --profile-file=memory.prof --profile-type=heap

# Profile detailed execution
atmos terraform plan large-component -s prod --profile-file=trace.out --profile-type=trace

# Analyze results
go tool pprof -http=:8080 slow-plan.prof
go tool pprof -http=:8080 memory.prof
go tool trace trace.out
```

**Memory Analysis:**
```shell
# File-based memory profiling
atmos describe stacks --profile-file=heap.prof --profile-type=heap
atmos describe stacks --profile-file=allocs.prof --profile-type=allocs

# Analyze
go tool pprof -http=:8080 heap.prof
go tool pprof -http=:8080 allocs.prof

# Server-based memory profiling
atmos describe stacks --profiler-enabled

# In another terminal
go tool pprof http://localhost:6060/debug/pprof/heap
go tool pprof http://localhost:6060/debug/pprof/allocs
```

**Custom Server Configuration:**
```shell
atmos terraform apply vpc -s prod \
  --profiler-enabled \
  --profiler-host=0.0.0.0 \
  --profiler-port=8060
```

## Choosing the Right Tool

| Feature            | Performance Heatmap            | pprof Profiling                       |
|--------------------|--------------------------------|---------------------------------------|
| **Overhead**       | Minimal (microsecond tracking) | Higher (full runtime instrumentation) |
| **Granularity**    | Function-level metrics         | Line-level profiling                  |
| **Setup**          | CLI flag only (`--heatmap`)    | Requires flags or config              |
| **Output**         | Real-time visualization        | File or HTTP endpoint                 |
| **Use Case**       | Quick performance checks       | Deep performance analysis             |
| **Visualization**  | Built-in interactive UI        | Requires `go tool pprof`              |
| **Data Retention** | In-memory (command lifetime)   | Persistent files                      |
| **Profile Types**  | Function timing only           | CPU, memory, goroutines, blocking, etc. |

**Use Performance Heatmap when:**
- Validating performance during development
- Identifying which functions are called most
- Comparing execution times between commands
- Need quick insights without external tools
- Monitoring lightweight operations

**Use pprof Profiling when:**
- Deep diving into performance bottlenecks
- Analyzing CPU and memory usage patterns
- Investigating memory leaks
- Profiling goroutines and lock contention
- Need flame graphs and detailed visualizations
- Historical comparison across runs

**Use Both Together:**
```bash
# Combine for comprehensive analysis
atmos terraform plan vpc -s prod \
  --heatmap \
  --profile-file=cpu.prof \
  --profile-type=cpu

# View quick heatmap summary
# Then deep dive with pprof
go tool pprof -http=:8080 cpu.prof
```

## Dependencies

### Graphviz (Optional for pprof)

The pprof web interface requires [Graphviz](https://graphviz.org/) for visual graphs:

**macOS:**
```shell
brew install graphviz
```

**Ubuntu/Debian:**
```shell
sudo apt-get install graphviz
```

**CentOS/RHEL:**
```shell
sudo yum install graphviz
```

Without Graphviz, text-based pprof analysis still works.

## Best Practices

### File-Based vs Server-Based (pprof)

- **Use file-based profiling** for most CLI operations and performance analysis
- **Use server-based profiling** for long-running operations or interactive profiling
- Supports all profile types: `cpu`, `heap`, `allocs`, `goroutine`, `block`, `mutex`, `threadcreate`, `trace`

### Performance Heatmap Usage

- **Enable only when needed**: Zero overhead when not using `--heatmap` flag
- **Use appropriate modes**: `bar` for quick comparison, `table` for detailed analysis
- **Combine with logging**: Correlate performance with debug logs
- **Automate in CI/CD**: Track performance regressions in pipelines

### Profile Regularly

- Profile before and after optimizations
- Establish baseline profiles for typical operations
- Profile different stack sizes and complexity levels
- Compare heatmap metrics for quick validation, pprof for deep analysis

### Security Considerations

- Server-based profiling exposes runtime information through HTTP
- Use `localhost` binding in production environments
- Disable profiling in production unless actively debugging
- Performance heatmap is safe (in-memory only, no network exposure)

## Troubleshooting

### Performance Heatmap Issues

**Heatmap Not Showing:**
```bash
# Ensure you're using the latest Atmos version
atmos version --heatmap
```

**No Functions Tracked:**
- Performance tracking is added incrementally
- Run commands that process stacks/components for more data:
```bash
atmos describe stacks --heatmap
atmos terraform plan component -s stack --heatmap
```

**P95 Shows Zero:**
- P95 requires multiple calls to be meaningful
- Run commands that execute functions multiple times

**Interactive Mode Not Working:**
- Requires TTY (terminal)
- In scripts/CI/CD, static summary is displayed instead:
```
âš ï¸  No TTY available for interactive visualization. Summary displayed above.
```

### pprof Issues

**Profile File Creation Errors:**
```shell
# Ensure directory exists
mkdir -p /path/to/profile/
atmos command --profile-file=/path/to/profile/cpu.prof --profile-type=cpu
```

**Invalid Profile Type:**
```shell
# Check supported types
echo "Supported: cpu, heap, allocs, goroutine, block, mutex, threadcreate, trace"
atmos command --profile-file=profile.out --profile-type=heap
```

**Graphviz Not Found:**
```shell
# Use text-based analysis instead
go tool pprof -top cpu.prof
go tool pprof -list=functionName cpu.prof
```

**Server Already Running:**
```shell
# Use different port
atmos command --profiler-enabled --profiler-port=7070
```

## Examples

### Development Workflow
```bash
# Quick performance check with heatmap
atmos describe stacks --heatmap

# Deep analysis with pprof if issues found
atmos describe stacks --profile-file=cpu.prof
go tool pprof -http=:8080 cpu.prof
```

### CI/CD Integration
```bash
#!/bin/bash
# ci-performance-check.sh

# Run with heatmap for quick metrics
atmos validate stacks --heatmap 2>&1 | tee perf-output.txt

# Extract elapsed time
elapsed=$(grep "Elapsed:" perf-output.txt | awk '{print $2}')

# Check for regression (e.g., > 500ms)
if [ $(echo "$elapsed > 500" | bc) -eq 1 ]; then
  echo "Performance regression: ${elapsed} > 500ms"
  # Deep profile for investigation
  atmos validate stacks --profile-file=cpu.prof --profile-type=cpu
  exit 1
fi
```

### Combined Analysis
```bash
# Use both tools for comprehensive analysis
atmos terraform plan large-stack -s prod \
  --heatmap \
  --heatmap-mode=bar \
  --profile-file=analysis.prof \
  --profile-type=cpu \
  2>&1 | tee combined-analysis.log

# View heatmap summary immediately
# Then analyze with pprof
go tool pprof -http=:8080 analysis.prof
```

## Related Documentation

- [Error Messages](/cli/errors) - Common error messages and solutions
