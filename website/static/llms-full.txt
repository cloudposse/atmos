# atmos

> Universal tool for DevOps and Cloud Automation

This file contains all documentation content in a single document following the llmstxt.org standard.

## Best Practices

import DocCardList from '@theme/DocCardList'

> <q>Physics is the law, everything else is a recommendation.
> Anyone can break laws created by people, but I have yet to see anyone break the laws of physics.
> — **Elon Musk**</q>

Learn how to best leverage Stacks and Components together with Atmos.

<DocCardList/>

---

## Component Best Practices

import Intro from '@site/src/components/Intro'

<Intro>
Here are some essential best practices to follow when designing architectures using infrastructure as code (IaC), focusing on optimizing
component design, reusability, and lifecycle management. These guidelines are designed to help developers and operators build efficient,
scalable, and reliable systems, ensuring a smooth and effective infrastructure management process.
</Intro>

Also, be sure to review the [Terraform Best Practices](/best-practices/terraform) for additional guidance on using Terraform with Atmos.

> <q>Physics is the law, everything else is a recommendation.
> Anyone can break laws created by people, but I have yet to see anyone break the laws of physics.
> — **Elon Musk**</q>

## Keep Your Components Small to Reduce the Blast Radius of Changes

Focus on creating single purpose components that small, reusable components that adhere to the UNIX philosophy by doing one thing well.
This strategy leads to simpler updates, more straightforward troubleshooting, quicker plan/apply cycles, and a
clearer separation of responsibilities. Best of all, your state remains small and complexity remains manageable.

Anti-patterns to avoid include:
- Combining VPCs with databases in the same component
- Defining every dependency needed by an application in a single component (provided there's no shared lifecycle)

## Split Components By Lifecycle

To keep your component small, consider breaking them apart by their Software Development Lifecycle (SDLC).
Things that always change together, go together. Things that seldom change together, should be managed separately.
Keep the coupling loose, and use remote state for cohesion.

For instance, a VPC, which is rarely destroyed, should be managed separately from more dynamic resources like clusters
or databases that may frequently scale or undergo updates.

## Make Them Opinionated, But Not Too Opinionated

Ensure components are generalized to prevent the proliferation of similar components, thereby promoting easier testing,
reuse, and maintenance.

:::important Don't Treat Components like Child Modules
Don't force users to use generic components if that will radically complicate the configuration.
The goal is to make 80% of your infrastructure highly reusable with generic single purpose components.
The remaining 20% might need to be specialized for your use case, and that's okay.
:::

## Avoid Single Resource Components

If you find yourself writing a component that is so small, it manages only a single resource e.g. (an IAM Policy),
consider if it should be part of a larger component.

:::tip Stack Configurations are Not a Replacement for Terraform
The biggest risk for newcomers to Atmos is to over architect components into extremely DRY single-purpose components.
Stack configurations in YAML should not just be a proxy for terraform resources.

Use terraform for its strengths, compliment it with YAML when it makes sense for very straight forward configuration.
:::

## Use Parameterization, But Avoid Over-Parameterization

Good parameterization ensures components are reusable, but components become difficult to test and document with too many parameters.

Often time, child modules might accept more parameters than the root module. You can always add more parameters to the root module
as needed, but it's hard to remove them once they are there.

## Avoid Creating Factories Inside of Components

[Factories are common software design patterns](https://en.wikipedia.org/wiki/Factory_(object-oriented_programming)) that allow you
to create multiple instances of a component.

To minimize the blast radius of changes and maintain fast plan/apply cycles, do not embed factories within components that
provision lists of resources.

Examples of anti-patterns include:
- Reading a configuration file inside of Terraform to create multiple Buckets
- Using a `for_each` loop to create multiple DNS records from a variable input
  (you may hit rate limits when you zones get large enough; it's happened to us)

Instead, leverage [Stack configurations to serve as factories](/core-concepts/stacks) for provisioning multiple component instances.
This approach keeps the state isolated and scales efficiently with the increasing number of component instances.

Please note, it's perfectly fine to use `for_each` loops sometimes to provision groups of resources, just use them with moderation
and be aware of the potential downsides, such as creating massive states with a wide blast radius. For example, maybe you can safely manage a collection of resources this way.

:::note Do as we say, not as we do
It is with humility that we state this best practice. Even many of our own Cloud Posse components, do not follow this because
they were written before we realized the overwhelming benefits of this approach.
:::

## Use Components Inside of Factories

Google discusses the "factories" approach in the post [Resource Factories: A descriptive approach to Terraform](https://medium.com/google-cloud/resource-factories-a-descriptive-approach-to-terraform-581b3ebb59c). This concept is familiar to every major programming framework, and you can apply it to Terraform too.

However, unlike Google's approach of creating the factory inside the component ([which we don't recommend](#avoid-creating-factories-inside-of-components)), we suggest using the stack configuration as the factory and the component as the product.

By following this method, you create a single component for a specific purpose, such as a VPC, database, or Kubernetes cluster. Then, you can instantiate multiple instances of that component in your stack configuration.

In the factory pattern, the component acts like the "factory class," and when defined in the stack configuration, it is used to create and configure multiple component instances.

A component provides specific functionality but is not responsible for its own instantiation or configuration; this responsibility is delegated to the factory.

This approach decouples your architecture from the configuration, resulting in smaller state files and independent lifecycle management for each instance. Most importantly, it maximizes the reusability of your components.

## Use Component Libraries & Vendoring

Utilize a centralized [component library](/core-concepts/components/library) to distribute and share components across the
organization efficiently. This approach enhances discoverability by centralizing where components are stored, preventing
sprawl, and ensuring components are easily accessible to everyone. Employ vendoring to retrieve remote dependencies, like
components, ensuring the practice of immutable infrastructure.

## Organize Related Components with Folders

Organize multiple related components in a common folder. Use nested folders as necessary, to logically group components.
For example, by grouping components by cloud provider and layer (e.g. `components/terraform/aws/network/<vpc>`)

## Document Component Interfaces and Usage

Utilize tools such as [terraform-docs](https://terraform-docs.io) to thoroughly document the input variables and outputs
of your component. Include snippets of stack configuration to simplify understanding for developers on integrating the component
into their stack configurations. Providing examples that cover common use-cases of the component is particularly effective.

## Version Components for Breaking Changes

Use versioned folders within the component to delineate major versions (e.g. `/components/terraform/<something>/v1/`)

## Use a Monorepo for Your Components

For streamlined development and simplified dependency management, smaller companies should consolidate stacks and components
in a single monorepo, facilitating easier updates and unified versioning. Larger companies and enterprises with multiple monorepos
can benefit from a central repository for upstream components, and then use vendoring to easily pull in these shared components to
team-specific monorepos.

## Maintain Loose Coupling Between Components

Avoid directly invoking one component from within another to ensure components remain loosely coupled. Specifically for Terraform
components (root modules), this practice is unsupported due to the inability to define a backend in a child module, potentially
leading to unexpected outcomes. It's crucial to steer clear of this approach to maintain system integrity.

## Reserve Code Generation as an Escape Hatch for Emergencies

We generally advise against using code generation for application logic (components), because it's challenging to ensure good test
coverage (e.g. with `terratest`) and no one likes to code review machine-generated boilerplate in Pull Requests.

If you find yourself in a situation that seems to require code generation, take a step back and consider if that's the right approach.
- Do not code generate providers to [overcome "limitations" in Terraform](https://github.com/hashicorp/terraform/issues/19932#issuecomment-1817043906),
  for example, to iterate over providers. This is a red flag. Instead, architect your components to work with a single provider
- If you are programmatically combining several child modules, consider if they should instead be separated by lifecycle.

When you follow these rules, root modules become highly reusable, and you reduce the amount of state managed by a single component,
and therefore, the blast radius of changes.

## Separate Your State by Region

For Disaster Recovery purposes, always strive to keep the state of your components separate by region.
You don't want a regional outage to affect your ability to manage infrastructure in other regions.

## Limit Providers to One or Two Per Component

Avoid using multiple providers in a single component, as it reduces the reusability of the component and increases
the complexity and blast radius of what it manages.

Consider instead "hub" and "spoke" models, where each spoke is its own component with its own lifecycle.
In this model, the "spoke" will usually have two providers, one for the current context and one for the "hub."

---

## Stacks Best Practices

import Intro from '@site/src/components/Intro'

<Intro>
Here are some essential best practices to follow when designing the Stack configurations that describe your architectures. These guidelines are intended to help developers and operators think about how they model the configuration of their infrastructure in Atmos, for maximum clarity and long-term maintainability.
</Intro>

> <q>Physics is the law, everything else is a recommendation.
> Anyone can break laws created by people, but I have yet to see anyone break the laws of physics.
> — **Elon Musk**</q>

## Define Factories in Stack Configurations

Avoid creating factories inside of components, which make them overly complicate and succumb to their massive state.
Instead, use stack configurations to serve as factories for provisioning multiple component instances.
This approach keeps the state isolated and scales efficiently with the increasing number of component instances.

## Treat Stack Templates like an Escape Hatch

Apply them carefully and only when necessary. Using templates instead of inheritance can make stack configurations complex
and hard to manage. Be careful using stack templates together with the [factory pattern](#define-factories-in-stack-configurations).

The simplest templates are the best templates. Using variable interpolation is perfectly fine, but avoid using complex logic,
conditionals, and loops in templates. If you find yourself needing to do this, consider if you are solving the problem in the right way.

## Avoid Too Many Levels of Imports

It's very difficult for others to follow relationships when there are too many nested levels and overrides.

:::warning Complexity rashes

**If you have more than (3) levels of imports, you're probably developing a complexity rash.**

Overly DRY configurations can lead to complexity rashes that are difficult to debug and maintain,
and impossible for newcomers to understand.

:::

## Balance DRY Principles with Configuration Clarity

Avoid overly DRY configuration as it leads to complexity rashes. Sometimes repeating configuration is beneficial
for maintenance and clarity.

In recent years, the DevOps industry has often embraced the DRY (Don’t Repeat Yourself) principle to an extreme.
(And Atmos delivers!) While DRY aims to reduce redundancy and improve maintainability by eliminating duplicate code,
overzealous application of this principle leads to complications and rigidity.

DRY is not a panacea. In fact, sometimes a bit of repetition is **beneficial**, particularly when anticipating future
divergence in configurations or functionality. A balance between DRY and WET (Write Everything Twice) can offer more
flexibility, and make it easier to see the entire context in one place without needing to trace through multiple abstractions
or indirections

Here’s why:

1. **Cognitive Load:** The more you strive for DRYness, the more indirection and abstraction layers you introduce.
    This makes it harder for developers because they need to navigate through multiple layers of imports and abstractions
    to grasp the complete picture.
2. **Plan for Future Divergence:** When initially similar configurations are likely diverge over time,
    keeping them separate will make future changes easier.
3. **Premature Optimization:** Over-optimizing for DRYness may be a form of premature optimization. It’s important to recognize
    when to prioritize flexibility and clarity over minimal repetition.

## Reserve Code Generation for Stack Configuration

While we generally advise against using code generation for application logic (components), it's beneficial for
creating configurations where appropriate, such as developer environments and SaaS tenants.
These configurations ought to be committed.

Also, consider if you can [use templates](/core-concepts/stacks/templates) instead.

## Use Mixin Pattern for Snippets of Stack Configuration

Employ the [mixin pattern](/core-concepts/stacks/inheritance/mixins) for clarity when there there is brief configuration snippets that are reusable. Steer clear
of minimal stack configurations simply for the sake of DRYness as it frequently leads to too many levels of imports.

## Use YAML Anchors to DRY Configuration

YAML anchors are pretty sweet and you don’t get those with tfvars.

:::important YAML Anchors Gotchas
When you define [YAML anchors](https://yaml.org/spec/1.2.2/#3222-anchors-and-aliases), they can only be used within the scope of the
same file. This is not an Atmos limitation, but how YAML works. For example, do not work together with [imports](/core-concepts/stacks/imports),
where you define an anchor in one stack configuration and try to use it in another.
:::

## Enforce Standards using OPA Policies

Apply OPA or JSON Schema validation within stacks to establish policies governing component usage. These policies can be tailored
as needed, allowing the same component to be validated differently depending on its context of use.

---

## Terraform Best Practices with Atmos

import Intro from '@site/src/components/Intro'

<Intro>
These are some of the best practices we recommend when using Terraform with Atmos. They are opinionated and based on our experience working with Terraform and Atmos. When followed, they lead to more reusable and maintainable infrastructure as code.
</Intro>

> <q>Physics is the law, everything else is a recommendation.
> Anyone can break laws created by people, but I have yet to see anyone break the laws of physics.
> — **Elon Musk**</q>

Also, since [Terraform "root modules" are components](/core-concepts/components/terraform), be sure to review the [Component Best Practices](/best-practices/components) for additional guidance on using components with Atmos.

:::tip
[Cloud Posse](https://github.com/cloudposse) publishes their general [Terraform Best Practices](https://docs.cloudposse.com/reference/best-practices/terraform-best-practices/), which are more general and not specific to Atmos.
:::

##  Never Include Components Inside of Other Components

We do not recommend consuming one terraform component inside of another as that would defeat the purpose; each component is intended to be a loosely coupled unit of IaC with its own lifecycle.

Furthermore, since components define a state backend and providers, it's not advisable to call one root module from another root module. As only the stack backend of the first root module will be used, leading to unpredictable results.

## Use Terraform Overrides to Extend ("Monkey Patch") Vendored Components

When you need to extend a component, we recommend using [Terraform Overrides](https://developer.hashicorp.com/terraform/language/files/override).
It's essentially a Terraform-native way of [Monkey Patching](https://en.wikipedia.org/wiki/Monkey_patch).
This way, you can maintain the original component as a dependency and only override the parts you need to change.

:::warning Pitfall!
Use this technique cautiously because your overrides may break if the upstream interfaces change. There’s no contract that an upstream component will remain the same.
:::

To gain a deeper understanding of how this works, you have to understand how [Terraform overrides work](https://developer.hashicorp.com/terraform/language/files/override), and then it will make sense how [vendoring with Atmos](/core-concepts/vendor) can be used to extend components.

<details>
<summary>Comparison to Other Languages or Frameworks</summary>

#### Swizzling

In [Objective-C](https://spin.atomicobject.com/method-swizzling-objective-c/) and [Swift-UI](https://medium.com/@pallavidipke07/method-swizzling-in-swift-5c9d9ab008e4), swizzling is the method of changing the implementation of an existing selector.

In Docusaurus, [swizzling a component](https://docusaurus.io/docs/swizzling) means providing an alternative implementation that takes precedence over the component provided by the theme.

#### Monkey Patching

You can think of it also like [Monkey Patching](https://en.wikipedia.org/wiki/Monkey_patch) in [Ruby](http://blog.headius.com/2012/11/refining-ruby.html) or [React components](https://medium.com/@singhalaryan06/monkey-patching-mocking-hooks-and-methods-in-react-f6afef73e423), enabling you to override the default implementation. Gatsby has a similar concept called theme [shadowing](https://www.gatsbyjs.com/docs/how-to/plugins-and-themes/shadowing/).
</details>

---

## CLI Commands Cheat Sheet

import Link from '@docusaurus/Link'
import Card from '@site/src/components/Card'
import CardGroup from '@site/src/components/CardGroup'

<CardGroup title="General" className="cheatsheet">

  <Card url="/cli/commands/help">
    ```
    atmos
    ```
    Start an interactive UI to select an Atmos command, component and stack. Press "Enter" to execute the command.
  </Card>

  <Card url="/cli/commands/help">
    ```
    atmos help
    ```
    Show help for all Atmos CLI commands
  </Card>

  <Card url="/cli/commands/docs">
    ```
    atmos docs
    ```
   Open the Atmos documentation in a web browser
  </Card>

  <Card url="/cli/commands/version">
    ```
    atmos version
    ```
    Get the Atmos CLI version
  </Card>

  <Card url="/cli/commands/completion">
    ```
    atmos completion
    ```
    Generate completion scripts for `Bash`, `Zsh`, `Fish` and `PowerShell`
  </Card>
</CardGroup>

<CardGroup title="Describe Commands" className="cheatsheet">
  <Card url="/cli/commands/describe/affected">
    ```
    atmos describe affected
    ```
    Generate a list of the affected Atmos components and stacks given two Git commits
  </Card>

  <Card url="/cli/commands/describe/component">
    ```
    atmos describe component
    ```
    Describe the complete configuration for an Atmos component in an Atmos stack
  </Card>

  <Card url="/cli/commands/describe/config">
    ```
    atmos describe config
    ```
    Show the final (deep-merged) CLI configuration of all `atmos.yaml` file(s)
  </Card>

  <Card url="/cli/commands/describe/dependents">
    ```
    atmos describe dependents
    ```
    Show a list of Atmos components in Atmos stacks that depend on the provided Atmos component
  </Card>

  <Card url="/cli/commands/describe/stacks">
    ```
    atmos describe stacks
    ```
    Show the fully deep-merged configuration for all Atmos stacks and the components in the stacks
  </Card>

  <Card url="/cli/commands/describe/workflows">
    ```
    atmos describe workflows
    ```
    Show the configured Atmos workflows
  </Card>
</CardGroup>

<CardGroup title="Terraform Commands" className="cheatsheet">
  <Card url="/cli/commands/terraform/usage">
    ```
    atmos terraform
    ```
    Execute `terraform` commands
  </Card>

  <Card url="/cli/commands/terraform/clean">
    ```
    atmos terraform clean
    ```
    Delete the `.terraform` folder, the folder that `TF_DATA_DIR` ENV var points to, `.terraform.lock.hcl` file, `varfile` and `planfile` for a component in a stack
  </Card>

  <Card url="/cli/commands/terraform/deploy">
    ```
    atmos terraform deploy
    ```
    Execute `terraform apply -auto-approve` on an Atmos component in an Atmos stack
  </Card>

  <Card url="/cli/commands/terraform/generate-backend">
    ```
    atmos terraform generate backend
    ```
    Generate a Terraform backend config file for an Atmos terraform component in an Atmos stack
  </Card>

  <Card url="/cli/commands/terraform/generate-backends">
    ```
    atmos terraform generate backends
    ```
    Generate the Terraform backend config files for all Atmos terraform components in all stacks
  </Card>

  <Card url="/cli/commands/terraform/generate-varfile">
    ```
    atmos terraform generate varfile
    ```
    Generate a varfile (`.tfvar` ) for an Atmos terraform component in an Atmos stack
  </Card>

  <Card url="/cli/commands/terraform/generate-varfiles">
    ```
    atmos terraform generate varfiles
    ```
    Generate the terraform varfiles (`.tfvar`) for all Atmos terraform components in all stacks
  </Card>

  <Card url="/cli/commands/terraform/shell">
    ```
    atmos terraform shell
    ```
    Start a new `SHELL` configured with the environment for an Atmos component in a stack to allow executing all native terraform commands inside the shell without using any atmos-specific arguments and flags
  </Card>

  <Card url="/cli/commands/terraform/workspace">
    ```
    atmos terraform workspace
    ```
    Calculate the Terraform workspace for an Atmos component (from the context variables and stack config), then run `terraform init -reconfigure`, then select the workspace by executing the `terraform workspace select` command
  </Card>
</CardGroup>

<CardGroup title="Helmfile Commands" className="cheatsheet">
  <Card url="/cli/commands/helmfile/usage">
    ```
    atmos helmfile
    ```
    Execute `helmfile` commands
  </Card>

  <Card url="/cli/commands/helmfile/generate-varfile">
    ```
    atmos helmfile generate varfile
    ```
    Generate a varfile for a helmfile component in an Atmos stack
  </Card>
</CardGroup>

<CardGroup title="Validate Commands" className="cheatsheet">
  <Card url="/cli/commands/validate/component">
    ```
    atmos validate component
    ```
    Validate an Atmos component in a stack using JSON Schema and OPA policies
  </Card>

  <Card url="/cli/commands/validate/stacks">
    ```
    atmos validate stacks
    ```
    Validate all Atmos stack configurations
  </Card>
</CardGroup>

<CardGroup title="Core Commands" className="cheatsheet">
  <Card url="/cli/commands/vendor/pull">
    ```
    atmos vendor pull
    ```
    Pull sources and mixins from remote repositories for Terraform and Helmfile components and other artifacts
  </Card>

  <Card url="/cli/commands/workflow">
    ```
    atmos workflow
    ```
    Perform sequential execution of `atmos` and `shell` commands defined as workflow steps
  </Card>
</CardGroup>

<CardGroup title="Integrations" className="cheatsheet">
  <Card url="/cli/commands/aws/eks-update-kubeconfig">
    ```
    atmos aws eks update-kubeconfig
    ```
    Download `kubeconfig` from an EKS cluster and save it to a file
  </Card>

  <Card url="/cli/commands/atlantis/generate-repo-config">
    ```
    atmos atlantis generate repo-config
    ```
    Generates repository configuration for Atlantis
  </Card>
</CardGroup>

---

## Atmos Cheatsheet

import Link from '@docusaurus/Link'
import Card from '@site/src/components/Card'
import CardGroup from '@site/src/components/CardGroup'

<CardGroup title="Stacks" className="cheatsheet">
    <Card title="List Stacks">
    ```shell
    atmos list stacks
    ```
    </Card>
    <Card title="Folder Structure">
    ```
    ├── atmos.yaml
    ├── components
    │   └── myapp
    │       ├── main.tf
    │       ├── outputs.tf
    │       └── variables.tf
    └── stacks
        ├── catalog
        │   └── myapp.yaml
        └── deploy
            ├── dev.yaml
            ├── prod.yaml
            └── staging.yaml
    ```
    </Card>
    <Card title="Stack Schema">
    ```
    import:
    - catalog/something
    vars:
      key: value
    components:
      terraform:
        $component:
          vars:
            foo: "bar"
    ```
    </Card>
    <Card title="Stack Imports Schema">
    ```
    import:
    - catalog/something
    - path: "catalog/something/else"
      context:
        key: value
      skip_templates_processing: false
      ignore_missing_template_values: false
      skip_if_missing: false
    ```
    </Card>
    <Card title="Validate Stacks">
    ```shell
    atmos validate stacks
    ```
    </Card>
</CardGroup>

<CardGroup title="Components" className="cheatsheet">
    <Card title="List Components">
    ```shell
    atmos list components
    ```
    </Card>
    <Card title="Validate Components">
    ```shell
    atmos validate component $component -s $stack
    atmos validate component $component -s $stack --schema-type jsonschema --schema-path $component.json
    atmos validate component $component -s $stack --schema-type opa --schema-path $component.rego
    atmos validate component $component -s $stack --schema-type opa --schema-path $component.rego  --module-paths catalog
    atmos validate component $component -s $stack --timeout 15
    ```
    </Card>
</CardGroup>

<CardGroup title="Workflows" className="cheatsheet">
    <Card title="List Workflows">
    ```shell
    atmos list workflows
    ```
    </Card>
</CardGroup>

<CardGroup title="Terraform" className="cheatsheet">
    <Card title="Plan Root Modules">
    ```shell
    atmos terraform plan
    ```
    </Card>
    <Card title="Apply Root Modules">
    ```shell
    atmos terraform apply $component --stack $stack
    atmos terraform apply $component --stack $stack -auto-approve
    atmos terraform apply $component --stack $stack $planfile
    ```
    </Card>
    <Card title="Deploy Root Modules">
    ```shell
    atmos terraform apply
    atmos terraform apply $component --stack $stack -out $planfile
    atmos terraform apply $component --stack $stack -var "key=value"
    ```
    </Card>
</CardGroup>

<CardGroup title="Describe" className="cheatsheet">
    <Card title="Describe Affected">
    ```shell
    atmos describe affected
    atmos describe affected --verbose=true
    atmos describe affected --ref refs/heads/main
    atmos describe affected --ref refs/heads/my-new-branch --verbose=true
    atmos describe affected --ref refs/heads/main --format json
    atmos describe affected --ref refs/tags/v1.16.0 --file affected.yaml --format yaml
    atmos describe affected --sha 3a5eafeab90426bd82bf5899896b28cc0bab3073 --file affected.json
    atmos describe affected --sha 3a5eafeab90426bd82bf5899896b28cc0bab3073
    atmos describe affected --ssh-key <path_to_ssh_key>
    atmos describe affected --ssh-key <path_to_ssh_key> --ssh-key-password <password>
    atmos describe affected --repo-path <path_to_already_cloned_repo>
    atmos describe affected --include-spacelift-admin-stacks=true
    ```
    </Card>
</CardGroup>

---

## Components Cheatsheet

import Link from '@docusaurus/Link'
import Card from '@site/src/components/Card'
import CardGroup from '@site/src/components/CardGroup'

<CardGroup title="Configuration" className="cheatsheet">
  <Card title="Folder Structure">
  ```
  ├── atmos.yaml
  ├── components
  │   └── myapp
  │       ├── main.tf
  │       ├── outputs.tf
  │       └── variables.tf
  └── stacks
      ├── catalog
      │   └── myapp.yaml
      └── deploy
          ├── dev.yaml
          ├── prod.yaml
          └── staging.yaml
  ```
  </Card>
</CardGroup>

<CardGroup title="Commands" className="cheatsheet">
    <Card title="List Components">
    ```shell
    atmos list components
    ```
    </Card>
    <Card title="Validate Components">
    ```shell
    atmos validate component $component -s $stack
    atmos validate component $component -s $stack --schema-type jsonschema --schema-path $component.json
    atmos validate component $component -s $stack --schema-type opa --schema-path $component.rego
    atmos validate component $component -s $stack --schema-type opa --schema-path $component.rego  --module-paths catalog
    atmos validate component $component -s $stack --timeout 15
    ```
    </Card>
</CardGroup>

<CardGroup title="Terraform" className="cheatsheet">
    <Card title="Plan Root Modules">
    ```shell
    atmos terraform plan $component --stack $stack
    atmos terraform plan $component --stack $stack -out $planfile
    ```
    </Card>
    <Card title="Apply Root Modules">
    ```shell
    atmos terraform apply $component --stack $stack
    atmos terraform apply $component --stack $stack -auto-approve
    atmos terraform apply $component --stack $stack $planfile
    ```
    </Card>
    <Card title="Deploy Root Modules">
    ```shell
    atmos terraform deploy
    atmos terraform deploy $component --stack $stack -out $planfile
    atmos terraform deploy $component --stack $stack -var "key=value"
    ```
    </Card>
</CardGroup>

---

## Stacks Cheatsheet

import Link from '@docusaurus/Link'
import Card from '@site/src/components/Card'
import CardGroup from '@site/src/components/CardGroup'

<CardGroup title="Configuration" className="cheatsheet">
  <Card title="Folder Structure">
  ```
  ├── atmos.yaml
  ├── components
  │   └── myapp
  │       ├── main.tf
  │       ├── outputs.tf
  │       └── variables.tf
  └── stacks
      ├── catalog
      │   └── myapp.yaml
      └── deploy
          ├── dev.yaml
          ├── prod.yaml
          └── staging.yaml
  ```
  </Card>
  <Card title="Stack Schema">
  ```yaml
  import:
  - catalog/something
  vars:
    key: value
  components:
    terraform:
      $component:
        vars:
          foo: "bar"
  ```
  </Card>

  <Card title="Stack Overrides">
    ```yaml
    terraform:
      overrides:
        env: {}
        settings: {}
        vars: {}
        command: "opentofu"
    ```
  </Card>
  <Card title="Spacelift Settings">
    ```yaml
    terraform:
      components:
        $component:
          settings:
            spacelift:
              # The `autodeploy` setting was overridden with the value
              # from `terraform.overrides.settings.spacelift.autodeploy`
              autodeploy: true
              workspace_enabled: true
    ```
  </Card>
</CardGroup>

<CardGroup title="Commands" className="cheatsheet">
    <Card title="List Components">
    ```shell
    atmos list components
    ```
    </Card>
    <Card title="Validate Components">
    ```shell
    atmos validate component $component -s $stack
    atmos validate component $component -s $stack --schema-type jsonschema --schema-path $component.json
    atmos validate component $component -s $stack --schema-type opa --schema-path $component.rego
    atmos validate component $component -s $stack --schema-type opa --schema-path $component.rego  --module-paths catalog
    atmos validate component $component -s $stack --timeout 15
    ```
    </Card>
</CardGroup>

---

## Vendoring Cheatsheet

import Card from '@site/src/components/Card'
import CardGroup from '@site/src/components/CardGroup'

<CardGroup title="Configuration" className="cheatsheet">
  <Card title="Filesystem Layout">
  ```
  ├── atmos.yaml
  ├── vendor.yaml
  └── components
      └── myapp
         ├── main.tf
         ├── outputs.tf
         └── variables.tf
  ```
  </Card>

  <Card title="Vendor Schema">
  ```yaml title="vendor.yaml"
  apiVersion: atmos/v1
  kind: AtmosVendorConfig
  metadata:
    name: example-vendor-config
    description: Atmos vendoring manifest
  spec:
    imports:
      - "vendor/something"
    sources:
      - component: "vpc"
        source: "oci://public.ecr.aws/cloudposse/components/terraform/stable/aws/vpc:{{.Version}}"
        version: "latest"
        targets: ["components/terraform/infra/vpc/{{.Version}}"]
        included_paths: ["**/*.tf"]
        tags:
          - test
          - networking
    ```
  </Card>

  <Card title="Component Schema">
    ```yaml title="components/$component/component.yaml"
    apiVersion: atmos/v1
    kind: ComponentVendorConfig
    metadata:
      name: vpc-flow-logs-bucket-vendor-config
      description: Source and mixins config for vendoring of 'vpc-flow-logs-bucket' component
    spec:
      source:
        uri: github.com/cloudposse/terraform-aws-components.git//modules/vpc-flow-logs-bucket?ref={{.Version}}
        version: 1.398.0
        included_paths: ["**/*.tf"]
        excluded_paths: ["**/context.tf"]
      mixins:
        - uri: https://raw.githubusercontent.com/cloudposse/terraform-null-label/0.25.0/exports/context.tf
          filename: context.tf
      ```
  </Card>
</CardGroup>

<CardGroup title="Commands" className="cheatsheet">
    <Card title="Vendor Pull">
    ```shell
    atmos vendor pull
    atmos vendor pull --everything
    atmos vendor pull --component vpc-mixin-1
    atmos vendor pull -c vpc-mixin-2
    atmos vendor pull -c vpc-mixin-3
    atmos vendor pull -c vpc-mixin-4
    atmos vendor pull --tags test
    atmos vendor pull --tags networking,storage
    ```
    </Card>
</CardGroup>

---

## Atmos CLI

import Screengrab from '@site/src/components/Screengrab'
import Terminal from '@site/src/components/Terminal'
import Intro from '@site/src/components/Intro'

<Intro>
Use this command to start an interactive UI to run Atmos commands against any component or stack. Press `Enter` to execute the command for the selected
stack and component
</Intro>

## Usage

Just run the `atmos` command in your terminal to start the interactive UI. Use the arrow keys to select stacks and components to deploy.

```shell
atmos
```

- Use the `right/left` arrow keys to navigate between the "Commands", "Stacks" and "Components" views

- Use the `up/down` arrow keys (or the mouse wheel) to select a command to execute, component and stack

- Use the `/` key to filter/search for the commands, components, and stacks in the corresponding views

- Use the `Tab` key to flip the "Stacks" and "Components" views. This is useful to be able to use the UI in two different modes:

  * `Mode 1: Components in Stacks`. Display all available stacks, select a stack, then show all the components that are defined in the selected stack

  * `Mode 2: Stacks for Components`. Display all available components, select a component, then show all the stacks where the selected component is
    configured

- Press `Enter` to execute the selected command for the selected stack and component

## Screenshots

To get an idea of what it looks like using `atmos` on the command line, just [try our quickstart](/quick-start/) and run the [`atmos`](/cli) command to start
an interactive UI in the terminal. Use the arrow keys to select stacks and components to deploy.

![Atmos Screenshot](../../../docs/demo.gif)

### Components in Stacks (Mode 1)

In Atmos, you can easily search and navigate your configuration from the built-in UI.

<Terminal title="atmos (interactive)">
![`atmos` CLI command mode 1](/img/cli/atmos/atmos-cli-command-1.png)
</Terminal>

### Stacks for Components (Mode 2)

You can also search for the stacks where a component is configured.

<Terminal title="atmos (interactive)">
![`atmos` CLI command mode 2](/img/cli/atmos/atmos-cli-command-2.png)
</Terminal>

---

## atmos atlantis generate repo-config

import Screengrab from '@site/src/components/Screengrab'
import Intro from '@site/src/components/Intro'

<Intro>
Use this command to generate a repository configuration (`atlantis.yaml`) for Atlantis.
</Intro>

<Screengrab title="atmos atlantis generate repo-config --help" slug="atmos-atlantis-generate-repo-config--help"/>

```shell
atmos atlantis generate repo-config [options]
```

:::tip
Run `atmos atlantis generate repo-config --help` to see all the available options
:::

## Examples

```shell
atmos atlantis generate repo-config

atmos atlantis generate repo-config --output-path /dev/stdout

atmos atlantis generate repo-config --config-template config-1 --project-template project-1

atmos atlantis generate repo-config --config-template config-1 --project-template project-1 --stacks <stack1, stack2>

atmos atlantis generate repo-config --config-template config-1 --project-template project-1 --components <component1, component2>

atmos atlantis generate repo-config --config-template config-1 --project-template project-1 --stacks <stack1> --components <component1, component2>

atmos atlantis generate repo-config --affected-only=true

atmos atlantis generate repo-config --affected-only=true --output-path /dev/stdout

atmos atlantis generate repo-config --affected-only=true --verbose=true

atmos atlantis generate repo-config --affected-only=true --output-path /dev/stdout --verbose=true

atmos atlantis generate repo-config --affected-only=true --repo-path <path_to_cloned_target_repo>

atmos atlantis generate repo-config --affected-only=true --ref refs/heads/main

atmos atlantis generate repo-config --affected-only=true --ref refs/tags/v1.1.0

atmos atlantis generate repo-config --affected-only=true --sha 3a5eafeab90426bd82bf5899896b28cc0bab3073

atmos atlantis generate repo-config --affected-only=true --ref refs/tags/v1.2.0 --sha 3a5eafeab90426bd82bf5899896b28cc0bab3073

atmos atlantis generate repo-config --affected-only=true --ssh-key <path_to_ssh_key>

atmos atlantis generate repo-config --affected-only=true --ssh-key <path_to_ssh_key> --ssh-key-password <password>

atmos atlantis generate repo-config --affected-only=true --clone-target-ref=true
```

## Flags

<dl>
  <dt>`--config-template` (optional)</dt>
  <dd>Atlantis config template name.</dd>

  <dt>`--project-template` (optional)</dt>
  <dd>Atlantis project template name.</dd>

  <dt>`--output-path` (optional)</dt>
  <dd>Output path to write `atlantis.yaml` file.</dd>

  <dt>`--stacks` (optional)</dt>
  <dd>Generate Atlantis projects for the specified stacks only (comma-separated values).</dd>

  <dt>`--components` (optional)</dt>
  <dd>Generate Atlantis projects for the specified components only (comma-separated values).</dd>

  <dt>`--affected-only` (optional)</dt>
  <dd>Generate Atlantis projects only for the Atmos components changedbetween two Git commits.</dd>

  <dt>`--ref` (optional)</dt>
  <dd>[Git Reference](https://git-scm.com/book/en/v2/Git-Internals-Git-References) with which to compare the current working branch.</dd>

  <dt>`--sha` (optional)</dt>
  <dd>Git commit SHA with which to compare the current working branch.</dd>

  <dt>`--ssh-key` (optional)</dt>
  <dd>Path to PEM-encoded private key to clone private repos using SSH.</dd>

  <dt>`--ssh-key-password` (optional)</dt>
  <dd>Encryption password for the PEM-encoded private key if the key containsa password-encrypted PEM block.</dd>

  <dt>`--repo-path` (optional)</dt>
  <dd>Path to the already cloned target repository with which to compare the current branch.Conflicts with `--ref`, `--sha`, `--ssh-key` and `--ssh-key-password`.</dd>

  <dt>`--verbose` (optional)</dt>
  <dd>Print more detailed output when cloning and checking out the targetGit repository and processing the result.</dd>

  <dt>`--clone-target-ref` (optional)</dt>
  <dd>Clone the target reference with which to compare the current branch.`atmos atlantis generate repo-config --affected-only=true --clone-target-ref=true`The flag is only used when `--affected-only=true`If set to `false` (default), the target reference will be checked out insteadThis requires that the target reference is already cloned by Git,and the information about it exists in the `.git` directory.</dd>
</dl>

:::info

Refer to [Atlantis Integration](/integrations/atlantis) for more details on the Atlantis integration in Atmos

:::

---

## atmos atlantis

import DocCardList from '@theme/DocCardList';
import Screengrab from '@site/src/components/Screengrab'
import Intro from '@site/src/components/Intro'

<Intro>
Use these subcommands to execute commands that generate Atlantis configurations.
</Intro>

## Usage

<Screengrab title="atmos atlantis --help" slug="atmos-atlantis--help" />

## Subcommands

<DocCardList/>

---

## atmos aws eks update-kubeconfig

import Screengrab from '@site/src/components/Screengrab'
import Intro from '@site/src/components/Intro'

<Intro>
Use this command to download `kubeconfig` from an EKS cluster and save it to a file.
</Intro>

<Screengrab title="atmos aws eks-update-kubeconfig --help" slug="atmos-aws-eks-update-kubeconfig--help" />

```shell
atmos aws eks update-kubeconfig [options]
```

This command executes `aws eks update-kubeconfig` command to download `kubeconfig` from an EKS cluster and saves it to a file.

The command can execute `aws eks update-kubeconfig` in three different ways:

1. If all the required parameters (cluster name and AWS profile/role) are provided on the command-line, then Atmos executes the command without
   requiring the `atmos.yaml` CLI config and context.

For example:

  ```shell
  atmos aws eks update-kubeconfig --profile=<profile> --name=<cluster_name>
  ```

1. If `component` and `stack` are provided on the command-line, then Atmos executes the command using the `atmos.yaml` CLI config and stack's context
   by searching for the following settings:

- `components.helmfile.cluster_name_pattern` in the `atmos.yaml` CLI config (and calculates the `--name` parameter using the pattern)
- `components.helmfile.helm_aws_profile_pattern` in the `atmos.yaml` CLI config (and calculates the `--profile` parameter using the pattern)
- `components.helmfile.kubeconfig_path` in the `atmos.yaml` CLI config the variables for the component in the provided stack
- `region` from the variables for the component in the stack

For example:

  ```shell
  atmos aws eks update-kubeconfig <component> -s <stack>
  ```

1. Combination of the above. Provide a component and a stack, and override other parameters on the command line.

For example:

  ```shell
  atmos aws eks update-kubeconfig <component> -s <stack> --kubeconfig=<path_to_kubeconfig> --region=us-east-1
  ```

:::info
Refer to [Update kubeconfig](https://docs.aws.amazon.com/cli/latest/reference/eks/update-kubeconfig.html) for more information
:::

:::tip
Run `atmos aws eks update-kubeconfig --help` to see all the available options
:::

## Examples

```shell
atmos aws eks update-kubeconfig <component> -s <stack>
atmos aws eks update-kubeconfig --profile=<profile> --name=<cluster_name>
atmos aws eks update-kubeconfig <component> -s <stack> --kubeconfig=<path_to_kubeconfig> --region=<region>
atmos aws eks update-kubeconfig --role-arn <ARN>
atmos aws eks update-kubeconfig --alias <cluster context name alias>
atmos aws eks update-kubeconfig --dry-run=true
atmos aws eks update-kubeconfig --verbose=true
```

## Arguments

<dl>
  <dt>`component` (optional)</dt>
  <dd>Atmos component.</dd>
</dl>

## Flags

<dl>
  <dt>`--stack` / `-s` (optional)</dt>
  <dd>Atmos stack.</dd>

  <dt>`--profile` (optional)</dt>
  <dd>AWS profile to use to authenticate to the EKS cluster.</dd>

  <dt>`--role-arn` (optional)</dt>
  <dd>AWS IAM role ARN to use to authenticate to the EKS cluster.</dd>

  <dt>`--name` (optional)</dt>
  <dd>EKS cluster name.</dd>

  <dt>`--region` (optional)</dt>
  <dd>AWS region.</dd>

  <dt>`--kubeconfig` (optional)</dt>
  <dd>`kubeconfig` filename to append with the configuration.</dd>

  <dt>`--alias` (optional)</dt>
  <dd>Alias for the cluster context name. Defaults to match cluster ARN.</dd>

  <dt>`--dry-run` (optional)</dt>
  <dd>Print the merged kubeconfig to stdout instead of writing it to the specified file.</dd>

  <dt>`--verbose` (optional)</dt>
  <dd>Print more detailed output when writing the kubeconfig file, including the appended entries.</dd>
</dl>

---

## atmos aws

import Screengrab from '@site/src/components/Screengrab'
import DocCardList from '@theme/DocCardList';
import Intro from '@site/src/components/Intro'

<Screengrab title="atmos aws --help" slug="atmos-aws--help" />

## Subcommands

<Intro>
Use these subcommands to interact with AWS.
</Intro>

<DocCardList/>

---

## Atmos CLI Commands

import Screengrab from '@site/src/components/Screengrab'
import DocCardList from '@theme/DocCardList'
import Intro from '@site/src/components/Intro'

<Intro>
Use these commands to perform operations.
</Intro>

<Screengrab title="atmos --help" slug="atmos--help" />

# Commands

<DocCardList/>

---

## atmos completion

import Screengrab from '@site/src/components/Screengrab'
import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';
import Intro from '@site/src/components/Intro'

<Intro>
Use this command to generate completion scripts for `Bash`, `Zsh`, `Fish` and `PowerShell`.
</Intro>

<Screengrab title="atmos completion --help" slug="atmos-completion--help" />

## Usage

Execute the `completion` command like this:

```shell
atmos completion [bash|zsh|fish|powershell]
```

This command generates completion scripts for `Bash`, `Zsh`, `Fish` and `powershell`.

When the generated completion script is loaded into the shell, pressing the tab key twice displays the available commands and the help.

:::tip
Run `atmos completion --help` to see all the available options
:::

## Configuring Your Shell

To enable command completion, you need to configure your shell. The setup process depends on which shell you’re using (e.g., `zsh` or `bash`).

Select your shell below for detailed setup instructions.

<Tabs>
  <TabItem value="bash" label="Bash" default>
## Bash Completion Setup

To enable tab completion for Atmos in Bash, add the following to your `~/.bashrc` or `~/.bash_profile`:

```bash
# Enable Atmos CLI completion
source <(atmos completion bash)
```
After saving the file, apply the changes by running:

```zsh
source ~/.bashrc
```

Now, you can run any `atmos` command, and pressing `<Tab>` after typing `atmos` will show the available subcommands. The same applies to `--stack` arguments and commands requiring a component (e.g., `atmos terraform plan`).

</TabItem>
<TabItem value="zsh" name="Zsh">

## Zsh Completion Setup

To enable tab completion for Atmos in `Zsh`, add the following to your `~/.zshrc`:

```zsh
# Initialize Zsh completion system
autoload -Uz compinit && compinit

# Enable Atmos CLI completion
source <(atmos completion zsh)

# Improve completion behavior
zstyle ':completion:*' menu select      # Enable menu selection
zstyle ':completion:*' force-list always # Force vertical menu listing

# Ensure the Tab key triggers autocompletion
bindkey '\t' expand-or-complete
```

After saving the file, apply the changes by running:

```zsh
source ~/.zshrc
```

Now, you can run any `atmos` command, and pressing `<Tab>` after typing `atmos` will show the available subcommands. The same applies to `--stack` arguments and commands requiring a component (e.g., `atmos terraform plan`).

If completions do not work, try regenerating the completion cache:

```zsh
rm -f ~/.zcompdump && compinit
```
</TabItem>
</Tabs>

:::warning
The Atmos completion script statically completes [custom commands](/core-concepts/custom-commands) based on the Atmos configuration. If completions are generated without this configuration (e.g., outside a project directory), custom commands won’t be included. To ensure accuracy, generate or regenerate the script from the correct working directory. This only affects custom commands. Components, stacks, and built-in commands remain fully dynamic.
:::

### Examples

```shell
atmos completion bash
atmos completion zsh
atmos completion fish
atmos completion powershell
```

You can generate and load the shell completion script for `Bash` by executing the following commands:

```shell
atmos completion bash > /tmp/completion
source /tmp/completion
```

or

```shell
source <(atmos completion bash)
```

## Arguments

<dl>
  <dt>`shell_name` (required)</dt>
  <dd>Shell name. Valid values are `bash`, `zsh`, `fish` and `powershell`.</dd>
</dl>

:::info
Refer to [Command-line completion](https://en.wikipedia.org/wiki/Command-line_completion) for more details
:::

---

## atmos describe affected

import Terminal from '@site/src/components/Terminal'
import File from '@site/src/components/File'
import Screengrab from '@site/src/components/Screengrab'
import Intro from '@site/src/components/Intro'

<Intro>
Use this command to show a list of the affected Atmos components and stacks given two Git commits.
</Intro>

<Screengrab title="atmos describe affected --help" slug="atmos-describe-affected--help" />

## Description

The command uses two different Git commits to produce a list of affected Atmos components and stacks.

For the first commit, the command assumes that the current repo root is a Git checkout. An error will be thrown if the
current repo is not a Git repository (the `.git/` folder does not exist or is configured incorrectly).

The second commit can be specified on the command line by using
the `--ref` ([Git References](https://git-scm.com/book/en/v2/Git-Internals-Git-References)) or `--sha` (commit SHA) flags.
The `--sha` takes precedence over the `--ref` flag.

:::tip
If the flags are not provided, the `ref` will be set automatically to the reference to the default branch
(`refs/remotes/origin/HEAD` Git ref, usually the `main` branch).
:::

## How does it work?

The command performs the following:

- If the `--repo-path` flag is passed, the command uses it as the path to the already cloned target repo with which to
  compare the current working branch. I this case, the command will not clone and checkout the
  target reference, but instead will use the already cloned one to compare the current branch with. In this case, the
  `--ref`, `--sha`, `--ssh-key` and `--ssh-key-password` flags are not used, and an error will be thrown if the `--repo-path`
  flag and any of the `--ref`, `--sha`, `--ssh-key` or `--ssh-key-password` flags are provided at the same time

- Otherwise, if the `--clone-target-ref=true` flag is specified, the command clones (into a temp directory) the remote
  target with which to compare the current working branch. If the `--ref` flag or the commit SHA flag `--sha` are provided,
  the command uses them to clone and checkout the remote target. Otherwise, the `HEAD` of the remote origin is
  used (`refs/remotes/origin/HEAD` Git ref, usually the `main` branch)

- Otherwise, (if the `--repo-path` and `--clone-target-ref=true` flags are not passed), the command does not clone anything
  from the remote origin, but instead just copies the current repo into a temp directory and checks out the target
  reference with which to compare the current working branch.
  If the `--ref` flag or the commit SHA flag `--sha` are
  provided, the command uses them to check out. Otherwise, the `HEAD` of the remote origin is used
  (`refs/remotes/origin/HEAD` Git ref, usually the `main` branch).
  This requires that the target reference is already cloned by Git, and the information about it exists in
  the `.git` directory (in case of using a non-default branch as the target, Git deep clone needs to be executed instead
  of a shallow clone).
  This is the recommended way to execute the `atmos describe affected` command since it allows
  [working with private repositories](#working-with-private-repositories) without providing the SSH credentials
  (`--ssh-key` and `--ssh-key-password` flags), since in this case Atmos does not access the remote origin and instead
  just checks out the target reference (which is already on the local file system)

- The command deep-merges all stack configurations from both sources: the current working branch and the target reference

- The command searches for changes in the component directories

- The command compares each stack manifest section of the stack configurations from both sources looking for differences

- And finally, the command outputs a JSON or YAML document consisting of a list of the affected components and stacks
  and what caused it to be affected

Since Atmos first checks the component folders for changes, if it finds any affected files, it will mark all related
components and stacks as affected. Atmos will then skip evaluating the stacks for differences since it already
knows that they are affected.
:::tip Use our GitHub Action
Our [affected stacks](/integrations/github-actions/affected-stacks) GitHub Action provides a ready-to-go way to run
`describe affected` and produce a GitHub matrix.
:::

## Usage

```shell
atmos describe affected [options]
```

:::tip
Run `atmos describe affected --help` to see all the available options
:::

## Examples

```shell
atmos describe affected
atmos describe affected --verbose=true
atmos describe affected --ref refs/heads/main
atmos describe affected --ref refs/heads/my-new-branch --verbose=true
atmos describe affected --ref refs/heads/main --format json
atmos describe affected --ref refs/tags/v1.16.0 --file affected.yaml --format yaml
atmos describe affected --sha 3a5eafeab90426bd82bf5899896b28cc0bab3073 --file affected.json
atmos describe affected --sha 3a5eafeab90426bd82bf5899896b28cc0bab3073
atmos describe affected --ssh-key <path_to_ssh_key>
atmos describe affected --ssh-key <path_to_ssh_key> --ssh-key-password <password>
atmos describe affected --repo-path <path_to_already_cloned_repo>
atmos describe affected --include-spacelift-admin-stacks=true
atmos describe affected --clone-target-ref=true
atmos describe affected --include-dependents=true
atmos describe affected --include-settings=true
atmos describe affected --stack=plat-ue2-prod
atmos describe affected --upload=true
atmos describe affected --query <yq-expression>
atmos describe affected --process-templates=false
atmos describe affected --process-functions=false
atmos describe affected --skip=terraform.output
atmos describe affected --skip=terraform.output --skip=include
atmos describe affected --skip=include,eval
atmos describe affected --exclude-locked
```

# Example Output

<Terminal title="atmos describe affected --verbose=true">
```shell
> atmos describe affected --verbose=true

Cloning repo 'https://github.com/cloudposse/atmos' into the temp dir '/var/folders/g5/lbvzy_ld2hx4mgrgyp19bvb00000gn/T/16710736261366892599'

Checking out the HEAD of the default branch ...

Enumerating objects: 4215, done.
Counting objects: 100% (1157/1157), done.
Compressing objects: 100% (576/576), done.
Total 4215 (delta 658), reused 911 (delta 511), pack-reused 3058

Checked out Git ref 'refs/heads/main'

Current HEAD: 7d37c1e890514479fae404d13841a2754be70cbf refs/heads/describe-affected
BASE: 40210e8d365d3d88ac13c0778c0867b679bbba69 refs/heads/main

Changed files:

tests/fixtures/scenarios/complete/components/terraform/infra/vpc/main.tf
internal/exec/describe_affected.go
website/docs/cli/commands/describe/describe-affected.md

Affected components and stacks:

[
   {
      "component": "infra/vpc",
      "component_type": "terraform",
      "component_path": "components/terraform/infra/vpc",
      "stack": "tenant1-ue2-dev",
      "stack_slug": "tenant1-ue2-dev-infra-vpc",
      "spacelift_stack": "tenant1-ue2-dev-infra-vpc",
      "atlantis_project": "tenant1-ue2-dev-infra-vpc",
      "affected": "component"
   },
   {
      "component": "infra/vpc",
      "component_type": "terraform",
      "component_path": "components/terraform/infra/vpc",
      "stack": "tenant1-ue2-prod",
      "stack_slug": "tenant1-ue2-prod-infra-vpc",
      "spacelift_stack": "tenant1-ue2-prod-infra-vpc",
      "atlantis_project": "tenant1-ue2-prod-infra-vpc",
      "affected": "component"
   },
   {
      "component": "infra/vpc",
      "component_type": "terraform",
      "component_path": "components/terraform/infra/vpc",
      "stack": "tenant1-ue2-staging",
      "stack_slug": "tenant1-ue2-staging-infra-vpc",
      "spacelift_stack": "tenant1-ue2-staging-infra-vpc",
      "atlantis_project": "tenant1-ue2-staging-infra-vpc",
      "affected": "component"
   },
     {
    "component": "top-level-component3",
    "component_type": "terraform",
    "component_path": "components/terraform/top-level-component1",
    "stack": "tenant1-ue2-test-1",
    "stack_slug": "tenant1-ue2-test-1-top-level-component3",
    "atlantis_project": "tenant1-ue2-test-1-top-level-component3",
    "affected": "file",
    "file": "tests/fixtures/scenarios/complete/components/terraform/mixins/introspection.mixin.tf"
  },
  {
    "component": "top-level-component3",
    "component_type": "terraform",
    "component_path": "components/terraform/top-level-component1",
    "stack": "tenant1-ue2-test-1",
    "stack_slug": "tenant1-ue2-test-1-top-level-component3",
    "atlantis_project": "tenant1-ue2-test-1-top-level-component3",
    "affected": "folder",
    "folder": "tests/fixtures/scenarios/complete/components/helmfile/infra/infra-server"
  }
]
```
</Terminal>

## Flags

<dl>
    <dt>`--ref` (optional)</dt>
    <dd>
        [Git Reference](https://git-scm.com/book/en/v2/Git-Internals-Git-References) with which to compare the current working branch
    </dd>

    <dt>`--sha` (optional)</dt>
    <dd>
        Git commit SHA with which to compare the current working branch
    </dd>

    <dt>`--file` (optional)</dt>
    <dd>
        If specified, write the result to the file
    </dd>

    <dt>`--format` (optional)</dt>
    <dd>
        Specify the output format: `json` or `yaml` (`json` is default)
    </dd>

    <dt>`--ssh-key` (optional)</dt>
    <dd>
        Path to PEM-encoded private key to clone private repos using SSH
    </dd>

    <dt>`--ssh-key-password` (optional)</dt>
    <dd>
        Encryption password for the PEM-encoded private key if the key contains a password-encrypted PEM block
    </dd>

    <dt>`--repo-path` (optional)</dt>
    <dd>
        Path to the already cloned target repository with which to compare the current branch. Conflicts with `--ref`, `--sha`, `--ssh-key` and `--ssh-key-password`
    </dd>

    <dt>`--verbose` (optional)</dt>
    <dd>
        Print more detailed output when cloning and checking out the target Git repository and processing the result
    </dd>

    <dt>`--include-spacelift-admin-stacks` (optional)</dt>
    <dd>
        Include the Spacelift admin stack of any stack that is affected by config changes
    </dd>

    <dt>`--clone-target-ref` (optional)</dt>
    <dd>
        Clone the target reference with which to compare the current branch.

        `atmos describe affected --clone-target-ref=true`

        If set to `false` (default), the target reference will be checked out instead.
        This requires that the target reference is already cloned by Git, and the information about it exists in the `.git` directory
    </dd>

    <dt>`--stack` (optional)</dt>
    <dd>
        Only show results for the specific stack.

        `atmos describe affected --stack=plat-ue2-prod`
    </dd>

    <dt>`--include-dependents` (optional)</dt>
    <dd>
        Include the dependent components and stacks.

        `atmos describe affected --include-dependents=true`
    </dd>

    <dt>`--include-settings` (optional)</dt>
    <dd>
        Include the `settings` section for each affected component.

        `atmos describe affected --include-settings=true`
    </dd>

    <dt>`--query` (optional)</dt>
    <dd>
        Query the results of the command using YQ expressions.

        `atmos describe affected --query=<yq-expression>`

        For more details, refer to [YQ - a lightweight and portable command-line YAML processor](https://mikefarah.gitbook.io/yq)
    </dd>

    <dt>`--process-templates` (optional)</dt>
    <dd>
        Enable/disable processing of `Go` templates in Atmos stacks manifests when executing the command.
        If the flag is not provided, it's set to `true` by default.

        `atmos describe affected --process-templates=false`
    </dd>

    <dt>`--process-functions` (optional)</dt>
    <dd>
        Enable/disable processing of Atmos YAML functions in Atmos stacks manifests when executing the command.
        If the flag is not provided, it's set to `true` by default.

        `atmos describe affected --process-functions=false`
    </dd>

    <dt>`--skip` (optional)</dt>
    <dd>
        Skip processing a specific Atmos YAML function in Atmos stacks manifests when executing the command.
        To specify more than one function, use multiple `--skip` flags, or separate the functions with a comma:

        `atmos describe affected --skip=terraform.output --skip=include`

        `atmos describe affected --skip=terraform.output,include`
    </dd>

    <dt>`--exclude-locked` (optional)</dt>
    <dd>
        Exclude the locked components (`metadata.locked: true`) from the output.

        Refer to [Locking Components with `metadata.locked`](/core-concepts/stacks/define-components/#locking-components-with-metadatalocked)

        `atmos describe affected --exclude-locked`
    </dd>

    <dt>`--upload` (optional)</dt>
    <dd>
        Upload the affected components and stacks to a specified HTTP endpoint.

        `atmos describe affected --upload=true`

        Atmos will perform an HTTP POST request to the URL `${ATMOS_PRO_BASE_URL}/${ATMOS_PRO_ENDPOINT}`,
        where the base URL is defined by the `ATMOS_PRO_BASE_URL` environment variable,
        and the URL path is defined by the `ATMOS_PRO_ENDPOINT`environment variable
    </dd>
</dl>

## Output

The command outputs a list of objects (in JSON or YAML format).

Each object has the following schema:

```json
{
  "component": "....",
  "component_type": "....",
  "component_path": "....",
  "stack": "....",
  "stack_slug": "....",
  "spacelift_stack": ".....",
  "atlantis_project": ".....",
  "affected": ".....",
  "affected_all": [],
  "file": ".....",
  "folder": ".....",
  "dependents": [],
  "included_in_dependents": "true | false",
  "settings": {}
}
```

where:

<dl>
    <dt>`component`</dt>
    <dd>
        The affected Atmos component.
    </dd>

    <dt>`component_type`</dt>
    <dd>
        The type of the component (`terraform` or `helmfile`).
    </dd>

    <dt>`component_path`</dt>
    <dd>
        The filesystem path to the `terraform` or `helmfile` component.
    </dd>

    <dt>`stack`</dt>
    <dd>
        The affected Atmos stack.
    </dd>

    <dt>`stack_slug`</dt>
    <dd>
        The Atmos stack slug (concatenation of the Atmos stack and Atmos component).
    </dd>

    <dt>`spacelift_stack`</dt>
    <dd>
        The affected Spacelift stack. It will be included only if the Spacelift workspace is enabled for the Atmos component in the
        Atmos stack in the `settings.spacelift.workspace_enabled` section (either directly in the component's `settings.spacelift.workspace_enabled` section
        or via inheritance).
    </dd>

    <dt>`atlantis_project`</dt>
    <dd>
        The affected Atlantis project name. It will be included only if the Atlantis integration is configured in
        the `settings.atlantis` section in the stack config. Refer to [Atlantis Integration](/integrations/atlantis) for more details.
    </dd>

    <dt>`file`</dt>
    <dd>
        If the Atmos component depends on an external file, and the file was changed,
        the `file` attributes shows the modified file.
    </dd>

    <dt>`folder`</dt>
    <dd>
        If the Atmos component depends on an external folder, and any file in the folder was changed,
        the `folder` attributes shows the modified folder.
    </dd>

    <dt>`dependents`</dt>
    <dd>
        A list of components that depend on the current affected component. It will be populated only if the
        command-line flag `--include-dependents=true` is passed (to take dependencies into account) and there are other components
        that depend on the affected component in the stack.
        Refer to [`atmos describe dependents`](/cli/commands/describe/dependents) for more details. The `dependents` property is
        hierarchical - each component in the list will also contain a `dependents` property if that component has dependent
        components as well.
    </dd>

    <dt>`settings`</dt>
    <dd>
        The `settings` section of the component in the stack. It will be included only if the
        command-line flag `--include-settings=true` is passed. The `settings` sections is a free-form map used to pass
        configuration information to [integrations](/integrations).
    </dd>

    <dt>`included_in_dependents`</dt>
    <dd>
        A boolean flag indicating if the affected component in the stack is also present in any of the `dependents`
        properties of the other affected components. It will be included only if the command-line flag `--include-dependents=true`
        is passed. If `included_in_dependents` is set to `true`, it indicates that the affected component in the stack is also
        present in any of the `dependents` lists in the dependency hierarchy of the other affected components.
        This flag can be used to decide whether to plan/apply the affected component - you might skip planning/applying the component
        since it's also a dependency of another affected component and will be triggered in the dependency order of the other
        affected component.
    </dd>

    <dt>`affected`</dt>
    <dd>
        Shows the first (in the processing order) section that was changed. The possible values are:

        <dl>
            <dt>`stack.vars`</dt>
            <dd>
                The `vars` component section in the stack config has been modified.
            </dd>

            <dt>`stack.env`</dt>
            <dd>
                The `env` component section in the stack config has been modified.
            </dd>

            <dt>`stack.settings`</dt>
            <dd>
                The `settings` component section in the stack config has been modified.
            </dd>

            <dt>`stack.metadata`</dt>
            <dd>
                The `metadata` component section in the stack config has been modified.
            </dd>

            <dt>`component`</dt>
            <dd>
                The Terraform or Helmfile component that the Atmos component provisions has been changed.
            </dd>

            <dt>`component.module`</dt>
            <dd>
                The Terraform component is affected because it uses a local Terraform module (not from the Terraform registry, but from the
                local filesystem), and that local module has been changed.

                For example, let's suppose that we have a catalog of reusable Terraform modules in the `modules` folder (outside the `components` folder), and
                we have defined the following `label` Terraform module in `modules/label`:

                ```hcl title="modules/label"
                  module "label" {
                    source  = "cloudposse/label/null"
                    version = "0.25.0"
                    context = module.this.context
                  }

                  output "label" {
                    value       = module.label
                    description = "Label outputs"
                  }
                ```

                We then use the Terraform module in the `components/terraform/top-level-component1` component:

                ```hcl title="components/terraform/top-level-component1"
                  module "service_2_label" {
                    source  = "../../../modules/label"
                    context = module.this.context
                  }

                  output "service_2_id" {
                    value       = module.service_2_label.label.id
                    description = "Service 2 ID"
                  }
                ```

                The `label` module is not in the stack config of the `top-level-component1` component (not in the YAML stack config files), but Atmos
                understands Terraform dependencies (using a Terraform parser from HashiCorp), and can automatically detect any changes to the module.

                For example, if you make changes to any files in the folder `modules/label`, Atmos will detect the module changes, and since the module is a
                Terraform dependency of the `top-level-component1` component, Atmos will mark the component as affected with the `affected` attribute
                set to `component.module`:

                ```json
                  [
                    {
                      "component": "top-level-component1",
                      "component_type": "terraform",
                      "component_path": "tests/fixtures/scenarios/complete/components/terraform/top-level-component1",
                      "stack": "tenant1-ue2-staging",
                      "stack_slug": "tenant1-ue2-staging-top-level-component1",
                      "spacelift_stack": "tenant1-ue2-staging-top-level-component1",
                      "atlantis_project": "tenant1-ue2-staging-top-level-component1",
                      "affected": "component.module",
                      "affected_all": [
                        "component.module"
                      ]
                    },
                    {
                      "component": "top-level-component1",
                      "component_type": "terraform",
                      "component_path": "tests/fixtures/scenarios/complete/components/terraform/top-level-component1",
                      "stack": "tenant2-ue2-staging",
                      "stack_slug": "tenant2-ue2-staging-top-level-component1",
                      "spacelift_stack": "tenant2-ue2-staging-top-level-component1",
                      "atlantis_project": "tenant2-ue2-staging-top-level-component1",
                      "affected": "component.module",
                      "affected_all": [
                        "component.module"
                      ]
                    }
                  ]
                ```
            </dd>

            <dt>`stack.settings.spacelift.admin_stack_selector`</dt>
            <dd>
                The Atmos component for the Spacelift admin stack.

                This will be included only if all of the following is true:

                - The `atmos describe affected` is executed with the `--include-spacelift-admin-stacks=true` flag

                - Any of the affected Atmos components has configured the section `settings.spacelift.admin_stack_selector` pointing to the Spacelift admin
                  stack that manages the components.

                  For example:

                  ```yaml title="stacks/orgs/cp/tenant1/_defaults.yaml"
                  settings:
                    spacelift:
                      # All Spacelift child stacks for the `tenant1` tenant are managed by the
                      # `tenant1-ue2-prod-infrastructure-tenant1` Spacelift admin stack.
                      # The `admin_stack_selector` attribute is used to find the affected Spacelift
                      # admin stack for each affected Atmos stack
                      # when executing the command
                      # `atmos describe affected --include-spacelift-admin-stacks=true`
                      admin_stack_selector:
                        component: infrastructure-tenant1
                        tenant: tenant1
                        environment: ue2
                        stage: prod
                  ```

                - The Spacelift admin stack is enabled by `settings.spacelift.workspace_enabled` set to `true`.

                  For example:

                  ```yaml title="stacks/catalog/terraform/spacelift/infrastructure-tenant1.yaml"
                  components:
                    terraform:
                      infrastructure-tenant1:
                        metadata:
                          component: spacelift
                          inherits:
                            - spacelift-defaults
                        settings:
                          spacelift:
                            workspace_enabled: true
                  ```
            </dd>

            <dt>`file`</dt>
            <dd>
                An external file on the local filesystem that the Atmos component depends on was changed.

                Dependencies on external files (not in the component's folder) are defined using the `file` attribute in the `settings.depends_on` map.

                For example:

                ```yaml title="stacks/catalog/terraform/top-level-component3.yaml"
                components:
                  terraform:
                    top-level-component3:
                      metadata:
                        component: "top-level-component1"
                      settings:
                        depends_on:
                          1:
                            file: "tests/fixtures/scenarios/complete/components/terraform/mixins/introspection.mixin.tf"
                ```

                In the configuration above, we specify that the Atmos component `top-level-component3` depends on the file
                `tests/fixtures/scenarios/complete/components/terraform/mixins/introspection.mixin.tf` (which is not in the component's folder). If the file gets modified,
                the component `top-level-component3` will be included in the `atmos describe affected` command output.

                For example:

                ```json
                  [
                    {
                      "component": "top-level-component3",
                      "component_type": "terraform",
                      "component_path": "components/terraform/top-level-component1",
                      "stack": "tenant1-ue2-test-1",
                      "stack_slug": "tenant1-ue2-test-1-top-level-component3",
                      "atlantis_project": "tenant1-ue2-test-1-top-level-component3",
                      "affected": "file",
                      "affected_all": [
                        "file"
                      ],
                      "file": "tests/fixtures/scenarios/complete/components/terraform/mixins/introspection.mixin.tf"
                    }
                  ]
                ```
            </dd>

            <dt>`folder`</dt>
            <dd>
                Any file in an external folder that the Atmos component depends on was changed.

                Dependencies on external folders are defined using the `folder` attribute in the `settings.depends_on` map.

                For example:

                ```yaml title="stacks/catalog/terraform/top-level-component3.yaml"
                components:
                  terraform:
                    top-level-component3:
                      metadata:
                        component: "top-level-component1"
                      settings:
                        depends_on:
                          1:
                            file: "tests/fixtures/scenarios/complete/components/terraform/mixins/introspection.mixin.tf"
                          2:
                            folder: "tests/fixtures/scenarios/complete/components/helmfile/infra/infra-server"
                ```

                In the configuration above, we specify that the Atmos component `top-level-component3` depends on the folder
                `tests/fixtures/scenarios/complete/components/helmfile/infra/infra-server`. If any file in the folder gets modified,
                the component `top-level-component3` will be included in the `atmos describe affected` command output.

                For example:

                ```json
                  [
                    {
                      "component": "top-level-component3",
                      "component_type": "terraform",
                      "component_path": "components/terraform/top-level-component1",
                      "stack": "tenant1-ue2-test-1",
                      "stack_slug": "tenant1-ue2-test-1-top-level-component3",
                      "atlantis_project": "tenant1-ue2-test-1-top-level-component3",
                      "affected": "folder",
                      "affected_all": [
                        "folder"
                      ],
                      "folder": "tests/fixtures/scenarios/complete/components/helmfile/infra/infra-server"
                    }
                  ]
                ```
            </dd>
        </dl>
    </dd>

    <dt>`affected_all`</dt>
    <dd>
        Shows all component sections and attributes that were changed.

        For example, if you make changes to the `vars` and `settings` sections of the component `component-1` in the
        `nonprod` stack, and execute `atmos describe affected`, you will get the following result:

        ```json
          [
            {
              "component": "component-1",
              "component_type": "terraform",
              "stack": "nonprod",
              "stack_slug": "nonprod-component-1",
              "affected": "stack.vars",
              "affected_all": [
                 "stack.vars",
                 "stack.settings"
              ]
            }
          ]
        ```

        If you create a new Terraform/Tofu component, configure a new Atmos component `component-1` in the
        `nonprod` stack, and execute `atmos describe affected`, you will get the following result:

        ```json
        [
          {
            "component": "component-1",
            "component_type": "terraform",
            "stack": "nonprod",
            "stack_slug": "nonprod-component-1",
            "affected": "stack.metadata",
            "affected_all": [
              "component",
              "stack.metadata",
              "stack.vars",
              "stack.env",
              "stack.settings"
            ]
          }
        ]
        ```

        where:

            <dl>
                <dt>`affected`</dt>
                <dd>
                    Shows that the Atmos component's `metadata` section was changed
                    (since the component is new and the `metadata` section is the first section that Atmos processes).
                </dd>

                <dt>`affected_all`</dt>
                <dd>
                    Shows all the affected sections and attributes:
                    <dl>
                        <dt>`component`</dt>
                        <dd>
                            The Terraform component (Terraform configuration) was affected (since it was just added).
                        </dd>
                        <dt>`stack.metadata`</dt>
                        <dd>
                            The Atmos component's `metadata` section was changed.
                        </dd>
                        <dt>`stack.vars`</dt>
                        <dd>
                            The Atmos component's `vars` section was changed.
                        </dd>
                        <dt>`stack.env`</dt>
                        <dd>
                            The Atmos component's `env` section was changed.
                        </dd>
                        <dt>`stack.settings`</dt>
                        <dd>
                            The Atmos component's `settings` section was changed.
                        </dd>
                    </dl>
                </dd>
            </dl>
    </dd>
</dl>

:::note

[Abstract Atmos components](/design-patterns/abstract-component) (`metadata.type` is set to `abstract`)
are not included in the output since they serve as blueprints for other Atmos components and are not meant to be provisioned.

[Disabled Atmos components](/core-concepts/stacks/define-components/#disabling-components-with-metadataenabled) (`metadata.enabled` is set to `false`)
are also not included in the output since they are explicitly disabled.

:::

## Output Example

<Terminal title="atmos describe affected --include-spacelift-admin-stacks=true">
```json
[
  {
    "component": "infrastructure-tenant1",
    "component_type": "terraform",
    "component_path": "tests/fixtures/scenarios/complete/components/terraform/spacelift",
    "stack": "tenant1-ue2-prod",
    "stack_slug": "tenant1-ue2-prod-infrastructure-tenant1",
    "spacelift_stack": "tenant1-ue2-prod-infrastructure-tenant1",
    "atlantis_project": "tenant1-ue2-prod-infrastructure-tenant1",
    "affected": "stack.settings.spacelift.admin_stack_selector",
    "affected_all": [
        "stack.settings.spacelift.admin_stack_selector"
    ]
  },
  {
    "component": "infrastructure-tenant2",
    "component_type": "terraform",
    "component_path": "tests/fixtures/scenarios/complete/components/terraform/spacelift",
    "stack": "tenant2-ue2-prod",
    "stack_slug": "tenant2-ue2-prod-infrastructure-tenant2",
    "spacelift_stack": "tenant2-ue2-prod-infrastructure-tenant2",
    "atlantis_project": "tenant2-ue2-prod-infrastructure-tenant2",
    "affected": "stack.settings.spacelift.admin_stack_selector",
    "affected_all": [
      "stack.settings.spacelift.admin_stack_selector"
    ]
  },
  {
    "component": "test/test-component-override-2",
    "component_type": "terraform",
    "component_path": "components/terraform/test/test-component",
    "stack": "tenant1-ue2-dev",
    "stack_slug": "tenant1-ue2-dev-test-test-component-override-2",
    "spacelift_stack": "tenant1-ue2-dev-new-component",
    "atlantis_project": "tenant1-ue2-dev-new-component",
    "affected": "stack.vars",
    "affected_all": [
      "stack.vars"
    ]
  },
  {
    "component": "infra/vpc",
    "component_type": "terraform",
    "component_path": "components/terraform/infra/vpc",
    "stack": "tenant2-ue2-staging",
    "stack_slug": "tenant1-ue2-staging-infra-vpc",
    "spacelift_stack": "tenant1-ue2-staging-infra-vpc",
    "atlantis_project": "tenant1-ue2-staging-infra-vpc",
    "affected": "component",
    "affected_all": [
      "component"
    ]
  },
  {
    "component": "test/test-component-override-3",
    "component_type": "terraform",
    "component_path": "components/terraform/test/test-component",
    "stack": "tenant1-ue2-prod",
    "stack_slug": "tenant1-ue2-prod-test-test-component-override-3",
    "atlantis_project": "tenant1-ue2-prod-test-test-component-override-3",
    "affected": "stack.env",
    "affected_all": [
      "stack.env"
    ]
  },
  {
    "component": "top-level-component3",
    "component_type": "terraform",
    "component_path": "components/terraform/top-level-component1",
    "stack": "tenant1-ue2-test-1",
    "stack_slug": "tenant1-ue2-test-1-top-level-component3",
    "atlantis_project": "tenant1-ue2-test-1-top-level-component3",
    "affected": "file",
    "affected_all": [
      "file",
      "folder"
    ]
    "file": "tests/fixtures/scenarios/complete/components/terraform/mixins/introspection.mixin.tf"
  },
  {
    "component": "top-level-component3",
    "component_type": "terraform",
    "component_path": "components/terraform/top-level-component1",
    "stack": "tenant1-ue2-test-1",
    "stack_slug": "tenant1-ue2-test-1-top-level-component3",
    "atlantis_project": "tenant1-ue2-test-1-top-level-component3",
    "affected": "folder",
    "affected_all": [
      "file",
      "folder"
    ]
    "folder": "tests/fixtures/scenarios/complete/components/helmfile/infra/infra-server"
  }
]
```
</Terminal>

## Affected Components with Dependencies

The output of the `atmos describe affected` command can include dependencies for the affected components.

If the command-line flag `--include-dependents=true` is passed to the `atmos describe affected` command, and there are
other components that depend on the affected components in the stack, the command will include a `dependents`
property (list) for each affected component. The `dependents` property is hierarchical - each component in the list will
also contain a `dependents` property if that component has dependent components as well.

For example, suppose that we have the following configuration for the Atmos components `component-1`, `component-2` and
`component-3` in the stack `plat-ue2-dev`:

<File title="stacks/orgs/acme/plat/dev/us-east-2.yaml">
  ```yaml
    components:
      terraform:
        component-1:
          metadata:
            component: "terraform-component-1"
          vars: {}

        component-2:
          metadata:
            component: "terraform-component-2"
          vars: {}
          settings:
            depends_on:
              1:
                component: "component-1"

        component-3:
          metadata:
            component: "terraform-component-3"
          vars: {}
          settings:
            depends_on:
              1:
                component: "component-2"
  ```
</File>

:::tip
For more details on how to configure component dependencies, refer to [`atmos describe dependents`](/cli/commands/describe/dependents)
:::

In the above configuration, `component-3` depends on `component-2`, whereas `component-2` depends on `component-1`.

If all the components are affected (modified) in the current working branch,
the `atmos describe affected --include-dependents=true` command will produce the following result:

<Terminal title="atmos describe affected --include-dependents=true">
 ```json
  [
    {
      "component": "component-1",
      "stack": "plat-ue2-dev",
      "stack_slug": "plat-ue2-dev-component-1",
      "included_in_dependents": false,
      "dependents": [
        {
          "component": "component-2",
          "stack": "plat-ue2-dev",
          "stack_slug": "plat-ue2-dev-component-2",
          "dependents": [
            {
              "component": "component-3",
              "stack": "plat-ue2-dev",
              "stack_slug": "plat-ue2-dev-component-3"
            }
          ]
        }
      ]
    },
    {
      "component": "component-2",
      "stack": "plat-ue2-dev",
      "stack_slug": "plat-ue2-dev-component-2",
      "included_in_dependents": true,
      "dependents": [
        {
          "component": "component-3",
          "stack": "plat-ue2-dev",
          "stack_slug": "plat-ue2-dev-component-3"
        }
      ]
    },
    {
      "component": "component-3",
      "stack": "plat-ue2-dev",
      "stack_slug": "plat-ue2-dev-component-3",
      "included_in_dependents": true
    }
  ]
 ```
</Terminal>

The `component-1` component does not depend on any other component, and therefore it has the `included_in_dependents`
attribute set to `false`. The `component-2` and `component-3` components depend on other components and are included in
the `dependents` property of the other components, and hence the `included_in_dependents` attribute is set to `true`.

When processing the above output, you might decide to not plan/apply the `component-2` and `component-3` components
since they are in the `dependents` property of the `component-1` component. Instead, you might just
trigger `component-1` and then `component-2` and `component-3` in the order of dependencies.

## Working with Private Repositories

There are a few ways to work with private repositories with which the current local branch is compared to detect the changed files and affected Atmos
stacks and components:

- Using the `--ssh-key` flag to specify the filesystem path to a PEM-encoded private key to clone private repos using SSH, and
  the `--ssh-key-password` flag to provide the encryption password for the PEM-encoded private key if the key contains a password-encrypted PEM block

- Execute the `atmos describe affected --repo-path <path_to_cloned_target_repo>` command in a [GitHub Action](https://docs.github.com/en/actions).
  For this to work, clone the remote private repository using the [checkout](https://github.com/actions/checkout) GitHub action. Then use
  the `--repo-path` flag to specify the path to the already cloned target repository with which to compare the current branch

- It should just also work with whatever SSH config/context has been already set up, for example, when
  using [SSH agents](https://www.ssh.com/academy/ssh/agent). In this case, you don't need to use the `--ssh-key`, `--ssh-key-password`
  and `--repo-path` flags to clone private repositories

## Using with GitHub Actions

If the `atmos describe affected` command is executed in a [GitHub Action](https://docs.github.com/en/actions), and you don't want to store or
generate a long-lived SSH private key on the server, you can do the following (__NOTE:__ This is only required if the action is attempting to clone a
private repo which is not itself):

- Create a GitHub
  [Personal Access Token (PAT)](https://docs.github.com/en/authentication/keeping-your-account-and-data-secure/creating-a-personal-access-token)
  with scope permissions to clone private repos

- Add the created PAT as a repository or GitHub organization [secret](https://docs.github.com/en/actions/security-guides/encrypted-secrets)

- In your GitHub action, clone the remote repository using the [checkout](https://github.com/actions/checkout) GitHub action

- Execute `atmos describe affected` command with the `--repo-path` flag set to the cloned repository path using
  the [`GITHUB_WORKSPACE`](https://docs.github.com/en/actions/learn-github-actions/variables) ENV variable (which points to the default working
  directory on the GitHub runner for steps, and the default location of the repository when using the [checkout](https://github.com/actions/checkout)
  action). For example:

    ```shell
    atmos describe affected --repo-path $GITHUB_WORKSPACE
    ```

## Upload the affected components and stacks to an HTTP endpoint

If the `--upload=true` command-line flag is passed, Atmos will upload the affected components and stacks to a
specified HTTP endpoint.

The endpoint can process the affected components and their dependencies in a CI/CD pipeline (e.g. execute
`terraform apply` on all the affected components in the stacks and all the dependencies).

Atmos will perform an HTTP POST request to the URL `${ATMOS_PRO_BASE_URL}/${ATMOS_PRO_ENDPOINT}`, where the base URL
is defined by the `ATMOS_PRO_BASE_URL` environment variable, and the URL path is defined by the `ATMOS_PRO_ENDPOINT`
environment variable.

An [Authorization](https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Authorization) header
`Authorization: Bearer $ATMOS_PRO_TOKEN` will be added to the HTTP request (if the `ATMOS_PRO_TOKEN` environment
variable is set) to provide credentials to authenticate with the server.

:::note
If the `--upload=true` command-line flag is passed, the `--include-dependencies` and `--include-settings` flags are
automatically set to `true`, so the affected components will be uploaded with their dependencies and settings
(if they are configured in Atmos stack manifests).
:::

The payload of the HTTP POST request will be a JSON object with the following schema:

<Terminal title="Payload of 'atmos describe affected --upload=true' command">
  ```json
    {
       "base_sha": "6746ba4df9e87690c33297fe740011e5ccefc1f9",
       "head_sha": "5360d911d9bac669095eee1ca1888c3ef5291084",
       "repo_url": "https://github.com/cloudposse/atmos",
       "repo_host": "github.com",
       "repo_name": "atmos",
       "repo_owner": "cloudposse",
       "stacks": [
          {
            "component": "vpc",
            "component_type": "terraform",
            "component_path": "examples/quick-start-advanced/components/terraform/vpc",
            "stack": "plat-ue2-dev",
            "stack_slug": "plat-ue2-dev-vpc",
            "affected": "stack.vars",
            "included_in_dependents": false,
            "dependents": [],
            "settings": {}
          }
      ]
   }
  ```
</Terminal>

where:

<dl>
    <dt>`base_sha`</dt>
    <dd>
        the Git commit SHA of the base branch against which the changes in the current commit are compared
    </dd>

    <dt>`head_sha`</dt>
    <dd>
        the SHA of the current Git commit
    </dd>

    <dt>`repo_url`</dt>
    <dd>
        the URL of the current repository
    </dd>

    <dt>`repo_name`</dt>
    <dd>
        the name of the current repository
    </dd>

    <dt>`repo_owner`</dt>
    <dd>
        the owner of the current repository
    </dd>

    <dt>`repo_host`</dt>
    <dd>
        the host of the current repository
    </dd>

    <dt>`stacks`</dt>
    <dd>
        a list of affected components and stacks with their dependencies and settings
    </dd>
</dl>

---

## atmos describe component

import Terminal from '@site/src/components/Terminal'
import Screengrab from '@site/src/components/Screengrab'
import Intro from '@site/src/components/Intro'

<Intro>
Use this command to describe the complete configuration for an [Atmos component](/core-concepts/components) in
an [Atmos stack](/core-concepts/stacks).
</Intro>

<Screengrab title="atmos describe component --help" slug="atmos-describe-component--help" />

## Usage

Execute the `atmos describe component` command like this:

```shell
atmos describe component <component> -s <stack>
```

:::tip
Run `atmos describe component --help` to see all the available options
:::

## Examples

```shell
atmos describe component infra/vpc -s tenant1-ue2-dev

atmos describe component infra/vpc -s tenant1-ue2-dev --format json

atmos describe component infra/vpc -s tenant1-ue2-dev -f yaml

atmos describe component infra/vpc -s tenant1-ue2-dev --file component.yaml

atmos describe component echo-server -s tenant1-ue2-staging

atmos describe component test/test-component-override -s tenant2-ue2-prod

atmos describe component vpc -s tenant1-ue2-dev --process-templates=false

atmos describe component vpc -s tenant1-ue2-dev --process-functions=false

atmos describe component vpc -s tenant1-ue2-dev --skip=terraform.output

atmos describe component vpc -s tenant1-ue2-dev --skip=terraform.output --skip=include

atmos describe component vpc -s tenant1-ue2-dev --skip=include,eval

atmos describe component vpc -s plat-ue2-prod --query .vars.tags

atmos describe component vpc -s plat-ue2-prod -q .settings

atmos describe component vpc -s plat-ue2-prod --pager=more
```

## Arguments

<dl>
  <dt>`component` (required)</dt>
  <dd>Atmos component.</dd>
</dl>

## Flags

<dl>
  <dt>`--stack` / `-s` (required)</dt>
  <dd>Atmos stack.</dd>

  <dt>`--format` / `-f` (optional)</dt>
  <dd>Output format: `yaml` or `json` (`yaml` is default).</dd>

  <dt>`--file` (optional)</dt>
  <dd>If specified, write the result to the file.</dd>

  <dt>`--process-templates` (optional)</dt>
  <dd>Enable/disable processing of all `Go` templatesin Atmos stacks manifests when executing the command.Use the flag to see the component configurationbefore and after the templates are processed.If the flag is not provided, it's set to `true` by default.`atmos describe component <c> -s <stack> --process-templates=false`.</dd>

  <dt>`--process-functions` (optional)</dt>
  <dd>Enable/disable processing of all Atmos YAML functionsin Atmos stacks manifests when executing the command.Use the flag to see the component configurationbefore and after the functions are processed.If the flag is not provided, it's set to `true` by default.`atmos describe component <c> -s <stack> --process-functions=false`.</dd>

  <dt>`--skip` (optional)</dt>
  <dd>Skip processing a specific Atmos YAML functionin Atmos stacks manifests when executing the command.To specify more than one function,use multiple `--skip` flags, or separate the functions with a comma:`atmos describe component <c> -s <stack> --skip=terraform.output --skip=include``atmos describe component <c> -s <stack> --skip=terraform.output,include`.</dd>

  <dt>`--query` / `-q` (optional)</dt>
  <dd>Query the results of the command using `yq` expressions.`atmos describe component <c> -s <stack> --query .vars.tags`For more details, refer to https://mikefarah.gitbook.io/yq.</dd>

  <dt>`--pager` (optional)</dt>
  <dd>Disable/Enable the paging user experience.</dd>
</dl>

## Output

The command outputs the final deep-merged component configuration.

The output contains the following sections:

- `atlantis_project` - Atlantis project name (if [Atlantis Integration](/integrations/atlantis) is configured for the component in the stack)

- `atmos_cli_config` - information about Atmos CLI configuration from `atmos.yaml`

- `atmos_component` - [Atmos component](/core-concepts/components) name

- `atmos_stack` - [Atmos stack](/core-concepts/stacks) name

- `stack` - same as `atmos_stack`

- `atmos_stack_file` - the stack manifest where the Atmos stack is defined

- `atmos_manifest` - same as `atmos_stack_file`

- `backend` - Terraform/OpenTofu backend configuration

- `backend_type` - Terraform/OpenTofu backend type

- `command` - the binary to execute when provisioning the component (e.g. `terraform`, `terraform-1`, `tofu`, `helmfile`)

- `component` - the Terraform/OpenTofu component for which the Atmos component provides configuration

- `component_type` - the type of the component (`terraform` or `helmfile`)

- `component_info` - a block describing the Terraform or Helmfile components that the Atmos component manages. The `component_info` block has the
  following sections:
  - `component_path` - the filesystem path to the Terraform/OpenTofu or Helmfile component

  - `component_type` - the type of the component (`terraform` or `helmfile`)

  - `terraform_config` - if the component type is `terraform`, this sections describes the high-level metadata about the Terraform component from its
    source code, including variables, outputs and child Terraform modules (using a Terraform parser from HashiCorp). The file names and line numbers
    where the variables, outputs and child modules are defined are also included. Invalid Terraform configurations are also detected, and in case of
    any issues, the warnings and errors are shows in the `terraform_config.diagnostics` section

- `env` - a map of ENV variables defined for the Atmos component

- `inheritance` - component's [inheritance chain](/core-concepts/stacks/inheritance)

- `metadata` - component's metadata config

- `remote_state_backend` - Terraform/OpenTofu backend config for remote state

- `remote_state_backend_type` - Terraform/OpenTofu backend type for remote state

- `settings` - component settings (free-form map)

- `sources` - sources of the values from the component's sections (`vars`, `env`, `settings`)

- `spacelift_stack` - Spacelift stack name (if [Spacelift Integration](/integrations/spacelift) is configured for the component in the stack
  and `settings.spacelift.workspace_enabled` is set to `true`)

- `vars` - the final deep-merged component variables that are provided to Terraform/OpenTofu and Helmfile when executing
  `atmos terraform` and `atmos helmfile` commands

- `workspace` - Terraform/OpenTofu workspace for the Atmos component

- `imports` - a list of all imports in the Atmos stack (this shows all imports in the stack, related to the component and not)

- `deps_all` - a list of all component stack dependencies (stack manifests where the component settings are defined, either inline or via imports)

- `deps` - a list of component stack dependencies where the _final_ values of all component configurations are defined
  (after the deep-merging and processing all the inheritance chains and all the base components)

- `overrides` - a map of overrides for the component. Refer to [Component Overrides](/core-concepts/stacks/overrides) for more details

- `providers` - a map of provider configurations for the component

## Difference between `imports`, `deps_all` and `deps` outputs

The difference between the `imports`, `deps_all` and `deps` outputs is as follows:

- `imports` shows all imports in the stack for all components. This can be useful in GitHub actions and
   in [OPA validation policies](/core-concepts/validate/opa) to check whether an import is allowed in the stack or not

- `deps_all` shows all component stack dependencies (imports and root-level stacks) where any configuration for the component is present.
  This also can be useful in GitHub Actions and [OPA validation policies](/core-concepts/validate/opa) to check whether a user or a team
  is allowed to import a particular config file for the component in the stack

- `deps` shows all the component stack dependencies where the __FINAL__ values from all the component sections are defined
  (after the deep-merging and processing all the inheritance chains and all the base components). This is useful in CI/CD systems (e.g. Spacelift)
  to detect only the affected files that the component depends on. `deps` is usually a much smaller list than `deps_all` and can
  differ from it in the following ways:

  - An Atmos component can inherit configurations from many base components, see [Component Inheritance](/core-concepts/stacks/inheritance), and
    import those base component configurations

  - The component can override all the default variables from the base components, and the final values are not dependent on the base component
    configs anymore. For example, `derived-component-3` import the base component `base-component-4`, inherits from it, and overrides all
    the variables:

   ```yaml
   # Import the base component config
   import:
     - catalog/terraform/base-component-4

   components:
     terraform:
       derived-component-3:
         metadata:
           component: "test/test-component"  # Point to the Terraform/OpenTofu component
           inherits:
             # Inherit all the values from the base component
             - base-component-4
         vars:
           # Override all the variables from the base component
   ```

  - Atmos detects that and does not include the base component `base-component-4` config file into the `deps` output since the `derived-component-3`
    does not directly depend on `base-component-4` (all values are coming from the `derived-component-3`). This will help, for example,
    prevent unrelated Spacelift stack triggering

  - In the above case, the `deps_all` output will include both `derived-component-3` and `base-component-4`, but the `deps` output will not include
    `base-component-4`

## Command example

<Terminal title="atmos describe component test/test-component-override-3 -s tenant1-ue2-dev">
```yaml
atlantis_project: tenant1-ue2-dev-test-test-component-override-3
atmos_cli_config:
  base_path: ./tests/fixtures/scenarios/complete
  components:
    terraform:
      base_path: components/terraform
      apply_auto_approve: false
      deploy_run_init: true
      init_run_reconfigure: true
      auto_generate_backend_file: false
  stacks:
    base_path: stacks
    included_paths:
      - orgs/**/*
    excluded_paths:
      - '**/_defaults.yaml'
    name_pattern: '{tenant}-{environment}-{stage}'
  workflows:
    base_path: stacks/workflows
atmos_component: test/test-component-override-3
atmos_stack: tenant1-ue2-dev
atmos_stack_file: orgs/cp/tenant1/dev/us-east-2
backend:
  bucket: cp-ue2-root-tfstate
  dynamodb_table: cp-ue2-root-tfstate-lock
  key: terraform.tfstate
  region: us-east-2
  workspace_key_prefix: test-test-component
backend_type: s3
command: terraform
component: test/test-component
component_info:
  component_path: tests/fixtures/scenarios/complete/components/terraform/test/test-component
  component_type: terraform
  terraform_config:
    path: tests/fixtures/scenarios/complete/components/terraform/test/test-component
    variables:
      enabled:
        name: enabled
        type: bool
        description: Set to false to prevent the module from creating any resources
        default: null
        required: false
        sensitive: false
        pos:
          filename: tests/fixtures/scenarios/complete/components/terraform/test/test-component/context.tf
          line: 97
      name:
        name: name
        type: string
        description: |
          ID element. Usually the component or solution name, e.g. 'app' or 'jenkins'.
          This is the only ID element not also included as a `tag`.
          The "name" tag is set to the full `id` string. There is no tag with the value of the `name` input.
        default: null
        required: false
        sensitive: false
        pos:
          filename: tests/fixtures/scenarios/complete/components/terraform/test/test-component/context.tf
          line: 127
      service_1_name:
        name: service_1_name
        type: string
        description: Service 1 name
        default: null
        required: true
        sensitive: false
        pos:
          filename: tests/fixtures/scenarios/complete/components/terraform/test/test-component/variables.tf
          line: 6
    outputs:
      service_1_id:
        name: service_1_id
        description: Service 1 ID
        sensitive: false
        pos:
          filename: tests/fixtures/scenarios/complete/components/terraform/test/test-component/outputs.tf
          line: 1
      service_2_id:
        name: service_2_id
        description: Service 2 ID
        sensitive: false
        pos:
          filename: tests/fixtures/scenarios/complete/components/terraform/test/test-component/outputs.tf
          line: 6
    modulecalls:
      service_1_label:
        name: service_1_label
        source: cloudposse/label/null
        version: 0.25.0
        pos:
          filename: tests/fixtures/scenarios/complete/components/terraform/test/test-component/main.tf
          line: 1
    diagnostics: []
deps:
  - catalog/terraform/mixins/test-2
  - catalog/terraform/services/service-1-override-2
  - catalog/terraform/services/service-2-override-2
  - catalog/terraform/spacelift-and-backend-override-1
  - catalog/terraform/test-component
  - catalog/terraform/test-component-override-3
  - mixins/region/us-east-2
  - mixins/stage/dev
  - orgs/cp/_defaults
  - orgs/cp/tenant1/_defaults
  - orgs/cp/tenant1/dev/us-east-2
deps_all:
  - catalog/terraform/mixins/test-1
  - catalog/terraform/mixins/test-2
  - catalog/terraform/services/service-1
  - catalog/terraform/services/service-1-override
  - catalog/terraform/services/service-1-override-2
  - catalog/terraform/services/service-2
  - catalog/terraform/services/service-2-override
  - catalog/terraform/services/service-2-override-2
  - catalog/terraform/spacelift-and-backend-override-1
  - catalog/terraform/tenant1-ue2-dev
  - catalog/terraform/test-component
  - catalog/terraform/test-component-override
  - catalog/terraform/test-component-override-2
  - catalog/terraform/test-component-override-3
  - mixins/region/us-east-2
  - mixins/stage/dev
  - orgs/cp/_defaults
  - orgs/cp/tenant1/_defaults
  - orgs/cp/tenant1/dev/us-east-2
env:
  TEST_ENV_VAR1: val1-override-3
  TEST_ENV_VAR2: val2-override-3
  TEST_ENV_VAR3: val3-override-3
  TEST_ENV_VAR4: null
imports:
  - catalog/terraform/mixins/test-1
  - catalog/terraform/mixins/test-2
  - catalog/terraform/services/service-1
  - catalog/terraform/services/service-1-override
  - catalog/terraform/services/service-1-override-2
  - catalog/terraform/services/service-2
  - catalog/terraform/services/service-2-override
  - catalog/terraform/services/service-2-override-2
  - catalog/terraform/services/top-level-service-1
  - catalog/terraform/services/top-level-service-2
  - catalog/terraform/spacelift-and-backend-override-1
  - catalog/terraform/tenant1-ue2-dev
  - catalog/terraform/test-component
  - catalog/terraform/test-component-override
  - catalog/terraform/test-component-override-2
  - catalog/terraform/test-component-override-3
  - catalog/terraform/top-level-component1
  - catalog/terraform/vpc
  - mixins/region/us-east-2
  - mixins/stage/dev
  - orgs/cp/_defaults
  - orgs/cp/tenant1/_defaults
  - orgs/cp/tenant1/dev/_defaults
inheritance:
  - mixin/test-2
  - mixin/test-1
  - test/test-component-override-2
  - test/test-component-override
  - test/test-component
metadata:
  component: test/test-component
  inherits:
    - test/test-component-override
    - test/test-component-override-2
    - mixin/test-1
    - mixin/test-2
  terraform_workspace: test-component-override-3-workspace
remote_state_backend:
  bucket: cp-ue2-root-tfstate
  dynamodb_table: cp-ue2-root-tfstate-lock
  region: us-east-2
  workspace_key_prefix: test-test-component
remote_state_backend_type: s3
settings:
  config:
    is_prod: false
  spacelift:
    protect_from_deletion: true
    stack_destructor_enabled: false
    stack_name_pattern: '{tenant}-{environment}-{stage}-new-component'
    workspace_enabled: false
sources:
  backend:
    bucket:
      final_value: cp-ue2-root-tfstate
      name: bucket
      stack_dependencies:
        - stack_file: catalog/terraform/spacelift-and-backend-override-1
          stack_file_section: terraform.backend.s3
          dependency_type: import
          variable_value: cp-ue2-root-tfstate
        - stack_file: orgs/cp/_defaults
          stack_file_section: terraform.backend.s3
          dependency_type: import
          variable_value: cp-ue2-root-tfstate
    dynamodb_table:
      final_value: cp-ue2-root-tfstate-lock
      name: dynamodb_table
      stack_dependencies:
        - stack_file: catalog/terraform/spacelift-and-backend-override-1
          stack_file_section: terraform.backend.s3
          dependency_type: import
          variable_value: cp-ue2-root-tfstate-lock
        - stack_file: orgs/cp/_defaults
          stack_file_section: terraform.backend.s3
          dependency_type: import
          variable_value: cp-ue2-root-tfstate-lock
  env:
    TEST_ENV_VAR1:
      final_value: val1-override-3
      name: TEST_ENV_VAR1
      stack_dependencies:
        - dependency_type: import
          stack_file: catalog/terraform/test-component-override-3
          stack_file_section: components.terraform.env
          variable_value: val1-override-3
        - dependency_type: import
          stack_file: catalog/terraform/test-component-override-2
          stack_file_section: components.terraform.env
          variable_value: val1-override-2
        - dependency_type: import
          stack_file: catalog/terraform/test-component-override
          stack_file_section: components.terraform.env
          variable_value: val1-override
        - dependency_type: import
          stack_file: catalog/terraform/test-component
          stack_file_section: components.terraform.env
          variable_value: val1
  settings:
    spacelift:
      final_value:
        protect_from_deletion: true
        stack_destructor_enabled: false
        stack_name_pattern: '{tenant}-{environment}-{stage}-new-component'
        workspace_enabled: false
      name: spacelift
      stack_dependencies:
        - dependency_type: import
          stack_file: catalog/terraform/test-component-override-3
          stack_file_section: components.terraform.settings
          variable_value:
            workspace_enabled: false
        - dependency_type: import
          stack_file: catalog/terraform/test-component-override-2
          stack_file_section: components.terraform.settings
          variable_value:
            stack_name_pattern: '{tenant}-{environment}-{stage}-new-component'
            workspace_enabled: true
        - dependency_type: import
          stack_file: catalog/terraform/test-component
          stack_file_section: components.terraform.settings
          variable_value:
            workspace_enabled: true
        - dependency_type: import
          stack_file: catalog/terraform/spacelift-and-backend-override-1
          stack_file_section: settings
          variable_value:
            protect_from_deletion: true
            stack_destructor_enabled: false
            workspace_enabled: true
  vars:
    enabled:
      final_value: true
      name: enabled
      stack_dependencies:
        - dependency_type: import
          stack_file: catalog/terraform/test-component
          stack_file_section: components.terraform.vars
          variable_value: true
        - dependency_type: inline
          stack_file: orgs/cp/tenant1/dev/us-east-2
          stack_file_section: terraform.vars
          variable_value: false
    # Other variables are omitted for clarity
vars:
  enabled: true
  environment: ue2
  namespace: cp
  region: us-east-2
  service_1_map:
    a: 1
    b: 6
    c: 7
    d: 8
  service_1_name: mixin-2
  stage: dev
  tenant: tenant1
workspace: test-component-override-3-workspace
```
</Terminal>

## Sources of Component Variables

The `sources.vars` section of the output shows the final deep-merged component's variables and their inheritance chain.

Each variable descriptor has the following schema:

- `final_value` - the final value of the variable after Atmos processes and deep-merges all values from all stack manifests
- `name` - the variable name
- `stack_dependencies` - the variable's inheritance chain (stack manifests where the values for the variable were provided). It has the following
  schema:

  - `stack_file` - the stack manifest where the value for the variable was provided
  - `stack_file_section` - the section of the stack manifest where the value for the variable was provided
  - `variable_value` - the variable's value
  - `dependency_type` - how the variable was defined (`inline` or `import`). `inline` means the variable was defined in one of the sections
    in the stack manifest. `import` means the stack manifest where the variable is defined was imported into the parent Atmos stack

For example:

<Terminal title="atmos describe component test/test-component-override-3 -s tenant1-ue2-dev">
```yaml
sources:
  vars:
    enabled:
      final_value: true
      name: enabled
      stack_dependencies:
        - dependency_type: import
          stack_file: catalog/terraform/test-component
          stack_file_section: components.terraform.vars
          variable_value: true
        - dependency_type: inline
          stack_file: orgs/cp/tenant1/dev/us-east-2
          stack_file_section: terraform.vars
          variable_value: false
        - dependency_type: inline
          stack_file: orgs/cp/tenant1/dev/us-east-2
          stack_file_section: vars
          variable_value: true
    environment:
      final_value: ue2
      name: environment
      stack_dependencies:
        - dependency_type: import
          stack_file: mixins/region/us-east-2
          stack_file_section: vars
          variable_value: ue2
    namespace:
      final_value: cp
      name: namespace
      stack_dependencies:
        - dependency_type: import
          stack_file: orgs/cp/_defaults
          stack_file_section: vars
          variable_value: cp
    region:
      final_value: us-east-2
      name: region
      stack_dependencies:
        - dependency_type: import
          stack_file: mixins/region/us-east-2
          stack_file_section: vars
          variable_value: us-east-2
    service_1_map:
      final_value:
        a: 1
        b: 6
        c: 7
        d: 8
      name: service_1_map
      stack_dependencies:
        - dependency_type: import
          stack_file: catalog/terraform/services/service-1-override-2
          stack_file_section: components.terraform.vars
          variable_value:
            b: 6
            c: 7
            d: 8
        - dependency_type: import
          stack_file: catalog/terraform/services/service-1-override
          stack_file_section: components.terraform.vars
          variable_value:
            a: 1
            b: 2
            c: 3
    service_1_name:
      final_value: mixin-2
      name: service_1_name
      stack_dependencies:
        - dependency_type: import
          stack_file: catalog/terraform/mixins/test-2
          stack_file_section: components.terraform.vars
          variable_value: mixin-2
        - dependency_type: import
          stack_file: catalog/terraform/mixins/test-1
          stack_file_section: components.terraform.vars
          variable_value: mixin-1
        - dependency_type: import
          stack_file: catalog/terraform/services/service-1-override-2
          stack_file_section: components.terraform.vars
          variable_value: service-1-override-2
        - dependency_type: import
          stack_file: catalog/terraform/tenant1-ue2-dev
          stack_file_section: components.terraform.vars
          variable_value: service-1-override-2
        - dependency_type: import
          stack_file: catalog/terraform/services/service-1-override
          stack_file_section: components.terraform.vars
          variable_value: service-1-override
        - dependency_type: import
          stack_file: catalog/terraform/services/service-1
          stack_file_section: components.terraform.vars
          variable_value: service-1
    stage:
      final_value: dev
      name: stage
      stack_dependencies:
        - dependency_type: import
          stack_file: mixins/stage/dev
          stack_file_section: vars
          variable_value: dev
```
</Terminal>

:::info

The `stack_dependencies` inheritance chain shows the variable sources in the reverse order the sources were processed.
The first item in the list was processed the last and its `variable_value` overrode all the previous values of the variable.

:::

For example, the component's `enabled` variable has the following inheritance chain:

```yaml
sources:
  vars:
    enabled:
      final_value: true
      name: enabled
      stack_dependencies:
        - dependency_type: import
          stack_file: catalog/terraform/test-component
          stack_file_section: components.terraform.vars
          variable_value: true
        - dependency_type: inline
          stack_file: orgs/cp/tenant1/dev/us-east-2
          stack_file_section: terraform.vars
          variable_value: false
        - dependency_type: inline
          stack_file: orgs/cp/tenant1/dev/us-east-2
          stack_file_section: vars
          variable_value: true
```

Which we can interpret as follows (reading from the last to the first item in the `stack_dependencies` list):

- In the `orgs/cp/tenant1/dev/us-east-2` stack manifest (the last item in the list), the value for `enabled` was set to `true` in the global `vars`
  section (inline)

- Then in the same `orgs/cp/tenant1/dev/us-east-2` stack manifest, the value for `enabled` was set to `false` in the `terraform.vars`
  section (inline). This value overrode the value set in the global `vars` section

- Finally, in the `catalog/terraform/test-component` stack manifest (which was imported into the parent Atmos stack
  via [`import`](/core-concepts/stacks/imports)), the value for `enabled` was set to `true` in the `components.terraform.vars` section of
  the `test/test-component-override-3` Atmos component. This value overrode all the previous values arriving at the `final_value: true` for the
  variable. This final value is then set for the `enabled` variable of the Terraform component `test/test-component` when Atmos
  executes `atmos terraform apply test/test-component-override-3 -s <stack>` command

## Sources of Component ENV Variables

The `sources.env` section of the output shows the final deep-merged component's environment variables and their inheritance chain.

Each variable descriptor has the following schema:

- `final_value` - the final value of the variable after Atmos processes and deep-merges all values from all stack manifests
- `name` - the variable name
- `stack_dependencies` - the variable's inheritance chain (stack manifests where the values for the variable were provided). It has the following
  schema:

  - `stack_file` - the stack manifest where the value for the variable was provided
  - `stack_file_section` - the section of the stack manifest where the value for the variable was provided
  - `variable_value` - the variable's value
  - `dependency_type` - how the variable was defined (`inline` or `import`). `inline` means the variable was defined in one of the sections
    in the stack manifest. `import` means the stack manifest where the variable is defined was imported into the parent Atmos stack

For example:

<Terminal title="atmos describe component test/test-component-override-3 -s tenant1-ue2-dev">
```yaml
sources:
  env:
    TEST_ENV_VAR1:
      final_value: val1-override-3
      name: TEST_ENV_VAR1
      stack_dependencies:
        - dependency_type: import
          stack_file: catalog/terraform/test-component-override-3
          stack_file_section: components.terraform.env
          variable_value: val1-override-3
        - dependency_type: import
          stack_file: catalog/terraform/test-component-override-2
          stack_file_section: components.terraform.env
          variable_value: val1-override-2
        - dependency_type: import
          stack_file: catalog/terraform/test-component-override
          stack_file_section: components.terraform.env
          variable_value: val1-override
        - dependency_type: import
          stack_file: catalog/terraform/test-component
          stack_file_section: components.terraform.env
          variable_value: val1
    TEST_ENV_VAR2:
      final_value: val2-override-3
      name: TEST_ENV_VAR2
      stack_dependencies:
        - dependency_type: import
          stack_file: catalog/terraform/test-component-override-3
          stack_file_section: components.terraform.env
          variable_value: val2-override-3
        - dependency_type: import
          stack_file: catalog/terraform/test-component-override-2
          stack_file_section: components.terraform.env
          variable_value: val2-override-2
        - dependency_type: import
          stack_file: catalog/terraform/test-component
          stack_file_section: components.terraform.env
          variable_value: val2
    TEST_ENV_VAR3:
      final_value: val3-override-3
      name: TEST_ENV_VAR3
      stack_dependencies:
        - dependency_type: import
          stack_file: catalog/terraform/test-component-override-3
          stack_file_section: components.terraform.env
          variable_value: val3-override-3
        - dependency_type: import
          stack_file: catalog/terraform/test-component-override
          stack_file_section: components.terraform.env
          variable_value: val3-override
        - dependency_type: import
          stack_file: catalog/terraform/test-component
          stack_file_section: components.terraform.env
          variable_value: val3
```
</Terminal>

:::info

The `stack_dependencies` inheritance chain shows the ENV variable sources in the reverse order the sources were processed.
The first item in the list was processed the last and its `variable_value` overrode all the previous values of the variable.

:::

For example, the component's `TEST_ENV_VAR1` ENV variable has the following inheritance chain:

```yaml
sources:
  env:
    TEST_ENV_VAR1:
      final_value: val1-override-3
      name: TEST_ENV_VAR1
      stack_dependencies:
        - dependency_type: import
          stack_file: catalog/terraform/test-component-override-3
          stack_file_section: components.terraform.env
          variable_value: val1-override-3
        - dependency_type: import
          stack_file: catalog/terraform/test-component-override-2
          stack_file_section: components.terraform.env
          variable_value: val1-override-2
        - dependency_type: import
          stack_file: catalog/terraform/test-component-override
          stack_file_section: components.terraform.env
          variable_value: val1-override
        - dependency_type: import
          stack_file: catalog/terraform/test-component
          stack_file_section: components.terraform.env
          variable_value: val1
```

Which we can interpret as follows (reading from the last to the first item in the `stack_dependencies` list):

- In the `catalog/terraform/test-component` stack manifest (the last item in the list), the value for the `TEST_ENV_VAR1` ENV variable was set
  to `val1` in the `components.terraform.env` section

- Then the value was set to `val1-override` in the `catalog/terraform/test-component-override` stack manifest. This value overrides the value set
  in the `catalog/terraform/test-component` stack manifest

- Then the value was set to `val1-override-2` in the `catalog/terraform/test-component-override-2` stack manifest. This value overrides the values
  set in the `catalog/terraform/test-component` and `catalog/terraform/test-component-override` stack manifests

- Finally, in the `catalog/terraform/test-component-override-3` stack manifest (which was imported into the parent Atmos stack
  via [`import`](/core-concepts/stacks/imports)), the value was set to `val1-override-3` in the `components.terraform.env` section of
  the `test/test-component-override-3` Atmos component. This value overrode all the previous values arriving at the `final_value: val1-override-3` for
  the ENV variable

## Sources of Component Settings

The `sources.settings` section of the output shows the final deep-merged component's settings and their inheritance chain.

Each setting descriptor has the following schema:

- `final_value` - the final value of the setting after Atmos processes and deep-merges all values from all stack manifests
- `name` - the setting name
- `stack_dependencies` - the setting's inheritance chain (stack manifests where the values for the variable were provided). It has the following
  schema:

  - `stack_file` - the stack manifest where the value for the setting was provided
  - `stack_file_section` - the section of the stack manifest where the value for the setting was provided
  - `variable_value` - the setting's value
  - `dependency_type` - how the setting was defined (`inline` or `import`). `inline` means the setting was defined in one of the sections
    in the stack manifest. `import` means the stack config file where the setting is defined was imported into the parent Atmos stack

For example:

<Terminal title="atmos describe component test/test-component-override-3 -s tenant1-ue2-dev">
```yaml
sources:
  settings:
    spacelift:
      final_value:
        protect_from_deletion: true
        stack_destructor_enabled: false
        stack_name_pattern: '{tenant}-{environment}-{stage}-new-component'
        workspace_enabled: false
      name: spacelift
      stack_dependencies:
        - dependency_type: import
          stack_file: catalog/terraform/test-component-override-3
          stack_file_section: components.terraform.settings
          variable_value:
            workspace_enabled: false
        - dependency_type: import
          stack_file: catalog/terraform/test-component-override-2
          stack_file_section: components.terraform.settings
          variable_value:
            stack_name_pattern: '{tenant}-{environment}-{stage}-new-component'
            workspace_enabled: true
        - dependency_type: import
          stack_file: catalog/terraform/test-component
          stack_file_section: components.terraform.settings
          variable_value:
            workspace_enabled: true
        - dependency_type: import
          stack_file: catalog/terraform/spacelift-and-backend-override-1
          stack_file_section: settings
          variable_value:
            protect_from_deletion: true
            stack_destructor_enabled: false
            workspace_enabled: true
```
</Terminal>

:::info

The `stack_dependencies` inheritance chain shows the sources of the setting in the reverse order the sources were processed.
The first item in the list was processed the last and its `variable_value` overrode all the previous values of the setting.

:::

---

## atmos describe config

import Terminal from '@site/src/components/Terminal'
import Screengrab from '@site/src/components/Screengrab'
import Intro from '@site/src/components/Intro'

<Intro>
Use this command to show the final (deep-merged) [CLI configuration](/cli/configuration) of all `atmos.yaml` file(s).
</Intro>

<Screengrab title="atmos describe config --help" slug="atmos-describe-config--help" />

## Usage

Execute the `describe config` command like this:

```shell
atmos describe config [options]
```

This command shows the final (deep-merged) [CLI configuration](/cli/configuration) (from `atmos.yaml` file(s)).

:::tip
Run `atmos describe config --help` to see all the available options
:::

## Examples

```shell
atmos describe config
atmos describe config -f yaml
atmos describe config --format yaml
atmos describe config -f json
atmos describe config --query <yq-expression>
```

## Flags

<dl>
  <dt>`--format` / `-f` (optional)</dt>
  <dd>Output format: `json` or `yaml` (`json` is default).</dd>

  <dt>`--query` / `-q` (optional)</dt>
  <dd>Query the results of the command using `yq` expressions.`atmos describe config --query <yq-expression>`.For more details, refer to https://mikefarah.gitbook.io/yq.</dd>
</dl>

---

## atmos describe dependents

import Terminal from '@site/src/components/Terminal'
import Screengrab from '@site/src/components/Screengrab'
import Intro from '@site/src/components/Intro'

<Intro>
Use this command to show a list of Atmos components in Atmos stacks that depend on the provided Atmos component.
</Intro>

<Screengrab title="atmos describe dependents --help" slug="atmos-describe-dependents--help"/>

## Description

In Atmos, you can define component dependencies by using the `settings.depends_on` section. The section used to define
all the Atmos components (in the same or different stacks) that the current component depends on.

The `settings.depends_on` section is a map of objects. The map keys are just the descriptions of dependencies and can be strings or numbers.
Provide meaningful descriptions so that people can understand what the dependencies are about.

Each object in the `settings.depends_on` section has the following schema:

<dl>
    <dt>file (optional)</dt>
    <dd>A file on the local filesystem that the current component depends on</dd>

    <dt>folder (optional)</dt>
    <dd>A folder on the local filesystem that the current component depends on</dd>

    <dt>component (required if `file` or `folder` is not specified)</dt>
    <dd>an Atmos component that the current component depends on</dd>

    <dt>stack (optional)</dt>
    <dd>Atmos stack where the `component` is provisioned</dd>

    <dt>namespace (optional)</dt>
    <dd>The `namespace` where the `component` is provisioned</dd>

    <dt>tenant (optional)</dt>
    <dd>The `tenant` where the `component` is provisioned</dd>

    <dt>environment (optional)</dt>
    <dd>The `environment` where the `component` is provisioned</dd>

    <dt>stage (optional)</dt>
    <dd>The `stage` where the `component` is provisioned</dd>
</dl>

One of `component`, `file` or `folder` is required.

Dependencies on external files (not in the component's folder) are defined using the `file` attribute. For example:

```yaml title="stacks/catalog/terraform/top-level-component3.yaml"
components:
  terraform:
    top-level-component3:
      metadata:
        component: "top-level-component1"
      settings:
        depends_on:
          1:
            file: "tests/fixtures/scenarios/complete/components/terraform/mixins/introspection.mixin.tf"
```

In the configuration above, we specify that the Atmos component `top-level-component3` depends on the file
`tests/fixtures/scenarios/complete/components/terraform/mixins/introspection.mixin.tf` (which is not in the component's folder).

Dependencies on external folders are defined using the `folder` attribute. For example:

```yaml title="stacks/catalog/terraform/top-level-component3.yaml"
components:
  terraform:
    top-level-component3:
      metadata:
        component: "top-level-component1"
      settings:
        depends_on:
          1:
            file: "tests/fixtures/scenarios/complete/components/terraform/mixins/introspection.mixin.tf"
          2:
            folder: "tests/fixtures/scenarios/complete/components/helmfile/infra/infra-server"
```

In the configuration above, we specify that the Atmos component `top-level-component3` depends on the folder
`tests/fixtures/scenarios/complete/components/helmfile/infra/infra-server`.

If `component` is specified, the rest of the attributes are the context variables and are used to define Atmos stacks other than the current stack.
For example, you can specify:

- `namespace` if the `component` is from a different Organization
- `tenant` if the `component` is from a different Organizational Unit
- `environment` if the `component` is from a different region
- `stage` if the `component` is from a different account
- `tenant`, `environment` and `stage` if the component is from a different Atmos stack (e.g. `tenant1-ue2-dev`)

In the following example, we define that the `top-level-component1` component depends on the following:

- The `test/test-component-override` component in the same Atmos stack
- The `test/test-component` component in Atmos stacks identified by the `dev` stage
- The `my-component` component from the `tenant1-ue2-staging` Atmos stack

```yaml title="tests/fixtures/scenarios/complete/stacks/catalog/terraform/top-level-component1.yaml"
components:
  terraform:
    top-level-component1:
      settings:
        depends_on:
          1:
            # If the `context` (namespace, tenant, environment, stage) is not provided,
            # the `component` is from the same Atmos stack as this component
            component: "test/test-component-override"
          2:
            # This component (in any stage) depends on `test/test-component`
            # from the `dev` stage (in any `environment` and any `tenant`)
            component: "test/test-component"
            stage: "dev"
          3:
            # This component depends on `my-component`
            # from the `tenant1-ue2-staging` Atmos stack
            component: "my-component"
            tenant: "tenant1"
            environment: "ue2"
            stage: "staging"
      vars:
        enabled: true
```

In the following example, we specify that the `top-level-component2` component depends on the following:

- The `test/test-component` component in the same Atmos stack
- The `test/test2/test-component-2` component in the same Atmos stack

```yaml title="tests/fixtures/scenarios/complete/stacks/catalog/terraform/top-level-component2.yaml"
components:
  terraform:
    top-level-component2:
      metadata:
        # Point to Terraform component
        component: "top-level-component1"
      settings:
        depends_on:
          1:
            # If the `context` (namespace, tenant, environment, stage) is not provided,
            # the `component` is from the same Atmos stack as this component
            component: "test/test-component"
          2:
            # If the `context` (namespace, tenant, environment, stage) is not provided,
            # the `component` is from the same Atmos stack as this component
            component: "test/test2/test-component-2"
      vars:
        enabled: true
```

Having the `top-level-component` and `top-level-component2` components configured as shown above, we can now execute the following Atmos command
to show all the components that depend on the `test/test-component` component in the `tenant1-ue2-dev` stack:

<Terminal title="atmos describe dependents test/test-component -s tenant1-ue2-dev">
    ```json
    [
        {
            "component": "top-level-component1",
            "component_type": "terraform",
            "component_path": "tests/fixtures/scenarios/complete/components/terraform/top-level-component1",
            "namespace": "cp",
            "tenant": "tenant1",
            "environment": "ue2",
            "stage": "dev",
            "stack": "tenant1-ue2-dev",
            "stack_slug": "tenant1-ue2-dev-top-level-component1",
            "spacelift_stack": "tenant1-ue2-dev-top-level-component1",
            "atlantis_project": "tenant1-ue2-dev-top-level-component1"
        }
    ]
    ```
</Terminal>

Similarly, the following Atmos command shows all the components that depend on the `test/test-component` component in
the `tenant1-ue2-test-1` stack:

<Terminal command="atmos describe dependents test/test-component -s tenant1-ue2-test-1">
    ```json
    [
        {
            "component": "top-level-component1",
            "component_type": "terraform",
            "component_path": "tests/fixtures/scenarios/complete/components/terraform/top-level-component1",
            "namespace": "cp",
            "tenant": "tenant1",
            "environment": "ue2",
            "stage": "test-1",
            "stack": "tenant1-ue2-test-1",
            "stack_slug": "tenant1-ue2-dev-top-level-component1",
            "spacelift_stack": "tenant1-ue2-test-1-top-level-component1",
            "atlantis_project": "tenant1-ue2-test-1-top-level-component1"
        },
        {
            "component": "top-level-component2",
            "component_type": "terraform",
            "component_path": "tests/fixtures/scenarios/complete/components/terraform/top-level-component1",
            "namespace": "cp",
            "tenant": "tenant1",
            "environment": "ue2",
            "stage": "test-1",
            "stack": "tenant1-ue2-test-1",
            "stack_slug": "tenant1-ue2-test-1-top-level-component2",
            "atlantis_project": "tenant1-ue2-test-1-top-level-component2"
        }
    ]
    ```
</Terminal>

After the `test/test-component` has been provisioned, you can use the outputs to perform the following actions:

- Provision the dependent components by executing the Atmos commands `atmos terraform apply top-level-component1 -s tenant1-ue2-test-1` and
`atmos terraform apply top-level-component2 -s tenant1-ue2-test-1` (on the command line or from a GitHub Action)

- Trigger the dependent Spacelift stack (from a GitHub Action by using the [spacectl](https://github.com/spacelift-io/spacectl) CLI, or by using an
OPA [Trigger](https://docs.spacelift.io/concepts/policy/trigger-policy)
policy, or by using
the [spacelift_stack_dependency](https://registry.terraform.io/providers/spacelift-io/spacelift/latest/docs/resources/stack_dependency) resource)

- Trigger the dependent Atlantis project

## Usage

```shell
atmos describe dependents [options]
```

:::tip
Run `atmos describe dependents --help` to see all the available options
:::

## Examples

```shell
atmos describe dependents test/test-component -s tenant1-ue2-test-1
atmos describe dependents test/test-component -s tenant1-ue2-dev --format yaml
atmos describe dependents test/test-component -s tenant1-ue2-test-1 -f yaml
atmos describe dependents test/test-component -s tenant1-ue2-test-1 --file dependents.json
atmos describe dependents test/test-component -s tenant1-ue2-test-1 --format yaml --file dependents.yaml
atmos describe dependents test/test-component -s tenant1-ue2-test-1 --query <yq-expression>
```

## Arguments

<dl>
    <dt>`component` (required)</dt>
    <dd>
        Atmos component.
    </dd>
</dl>

## Flags

<dl>
    <dt>`--stack` (alias `-s`)(required)</dt>
    <dd>
        Atmos stack.
    </dd>

    <dt>`--format` (alias `-f`)(optional)</dt>
    <dd>
        Output format: `json` or `yaml` (`json` is default).
    </dd>

    <dt>`--file` (optional)</dt>
    <dd>
        If specified, write the result to the file.
    </dd>

    <dt>`--query` (alias `-q`)(optional)</dt>
    <dd>
        Query the results of the command using YQ expressions.

        `atmos describe dependents <component> -s <stack> --query <yq-expression>`.

        For more details, refer to https://mikefarah.gitbook.io/yq.
    </dd>

    <dt>`--process-templates` (optional)</dt>
    <dd>
        Enable/disable processing of `Go` templates in Atmos stacks manifests when executing the command.
        If the flag is not provided, it's set to `true` by default.

        `atmos describe dependents <component> -s <stack> --process-templates=false`
    </dd>

    <dt>`--process-functions` (optional)</dt>
    <dd>
        Enable/disable processing of Atmos YAML functions in Atmos stacks manifests when executing the command.
        If the flag is not provided, it's set to `true` by default.

        `atmos describe dependents <component> -s <stack> --process-functions=false`
    </dd>

    <dt>`--skip` (optional)</dt>
    <dd>
        Skip processing a specific Atmos YAML function in Atmos stacks manifests when executing the command.
        To specify more than one function, use multiple `--skip` flags, or separate the functions with a comma:

        `atmos describe dependents <component> -s <stack> --skip=terraform.output --skip=include`

        `atmos describe dependents <component> -s <stack> --skip=terraform.output,include`
    </dd>
</dl>

## Output

The command outputs a list of objects (in JSON or YAML format).

Each object has the following schema:

```json
{
  "component": "....",
  "component_type": "....",
  "component_path": "....",
  "namespace": "....",
  "tenant": "....",
  "environment": "....",
  "stage": "....",
  "stack": "....",
  "stack_slug": "",
  "spacelift_stack": ".....",
  "atlantis_project": "....."
}
```

where:

- `component` - the dependent Atmos component

- `component_type` - the type of the dependent component (`terraform` or `helmfile`)

- `component_path` - the filesystem path to the `terraform` or `helmfile` component

- `namespace` - the `namespace` where the dependent Atmos component is provisioned

- `tenant` - the `tenant` where the dependent Atmos component is provisioned

- `environment` - the `environment` where the dependent Atmos component is provisioned

- `stage` - the `stage` where the dependent Atmos component is provisioned

- `stack` - the Atmos stack where the dependent Atmos component is provisioned

- `stack_slug` - the Atmos stack slug (concatenation of the Atmos stack and Atmos component)

- `spacelift_stack` - the dependent Spacelift stack. It will be included only if the Spacelift workspace is enabled for the dependent Atmos component
in the Atmos stack in the `settings.spacelift.workspace_enabled` section (either directly in the component's `settings.spacelift.workspace_enabled`
section or via inheritance)

- `atlantis_project` - the dependent Atlantis project name. It will be included only if the Atlantis integration is configured in
the `settings.atlantis` section in the stack manifest. Refer to [Atlantis Integration](/integrations/atlantis) for more details

:::note

Abstract Atmos components (`metadata.type` is set to `abstract`) are not included in the output since they serve as blueprints for other
Atmos components and are not meant to be provisioned.

:::

## Output Example

<Terminal command="atmos describe dependents test/test-component -s tenant1-ue2-test-1">
    ```json
    [
    {
        "component": "top-level-component2",
        "component_type": "terraform",
        "component_path": "tests/fixtures/scenarios/complete/components/terraform/top-level-component1",
        "namespace": "cp",
        "tenant": "tenant1",
        "environment": "ue2",
        "stage": "test-1",
        "stack": "tenant1-ue2-test-1",
        "stack_slug": "tenant1-ue2-dev-top-level-component2",
        "atlantis_project": "tenant1-ue2-test-1-top-level-component2"
    },
    {
        "component": "top-level-component1",
        "component_type": "terraform",
        "component_path": "tests/fixtures/scenarios/complete/components/terraform/top-level-component1",
        "namespace": "cp",
        "tenant": "tenant1",
        "environment": "ue2",
        "stage": "dev",
        "stack": "tenant1-ue2-dev",
        "stack_slug": "tenant1-ue2-test-1-top-level-component1",
        "spacelift_stack": "tenant1-ue2-dev-top-level-component1",
        "atlantis_project": "tenant1-ue2-dev-top-level-component1"
    }
    ]
    ```
</Terminal>

---

## atmos describe stacks

import Screengrab from '@site/src/components/Screengrab'
import Intro from '@site/src/components/Intro'

<Intro>
Use this command to show the fully deep-merged configuration for all stacks and the components in the stacks.
</Intro>

<Screengrab title="atmos describe stacks --help" slug="atmos-describe-stacks--help" />

## Usage

Execute the `describe stacks` command like this:

```shell
atmos describe stacks [options]
```

This command shows configuration for stacks and components in the stacks.

:::tip
Run `atmos describe stacks --help` to see all the available options
:::

## Examples

```shell
atmos describe stacks
atmos describe stacks -s tenant1-ue2-dev
atmos describe stacks --file=stacks.yaml
atmos describe stacks --file=stacks.json --format=json
atmos describe stacks --components=infra/vpc
atmos describe stacks --components=echo-server,infra/vpc
atmos describe stacks --components=echo-server,infra/vpc --sections=none
atmos describe stacks --components=echo-server,infra/vpc --sections=none
atmos describe stacks --components=none --sections=metadata
atmos describe stacks --components=echo-server,infra/vpc --sections=vars,settings,metadata
atmos describe stacks --components=test/test-component-override-3 --sections=vars,settings,component,deps,inheritance --file=stacks.yaml
atmos describe stacks --components=test/test-component-override-3 --sections=vars,settings --format=json --file=stacks.json
atmos describe stacks --components=test/test-component-override-3 --sections=deps,vars -s=tenant2-ue2-staging
atmos describe stacks --process-templates=false
atmos describe stacks --process-functions=false
atmos describe stacks --skip=terraform.output
atmos describe stacks --skip=terraform.output --skip=include
atmos describe stacks --skip=include,eval
atmos describe stacks --query <yq-expression>
```

:::tip
Use the `--query` flag (shorthand `-q`) to filter the output.
:::

## Flags

<dl>
  <dt>`--stack` / `-s` (optional)</dt>
  <dd>Filter by a specific stack.Supports names of the top-level stack manifests(including subfolder paths),and Atmos stack names (derived from the context vars).</dd>

  <dt>`--file` (optional)</dt>
  <dd>If specified, write the result to the file.</dd>

  <dt>`--format` (optional)</dt>
  <dd>Specify the output format: `yaml` or `json` (`yaml` is default).</dd>

  <dt>`--components` (optional)</dt>
  <dd>Filter by specific Atmos components(comma-separated string of component names).</dd>

  <dt>`--component-types` (optional)</dt>
  <dd>Filter by specific component types: `terraform` or `helmfile`.</dd>

  <dt>`--sections` (optional)</dt>
  <dd>Output only the specified component sections.Available component sections: `backend`, `backend_type`, `component`, `deps`,`env`, `inheritance`, `metadata`, `remote_state_backend`,`remote_state_backend_type`, `settings`, `vars`.</dd>

  <dt>`--process-templates` (optional)</dt>
  <dd>Enable/disable processing of all `Go` templatesin Atmos stacks manifests when executing the command.Use the flag to see the stack configurationsbefore and after the templates are processed.If the flag is not provided, it's set to `true` by default.`atmos describe stacks --process-templates=false`.</dd>

  <dt>`--process-functions` (optional)</dt>
  <dd>Enable/disable processing of all Atmos YAML functionsin Atmos stacks manifests when executing the command.Use the flag to see the stack configurationsbefore and after the functions are processed.If the flag is not provided, it's set to `true` by default.`atmos describe stacks --process-functions=false`.</dd>

  <dt>`--skip` (optional)</dt>
  <dd>Skip processing a specific Atmos YAML functionin Atmos stacks manifests when executing the command.To specify more than one function,use multiple `--skip` flags, or separate the functions with a comma:`atmos describe stacks --skip=terraform.output --skip=include``atmos describe stacks --skip=terraform.output,include`.</dd>

  <dt>`--query` / `-q` (optional)</dt>
  <dd>Query the results of the command using `yq` expressions.`atmos describe stacks --query <yq-expression>`.For more details, refer to https://mikefarah.gitbook.io/yq.</dd>
</dl>

---

## atmos describe workflows

import Terminal from '@site/src/components/Terminal'
import Screengrab from '@site/src/components/Screengrab'
import Intro from '@site/src/components/Intro'

<Intro>
Use this command to show all configured Atmos workflows.
</Intro>

<Screengrab title="atmos describe workflows --help" slug="atmos-describe-workflows--help" />

## Usage

Execute the `describe workflows` command like this:

```shell
atmos describe workflows [options]
```

:::tip
Run `atmos describe workflows --help` to see all the available options
:::

## Examples

```shell
atmos describe workflows
atmos describe workflows --output map
atmos describe workflows -o list
atmos describe workflows -o all
atmos describe workflows -o list --format json
atmos describe workflows -o all -f yaml
atmos describe workflows -f json
atmos describe workflows --query <yq-expression>
```

## Flags

<dl>
  <dt>`--format` / `-f` (optional)</dt>
  <dd>Specify the output format: `yaml` or `json` (`yaml` is default).</dd>

  <dt>`--output` / `-o` (optional)</dt>
  <dd>Specify the output type: `list`, `map` or `all` (`list` is default).</dd>

  <dt>`--query` / `-q` (optional)</dt>
  <dd>Query the results of the command using `yq` expressions.`atmos describe workflows --query <yq-expression>`.For more details, refer to https://mikefarah.gitbook.io/yq.</dd>
</dl>

When the `--output list` flag is passed (default), the output of the command is a list of objects. Each object has the
following schema:

- `file` - the workflow manifest file name
- `workflow` - the name of the workflow defined in the workflow manifest file

For example:

```shell
atmos describe workflows
atmos describe workflows -o list
```

```yaml
- file: compliance.yaml
  workflow: deploy/aws-config/global-collector
- file: compliance.yaml
  workflow: deploy/aws-config/superadmin
- file: compliance.yaml
  workflow: destroy/aws-config/global-collector
- file: compliance.yaml
  workflow: destroy/aws-config/superadmin
- file: datadog.yaml
  workflow: deploy/datadog-integration
- file: helpers.yaml
  workflow: save/docker-config-json
- file: networking.yaml
  workflow: apply-all-components
- file: networking.yaml
  workflow: plan-all-vpc
- file: networking.yaml
  workflow: plan-all-vpc-flow-logs
```

When the `--output map` flag is passed, the output of the command is a map of workflow manifests to the lists of
workflows defined in each manifest.
For example:

```shell
atmos describe workflows -o map
```

```yaml
compliance.yaml:
  - deploy/aws-config/global-collector
  - deploy/aws-config/superadmin
  - destroy/aws-config/global-collector
  - destroy/aws-config/superadmin
datadog.yaml:
  - deploy/datadog-integration
helpers.yaml:
  - save/docker-config-json
networking.yaml:
  - apply-all-components
  - plan-all-vpc
  - plan-all-vpc-flow-logs
```

When the `--output all` flag is passed, the output of the command is a map of workflow manifests to the maps of all
workflow definitions. For example:

```shell
atmos describe workflows -o all
```

```yaml
networking.yaml:
  name: Networking & Logging
  description: Atmos workflows for managing VPCs and VPC Flow Logs
  workflows:
    apply-all-components:
      description: |
        Run 'terraform apply' on all components in all stacks
      steps:
        - command: terraform apply vpc-flow-logs-bucket -s plat-ue2-dev -auto-approve
        - command: terraform apply vpc -s plat-ue2-dev -auto-approve
        - command: terraform apply vpc-flow-logs-bucket -s plat-uw2-dev -auto-approve
        - command: terraform apply vpc -s plat-uw2-dev -auto-approve
        - command: terraform apply vpc-flow-logs-bucket -s plat-ue2-staging -auto-approve
        - command: terraform apply vpc -s plat-ue2-staging -auto-approve
        - command: terraform apply vpc-flow-logs-bucket -s plat-uw2-staging -auto-approve
        - command: terraform apply vpc -s plat-uw2-staging -auto-approve
        - command: terraform apply vpc-flow-logs-bucket -s plat-ue2-prod -auto-approve
        - command: terraform apply vpc -s plat-ue2-prod -auto-approve
        - command: terraform apply vpc-flow-logs-bucket -s plat-uw2-prod -auto-approve
        - command: terraform apply vpc -s plat-uw2-prod -auto-approve
    plan-all-vpc:
      description: |
        Run 'terraform plan' on all 'vpc' components in all stacks
      steps:
        - command: terraform plan vpc -s plat-ue2-dev
        - command: terraform plan vpc -s plat-uw2-dev
        - command: terraform plan vpc -s plat-ue2-staging
        - command: terraform plan vpc -s plat-uw2-staging
        - command: terraform plan vpc -s plat-ue2-prod
        - command: terraform plan vpc -s plat-uw2-prod
    plan-all-vpc-flow-logs:
      description: |
        Run 'terraform plan' on all 'vpc-flow-logs-bucket' components in all stacks
      steps:
        - command: terraform plan vpc-flow-logs-bucket -s plat-ue2-dev
        - command: terraform plan vpc-flow-logs-bucket -s plat-uw2-dev
        - command: terraform plan vpc-flow-logs-bucket -s plat-ue2-staging
        - command: terraform plan vpc-flow-logs-bucket -s plat-uw2-staging
        - command: terraform plan vpc-flow-logs-bucket -s plat-ue2-prod
        - command: terraform plan vpc-flow-logs-bucket -s plat-uw2-prod
validation.yaml:
  name: Validation
  description: Atmos workflows for VPCs and VPC Flow Logs validation
  workflows:
    validate-all-vpc:
      description: Validate all VPC components in all stacks
      steps:
        - command: validate component vpc -s plat-ue2-dev
        - command: validate component vpc -s plat-uw2-dev
        - command: validate component vpc -s plat-ue2-staging
        - command: validate component vpc -s plat-uw2-staging
        - command: validate component vpc -s plat-ue2-prod
        - command: validate component vpc -s plat-uw2-prod
    validate-all-vpc-flow-logs:
      description: Validate all VPC Flow Logs bucket components in all stacks
      steps:
        - command: validate component vpc-flow-logs-bucket -s plat-ue2-dev
        - command: validate component vpc-flow-logs-bucket -s plat-uw2-dev
        - command: validate component vpc-flow-logs-bucket -s plat-ue2-staging
        - command: validate component vpc-flow-logs-bucket -s plat-uw2-staging
        - command: validate component vpc-flow-logs-bucket -s plat-ue2-prod
        - command: validate component vpc-flow-logs-bucket -s plat-uw2-prod
```

:::tip
Use the [atmos workflow](/cli/commands/workflow) CLI command to execute an Atmos workflow
:::

---

## atmos describe

import Screengrab from '@site/src/components/Screengrab'
import DocCardList from '@theme/DocCardList';

<Screengrab title="atmos describe --help" slug="atmos-describe--help" />

## Subcommands

<DocCardList />

---

## atmos docs generate

import Screengrab from '@site/src/components/Screengrab'
import Intro from '@site/src/components/Intro'

<Intro>

Use this command to generate one of your documentation artifacts (e.g. a README) as defined by the **named** section under `docs.generate.<KEY>` in `atmos.yaml`.

Replace `<KEY>` with the name of the section you want to run (for example, `readme`, `release-notes`, etc.).

</Intro>

In `atmos.yaml`, you can define **one or more** documentation‐generation blocks under `docs.generate`.  Each top‐level key becomes a CLI argument:

```yaml
docs:
  generate:
    readme:
      base-dir: .
      input:
        - "./README.yaml"
      template: "https://.../README.md.gotmpl"
      output: "./README.md"
      terraform:
        source: src/
        enabled: false
        format: "markdown"
        show_providers: false
        show_inputs: true
        show_outputs: true
        sort_by: "name"
        hide_empty: false
        indent_level: 2

    release-notes:
      base-dir: .
      input:
        - "./CHANGELOG.yaml"
      template: "./release-notes.gotmpl"
      output: "./RELEASE_NOTES.md"
```

For each CLI argument the command combines all local or remote YAML files specified at `input` and template file then generates the documentation artifact at the respective `output` folder. In case the template contains `terraform_docs` key, e.g.

    ```yaml
{{- $data := (ds "config") -}}

{{ $data.name | default "Project Title" }}

{{ $data.description | default "No description provided." }}

{{ if has $data "extra_info" }}
Extra info: {{ $data.extra_info }}
{{ end }}

{{ if has $data "terraform_docs" }}
## Terraform Docs
{{ $data.terraform_docs }}
{{ end }}

    ```
the resultant file will also have a corresponding section rendered. By default `terraform.format` is set to `markdown table`, and can also be `markdown`, `tfvars hcl`, and `tfvars json`.

## Dynamic Keys

If you add a new key under docs.generate—say readme2 or release-notes —you simply pass that key to the CLI:

```shell
atmos docs generate readme2
atmos docs generate release-notes
```

## Usage

<Screengrab title="atmos docs generate --help" slug="atmos-docs-generate--help" />

```shell
atmos docs generate readme
```

## Supported Sources for README.yaml and template

### Local Sources

It supports the following local file sources:

  - Absolute paths

    ```yaml
    docs:
      generate:
        readme:
          input:
            - "/Users/me/Documents/README.yaml"
          template: "/Users/me/Documents/README.md.gotmpl"
    ```

  - Paths relative to the current working directory

    ```yaml
    docs:
      generate:
        readme:
          input:
            - "./README.yaml"
          template: "./README.md.gotmpl"
    ```

  - Paths relative to the `base_dir` defined in `atmos.yaml` CLI config file (then resolved as relative to cwd)

    ```yaml
    docs:
      generate:
        readme:
          input:
            - "terraform/README.yaml"
          template: "terraform/README.md.gotmpl"
    ```

### Remote Sources

To download remote files, Atmos uses [`go-getter`](https://github.com/hashicorp/go-getter)
(used by [Terraform](https://www.terraform.io/) for downloading modules)

---

## atmos docs

import Screengrab from '@site/src/components/Screengrab'
import DocCardList from '@theme/DocCardList';
import Intro from '@site/src/components/Intro'

<Intro>
Use this command to open the [Atmos docs](https://atmos.tools/)
</Intro>

<Screengrab title="atmos docs --help" slug="atmos-docs--help" />

## Usage

When run on its own, the `atmos docs` command opens [Atmos docs](https://atmos.tools/), but it can also display documentation for specified components. For example:

```shell
atmos docs
atmos docs vpc
atmos docs eks/cluster
```

## Subcommands
<DocCardList />

---

## atmos helmfile generate varfile

import Screengrab from '@site/src/components/Screengrab'
import Terminal from '@site/src/components/Terminal'
import Intro from '@site/src/components/Intro'

<Intro>
Use this command to generate a varfile for a `helmfile` component in a stack.
</Intro>

<Screengrab title="atmos helmfile generate varfile --help" slug="atmos-helmfile-generate-varfile--help" />

## Usage

Execute the `helmfile generate varfile` command like this:

```shell
atmos helmfile generate varfile <component> -s <stack> [options]
```

This command generates a varfile for a `helmfile` component in a stack.

:::tip
Run `atmos helmfile generate varfile --help` to see all the available options
:::

## Examples

```shell
atmos helmfile generate varfile echo-server -s tenant1-ue2-dev
atmos helmfile generate varfile echo-server -s tenant1-ue2-dev
atmos helmfile generate varfile echo-server -s tenant1-ue2-dev -f vars.yaml
atmos helmfile generate varfile echo-server --stack tenant1-ue2-dev --file=vars.yaml
```

## Arguments

<dl>
  <dt>`component` (required)</dt>
  <dd>Atmos helmfile component.</dd>
</dl>

## Flags

<dl>
  <dt>`--stack` / `-s` (required)</dt>
  <dd>Atmos stack.</dd>

  <dt>`--file` / `-f` (optional)</dt>
  <dd>File name to write the varfile to.If not specified, the varfile name is generated automatically from the context.</dd>

  <dt>`--dry-run` (optional)</dt>
  <dd>Dry run.</dd>
</dl>

---

## atmos helmfile

import Screengrab from '@site/src/components/Screengrab'
import Terminal from '@site/src/components/Terminal'
import DocCardList from '@theme/DocCardList';
import Intro from '@site/src/components/Intro'

<Intro>
Use these subcommands to run `helmfile` commands.
</Intro>

<Screengrab title="atmos helmfile --help" slug="atmos-helmfile--help" />

# Usage

The `helmfile` integration passes through all arguments to the `helmfile` command.

Executes `helmfile` commands.

```shell
atmos helmfile <command> <component> -s <stack> [options]
atmos helmfile <command> <component> --stack <stack> [options]
```

:::info
Atmos supports all `helmfile` commands and options described in [Helmfile CLI reference](https://github.com/helmfile/helmfile#cli-reference).

In addition, the `component` argument and `stack` flag are required to generate variables for the component in the stack.
:::

**Additions and differences from native Helmfile:**

- `atmos helmfile generate varfile` command generates a varfile for the component in the stack

- `atmos helmfile` commands support [GLOBAL OPTIONS](https://github.com/roboll/helmfile#cli-reference) using the command-line flag `--global-options`.
  Usage: `atmos helmfile <command> <component> -s <stack> [command options] [arguments] --global-options="--no-color --namespace=test"`

- before executing the `helmfile` commands, Atmos runs `aws eks update-kubeconfig` to read kubeconfig from the EKS cluster and use it to
  authenticate with the cluster. This can be disabled in `atmos.yaml` CLI config by setting `components.helmfile.use_eks` to `false`

- double-dash `--` can be used to signify the end of the options for Atmos and the start of the additional native arguments and flags for
  the `helmfile` commands.

:::tip
Run `atmos helmfile --help` to see all the available options
:::

## Examples

```shell
atmos helmfile diff echo-server -s tenant1-ue2-dev
atmos helmfile diff echo-server -s tenant1-ue2-dev --redirect-stderr /dev/null

atmos helmfile apply echo-server -s tenant1-ue2-dev
atmos helmfile apply echo-server -s tenant1-ue2-dev --redirect-stderr /dev/stdout

atmos helmfile sync echo-server --stack tenant1-ue2-dev
atmos helmfile sync echo-server --stack tenant1-ue2-dev --redirect-stderr ./errors.txt

atmos helmfile destroy echo-server --stack=tenant1-ue2-dev
atmos helmfile destroy echo-server --stack=tenant1-ue2-dev --redirect-stderr /dev/stdout
```

## Arguments

<dl>
  <dt>`component` (required)</dt>
  <dd>Atmos component.</dd>
</dl>

## Flags

<dl>
  <dt>`--stack` / `-s` (required)</dt>
  <dd>Atmos stack.</dd>

  <dt>`--dry-run` (optional)</dt>
  <dd>Dry run.</dd>

  <dt>`--redirect-stderr` (optional)</dt>
  <dd>File descriptor to redirect `stderr` to.Errors can be redirected to any file or any standard file descriptor(including `/dev/null`).</dd>
</dl>

:::note

All native `helmfile` flags, command options, and arguments are supported

:::

## Subcommands

<DocCardList/>

---

## atmos help

import Screengrab from '@site/src/components/Screengrab'
import Terminal from '@site/src/components/Terminal'

<Screengrab title="atmos help" slug="atmos--help" />

## Usage

The `atmos --help` and `atmos -h` commands show help for all Atmos CLI commands.

From time to time, Atmos will check for a newer release and let you know if one is available.
Please see the [`atmos version`](/cli/commands/version) documentation to configure this behavior.

```shell
atmos help
atmos --help
atmos -h
```

## Examples

```shell
atmos help               # Starts an interactive help UI in the terminal
atmos --help             # Shows help for all Atmos CLI commands
atmos -h                 # Shows help for all Atmos CLI commands
atmos atlantis --help    # Executes 'atlantis' commands
atmos aws --help         # Executes 'aws' commands
atmos completion --help  # Executes 'completion' commands
atmos describe --help    # Executes 'describe' commands
atmos terraform --help   # Executes 'terraform' commands
atmos helmfile --help    # Executes 'helmfile' commands
atmos packer --help      # Executes 'packer' commands
atmos validate --help    # Executes 'validate' commands
atmos vendor --help      # Executes 'vendor' commands
atmos workflow --help    # Executes 'workflow' commands
```

## Screenshots

The `atmos help` starts an interactive help UI in the terminal:

<Terminal title="atmos help (interactive)">
![`atmos help` command](/img/cli/help/atmos-help-command.png)
</Terminal>

---

## atmos list components

import Screengrab from '@site/src/components/Screengrab'

:::note purpose
Use this command to list all Atmos components or Atmos components in a specified stack.
:::

<Screengrab title="atmos list components --help" slug="atmos-list-components--help" />

## Usage

Execute the `list components` command like this:

```shell
atmos list components
```

This command lists Atmos components in a specified stack.

```shell
atmos list components -s <stack>
```

:::tip
Run `atmos list components --help` to see all the available options
:::

## Examples

```shell
atmos list components
atmos list components -s tenant1-ue2-dev
```

### Custom Columns for Components

This configuration customizes the output of `atmos list components`:

```yaml
# In atmos.yaml
components:
  list:
    columns:
      - name: Component Name
        value: "{{ .component_name }}"
      - name: Component Type
        value: "{{ .component_type }}"
      - name: Component Path
        value: "{{ .component_path }}"
```
Running `atmos list components` will produce a table with these custom columns.

## Flags

<dl>
  <dt>`--stack` / `-s` (optional)</dt>
  <dd>Atmos stack.</dd>
</dl>

---

## atmos list metadata


The `atmos list metadata` command displays component metadata across all stacks.

## Usage

```shell
atmos list metadata [flags]
```

## Description

The `atmos list metadata` command helps you inspect component metadata across different stacks. It provides a tabular view where:

- Each column represents a stack (e.g., dev-ue1, staging-ue1, prod-ue1)
- Each row represents a key in the component's metadata
- Cells contain the metadata values for each key in each stack

The command is particularly useful for:
- Comparing component metadata across different environments
- Verifying component types and versions across stacks
- Understanding component organization patterns across your infrastructure

## Flags

<dl>
  <dt>`--query string`</dt>
  <dd>JMESPath query to filter metadata (default: `.metadata`)</dd>
  <dt>`--max-columns int`</dt>
  <dd>Maximum number of columns to display (default: `50`)</dd>
  <dt>`--format string`</dt>
  <dd>Output format: `table`, `json`, `yaml`, `csv`, `tsv` (default: `table`)</dd>
  <dt>`--delimiter string`</dt>
  <dd>Delimiter for csv/tsv output (default: `,` for csv, `\t` for tsv)</dd>
  <dt>`--stack string`</dt>
  <dd>Filter by stack pattern (e.g., `*-dev-*`, `prod-*`, `*-{dev,staging}-*`)</dd>
</dl>

## Examples

List all metadata:
```shell
atmos list metadata
```

List metadata for specific stacks:
```shell
# List metadata for dev stacks
atmos list metadata --stack '*-dev-*'

# List metadata for production stacks
atmos list metadata --stack 'prod-*'
```

List specific metadata using JMESPath queries:
```shell
# Query component names
atmos list metadata --query '.metadata.component'

# Query component types
atmos list metadata --query '.metadata.type'

# Query component versions
atmos list metadata --query '.metadata.version'
```

Output in different formats:
```shell
# JSON format for machine processing
atmos list metadata --format json

# YAML format for configuration files
atmos list metadata --format yaml

# CSV format for spreadsheet compatibility
atmos list metadata --format csv

# TSV format with tab delimiters
atmos list metadata --format tsv
```

### Custom Column using Stack Name

You can use available variables like `.stack_name` in your column definitions:

```yaml
# In atmos.yaml, under the appropriate scope (values, vars, settings, or metadata)
list:
  columns:
    - name: "Stack"
      value: "{{ .stack_name }}"
    - name: "Metadata"
      value: "{{ .key }}"
    - name: "Value"
      value: "{{ .value }}"
```

## Example Output

```shell
> atmos list metadata
┌──────────────┬──────────────┬──────────────┬──────────────┐
│              │   dev-ue1    │  staging-ue1 │   prod-ue1   │
├──────────────┼──────────────┼──────────────┼──────────────┤
│ component    │ vpc          │ vpc          │ vpc          │
│ type         │ terraform    │ terraform    │ terraform    │
│ version      │ 1.0.0        │ 1.0.0        │ 1.0.0        │
└──────────────┴──────────────┴──────────────┴──────────────┘
```

:::tip
- For wide tables, try using more specific queries or reduce the number of stacks
- Stack patterns support glob matching (e.g., `*-dev-*`, `prod-*`, `*-{dev,staging}-*`)
- Metadata is typically found under component configurations
:::

---

## atmos list settings


The `atmos list settings` command displays component settings across all stacks.

## Usage

```shell
atmos list settings [flags]
```

## Description

The `atmos list settings` command helps you inspect component settings across different stacks. It provides a tabular view where:

- Each column represents a stack (e.g., dev-ue1, staging-ue1, prod-ue1)
- Each row represents a key in the component's settings
- Cells contain the settings values for each key in each stack (only scalars at this time)

The command is particularly useful for:
- Comparing component settings across different environments
- Verifying settings are configured correctly in each stack
- Understanding component configuration patterns across your infrastructure

## Flags

<dl>
  <dt>`--query string`</dt>
  <dd>Dot-notation path query to filter settings (e.g., `.settings.templates`). Uses a simplified path syntax, not full JMESPath.</dd>
  <dt>`--max-columns int`</dt>
  <dd>Maximum number of columns to display (default: `50`)</dd>
  <dt>`--format string`</dt>
  <dd>Output format: `table`, `json`, `yaml`, `csv`, `tsv` (default: `table`)</dd>
  <dt>`--delimiter string`</dt>
  <dd>Delimiter for csv/tsv output (default: `,` for csv, `\t` for tsv)</dd>
  <dt>`--stack string`</dt>
  <dd>Filter by stack by wildcard pattern (e.g., `*-dev-*`, `prod-*`, `*-{dev,staging}-*`)</dd>
</dl>

## Examples

List all settings:
```shell
atmos list settings
```

List settings for specific stacks:
```shell
# List settings for dev stacks
atmos list settings --stack '*-dev-*'

# List settings for production stacks
atmos list settings --stack 'prod-*'
```

List specific settings using path queries:
```shell
# Query template settings
atmos list settings --query '.settings.templates'

# Query validation settings
atmos list settings --query '.settings.validation'

# Query specific template configurations
atmos list settings --query '.settings.templates.gomplate'
```

Output in different formats:
```shell
# JSON format for machine processing
atmos list settings --format json

# YAML format for configuration files
atmos list settings --format yaml

# CSV format for spreadsheet compatibility
atmos list settings --format csv

# TSV format with tab delimiters
atmos list settings --format tsv
```

### Custom Column using Stack Name

You can use available variables like `.stack_name` in your column definitions:

```yaml
# In atmos.yaml, under the appropriate scope (values, vars, settings, or metadata)
list:
  columns:
    - name: "Stack"
      value: "{{ .stack_name }}"
    - name: "Setting"
      value: "{{ .key }}"
    - name: "Value"
      value: "{{ .value }}"
```

## Example Output

```shell
> atmos list settings
┌──────────────┬──────────────┬──────────────┬──────────────┐
│              │   dev-ue1    │  staging-ue1 │   prod-ue1   │
├──────────────┼──────────────┼──────────────┼──────────────┤
│ templates    │ {...}        │ {...}        │ {...}        │
│ validation   │ {...}        │ {...}        │ {...}        │
└──────────────┴──────────────┴──────────────┴──────────────┘
```

:::tip
- For wide tables, try using more specific queries or reduce the number of stacks
- Stack patterns support glob matching (e.g., `*-dev-*`, `prod-*`, `*-{dev,staging}-*`)
- Settings are typically found under component configurations
:::

---

## atmos list stacks

import Screengrab from '@site/src/components/Screengrab'
import Terminal from '@site/src/components/Terminal'
import Intro from '@site/src/components/Intro'

<Intro>
Use this command to list Atmos stacks.
</Intro>

<Screengrab title="atmos list stacks --help" slug="atmos-list-stacks--help" />

## Usage

Execute the `list stacks` command like this:

```shell
atmos list stacks
```

To view all stacks for a provided component, execute the `list stacks` command like this:

```shell
atmos list stacks -c <component>
```

:::tip
Run `atmos list stacks --help` to see all the available options
:::

## Examples

```shell
atmos list stacks
atmos list stacks -c vpc
```

### Customizing Output Columns

This configuration customizes the output of `atmos list stacks`:

```yaml
# In atmos.yaml
stacks:
  list:
    format: table
    columns:
      - name: Stack Name
        value: "{{ .stack_name }}"
      - name: Configuration Path
        value: "{{ .stack_path }}"
```

When you run `atmos list stacks`, the output table will have columns titled "Stack Name" and "Configuration Path".

## Flags

<dl>
  <dt>`--component` / `-c` (optional)</dt>
  <dd>Atmos component.</dd>
</dl>

---

## atmos list values


The `atmos list values` command displays component values across all stacks where the component is used.

## Usage

```shell
atmos list values [component] [flags]
```

## Description

The `atmos list values` command helps you inspect component values across different stacks. It provides a tabular view where:

- Each column represents a stack (e.g., dev-ue1, staging-ue1, prod-ue1)
- Each row represents a key in the component's configuration
- Cells contain the values for each key in each stack

The command is particularly useful for:
- Comparing component configurations across different environments
- Verifying values are set correctly in each stack
- Understanding how a component is configured across your infrastructure

## Flags

<dl>
  <dt>`--query string`</dt>
  <dd>Dot-notation path query to filter values (e.g., `.vars.enabled`). Uses a simplified path syntax, not full JMESPath.</dd>
  <dt>`--abstract`</dt>
  <dd>Include abstract components in the output</dd>
  <dt>`--max-columns int`</dt>
  <dd>Maximum number of columns to display (default: `10`)</dd>
  <dt>`--format string`</dt>
  <dd>Output format: `table`, `json`, `csv`, `tsv` (default: `table`)</dd>
  <dt>`--delimiter string`</dt>
  <dd>Delimiter for csv/tsv output (default: `,` for csv, `\t` for tsv)</dd>
</dl>

## Examples

List all values for a component:
```shell
atmos list values vpc
```

List only variables for a component (using the alias):
```shell
atmos list vars vpc
```

List values with a custom path query:
```shell
# Query specific variables
atmos list values vpc --query .vars.enabled

# Query environment settings
atmos list values vpc --query .vars.environment

# Query network configuration
atmos list values vpc --query .vars.ipv4_primary_cidr_block
```

Include abstract components:
```shell
atmos list values vpc --abstract
```

Limit the number of columns:
```shell
atmos list values vpc --max-columns 5
```

Output in different formats:
```shell
# JSON format for machine processing
atmos list values vpc --format json

# CSV format for spreadsheet compatibility
atmos list values vpc --format csv

# TSV format with tab delimiters
atmos list values vpc --format tsv

# Note: Use JSON or CSV formats when dealing with wide datasets
# The table format will show a width error if the data is too wide for your terminal
```

### Custom Column using Stack Name

You can use available variables like `.stack_name` in your column definitions:

```yaml
# In atmos.yaml, under the appropriate scope (values, vars, settings, or metadata)
list:
  columns:
    - name: "Stack"
      value: "{{ .stack_name }}"
    - name: "Key"
      value: "{{ .key }}"
    - name: "Value"
      value: "{{ .value }}"
```

## Example Output

```shell
> atmos list vars vpc
┌──────────────┬──────────────┬──────────────┬──────────────┐
│              │   dev-ue1    │  staging-ue1 │   prod-ue1   │
├──────────────┼──────────────┼──────────────┼──────────────┤
│ enabled      │ true         │ true         │ true         │
│ name         │ dev-vpc      │ staging-vpc  │ prod-vpc     │
│ cidr_block   │ 10.0.0.0/16  │ 10.1.0.0/16  │ 10.2.0.0/16  │
│ environment  │ dev          │ staging      │ prod         │
│ namespace    │ example      │ example      │ example      │
│ stage        │ dev          │ staging      │ prod         │
│ region       │ us-east-1    │ us-east-1    │ us-east-1    │
└──────────────┴──────────────┴──────────────┴──────────────┘
```

### Nested Object Display

When listing values that contain nested objects:

1. In table format, nested objects appear as `{...}` placeholders
2. Use `--format json` or `--format yaml` to see the complete nested structure
3. You can query specific nested paths using the dot notation: `--query .vars.tags.Environment`

Example JSON output with nested objects:
```json
{
  "dev-ue1": {
    "cidr_block": "10.0.0.0/16",
    "tags": {
      "Environment": "dev",
      "Team": "devops"
    },
    "subnets": [
      "10.0.1.0/24",
      "10.0.2.0/24"
    ]
  }
}
```

## Related Commands

- [atmos list components](/cli/commands/list/components) - List available components
- [atmos describe component](/cli/commands/describe/component) - Show detailed information about a component

---

## atmos list vars


The `atmos list vars` command displays component variables across all stacks where the component is used.

## Usage

```shell
atmos list vars <component> [flags]
```

## Description

The `atmos list vars` command helps you inspect component variables across different stacks. It provides a tabular view where:

- Each column represents a stack (e.g., dev-ue1, staging-ue1, prod-ue1)
- Each row represents a variable in the component's configuration
- Cells contain the variable values for each stack

This command is an alias for `atmos list values --query .vars` and is useful for:
- Comparing component variables across different environments
- Verifying configuration consistency across stacks
- Troubleshooting configuration issues

## Arguments

<dl>
  <dt>`component`</dt>
  <dd>The component to list variables for</dd>
</dl>

## Flags

<dl>
  <dt>`--query string`</dt>
  <dd>Filter the results using YQ expressions (default: `.vars`)</dd>
  <dt>`--abstract`</dt>
  <dd>Include abstract components</dd>
  <dt>`--max-columns int`</dt>
  <dd>Maximum number of columns to display (default: `50`)</dd>
  <dt>`--format string`</dt>
  <dd>Output format: `table`, `json`, `yaml`, `csv`, `tsv` (default: `table`)</dd>
  <dt>`--delimiter string`</dt>
  <dd>Delimiter for csv/tsv output (default: `,` for csv, `\t` for tsv)</dd>
  <dt>`--stack string`</dt>
  <dd>Filter by stack pattern (e.g., `*-dev-*`, `prod-*`, `*-{dev,staging}-*`)</dd>
</dl>

## Examples

List all variables for a component:
```shell
atmos list vars vpc
```

List specific variables using query:
```shell
# List specific variable
atmos list vars vpc --query .vars.tags

# List a nested variable
atmos list vars vpc --query .vars.settings.vpc
```

Filter by stack pattern:
```shell
# List variables for dev stacks
atmos list vars vpc --stack '*-dev-*'

# List variables for production stacks
atmos list vars vpc --stack 'prod-*'
```

Output in different formats:
```shell
# JSON format for machine processing
atmos list vars vpc --format json

# YAML format for configuration files
atmos list vars vpc --format yaml

# CSV format for spreadsheet compatibility
atmos list vars vpc --format csv

# TSV format with tab delimiters
atmos list vars vpc --format tsv
```

Include abstract components:
```shell
atmos list vars vpc --abstract
```

Filter by stack and specific variables:
```shell
atmos list vars vpc --stack '*-ue2-*' --query .vars.region
```

### Custom Column using Stack Name

You can use available variables like `.stack_name` in your column definitions:

```yaml
# In atmos.yaml, under the appropriate scope (values, vars, settings, or metadata)
list:
  columns:
    - name: "Stack"
      value: "{{ .stack_name }}"
    - name: "Variable"
      value: "{{ .key }}"
    - name: "Value"
      value: "{{ .value }}"
```

## Example Output

```shell
> atmos list vars vpc
┌─────────────┬──────────────┬──────────────┬──────────────┐
│             │   dev-ue1    │  staging-ue1 │   prod-ue1   │
├─────────────┼──────────────┼──────────────┼──────────────┤
│ name        │ platform-vpc │ platform-vpc │ platform-vpc │
│ region      │ us-east-1    │ us-east-1    │ us-east-1    │
│ environment │ dev          │ staging      │ prod         │
└─────────────┴──────────────┴──────────────┴──────────────┘
```

:::tip
- For wide tables, try using more specific queries or reduce the number of stacks
- Stack patterns support glob matching (e.g., `*-dev-*`, `prod-*`, `*-{dev,staging}-*`)
- Use `--abstract` to include abstract components in the results
:::

---

## atmos list workflows


The `atmos list workflows` command displays all Atmos workflows defined in your project.

## Usage

```shell
atmos list workflows [flags]
```

## Description

The `atmos list workflows` command helps you inspect all Atmos workflows defined in your project's workflow manifests. It provides a tabular view where:

- Each row represents a workflow
- Columns show the file, workflow name, and description

This command is useful for:
- Getting an overview of all available workflows
- Finding workflows for specific tasks
- Understanding workflow organization in your project

## Flags

<dl>
  <dt>`--file, -f string`</dt>
  <dd>Filter workflows by file (e.g., `atmos list workflows -f workflow1`)</dd>
  <dt>`--format string`</dt>
  <dd>Output format: `table`, `json`, `yaml`, `csv`, `tsv` (default: `table`)</dd>
  <dt>`--delimiter string`</dt>
  <dd>Delimiter for csv/tsv output (default: `\t`)</dd>
</dl>

## Examples

List all workflows:
```shell
atmos list workflows
```

Filter workflows by file:
```shell
atmos list workflows -f networking.yaml
```

Output in different formats:
```shell
# JSON format for machine processing
atmos list workflows --format json

# YAML format for configuration files
atmos list workflows --format yaml

# CSV format for spreadsheet compatibility
atmos list workflows --format csv

# TSV format with tab delimiters
atmos list workflows --format tsv
```

Specify delimiter for CSV output:
```shell
atmos list workflows --format csv --delimiter ','
```

## Example Output

```shell
> atmos list workflows
┌────────────────┬─────────────────────────────┬─────────────────────────────────────────┐
│      File      │          Workflow           │               Description               │
├────────────────┼─────────────────────────────┼─────────────────────────────────────────┤
│ compliance.yaml│ deploy/aws-config/global    │ Deploy AWS Config Global                │
│ networking.yaml│ apply-all-components        │ Apply all networking components         │
│ networking.yaml│ plan-all-vpc                │ Plan all VPC changes                    │
│ datadog.yaml   │ deploy/datadog-integration  │ Deploy Datadog integration              │
└────────────────┴─────────────────────────────┴─────────────────────────────────────────┘
```

:::tip
- Use the `--file` flag to filter workflows from a specific manifest file
- The `describe workflows` command provides more detailed information about workflows
:::

## Examples

### Custom Columns for Workflows

This configuration customizes the output of `atmos list workflows`:

```yaml
# In atmos.yaml
workflows:
  list:
    columns:
      - name: Workflow
        value: "{{ .workflow_name }}"
      - name: Definition File
        value: "{{ .workflow_file }}"
      - name: Description
        value: "{{ .workflow_description }}"
```
Running `atmos list workflows` will display these columns.

## Examples

### Custom Columns for Workflows

This configuration customizes the output of `atmos list workflows`:

```yaml
# In atmos.yaml
workflows:
  list:
    columns:
      - name: Workflow
        value: "{{ .workflow_name }}"
      - name: Definition File
        value: "{{ .workflow_file }}"
      - name: Description
        value: "{{ .workflow_description }}"
```
Running `atmos list workflows` will display these columns.

---

## atmos list

import Screengrab from '@site/src/components/Screengrab';
import DocCardList from '@theme/DocCardList';
import Intro from '@site/src/components/Intro'

<Intro>
Use these subcommands to list sections of Atmos configurations.
</Intro>

<Screengrab title="atmos list --help" slug="atmos-list--help" />

Atmos provides a powerful feature to customize the columns displayed by various `atmos list` commands (e.g., `atmos list stacks`, `atmos list components`, `atmos list workflows`). This allows you to tailor the tabular output to show precisely the information you need for different contexts.

Column customization is configured within your `atmos.yaml` file using Go template expressions, enabling dynamic values based on stack, component, or workflow data. This guide explains how to configure and use this feature.

## Subcommands

<DocCardList />

## Supported List Commands

| Command                    | Description                                                                                   |
|---------------------------|-----------------------------------------------------------------------------------------------|
| `atmos list stacks`       | Lists all defined **stacks** in your project. A *stack* is a named configuration representing a deployment environment (e.g., `dev/us-east-1`, `prod/eu-west-1`). |
| `atmos list components`   | Lists all available **components** (Terraform, Helmfile, etc.) defined in the project. Components are reusable infrastructure building blocks. |
| `atmos list workflows`    | Lists all defined **workflows**, which are custom command sequences defined in `atmos.yaml` to streamline repetitive tasks. |
| `atmos list values`       | Displays the fully resolved **configuration values** for a specified component in a stack, after inheritance and imports are applied. |
| `atmos list vars`         | Lists the **Terraform `vars`** (input variables) that will be passed to a component for a given stack. Useful for debugging variable resolution. |
| `atmos list settings`     | Shows the **`settings` block**, typically used for configuring a component’s behavior (e.g., module version, backend type). |
| `atmos list metadata`     | Displays the **`metadata` block** associated with a component in a stack, including attributes like `stage`, `tenant`, `environment`, and `namespace`. |

You can define custom columns for each of these commands individually in your `atmos.yaml`.

## How Column Customization Works

To customize columns for a specific `list` command, navigate to the relevant section (e.g., `stacks`, `components`, `workflows`) in your `atmos.yaml` configuration file. Within that section, define a `list` block.

Inside the `list` block:
1.  Specify the output `format` (optional, defaults to `table`). Other options include `json`, `yaml`, `csv`, `tsv`.
2.  Define a `columns` array. Each element in this array represents a column in the output table and must have:
    *   `name`: The string that will appear as the column header.
    *   `value`: A Go template string that dynamically determines the value for each row in that column.

**Example Structure:**
```yaml
# In atmos.yaml
stacks: # Or components, workflows, etc.
  list:
    format: table # Optional
    columns:
      - name: "Header 1"
        value: "{{ .some_template_variable }}"
      - name: "Header 2"
        value: "Static Text or {{ .another_variable }}"
      # ... more columns
```

## YAML Template Syntax

The `value` field in each column definition supports Go templates. The available variables within the template depend on the specific `atmos list` command being customized:

### For `atmos list stacks`:
```yaml
{{ .stack_name }}  # Name of the stack
{{ .stack_path }}  # Filesystem path to the stack configuration file
```

### For `atmos list components`:
```yaml
{{ .component_name }}  # Name of the component
{{ .component_type }}  # Type of the component (e.g., terraform, helmfile)
{{ .component_path }}  # Filesystem path to the component directory
```

### For `atmos list workflows`:
```yaml
{{ .name }}      # The name of the workflow
{{ .file }}      # The manifest name
{{ .description }}  # The description provided for the workflow
```

### For `atmos list values`, `atmos list vars`, `atmos list settings`, and `atmos list metadata`:
```yaml
{{ .stack_name }}  # Name of the stack context
{{ .key }}         # The key or property name being listed
{{ .value }}       # The corresponding value for the key
```

## Full Reference: atmos.yaml Structure

Here's the general structure for defining custom list columns in `atmos.yaml`:

```yaml
<command_scope>: # e.g., stacks, components, workflows, values, vars, settings, metadata
  list:
    format: table|json|csv|yaml|tsv  # Optional, default is 'table'
    columns:
      - name: "<Your Column Header>"
        value: "<Go template string using available variables>"
      # ... add more column definitions as needed
```

- Replace `<command_scope>` with the specific scope corresponding to the `atmos list` command you want to customize (e.g., `stacks` for `atmos list stacks`).
- The `columns` array is mandatory if you want to override the default columns. If `columns` is omitted, the command uses its default output columns.

### Custom Columns for Workflows

```yaml
# In atmos.yaml
workflows:
  list:
    columns:
      - name: Workflow
        value: "{{ .name }}"  # Corresponds to the workflow key in the manifest
      - name: Manifest Name
        value: "{{ .file }}"   # Corresponds to the 'name' field within the manifest file
      - name: Description
        value: "{{ .description }}" # Corresponds to the 'description' field for the workflow
```

:::info
Note that `{{ .file }}` in this context refers to the value of the top-level `name` attribute within the workflow manifest file itself, not the path to the file.
:::

## Display Behavior

### TTY vs Non-TTY Output

The appearance of the output table depends on whether `atmos` detects an interactive terminal (TTY) or not:

- **TTY Output (e.g., running in your terminal)**
  - Displays a formatted table with borders and styling.
  - Attempts to fit within the terminal width.
  - Uses standard padding between columns (TableColumnPadding = 3).
  - Defaults to `format: table` if not specified.

- **Non-TTY Output (e.g., redirecting to a file, piping to another command)**
  - Produces a simpler, machine-readable format suitable for scripting or automation.
  - Ensures consistent structure for programmatic parsing.

## Troubleshooting & Tips

- **Blank Columns:** If a column appears empty, double-check the template variable name (`{{ .variable }}`) against the [YAML Template Syntax](#yaml-template-syntax) section for the specific command. Ensure the data context actually contains that variable for the items being listed.
- **Inspecting Available Data:** Use the `describe` command with `--format json` or `--format yaml` (e.g., `atmos describe stacks --format json`) to see the raw data structure and available fields you can use in your templates.
- **Wide Tables:** If the table is too wide for your terminal or you encounter errors about content width:
    - Reduce the number of columns defined in your `atmos.yaml`.
    - Use a different output format like `json` or `yaml`.
    - Some `list` commands might support a `--max-columns` flag (check command help).
- **Filtering:** Use command-specific flags like `--stacks 'pattern'` for `atmos list stacks` to filter the rows, which can indirectly simplify the output. Query flags (`--query`) might also help narrow down data.

---

## atmos packer build

import File from '@site/src/components/File'
import Terminal from '@site/src/components/Terminal'
import useBaseUrl from '@docusaurus/useBaseUrl';

:::note purpose
Use this command to process a Packer template configured for an Atmos component in a stack, and build it to generate a set of artifacts.
The builds specified within a template are executed in parallel, unless otherwise specified.
The artifacts that are created will be outputted at the end of the build, and a Packer manifest
(if configured in the Atmos component) will be updated with the results of the build.
:::

## Usage

Execute the `packer build` command like this:

```shell
atmos packer build <component> --stack <stack> [flags] -- [packer-options]
```

:::tip
For more details on the `packer build` command and options, refer to [Packer build command reference](https://developer.hashicorp.com/packer/docs/commands/build).
:::

## Arguments

<dl>
    <dt>`component` (required)</dt>
    <dd>
        Atmos Packer component.
    </dd>
</dl>

## Flags

<dl>
    <dt>`--stack` (alias `-s`)(required)</dt>
    <dd>
        Atmos stack.
    </dd>

    <dt>`--template` (alias `-t`)(optional)</dt>
    <dd>
        Packer template.
        It can be specified in the `settings.packer.template` section in the Atmos component manifest,
        or on the command line via the flag `--template <template>` (shorthand `-t`).
        The command line flag takes precedence over `settings.packer.template`.
    </dd>
</dl>

## Examples

<Terminal>
```shell
atmos packer build aws/bastion --stack nonprod
atmos packer build aws/bastion -s prod --template main.pkr.hcl
atmos packer build aws/bastion -s nonprod -t main.nonprod.pkr.hcl
```
</Terminal>

<Terminal title="atmos packer build aws/bastion --stack nonprod">
```shell
> atmos packer build aws/bastion --stack nonprod

amazon-ebs.al2023:

==> amazon-ebs.al2023: Prevalidating any provided VPC information
==> amazon-ebs.al2023: Prevalidating AMI Name: bastion-al2023-1754025080
==> amazon-ebs.al2023: Found Image ID: ami-0013ceeff668b979b
==> amazon-ebs.al2023: Setting public IP address to true on instance without a subnet ID
==> amazon-ebs.al2023: No VPC ID provided, Packer will use the default VPC
==> amazon-ebs.al2023: Inferring subnet from the selected VPC "vpc-xxxxxxx"
==> amazon-ebs.al2023: Set subnet as "subnet-xxxxxxx"
==> amazon-ebs.al2023: Creating temporary keypair: packer_688c4c79-f14a-b77e-ca1e-b5b4c17b4581
==> amazon-ebs.al2023: Creating temporary security group for this instance: packer_688c4c7b-3f16-69f9-0c39-88a3fcbe94fd
==> amazon-ebs.al2023: Authorizing access to port 22 from [0.0.0.0/0] in the temporary security groups...
==> amazon-ebs.al2023: Launching a source AWS instance...
==> amazon-ebs.al2023: changing public IP address config to true for instance on subnet "subnet-xxxxxxx"
==> amazon-ebs.al2023: Instance ID: i-0b621ca091aa4c240
==> amazon-ebs.al2023: Waiting for instance (i-0b621ca091aa4c240) to become ready...
==> amazon-ebs.al2023: Using SSH communicator to connect: 18.222.63.67
==> amazon-ebs.al2023: Waiting for SSH to become available...
==> amazon-ebs.al2023: Connected to SSH!
==> amazon-ebs.al2023: Provisioning with shell script: /var/folders/rt/fqmt0tmx3fs1qfzbf3qxxq700000gn/T/packer-shell653292668
==> amazon-ebs.al2023: Waiting for process with pid 2085 to finish.
==> amazon-ebs.al2023: Amazon Linux 2023 Kernel Livepatch repository   154 kB/s |  16 kB     00:00
==> amazon-ebs.al2023: Package jq-1.7.1-49.amzn2023.0.2.aarch64 is already installed.
==> amazon-ebs.al2023: Dependencies resolved.
==> amazon-ebs.al2023: Nothing to do.
==> amazon-ebs.al2023: Complete!
==> amazon-ebs.al2023: 17 files removed
==> amazon-ebs.al2023: Stopping the source instance...
==> amazon-ebs.al2023: Stopping instance
==> amazon-ebs.al2023: Waiting for the instance to stop...
==> amazon-ebs.al2023: Creating AMI bastion-al2023-1754025080 from instance i-0b621ca091aa4c240
==> amazon-ebs.al2023: Attaching run tags to AMI...
==> amazon-ebs.al2023: AMI: ami-0b2b3b68aa3c5ada8
==> amazon-ebs.al2023: Waiting for AMI to become ready...
==> amazon-ebs.al2023: Skipping Enable AMI deprecation...
==> amazon-ebs.al2023: Skipping Enable AMI deregistration protection...
==> amazon-ebs.al2023: Modifying attributes on AMI (ami-0b2b3b68aa3c5ada8)...
==> amazon-ebs.al2023: Modifying: ami org arns
==> amazon-ebs.al2023: Modifying attributes on snapshot (snap-09ad35550e1438fb2)...
==> amazon-ebs.al2023: Adding tags to AMI (ami-0b2b3b68aa3c5ada8)...
==> amazon-ebs.al2023: Tagging snapshot: snap-09ad35550e1438fb2
==> amazon-ebs.al2023: Creating AMI tags
==> amazon-ebs.al2023: Adding tag: "Stage": "nonprod"
==> amazon-ebs.al2023: Adding tag: "ScanStatus": "pending"
==> amazon-ebs.al2023: Adding tag: "SourceAMI": "ami-0013ceeff668b979b"
==> amazon-ebs.al2023: Adding tag: "SourceAMIDescription": "Amazon Linux 2023 AMI 2023.7.20250527.1 arm64 HVM kernel-6.12"
==> amazon-ebs.al2023: Adding tag: "SourceAMIName": "al2023-ami-2023.7.20250527.1-kernel-6.12-arm64"
==> amazon-ebs.al2023: Adding tag: "SourceAMIOwnerAccountId": "137112412989"
==> amazon-ebs.al2023: Creating snapshot tags
==> amazon-ebs.al2023: Terminating the source AWS instance...
==> amazon-ebs.al2023: Cleaning up any extra volumes...
==> amazon-ebs.al2023: No volumes to clean up, skipping
==> amazon-ebs.al2023: Deleting temporary security group...
==> amazon-ebs.al2023: Deleting temporary keypair...
==> amazon-ebs.al2023: Running post-processor:  (type manifest)
Build 'amazon-ebs.al2023' finished after 3 minutes 39 seconds.

==> Wait completed after 3 minutes 39 seconds

==> Builds finished. The artifacts of successful builds are:
--> amazon-ebs.al2023: AMIs were created:
us-east-2: ami-0b2b3b68aa3c5ada8

--> amazon-ebs.al2023: AMIs were created:
us-east-2: ami-0b2b3b68aa3c5ada8
```
</Terminal>

---

## atmos packer init

import File from '@site/src/components/File'
import Terminal from '@site/src/components/Terminal'
import useBaseUrl from '@docusaurus/useBaseUrl';

:::note purpose
Use this command to initialize Packer and install plugins according to an HCL template configuration for an Atmos component in a stack.
:::

## Usage

Execute the `packer init` command like this:

```shell
atmos packer init <component> --stack <stack> [flags] -- [packer-options]
```

:::tip
For more details on the `packer init` command and options, refer to [Packer init command reference](https://developer.hashicorp.com/packer/docs/commands/init).
:::

## Arguments

<dl>
    <dt>`component` (required)</dt>
    <dd>
        Atmos Packer component.
    </dd>
</dl>

## Flags

<dl>
    <dt>`--stack` (alias `-s`)(required)</dt>
    <dd>
        Atmos stack.
    </dd>

    <dt>`--template` (alias `-t`)(optional)</dt>
    <dd>
        Packer template.
        It can be specified in the `settings.packer.template` section in the Atmos component manifest,
        or on the command line via the flag `--template <template>` (shorthand `-t`).
        The command line flag takes precedence over `settings.packer.template`.
    </dd>
</dl>

## Examples

<Terminal>
```shell
atmos packer init aws/bastion --stack nonprod
atmos packer init aws/bastion -s prod --template main.pkr.hcl
atmos packer init aws/bastion -s nonprod -t main.nonprod.pkr.hcl
```
</Terminal>

<Terminal title="atmos packer init aws/bastion --stack nonprod">
```shell
> atmos packer init aws/bastion --stack nonprod

Installed plugin github.com/hashicorp/amazon v1.3.9 in "~/.config/packer/plugins/github.com/hashicorp/amazon/packer-plugin-amazon_v1.3.9_x5.0_darwin_arm64"
```
</Terminal>

---

## atmos packer inspect

import File from '@site/src/components/File'
import Terminal from '@site/src/components/Terminal'
import useBaseUrl from '@docusaurus/useBaseUrl';

:::note purpose
Use this command to inspect the various components that a Packer template configured for an Atmos component in a stack defines.
The command will show what variables a template accepts, the builders it defines, the provisioners it defines and the order they'll run, and more.
:::

## Usage

Execute the `packer inspect` command like this:

```shell
atmos packer inspect <component> --stack <stack> [flags] -- [packer-options]
```

:::tip
For more details on the `packer inspect` command and options, refer to [Packer inspect command reference](https://developer.hashicorp.com/packer/docs/commands/inspect).
:::

## Arguments

<dl>
    <dt>`component` (required)</dt>
    <dd>
        Atmos Packer component.
    </dd>
</dl>

## Flags

<dl>
    <dt>`--stack` (alias `-s`)(required)</dt>
    <dd>
        Atmos stack.
    </dd>

    <dt>`--template` (alias `-t`)(optional)</dt>
    <dd>
        Packer template.
        It can be specified in the `settings.packer.template` section in the Atmos component manifest,
        or on the command line via the flag `--template <template>` (shorthand `-t`).
        The command line flag takes precedence over `settings.packer.template`.
    </dd>
</dl>

## Examples

<Terminal>
```shell
atmos packer inspect aws/bastion --stack nonprod
atmos packer inspect aws/bastion -s prod --template main.pkr.hcl
atmos packer inspect aws/bastion -s nonprod -t main.nonprod.pkr.hcl
```
</Terminal>

<Terminal title="atmos packer inspect aws/bastion --stack nonprod">
```shell
> atmos packer inspect aws/bastion --stack nonprod

Packer Inspect: HCL2 mode

> input-variables:

var.ami_name: "bastion-al2023-1754457104"
var.ami_org_arns: "[\n  \"arn:aws:organizations::xxxxxxxxxxxx:organization/o-xxxxxxxxx\",\n]"
var.ami_ou_arns: "[]"
var.ami_tags: "{\n  \"ScanStatus\" = \"pending\"\n  \"SourceAMI\" = \"ami-0013ceeff668b979b\"\n  \"SourceAMIDescription\" = \"Amazon Linux 2023 AMI 2023.7.20250527.1 arm64 HVM kernel-6.12\"\n  \"SourceAMIName\" = \"al2023-ami-2023.7.20250527.1-kernel-6.12-arm64\"\n  \"SourceAMIOwnerAccountId\" = \"137112412989\"\n  \"Stage\" = \"nonprod\"\n}"
var.ami_users: "[]"
var.associate_public_ip_address: "true"
var.assume_role_arn: "null"
var.assume_role_duration_seconds: "1800"
var.assume_role_session_name: "atmos-packer"
var.encrypt_boot: "false"
var.force_delete_snapshot: "false"
var.force_deregister: "false"
var.instance_type: "t4g.small"
var.kms_key_arn: "null"
var.manifest_file_name: "manifest.json"
var.manifest_strip_path: "false"
var.provisioner_shell_commands: "[\n  \"sudo systemctl enable --now amazon-ssm-agent\",\n  \"sudo -E bash -c 'dnf install -y jq && dnf clean all && cloud-init clean'\",\n]"
var.region: "us-east-2"
var.skip_create_ami: "false"
var.source_ami: "ami-0013ceeff668b979b"
var.ssh_username: "ec2-user"
var.stage: "nonprod"
var.volume_size: "8"
var.volume_type: "gp3"

> local-variables:

> builds:

> <0>:
sources:
amazon-ebs.al2023

provisioners:
shell

post-processors:
0:
manifest
```
</Terminal>

---

## atmos packer output

import File from '@site/src/components/File'
import Terminal from '@site/src/components/Terminal'
import useBaseUrl from '@docusaurus/useBaseUrl';

:::note purpose
Use this command to get an output from a Packer manifest.
Manifests are generated by Packer when executing `packer build` commands (if configured in the Packer template and Atmos stack).
[YQ](https://mikefarah.gitbook.io/yq/) expressions and functions are supported to get any section or attribute from the manifest.
:::

## Usage

Execute the `packer output` command like this:

```shell
atmos packer output <component> --stack <stack> --query <yq-expression>
```

:::note
`atmos packer output` command is specific to Atmos (Packer itself does not have an `output` command).
The command is used to get an output from a Packer manifest.
Manifests are generated by Packer when executing `packer build` commands (if configured in the Packer template and Atmos stack).
:::

## Arguments

<dl>
    <dt>`component` (required)</dt>
    <dd>
        Atmos Packer component.
    </dd>
</dl>

## Flags

<dl>
    <dt>`--stack` (alias `-s`)(required)</dt>
    <dd>
        Atmos stack.
    </dd>

    <dt>`--query` (alias `-q`)(optional)</dt>
    <dd>
        [YQ](https://mikefarah.gitbook.io/yq/) expression to get sections and attributes from a [Packer manifest](https://developer.hashicorp.com/packer/docs/post-processors/manifest).
    </dd>
</dl>

## Examples

<Terminal>
```shell
atmos packer output aws/bastion -s prod
atmos packer output aws/bastion -s prod --query '.builds[0].artifact_id'
atmos packer output aws/bastion -s prod -q '.builds[0].artifact_id | split(":")[1]'
```
</Terminal>

<Terminal title="atmos packer output aws/bastion -s prod">
```shell
> atmos packer output aws/bastion -s prod

builds:
- artifact_id: us-east-2:ami-0c2ca16b7fcac7529
  build_time: 1.753281956e+09
  builder_type: amazon-ebs
  custom_data: null
  files: null
  name: al2023
  packer_run_uuid: 5114a723-92f6-060f-bae4-3ac2d0324557
- artifact_id: us-east-2:ami-0b2b3b68aa3c5ada8
  build_time: 1.7540253e+09
  builder_type: amazon-ebs
  custom_data: null
  files: null
  name: al2023
  packer_run_uuid: a57874d1-c478-63d7-cfde-9d91e513eb9a
  last_run_uuid: a57874d1-c478-63d7-cfde-9d91e513eb9a
```
</Terminal>

<Terminal title="atmos packer output aws/bastion -s prod --query '.builds[0].artifact_id'">
```shell
# Use a YQ expression to get a specific section or attribute from the Packer manifest,
# in this case, the `artifact_id` from the first build.

> atmos packer output aws/bastion -s nonprod --query '.builds[0].artifact_id'

us-east-2:ami-0c2ca16b7fcac7529
```
</Terminal>

<Terminal title="atmos packer output aws/bastion -s nonprod -q '.builds[0].artifact_id | split(:)[1]'">
```shell
# Use a YQ expression to get a specific section or attribute from the Packer manifest,
# in this case, the AMI (second part after the `:`) from the `artifact_id` from the first build.

> atmos packer output aws/bastion -s nonprod -q '.builds[0].artifact_id | split(":")[1]'

ami-0c2ca16b7fcac7529
```
</Terminal>

---

## atmos packer validate

import File from '@site/src/components/File'
import Terminal from '@site/src/components/Terminal'
import useBaseUrl from '@docusaurus/useBaseUrl';

:::note purpose
Use this command to validate the syntax and configuration of a Packer template configured for an Atmos component in a stack.
:::

## Usage

Execute the `packer validate` command like this:

```shell
atmos packer validate <component> --stack <stack> [flags] -- [packer-options]
```

:::tip
For more details on the `packer validate` command and options, refer to [Packer validate command reference](https://developer.hashicorp.com/packer/docs/commands/validate).
:::

## Arguments

<dl>
    <dt>`component` (required)</dt>
    <dd>
        Atmos Packer component.
    </dd>
</dl>

## Flags

<dl>
    <dt>`--stack` (alias `-s`)(required)</dt>
    <dd>
        Atmos stack.
    </dd>

    <dt>`--template` (alias `-t`)(optional)</dt>
    <dd>
        Packer template.
        It can be specified in the `settings.packer.template` section in the Atmos component manifest,
        or on the command line via the flag `--template <template>` (shorthand `-t`).
        The command line flag takes precedence over `settings.packer.template`.
    </dd>
</dl>

## Examples

<Terminal>
```shell
atmos packer validate aws/bastion --stack prod
atmos packer validate aws/bastion -s prod --template main.pkr.hcl
atmos packer validate aws/bastion -s nonprod -t main.nonprod.pkr.hcl
```
</Terminal>

---

## atmos packer version

import useBaseUrl from '@docusaurus/useBaseUrl';

:::note purpose
Use this command to display the currently installed Packer version.
:::

## Usage

Execute the `packer version` command like this:

```shell
atmos packer version
```

---

## atmos packer

import Screengrab from '@site/src/components/Screengrab'
import DocCardList from '@theme/DocCardList'
import File from '@site/src/components/File'
import Terminal from '@site/src/components/Terminal'
import useBaseUrl from '@docusaurus/useBaseUrl';
import Intro from '@site/src/components/Intro'

<Intro>
Use these subcommands to interact with [HashiCorp Packer](https://developer.hashicorp.com/packer)
to build automated machine images.
</Intro>

## Usage

<Terminal>
```shell
atmos packer <sub-command> <atmos-component> --stack <atmos-stack> [atmos-flags] -- [packer-options]
```
</Terminal>

:::tip
For more details on the Packer commands and options, refer to [Packer Commands](https://developer.hashicorp.com/packer/docs/commands).
:::

## Atmos Flags

<dl>
    <dt>`--stack` (alias `-s`)</dt>
    <dd>
        Atmos stack.
    </dd>

    <dt>`--template` (alias `-t`)(optional)</dt>
    <dd>
        Packer template.
        It can be specified in the `settings.packer.template` section in the Atmos component manifest,
        or on the command line via the flag `--template <template>` (shorthand `-t`).
        The command line flag takes precedence over `settings.packer.template`.
    </dd>

    <dt>`--query` (alias `-q`)(optional)</dt>
    <dd>
        [YQ](https://mikefarah.gitbook.io/yq/) expression to get sections and attributes from a [Packer manifest](https://developer.hashicorp.com/packer/docs/post-processors/manifest).
        Used in the `atmos packer output` command.
    </dd>
</dl>

## Examples

<Terminal>
```shell
atmos packer version

atmos packer validate aws/bastion --stack prod
atmos packer validate aws/bastion -s prod --template main.pkr.hcl
atmos packer validate aws/bastion -s nonprod -t main.nonprod.pkr.hcl

atmos packer inspect aws/bastion -s prod
atmos packer inspect aws/bastion -s prod --template main.pkr.hcl
atmos packer inspect aws/bastion -s nonprod -t main.nonprod.pkr.hcl

atmos packer init aws/bastion -s prod
atmos packer init aws/bastion -s prod --template main.pkr.hcl
atmos packer init aws/bastion -s nonprod -t main.nonprod.pkr.hcl

atmos packer build aws/bastion -s prod
atmos packer build aws/bastion -s prod --template main.pkr.hcl
atmos packer build aws/bastion -s nonprod -t main.nonprod.pkr.hcl

atmos packer output aws/bastion -s prod
atmos packer output aws/bastion -s prod --query '.builds[0].artifact_id'
atmos packer output aws/bastion -s prod -q '.builds[0].artifact_id | split(":")[1]'
```
</Terminal>

## Subcommands

<DocCardList />

---

## atmos pro lock

import Screengrab from "@site/src/components/Screengrab";
import Intro from '@site/src/components/Intro'

<Intro>
This command implements the locking feature of [Atmos Pro](https://atmos-pro.com/docs). Use this command to lock
a stack in Atmos Pro so that it cannot be planned or applied by another process (pull request, CI/CD, etc.)
</Intro>

## Usage

Execute the `pro lock` command like this:

```shell
atmos pro lock --component <component> --stack <stack> --ttl <ttl> --message <message>
```

## Description

Atmos pro supports locking a stack in Atmos Pro so that it cannot be planned or applied by another process (pull
request, CI/CD, etc.). Your CI/CD pipeline can use the `atmos pro lock` command to ensure it is the exclusive process
interacting with a stack at the current time. Once your work is complete, you can unlock the stack by running the `atmos
pro unlock` command.

:::tip
Run `atmos pro lock --help` to see all the available options
:::

## Examples

```shell
atmos pro lock --component vpc --stack plat-ue2-dev --ttl 300 --message "Locked by $GITHUB_RUN_ID"
atmos pro lock --component vpc --stack plat-ue2-dev --ttl 300
```

## Flags

<dl>
  <dt>`--component` (alias `-c`) (required)</dt>
  <dd>Atmos component to lock.</dd>

  <dt>`--stack` (alias `-s`) (required)</dt>
  <dd>Atmos stack to lock.</dd>

  <dt>`--ttl` (alias `-t`) (optional)</dt>
  <dd>The time to live (TTL) for the lock, in seconds. Defaults to 30.</dd>

  <dt>`--message` (alias `-m`) (optional)</dt>
  <dd>A message to display to other users who try to lock the stack. Defaults to "Locked by Atmos".</dd>
</dl>

---

## atmos pro unlock

import Screengrab from "@site/src/components/Screengrab";
import Intro from '@site/src/components/Intro'

<Intro>
This command implements the locking feature of [Atmos Pro](https://atmos-pro.com/docs). Use this command to unlock
a stack in Atmos Pro that has previously been locked by the lock command.
</Intro>

## Usage

Execute the `pro unlock` command like this:

```shell
atmos pro unlock --component <component> --stack <stack>
```

## Description

Atmos pro supports locking a stack in Atmos Pro so that it cannot be planned or applied by another process (pull
request, CI/CD, etc.). Your CI/CD pipeline can use the `atmos pro lock` command to ensure it is the exclusive process
interacting with a stack at the current time. Once your work is complete, you can unlock the stack by running the `atmos
pro unlock` command.

:::tip
Run `atmos pro unlock --help` to see all the available options
:::

## Examples

```shell
atmos pro unlock --component vpc --stack plat-ue2-dev
```

## Flags

<dl>
  <dt>`--component` (alias `-c`) (required)</dt>
  <dd>Atmos component to unlock.</dd>

  <dt>`--stack` (alias `-s`) (required)</dt>
  <dd>Atmos stack to unlock.</dd>
</dl>

---

## atmos pro

import Screengrab from "@site/src/components/Screengrab";
import DocCardList from "@theme/DocCardList";
import Intro from '@site/src/components/Intro'

<Intro>
Use these subcommands to interact with Atmos Pro.
</Intro>

## Subcommands

<DocCardList />

---

## atmos terraform clean

import Screengrab from '@site/src/components/Screengrab'
import Terminal from '@site/src/components/Terminal'

:::note purpose
Use this command to delete the `.terraform` folder, the folder that `TF_DATA_DIR` ENV var points to, `.terraform.lock.hcl` file, `varfile`
and `planfile` for a
component in a stack.
:::

<Screengrab title="atmos terraform clean --help" slug="atmos-terraform-clean--help" />

## Usage

Execute the `terraform clean` command like this:

```shell
atmos terraform clean <component> -s <stack> [--skip-lock-file] [--everything] [--force]
```

:::warning
The `clean` command, by default, deletes all Terraform-related files, including local state files, but will prompt for confirmation before proceeding. Using the `--force` flag skips the confirmation prompt and executes the deletion immediately.
Use these flags with extreme caution as they can lead to irreversible data loss.
:::

:::tip
Run `atmos terraform clean --help` to see all the available options
:::

## Examples

```shell
# Delete all Terraform-related files for all components (with confirmation)
atmos terraform clean
# Force delete all Terraform-related files for all components (no confirmation)
atmos terraform clean --force
atmos terraform clean top-level-component1 -s tenant1-ue2-dev
atmos terraform clean infra/vpc -s tenant1-ue2-staging
atmos terraform clean infra/vpc -s tenant1-ue2-staging --skip-lock-file
atmos terraform clean test/test-component -s tenant1-ue2-dev
atmos terraform clean test/test-component-override-2 -s tenant2-ue2-prod
atmos terraform clean test/test-component-override-3 -s tenant1-ue2-dev
```

## Arguments

<dl>
    <dt>`component` (required)</dt>
    <dd>
        Atmos terraform component.
    </dd>
</dl>

## Flags

<dl>
    <dt>`--stack` (alias `-s`) (required)</dt>
    <dd>
        Atmos stack.
    </dd>

    <dt>`--dry-run` (optional)</dt>
    <dd>
        Dry run.

        ```shell
        atmos terraform clean <component> -s <stack> --dry-run=true
        ```
    </dd>

    <dt>`--skip-lock-file` (optional)</dt>
    <dd>
        Skip deleting the `.terraform.lock.hcl` file.
    </dd>
</dl>

---

## atmos terraform deploy

import Screengrab from '@site/src/components/Screengrab'
import Terminal from '@site/src/components/Terminal'

:::note purpose
Use this command to execute `terraform apply -auto-approve` on an Atmos component in an Atmos stack.
:::

<Screengrab title="atmos terraform deploy --help" slug="atmos-terraform-deploy--help" />

## Usage

Execute the `terraform deploy` subcommand like this:

```shell
atmos terraform deploy <component> -s <stack>
```

- `atmos terraform deploy` command supports `--deploy-run-init=true|false` flag to enable/disable running `terraform init` before executing the
  command

- `atmos terraform deploy` command automatically sets `-auto-approve` flag when running `terraform apply`

- `atmos terraform deploy` command supports `--from-plan` flag. If the flag is specified, the command will use the planfile previously generated
  by `atmos terraform plan` command instead of generating a new planfile, e.g. `atmos terraform deploy <component> -s <stack> --from-plan`. Note that
  in this case, the planfile name is in the format supported by Atmos and is saved to the component's folder

- `atmos terraform deploy` command supports `--planfile` flag to specify the path to a planfile. The `--planfile` flag should be used instead of the
  planfile argument in the native `terraform apply <planfile>` command. For example, you can execute the command
  `atmos terraform plan <component> -s <stack> -out=<FILE>`, which will save the generated plan to a file on disk,
  and then execute the command `atmos terraform deploy <component> -s <stack> --planfile <FILE>` to apply the previously generated planfile

See [all flags](#flags).

:::tip
Run `atmos terraform deploy --help` to see all the available options
:::

## Examples

### Simple Example

Deploy the `top-level-component1` using the configuration specified in the `tenant1-ue2-dev` stack. This command explicitly targets a stack, which defines the environment and region settings for the deployment.

```shell
atmos terraform deploy top-level-component1 --stack tenant1-ue2-dev
```

### Planfiles

Deploy `top-level-component1` based on a previously generated execution plan. The `-s` flag specifies the `tenant1-ue2-dev` stack, and `--from-plan` indicates that the deploy should proceed with the plan that was previously created, ensuring that the deployment matches the plan's specifications.

```shell
atmos terraform deploy top-level-component1 -s tenant1-ue2-dev --from-plan
```

Or use `-s` for a specific execution plan file located at `<path_to_planfile>` to ensure precision in what is deployed.

```shell
atmos terraform deploy top-level-component1 -s tenant1-ue2-dev --planfile <path_to_planfile>
```

### Targeting Specific Stages

This demonstrates how Atmos can be used to deploy infrastructure components, like a VPC, specifying the stack to ensure the deployment occurs within the correct environment and configuration context.

```shell
atmos terraform deploy infra/vpc -s tenant1-ue2-staging
atmos terraform deploy test/test-component -s tenant1-ue2-dev
atmos terraform deploy test/test-component-override-2 -s tenant2-ue2-prod
atmos terraform deploy test/test-component-override-3 -s tenant1-ue2-dev
```

## Arguments

<dl>
    <dt>`component` (required)</dt>
    <dd>
        Atmos terraform component.
    </dd>
</dl>

## Flags

<dl>
    <dt>`--stack` (alias `-s`) (required)</dt>
    <dd>
        Atmos stack.
    </dd>

    <dt>`--dry-run` (optional)</dt>
    <dd>
        Dry run.

        ```shell
        atmos terraform deploy <component> -s <stack> --dry-run=true
        ```
    </dd>

    <dt>`--deploy-run-init` (optional)</dt>
    <dd>
        Enable/disable running `terraform init` before executing the command.

        ```shell
        atmos terraform deploy <component> -s <stack> --deploy-run-init
        ```
    </dd>

    <dt>`--from-plan` (optional)</dt>
    <dd>
        If the flag is specified, use the `planfile` previously generated by Atmos instead of generating a new `planfile`.
        The planfile name is in the format supported by Atmos and is saved to the component's folder.

        ```shell
        atmos terraform deploy <component> -s <stack> --from-plan
        ```
    </dd>

    <dt>`--planfile` (optional)</dt>
    <dd>
        The path to a planfile. The `--planfile` flag should be used instead of the planfile argument in the native `terraform apply <planfile>` command .

        ```shell
        atmos terraform apply <component> -s <stack> --planfile <planfile>
        ```
    </dd>

    <dt>`--process-templates` (optional)</dt>
    <dd>
        Enable/disable Go template processing in Atmos stack manifests when executing terraform commands.

        If the flag is not passed, template processing is enabled by default.

        ```shell
        atmos terraform deploy <component> -s <stack> --process-templates=false
        ```
    </dd>

    <dt>`--process-functions` (optional)</dt>
    <dd>
        Enable/disable YAML functions processing in Atmos stack manifestswhen executing terraform commands.

        If the flag is not passed, YAML function processing is enabled by default.

        ```shell
        atmos terraform deploy <component> -s <stack> --process-functions=false
        ```
    </dd>

    <dt>`--skip` (optional)</dt>
    <dd>
        Skip processing a specific Atmos YAML function in Atmos stacks manifests when executing terraform commands.

        To specify more than one function, use multiple `--skip` flags, or separate the functions with a comma.

        ```shell
        atmos terraform deploy <component> -s <stack> --skip=eval --skip=include
        atmos terraform deploy <component> -s <stack> --skip=terraform.output,include
        ```
    </dd>
</dl>

:::note

The `atmos terraform deploy` command supports all native `terraform apply` options described
in [Terraform apply options](https://developer.hashicorp.com/terraform/cli/commands/apply#apply-options), with the exception that a planfile argument
can't be provided on the command line. To use a previously generated planfile, use the `--from-plan` or `--planfile` command-line flags

:::

---

## atmos terraform generate backend

import Screengrab from '@site/src/components/Screengrab'
import Terminal from '@site/src/components/Terminal'

:::note purpose
Use this command to generate a Terraform backend config file for an Atmos terraform component in a stack.
:::

<Screengrab title="atmos terraform generate backend --help" slug="atmos-terraform-generate-backend--help" />

## Usage

Execute the `terraform generate backend` command like this:

```shell
atmos terraform generate backend <component> -s <stack>
```

This command generates a backend config file for an Atmos terraform component in a stack.

:::tip
Run `atmos terraform generate backend --help` to see all the available options
:::

## Examples

```shell
atmos terraform generate backend top-level-component1 -s tenant1-ue2-dev
atmos terraform generate backend infra/vpc -s tenant1-ue2-staging
atmos terraform generate backend test/test-component -s tenant1-ue2-dev
atmos terraform generate backend test/test-component-override-2 -s tenant2-ue2-prod
```

## Arguments

<dl>
    <dt>`component` (required)</dt>
    <dd>
        Atmos terraform component.
    </dd>
</dl>

## Flags

<dl>
    <dt>`--stack` (alias `-s`) (required)</dt>
    <dd>
        Atmos stack.
    </dd>

    <dt>`--dry-run` (optional)</dt>
    <dd>
        Dry run.

        ```shell
        atmos terraform generate backend <component> -s <stack> --dry-run=true
        ```
    </dd>

    <dt>`--process-templates` (optional)</dt>
    <dd>
        Enable/disable Go template processing in Atmos stack manifests when executing terraform commands.

        If the flag is not passed, template processing is enabled by default.

        ```shell
        atmos terraform generate backend <component> -s <stack> --process-templates=false
        ```
    </dd>

    <dt>`--process-functions` (optional)</dt>
    <dd>
        Enable/disable YAML functions processing in Atmos stack manifestswhen executing terraform commands.

        If the flag is not passed, YAML function processing is enabled by default.

        ```shell
        atmos terraform generate backend <component> -s <stack> --process-functions=false
        ```
    </dd>

    <dt>`--skip` (optional)</dt>
    <dd>
        Skip processing a specific Atmos YAML function in Atmos stacks manifests when executing terraform commands.

        To specify more than one function, use multiple `--skip` flags, or separate the functions with a comma.

        ```shell
        atmos terraform generate backend <component> -s <stack> --skip=eval --skip=include
        atmos terraform generate backend <component> -s <stack> --skip=terraform.output,include
        ```
    </dd>
</dl>

:::info
Refer to [Terraform backend configuration](https://developer.hashicorp.com/terraform/language/settings/backends/configuration) for more details
on `terraform` backends and supported formats
:::

---

## atmos terraform generate backends

import Screengrab from '@site/src/components/Screengrab'
import Terminal from '@site/src/components/Terminal'

:::note purpose
Use this command to generate the Terraform backend config files for all Atmos terraform [components](/core-concepts/components) in
all [stacks](/core-concepts/stacks).
:::

<Screengrab title="atmos terraform generate backends --help" slug="atmos-terraform-generate-backends--help" />

## Usage

Execute the `terraform generate backends` command like this:

```shell
atmos terraform generate backends [options]
```

This command generates backend config files for all Atmos terraform components in all stacks.

:::tip
Run `atmos terraform generate backends --help` to see all the available options
:::

## Examples

```shell
atmos terraform generate backends --file-template {component-path}/{tenant}/{environment}-{stage}.tf.json --format json
atmos terraform generate backends --file-template {component-path}/backends/{tenant}-{environment}-{stage}.tf.json --format json
atmos terraform generate backends --file-template backends/{tenant}/{environment}/{region}/{component}.tf --format hcl
atmos terraform generate backends --file-template backends/{tenant}-{environment}-{stage}-{component}.tf
atmos terraform generate backends --file-template /{tenant}/{stage}/{region}/{component}.tf
atmos terraform generate backends --file-template backends/{tenant}-{environment}-{stage}-{component}.tfbackend --format backend-config
atmos terraform generate backends --stacks orgs/cp/tenant1/staging/us-east-2,orgs/cp/tenant2/dev/us-east-2 --file-template <file_template>
atmos terraform generate backends --stacks tenant1-ue2-staging,tenant1-ue2-prod --file-template <file_template>
atmos terraform generate backends --stacks orgs/cp/tenant1/staging/us-east-2,tenant1-ue2-prod --file-template <file_template>
atmos terraform generate backends --components <component1>,<component2> --file-template <file_template>
atmos terraform generate backends --format hcl --file-template <file_template>
atmos terraform generate backends --format json --file-template <file_template>
atmos terraform generate backends --format backend-config --file-template <file_template>
```

## Flags

<dl>
    <dt>`--file-template` (optional)</dt>
    <dd>
        Backend file template (path, file name, and file extension).
        Supports absolute and relative paths.
        Supports context tokens: `{namespace}`, `{tenant}`, `{environment}`, `{region}`, `{stage}`, `{base-component}`, `{component}`, `{component-path}`.
        All subdirectories in the path will be created automatically.
        If the flag is not specified, all backend config files will be written to the corresponding terraform component folders.
    </dd>

    <dt>`--stacks` (optional)</dt>
    <dd>
        Only process the specified stacks (comma-separated values).
        The names of top-level stack manifests and Atmos stack names are supported.
    </dd>

    <dt>`--components` (optional)</dt>
    <dd>
        Only generate backend files for the specified Atmos components (comma-separated values).
    </dd>

    <dt>`--format` (optional)</dt>
    <dd>
        Backend file format: `json`, `hcl`, `backend-config` (`json` is default) .
    </dd>

    <dt>`--dry-run` (optional)</dt>
    <dd>
        Dry run.
    </dd>
</dl>

:::info

Refer to [Terraform backend configuration](https://developer.hashicorp.com/terraform/language/settings/backends/configuration) for more details
on `terraform` backends and supported formats

:::

---

## atmos terraform generate planfile

import Screengrab from '@site/src/components/Screengrab'

:::note purpose
Use this command to generate a planfile for an Atmos Terraform/OpenTofu [component](/core-concepts/components) in a [stack](/core-concepts/stacks).
:::

<Screengrab title="atmos terraform generate planfile --help" slug="atmos-terraform-generate-planfile--help" />

## Usage

Execute the `terraform generate planfile` command like this:

```shell
atmos terraform generate planfile <component> -s <stack> [options]
```

This command generates a Terraform planfile for a specified Atmos component in a stack.

Under the hood, Atmos executes `terraform plan` to create a binary planfile, then uses `terraform show` to convert it into a human-readable format (YAML or JSON). This enables easy integration with other tooling like `checkov`.

:::tip
Run `atmos terraform generate planfile --help` to see all the available options
:::

## Examples

```shell
atmos terraform generate planfile component1 -s plat-ue2-dev
atmos terraform generate planfile component1 -s plat-ue2-prod --format=json
atmos terraform generate planfile component1 -s plat-ue2-prod --format=yaml
atmos terraform generate planfile <component> -s <stack> --file=planfile.json
atmos terraform generate planfile <component> -s <stack> --format=yaml --file=planfiles/planfile.yaml
atmos terraform generate planfile <component> -s <stack> --file=/Users/me/Documents/atmos/infra/planfile.json
```

## Arguments

<dl>
    <dt>`component` (required)</dt>
    <dd>
        Atmos terraform component.
    </dd>
</dl>

## Flags

<dl>
    <dt>`--stack` (alias `-s`) (required)</dt>
    <dd>
        Atmos stack.
    </dd>

    <dt>`--format` (optional)</dt>
    <dd>
        Output format (`json` or `yaml`, `json` is default).

        ```shell
        atmos terraform generate planfile <component> -s <stack> --format=json
        atmos terraform generate planfile <component> -s <stack> --format=yaml
        ```
    </dd>

    <dt>`--file` (alias `-f`) (optional)</dt>
    <dd>
        Planfile name.
        Supports absolute and relative paths.

        If not provided, Atmos generates the planfile in the Terraform component directory with the name
        `<stack>-<component>.planfile.json` or `<stack>-<component>.planfile.yaml`, depending on the format specified
        with `--format` flag (`json` is default).

        If an absolute path is provided, the file will be created in the specified directory:

        ```shell
        atmos terraform generate planfile <component> -s <stack> --file=/Users/me/Documents/atmos/infra/planfile.json
        ```

        If a relative path is specified, the file will be created in the Terraform component directory:

        ```shell
        atmos terraform generate planfile <component> -s <stack> --file=planfile.json
        atmos terraform generate planfile <component> -s <stack> --format=yaml --file=planfiles/planfile.yaml
        ```
    </dd>

    <dt>`--process-templates` (optional)</dt>
    <dd>
        Enable/disable Go template processing in Atmos stack manifests when executing terraform commands.

        If the flag is not passed, template processing is enabled by default.

        ```shell
        atmos terraform generate planfile <component> -s <stack> --process-templates=false
        ```
    </dd>

    <dt>`--process-functions` (optional)</dt>
    <dd>
        Enable/disable YAML functions processing in Atmos stack manifestswhen executing terraform commands.

        If the flag is not passed, YAML function processing is enabled by default.

        ```shell
        atmos terraform generate planfile <component> -s <stack> --process-functions=false
        ```
    </dd>

    <dt>`--skip` (optional)</dt>
    <dd>
        Skip processing a specific Atmos YAML function in Atmos stacks manifests when executing terraform commands.

        To specify more than one function, use multiple `--skip` flags, or separate the functions with a comma.

        ```shell
        atmos terraform generate planfile <component> -s <stack> --skip=eval --skip=include
        atmos terraform generate planfile <component> -s <stack> --skip=terraform.output,include
        ```
    </dd>
</dl>

## Validate Terraform/OpenTofu planfiles using Checkov

You can generate a planfile for a component in a stack and validate it using [Checkov](https://www.checkov.io/).

```shell
atmos terraform generate planfile <component> -s <stack>
checkov --file components/terraform/<component>/<stack>-<component>.planfile.json --framework terraform_plan
```

Refer to [Evaluate Checkov Policies on Terraform Plan](https://www.checkov.io/7.Scan%20Examples/Terraform%20Plan%20Scanning.html)
for more information.

---

## atmos terraform generate varfile

import Screengrab from '@site/src/components/Screengrab'

:::note purpose
Use this command to generate a varfile (`.tfvar` ) for an Atmos terraform [component](/core-concepts/components) in a [stack](/core-concepts/stacks).
:::

<Screengrab title="atmos terraform generate varfile --help" slug="atmos-terraform-generate-varfile--help" />

## Usage

Execute the `terraform generate varfile` command like this:

```shell
atmos terraform generate varfile <component> -s <stack>
```

This command generates a varfile for an Atmos terraform component in a stack.

:::tip
Run `atmos terraform generate varfile --help` to see all the available options
:::

## Examples

```shell
atmos terraform generate varfile top-level-component1 -s tenant1-ue2-dev
atmos terraform generate varfile infra/vpc -s tenant1-ue2-staging
atmos terraform generate varfile test/test-component -s tenant1-ue2-dev
atmos terraform generate varfile test/test-component-override-2 -s tenant2-ue2-prod
atmos terraform generate varfile test/test-component-override-3 -s tenant1-ue2-dev -f vars.json
```

## Arguments

<dl>
    <dt>`component` (required)</dt>
    <dd>
        Atmos terraform component.
    </dd>
</dl>

## Flags

<dl>
    <dt>`--stack` (alias `-s`) (required)</dt>
    <dd>
        Atmos stack.
    </dd>

    <dt>`--dry-run` (optional)</dt>
    <dd>
        Dry run.

        ```shell
        atmos terraform generate varfile <component> -s <stack> --dry-run=true
        ```
    </dd>

    <dt>`--process-templates` (optional)</dt>
    <dd>
        Enable/disable Go template processing in Atmos stack manifests when executing terraform commands.

        If the flag is not passed, template processing is enabled by default.

        ```shell
        atmos terraform generate varfile <component> -s <stack> --process-templates=false
        ```
    </dd>

    <dt>`--process-functions` (optional)</dt>
    <dd>
        Enable/disable YAML functions processing in Atmos stack manifestswhen executing terraform commands.

        If the flag is not passed, YAML function processing is enabled by default.

        ```shell
        atmos terraform generate varfile <component> -s <stack> --process-functions=false
        ```
    </dd>

    <dt>`--skip` (optional)</dt>
    <dd>
        Skip processing a specific Atmos YAML function in Atmos stacks manifests when executing terraform commands.

        To specify more than one function, use multiple `--skip` flags, or separate the functions with a comma.

        ```shell
        atmos terraform generate varfile <component> -s <stack> --skip=eval --skip=include
        atmos terraform generate varfile <component> -s <stack> --skip=terraform.output,include
        ```
    </dd>
</dl>

---

## atmos terraform generate varfiles

import Screengrab from '@site/src/components/Screengrab'
import Intro from '@site/src/components/Intro'

<Intro>
Use this command to generate the Terraform varfiles (`.tfvar`) for all Atmos terraform [components](/core-concepts/components) in
all [stacks](/core-concepts/stacks).
</Intro>

<Screengrab title="atmos terraform generate varfiles --help" slug="atmos-terraform-generate-varfiles--help" />

## Usage

Executes `terraform generate varfiles` command.

```shell
atmos terraform generate varfiles [options]
```

This command generates varfiles for all Atmos terraform components in all stacks.

:::tip
Run `atmos terraform generate varfiles --help` to see all the available options
:::

## Examples

```shell
atmos terraform generate varfiles --file-template {component-path}/{environment}-{stage}.tfvars.json
atmos terraform generate varfiles --file-template /configs/{tenant}/{environment}/{stage}/{component}.json
atmos terraform generate varfiles --file-template /{tenant}/{stage}/{region}/{component}.yaml
atmos terraform generate varfiles --stacks orgs/cp/tenant1/staging/us-east-2,orgs/cp/tenant2/dev/us-east-2
atmos terraform generate varfiles --stacks tenant1-ue2-staging,tenant1-ue2-prod
atmos terraform generate varfiles --stacks orgs/cp/tenant1/staging/us-east-2,tenant1-ue2-prod
atmos terraform generate varfiles --components <component1>,<component2> --file-template <file_template>
atmos terraform generate varfiles --format hcl --file-template <file_template>
atmos terraform generate varfiles --format json --file-template <file_template>
atmos terraform generate varfiles --format yaml --file-template <file_template>
```

## Flags

<dl>
    <dt>`--file-template` (required)</dt>
    <dd>
        Varfile template (path, file name, and file extension).
        Supports absolute and relative paths.
        Supports context tokens: `{namespace}`, `{tenant}`, `{environment}`, `{region}`, `{stage}`, `{base-component}`, `{component}`, `{component-path}`.
        All subdirectories in the path will be created automatically.
    </dd>

    <dt>`--stacks` (optional)</dt>
    <dd>
        Only process the specified stacks (comma-separated values).
        The names of top-level stack manifests and Atmos stack names are supported.
    </dd>

    <dt>`--components` (optional)</dt>
    <dd>
        Only generate backend files for the specified Atmos components (comma-separated values).
    </dd>

    <dt>`--format` (optional)</dt>
    <dd>
        Backend file format: `json`, `hcl`, `backend-config` (`json` is default) .
    </dd>

    <dt>`--dry-run` (optional)</dt>
    <dd>
        Dry run.
    </dd>
</dl>

---

## atmos terraform plan-diff

import Tabs from "@theme/Tabs";
import TabItem from "@theme/TabItem";

# terraform plan-diff

The `atmos terraform plan-diff` command compares two Terraform plans and shows the differences between them.

It takes an original plan file (`--orig`) and optionally a new plan file (`--new`). If the new plan file is not provided, it will generate one by running `terraform plan` with the current configuration.

The command shows differences in variables, resources, and outputs between the two plans.

## Usage

```shell
atmos terraform plan-diff <component> -s <stack> --orig=<original-plan-file> [--new=<new-plan-file>] [options]
```

## Arguments

<dl>
  <dt>`component` (required)</dt>
  <dd>The name of the component to run the command against.</dd>
</dl>

## Flags

<dl>
  <dt>`-s` / `--stack` (required)</dt>
  <dd>The stack name to use.</dd>

  <dt>`--orig` (required)</dt>
  <dd>Path to the original Terraform plan file.</dd>

  <dt>`--new` (optional)</dt>
  <dd>Path to the new Terraform plan file.</dd>

  <dt>`--skip-init` (optional)</dt>
  <dd>Skip running `terraform init` before executing the command.</dd>
</dl>

You can also pass any additional flags and arguments that are supported by the `terraform plan` command when generating a new plan.

## Examples

### Compare an existing plan with a new plan generated with current configuration

```shell
atmos terraform plan-diff myapp -s dev --orig=orig.plan
```

### Compare two existing plan files

```shell
atmos terraform plan-diff myapp -s dev --orig=orig.plan --new=new.plan
```

## Output Format

When there are no differences between the two plan files:

```text
The planfiles are identical
```

When there are differences between the two plan files:

```text
Diff Output
=========

Variables:
----------
+ added_var: "new value"
- removed_var: "old value"
~ changed_var: "old value" => "new value"

Resources:
-----------
+ aws_s3_bucket.new_bucket
- aws_instance.removed_instance
~ aws_security_group.modified_group
  ~ ingress.cidr_blocks: ["10.0.0.0/16"] => ["10.0.0.0/8"]
  + egress.port: 443

Outputs:
--------
+ new_output: "value"
- removed_output: "value"
~ changed_output: "old" => "new"
```

## Exit Codes

| Exit Code | Description                               |
| --------- | ----------------------------------------- |
| 0         | Success - no differences found            |
| 1         | Error occurred during execution           |
| 2         | Success - differences found between plans |

## Use Cases

The `plan-diff` command is useful for:

1. **Validating changes**: Compare a previously saved plan with the current state to see what has changed.
2. **Reviewing variable impacts**: See how changing variables affects the infrastructure plan.
3. **CI/CD workflows**: Use the exit code to determine if changes are expected or unexpected.
4. **Documentation**: Generate human-readable diffs for change management and approvals.

## How It Works

The command:

1. Runs `terraform init` in the component directory
2. If `--new` is not specified, runs a plan and captures the output
3. Runs `terraform show -json` for each plan to get the JSON representation
4. Sorts the JSON for consistent comparison
5. Creates a diff between the two plans
6. Handles sensitive values properly by displaying `(sensitive value)`
7. Returns appropriate exit code based on whether differences were found

---

## atmos terraform shell

import Screengrab from '@site/src/components/Screengrab'
import Intro from '@site/src/components/Intro'

<Intro>
This command starts a new `SHELL` configured with the environment for an Atmos component in a Stack to allow executing all native terraform commands
inside the shell without using any atmos-specific arguments and flags.
</Intro>

<Screengrab title="atmos terraform shell --help" slug="atmos-terraform-shell--help" />

## Usage

Execute the `terraform shell` command like this:

```shell
atmos terraform shell <component> -s <stack>
```

The command configures the environment for an Atmos component in a stack and starts a new shell suitable for executing all terraform commands natively
without going through Atmos.

The command does the following:

- Processes the stack manifests, generates the required variables for the Atmos component in the stack, and writes them to a file in the
  component's folder

- Generates a backend config file for the Atmos component in the stack and writes it to a file in the component's folder (or as specified by the
  [Atmos configuration setting](/cli/configuration))

- Creates a `terraform` workspace for the component in the stack

- Drops the user into a separate shell (process) with all the required paths and ENV vars set

- Inside the shell, the user can execute all `terraform` commands using the native syntax

- Atmos sets the `ATMOS_SHLVL` environment variable to track the nesting level of shells:
    - If `ATMOS_SHLVL` is not already set, Atmos initializes it to `1`.
    - If `ATMOS_SHLVL` is already set, Atmos increments its value by `1` for each new nested shell.

:::tip
Run `atmos terraform shell --help` to see all the available options
:::

## Examples

```shell
atmos terraform shell top-level-component1 -s tenant1-ue2-dev
atmos terraform shell infra/vpc -s tenant1-ue2-staging
atmos terraform shell test/test-component-override-3 -s tenant2-ue2-prod
```

## Arguments

<dl>
    <dt>`component` (required)</dt>
    <dd>
        Atmos terraform component.
    </dd>
</dl>

## Flags

<dl>
    <dt>`--stack` (alias `-s`) (required)</dt>
    <dd>
        Atmos stack.
    </dd>

    <dt>`--dry-run` (optional)</dt>
    <dd>
        Dry run.

        ```shell
        atmos terraform shell <component> -s <stack> --dry-run=true
        ```
    </dd>

    <dt>`--process-templates` (optional)</dt>
    <dd>
        Enable/disable Go template processing in Atmos stack manifests when executing terraform commands.

        If the flag is not passed, template processing is enabled by default.

        ```shell
        atmos terraform shell <component> -s <stack> --process-templates=false
        ```
    </dd>

    <dt>`--process-functions` (optional)</dt>
    <dd>
        Enable/disable YAML functions processing in Atmos stack manifestswhen executing terraform commands.

        If the flag is not passed, YAML function processing is enabled by default.

        ```shell
        atmos terraform shell <component> -s <stack> --process-functions=false
        ```
    </dd>

    <dt>`--skip` (optional)</dt>
    <dd>
        Skip processing a specific Atmos YAML function in Atmos stacks manifests when executing terraform commands.

        To specify more than one function, use multiple `--skip` flags, or separate the functions with a comma.

        ```shell
        atmos terraform shell <component> -s <stack> --skip=eval --skip=include
        atmos terraform shell <component> -s <stack> --skip=terraform.output,include
        ```
    </dd>
</dl>

---

## atmos terraform workspace

import Screengrab from '@site/src/components/Screengrab'
import Intro from '@site/src/components/Intro'

<Intro>
Use this command to calculate the `terraform` workspace for an Atmos component (from the context variables and stack config). It will
run `terraform init -reconfigure` and then select the workspace by executing the `terraform workspace select` command.
</Intro>

<Screengrab title="atmos terraform workspace --help" slug="atmos-terraform-workspace--help" />

## Usage

Execute the `terraform workspace` command like this:

```shell
atmos terraform workspace <component> -s <stack>
```

This command calculates the `terraform` workspace for an Atmos component (from the context variables and stack config), then
runs `terraform init -reconfigure`, then selects the workspace by executing the `terraform workspace select` command.

If the workspace does not exist, the command creates it by executing the `terraform workspace new` command.

:::tip
Run `atmos terraform workspace --help` to see all the available options
:::

## Examples

```shell
atmos terraform workspace top-level-component1 -s tenant1-ue2-dev
atmos terraform workspace infra/vpc -s tenant1-ue2-staging
atmos terraform workspace test/test-component -s tenant1-ue2-dev
atmos terraform workspace test/test-component-override-2 -s tenant2-ue2-prod
atmos terraform workspace test/test-component-override-3 -s tenant1-ue2-dev
```

## Arguments

<dl>
    <dt>`component` (required)</dt>
    <dd>
        Atmos terraform component.
    </dd>
</dl>

## Flags

<dl>
    <dt>`--stack` (alias `-s`) (required)</dt>
    <dd>
        Atmos stack.
    </dd>

    <dt>`--dry-run` (optional)</dt>
    <dd>
        Dry run.

        ```shell
        atmos terraform workspace <component> -s <stack> --dry-run=true
        ```
    </dd>

    <dt>`--process-templates` (optional)</dt>
    <dd>
        Enable/disable Go template processing in Atmos stack manifests when executing terraform commands.

        If the flag is not passed, template processing is enabled by default.

        ```shell
        atmos terraform workspace <component> -s <stack> --process-templates=false
        ```
    </dd>

    <dt>`--process-functions` (optional)</dt>
    <dd>
        Enable/disable YAML functions processing in Atmos stack manifestswhen executing terraform commands.

        If the flag is not passed, YAML function processing is enabled by default.

        ```shell
        atmos terraform workspace <component> -s <stack> --process-functions=false
        ```
    </dd>

    <dt>`--skip` (optional)</dt>
    <dd>
        Skip processing a specific Atmos YAML function in Atmos stacks manifests when executing terraform commands.

        To specify more than one function, use multiple `--skip` flags, or separate the functions with a comma.

        ```shell
        atmos terraform workspace <component> -s <stack> --skip=eval --skip=include
        atmos terraform workspace <component> -s <stack> --skip=terraform.output,include
        ```
    </dd>
</dl>

---

## atmos terraform

import Screengrab from '@site/src/components/Screengrab'
import DocCardList from '@theme/DocCardList'
import File from '@site/src/components/File'
import Terminal from '@site/src/components/Terminal'
import Intro from '@site/src/components/Intro'

<Intro>
Use these subcommands to interact with Terraform and OpenTofu.
</Intro>

<Screengrab title="atmos terraform --help" slug="atmos-terraform--help" />

Atmos Terraform/OpenTofu commands fall into two categories:

  - Single-Component: Run Terraform for one component at a time

  - Multi-Component (Filtered/Bulk): Run Terraform across multiple components using stack names, selectors, or change detection

Atmos supports all Terraform and OpenTofu commands and options described in
[Terraform CLI Overview](https://developer.hashicorp.com/terraform/cli/commands)
and [OpenTofu Basic CLI Features](https://opentofu.org/docs/cli/commands/).

In addition, for the Single-Component commands, the `component` argument and `stack` flag are required to generate
variables and backend config for the component in the stack.

:::note Disambiguation
The term "Terraform" is used in this documentation to refer to generic concepts such as providers, modules, stacks, the
HCL-based domain-specific language and its interpreter. Atmos works with [OpenTofu](/core-concepts/projects/configuration/opentofu).
:::

## Single-Component Commands Usage

Use single-component commands when you want to execute Terraform operations on one component at a time, offering precise control over individual resources.
<Terminal>
```shell
# Execute `terraform <command>` on a `component` in a `stack`
atmos terraform <command> <component> -s <stack> [options]
atmos terraform <command> <component> --stack <stack> [options]
```
</Terminal>

## Multi-Component Commands (Bulk Operations) Usage

Use multi-component commands to run Terraform operations across multiple components simultaneously. You can target components by stack, selector, query, or change detection—often making this approach more efficient than using Atmos workflows for certain use cases.

<Terminal>
```shell
# Execute `terraform <command>` on all components in the stack `prod`
atmos terraform <command> --stack prod

# Execute `terraform <command>` on components `component-1` and `component-2` in all stacks
atmos terraform <command> --components component-1,component-2

# Execute `terraform <command>` on components `component-1` and `component-2` in the stack `prod`
atmos terraform <command> --stack prod --components component-1,component-2

# Execute `terraform <command>` on all components in all stacks
atmos terraform <command> --all

# Execute `terraform <command>` on all components in the stack `prod`
atmos terraform <command> --all --stack prod

# Execute `terraform <command>` on all the directly affected components in all stacks in dependency order
# (if component dependencies are configured)
atmos terraform <command> --affected

# Execute `terraform <command>` on all the directly affected components in the `prod` stack in dependency order
# (if component dependencies are configured)
atmos terraform <command> --affected --stack prod

# Execute `terraform <command>` on all the directly affected components in all stacks in dependency order.
# For each directly affected component, detect the dependent components and process them in dependency order, recursively.
# Dependents are components that are indirectly affected, meaning that nothing in the current branch modifies their code
# or configs, but they are configured as dependencies of the components that are modified
atmos terraform <command> --affected --include-dependents

# Execute `terraform <command>` on all the directly affected components in the `prod` stack in dependency order.
# For each directly affected component, detect the dependent components and process them in dependency order, recursively.
atmos terraform <command> --affected --include-dependents --stack prod

# Execute `terraform <command>` on all components that have `vars.tags.team == "data"`, in all stacks
atmos terraform <command> --query '.vars.tags.team == "data"'

# Execute `terraform <command>` on all components that have `vars.tags.team == "eks"`, in the stack `prod`
atmos terraform <command> --query '.vars.tags.team == "eks"' --stack prod

# Execute `terraform <command>` on all components that have `settings.context.account_id == 12345`, in all stacks
atmos terraform <command> --query '.settings.context.account_id == 12345'
```
</Terminal>

## Additions and differences from native Terraform and OpenTofu

- before executing other `terraform` commands, Atmos runs `terraform init`

- you can skip over atmos calling `terraform init` if you know your project is already in a good working state by using the `--skip-init` flag like
  so `atmos terraform <command> <component> -s <stack> --skip-init`

- `atmos terraform deploy` command executes `terraform apply -auto-approve` (sets `-auto-approve` flag when running `terraform apply`)

- `atmos terraform deploy` command supports `--deploy-run-init=true|false` flag to enable/disable running `terraform init` before executing the
  command

- `atmos terraform apply` and `atmos terraform deploy` commands support `--from-plan` flag. If the flag is specified, the commands will use
  the planfile previously generated by `atmos terraform plan` command instead of generating a new planfile,
  e.g. `atmos terraform apply <component> -s <stack> --from-plan`. Note that in this case, the planfile name is in the format supported by Atmos and
  is saved to the component's folder

- `atmos terraform apply` and `atmos terraform deploy` commands support `--planfile` flag to specify the path to a planfile.
  The `--planfile` flag should be used instead of the planfile argument in the native `terraform apply <planfile>` command.
  For example, you can execute the command `atmos terraform plan <component> -s <stack> -out=<FILE>`, which will save the generated plan to a
  file on disk, and then execute the command `atmos terraform apply <component> -s <stack> --planfile <FILE>` to apply the previously generated
  planfile

- `atmos terraform plan` command accepts a `--skip-planfile` flag to skip writing the plan to a file. If the flag is set to `true`
  (e.g., `atmos terraform plan <component> -s <stack> --skip-planfile=true`), Atmos will not pass the `-out` flag to Terraform
  when executing the command. Set it to `true` when using Terraform Cloud since the `-out` flag is not supported.
  Terraform Cloud automatically stores plans in its backend and can't store it in a local file

- `atmos terraform clean` command deletes the `.terraform` folder, `.terraform.lock.hcl` lock file, and the previously generated `planfile`
  and `varfile` for the specified component and stack. Use the `--skip-lock-file` flag to skip deleting the `.terraform.lock.hcl` file.
  It deletes all local Terraform state files and directories
  (including [`terraform.tfstate.d`](https://developer.hashicorp.com/terraform/cli/workspaces#workspace-internals)
  used for local state) for a component in a stack.
  The `--force` flag bypasses the safety confirmation prompt and forces the deletion. Use with caution.

  :::warning
  The `clean` command performs destructive operations that can lead to permanent state loss, if not using remote backends.
  Always ensure you have remote state configured in your components before proceeding.
  :::

- `atmos terraform workspace` command first runs `terraform init -reconfigure`, then `terraform workspace select`, and if the workspace was not
  created before, it then runs `terraform workspace new`

- `atmos terraform import` command searches for `region` in the variables for the specified component and stack, and if it finds it,
  sets `AWS_REGION=<region>` ENV var before executing the command

- `atmos terraform generate backend` command generates a backend config file for an Atmos component in a stack

- `atmos terraform generate backends` command generates backend config files for all Atmos components in all stacks

- `atmos terraform generate varfile` command generates a varfile for an Atmos component in a stack

- `atmos terraform generate varfiles` command generates varfiles for all Atmos components in all stacks

- `atmos terraform plan-diff` command compares two Terraform plans and shows the differences between them. It takes an original plan file (`--orig`) and optionally a new plan file (`--new`). If the new plan file is not provided, it will generate one by running `terraform plan` with the current configuration.

- `atmos terraform shell` command configures an environment for an Atmos component in a stack and starts a new shell allowing executing all native
  terraform commands inside the shell

- double-dash `--` can be used to signify the end of the options for Atmos and the start of the additional native arguments and flags for
  the `terraform` commands. For example:
    - `atmos terraform plan <component> -s <stack> -- -refresh=false`
    - `atmos terraform apply <component> -s <stack> -- -lock=false`

:::tip
Run `atmos terraform --help` to see all the available options
:::

## Examples

<Terminal>
```shell
atmos terraform plan test/test-component-override-3 -s tenant1-ue2-dev
atmos terraform plan test/test-component-override-3 -s tenant1-ue2-dev --skip-lock-file
atmos terraform plan test/test-component-override-2 -s tenant1-ue2-dev --redirect-stderr /dev/stdout
atmos terraform plan test/test-component-override -s tenant1-ue2-dev --redirect-stderr ./errors.txt

atmos terraform apply test/test-component-override-3 -s tenant1-ue2-dev
atmos terraform apply test/test-component-override-2 -s tenant1-ue2-dev --redirect-stderr /dev/stdout
atmos terraform apply test/test-component-override -s tenant1-ue2-dev --redirect-stderr ./errors.txt

atmos terraform destroy test/test-component-override-3 -s tenant1-ue2-dev
atmos terraform destroy test/test-component-override-2 -s tenant1-ue2-dev --redirect-stderr /dev/stdout
atmos terraform destroy test/test-component-override -s tenant1-ue2-dev --redirect-stderr /dev/null

atmos terraform init test/test-component-override-3 -s tenant1-ue2-dev

# Clean all components (with confirmation)
atmos terraform clean

# Clean a specific component
atmos terraform clean vpc

# Clean a specific component in a stack
atmos terraform clean vpc --stack dev

# Clean without confirmation prompt
atmos terraform clean --force
atmos terraform clean test/test-component-override-3 -s tenant1-ue2-dev

atmos terraform workspace test/test-component-override-3 -s tenant1-ue2-dev
atmos terraform workspace test/test-component-override-3 -s tenant1-ue2-dev --redirect-stderr /dev/null
atmos terraform workspace test/test-component-override-3 -s tenant1-ue2-dev --redirect-stderr /dev/stdout
atmos terraform workspace test/test-component-override-3 -s tenant1-ue2-dev --redirect-stderr ./errors.txt

atmos terraform plan test/test-component -s tenant1-ue2-dev -- -refresh=false -lock=false

atmos terraform plan test/test-component -s tenant1-ue2-dev --append-user-agent "Acme/1.0 (Build 1234; arm64)"
```
</Terminal>

## Arguments

<dl>
    <dt>`component` (required for Single-Component commands)</dt>
    <dd>
        Atmos Terraform/OpenTofu component.
    </dd>
</dl>

## Flags

<dl>
    <dt>`--stack` (alias `-s`) (required for Single-Component commands)</dt>
    <dd>
        Atmos stack.

        ```shell
        atmos terraform plan <component> --stack <stack>
        atmos terraform apply --all -s <stack>
        ```
    </dd>

    <dt>`--dry-run` (optional)</dt>
    <dd>
        Dry run.
        Simulate the command without making any changes.

        ```shell
        atmos terraform <command> <component> -s <stack> --dry-run
        atmos terraform <command> --all --dry-run
        atmos terraform <command> --affected --dry-run
        ```
    </dd>

    <dt>`--redirect-stderr` (optional)</dt>
    <dd>
        File descriptor to redirect `stderr` to.

        Errors can be redirected to any file or any standard file descriptor (including `/dev/null`).
    </dd>

    <dt>`--append-user-agent` (optional)</dt>
    <dd>
        Append a custom User-Agent to Terraform requests.

        Can also be set using the `ATMOS_COMPONENTS_TERRAFORM_APPEND_USER_AGENT` environment variable.
    </dd>

    <dt>`--skip-init` (optional)</dt>
    <dd>
        Skip running `terraform init` before executing terraform commands.

        ```shell
        atmos terraform apply <component> -s <stack> --skip-init
        ```
    </dd>

    <dt>`--skip-planfile` (optional)</dt>
    <dd>
        Skip writing the plan to a file.
        If the flag is set to `true`, Atmos will not pass the `-out` flag to Terraform
        when executing `terraform plan` commands. Set it to `true` when using Terraform Cloud since the `-out` flag is not supported.
        Terraform Cloud automatically stores plans in its backend and can't store it in a local file

        ```shell
        atmos terraform plan <component> -s <stack> --skip-planfile=true
        ```
    </dd>

    <dt>`--process-templates` (optional)</dt>
    <dd>
        Enable/disable Go template processing in Atmos stack manifests when executing terraform commands.

        If the flag is not passed, template processing is enabled by default.

        ```shell
        atmos terraform plan <component> -s <stack> --process-templates=false
        ```
    </dd>

    <dt>`--process-functions` (optional)</dt>
    <dd>
        Enable/disable YAML functions processing in Atmos stack manifestswhen executing terraform commands.

        If the flag is not passed, YAML function processing is enabled by default.

        ```shell
        atmos terraform plan <component> -s <stack> --process-functions=false
        ```
    </dd>

    <dt>`--skip` (optional)</dt>
    <dd>
        Skip processing a specific Atmos YAML function in Atmos stacks manifests when executing terraform commands.

        To specify more than one function, use multiple `--skip` flags, or separate the functions with a comma.

        ```shell
        atmos terraform plan <component> -s <stack> --skip=eval --skip=include
        atmos terraform apply <component> -s <stack> --skip=terraform.output,include
        ```
    </dd>

    <dt>`--components` (optional)</dt>
    <dd>
        Execute the command on the specified components in all stacks or in a specific stack.

        ```shell
        atmos terraform plan --components <component-1>
        atmos terraform plan --components <component-1>,<component-2>
        atmos terraform apply --components <component-1> --components <component-2>
        atmos terraform apply --components <component-1>,<component-2> --stack <stack> --logs-level=Debug
        ```
    </dd>

    <dt>`--all` (optional)</dt>
    <dd>
        Execute the command on all components in all stacks or in a specific stack.

        ```shell
        atmos terraform plan --all
        atmos terraform apply --all --stack <stack>
        atmos terraform apply --all --dry-run
        atmos terraform deploy --all --logs-level=Debug
        ```
    </dd>

    <dt>`--query` (optional)</dt>
    <dd>
        Execute the command on the components filtered by a [YQ](https://mikefarah.gitbook.io/yq) expression, in all stacks or in a specific stack.

        __NOTE__: All Atmos sections are available in the expression, e.g. `vars`, `settings`, `env`, `metadata`, `backend`, etc.

        ```shell
        atmos terraform plan --query '.vars.tags.team == "data"'
        atmos terraform apply --query '.vars.tags.team == "eks"' --stack <stack>
        atmos terraform apply --query '.settings.context.account_id == 12345'
        atmos terraform deploy --query '.vars.tags.team == "data"' --dry-run --logs-level=Debug
        ```
    </dd>

    <dt>`--affected` (optional)</dt>
    <dd>
        Execute the command on all the directly affected components, in all stacks or in a specific stack,
        in dependency order (if [component dependencies](/core-concepts/stacks/dependencies/) are configured).

        __NOTE__: When using the `--affected` flag, Atmos supports all the flags from the [`atmos describe affected`](/cli/commands/describe/affected) CLI command.

        ```shell
        atmos terraform plan --affected
        atmos terraform apply --affected --stack <stack>
        atmos terraform apply --affected --dry-run
        atmos terraform apply --affected --clone-target-ref=true
        atmos terraform deploy --affected --include-dependents
        atmos terraform apply --affected --include-dependents --dry-run --logs-level=Debug
        ```
    </dd>

    <dt>`--include-dependents` (optional; can only be used in conjunction with the `--affected` flag)</dt>
    <dd>
        For each directly affected component, detect the dependent components and process them in dependency order, recursively.
        Dependents are components that are indirectly affected, meaning that nothing in the current branch modifies their code
        or configs, but they are configured as [dependencies](/core-concepts/stacks/dependencies/) of the components that are modified.

        ```shell
        atmos terraform plan --affected --include-dependents --logs-level=Debug
        atmos terraform apply --affected --include-dependents --dry-run
        atmos terraform apply --affected --include-dependents --stack prod --dry-run
        ```
    </dd>

    <dt>`--ref` (optional; can only be used in conjunction with the `--affected` flag)</dt>
    <dd>
        [Git Reference](https://git-scm.com/book/en/v2/Git-Internals-Git-References) with which to compare the current working branch.

        If the reference is a branch, the command will compare the current working branch with the branch.

        If the reference is a tag, the command will compare the current working branch with the tag.

        If the flags are not provided, the ref will be set automatically to the head to the default branch (`refs/remotes/origin/HEAD` Git ref, usually the `main` branch)
    </dd>

    <dt>`--sha` (optional; can only be used in conjunction with the `--affected` flag)</dt>
    <dd>
        Git commit SHA with which to compare the current working branch
    </dd>

    <dt>`--ssh-key` (optional; can only be used in conjunction with the `--affected` flag)</dt>
    <dd>
        Path to PEM-encoded private key to clone private repos using SSH
    </dd>

    <dt>`--ssh-key-password` (optional; can only be used in conjunction with the `--affected` flag)</dt>
    <dd>
        Encryption password for the PEM-encoded private key if the key contains a password-encrypted PEM block
    </dd>

    <dt>`--repo-path` (optional; can only be used in conjunction with the `--affected` flag)</dt>
    <dd>
        Path to the already cloned target repository with which to compare the current branch. Conflicts with `--ref`, `--sha`, `--ssh-key` and `--ssh-key-password`
    </dd>

    <dt>`--clone-target-ref` (optional; can only be used in conjunction with the `--affected` flag)</dt>
    <dd>
        Clone the target reference with which to compare the current branch.

        ```shell
        atmos terraform plan --affected --clone-target-ref=true
        atmos terraform deploy --affected --clone-target-ref=true --dry-run
        atmos terraform apply --affected --clone-target-ref=true --dry-run --logs-level=Debug
        ```

        If the flag is not passed or set to `false` (default), the target reference will be checked out instead.
        This requires that the target reference is already cloned by Git, and the information about it exists in the `.git` directory
    </dd>

</dl>

:::note

All native Terraform/OpenTofu flags are supported.

:::

## Multi-Component Commands (Bulk Operations) Examples

Let's assume that we have the following Atmos stack manifests in the `prod` and `nonprod` stacks,
with [dependencies between the components](/core-concepts/stacks/dependencies/):

<File title="`prod` and `nonprod` stacks">
```yaml
components:
  terraform:
    vpc:
      vars:
        tags:
          # Team `network` manages the `vpc` component
          team: network
    eks/cluster:
      vars:
        tags:
          # Team `eks` manages the `eks/cluster` component
          team: eks
      settings:
        depends_on:
          # `eks/cluster` depends on the `vpc` component
          1:
            component: vpc
    eks/external-dns:
      vars:
        tags:
          # Team `eks` manages the `eks/external-dns` component
          team: eks
      settings:
        depends_on:
          # `eks/external-dns` depends on the `eks/cluster` component
          1:
            component: eks/cluster
    eks/karpenter:
      vars:
        tags:
          # Team `eks` manages the `eks/karpenter` component
          team: eks
      settings:
        depends_on:
          # `eks/karpenter` depends on the `eks/cluster` component
          1:
            component: eks/cluster
    eks/karpenter-node-pool:
      vars:
        tags:
          # Team `eks` manages the `eks/karpenter-node-pool` component
          team: eks
      settings:
        # `eks/karpenter-node-pool` depends on the `eks/cluster` and `eks/karpenter` components
        depends_on:
          1:
            component: eks/cluster
          2:
            component: eks/karpenter
    eks/istio/base:
      vars:
        tags:
          # Team `istio` manages the `eks/istio/base` component
          team: istio
      settings:
        # `eks/istio/base` depends on the `eks/cluster` component
        depends_on:
          1:
            component: eks/cluster
    eks/istio/istiod:
      vars:
        tags:
          # Team `istio` manages the `eks/istio/istiod` component
          team: istio
      settings:
        # `eks/istio/istiod` depends on the `eks/cluster` and `eks/istio/base` components
        depends_on:
          1:
            component: eks/cluster
          2:
            component: eks/istio/base
    eks/istio/test-app:
      vars:
        tags:
          # Team `istio` manages the `eks/istio/test-app` component
          team: istio
      settings:
        # `eks/istio/test-app` depends on the `eks/cluster`, `eks/istio/istiod` and `eks/istio/base` components
        depends_on:
          1:
            component: eks/cluster
          2:
            component: eks/istio/istiod
          3:
            component: eks/istio/base
```
</File>

Let's run the following Multi-Component commands in `dry-run` mode and review the output to understand what each command executes:

<Terminal title="atmos terraform apply --all --dry-run">
```shell
# Execute the `terraform apply` command on all components in all stacks

> atmos terraform apply --all --dry-run

Executing command="atmos terraform apply vpc -s nonprod"
Executing command="atmos terraform apply eks/cluster -s nonprod"
Executing command="atmos terraform apply eks/external-dns -s nonprod"
Executing command="atmos terraform apply eks/istio/base -s nonprod"
Executing command="atmos terraform apply eks/istio/istiod -s nonprod"
Executing command="atmos terraform apply eks/istio/test-app -s nonprod"
Executing command="atmos terraform apply eks/karpenter -s nonprod"
Executing command="atmos terraform apply eks/karpenter-node-pool -s nonprod"

Executing command="atmos terraform apply vpc -s prod"
Executing command="atmos terraform apply eks/cluster -s prod"
Executing command="atmos terraform apply eks/external-dns -s prod"
Executing command="atmos terraform apply eks/istio/base -s prod"
Executing command="atmos terraform apply eks/istio/istiod -s prod"
Executing command="atmos terraform apply eks/istio/test-app -s prod"
Executing command="atmos terraform apply eks/karpenter -s prod"
Executing command="atmos terraform apply eks/karpenter-node-pool -s prod"
```
</Terminal>

<Terminal title="atmos terraform apply --all --stack prod --dry-run">
```shell
# Execute the `terraform apply` command on all components in the `prod` stack

> atmos terraform apply --all --stack prod --dry-run

Executing command="atmos terraform apply vpc -s prod"
Executing command="atmos terraform apply eks/cluster -s prod"
Executing command="atmos terraform apply eks/external-dns -s prod"
Executing command="atmos terraform apply eks/istio/base -s prod"
Executing command="atmos terraform apply eks/istio/istiod -s prod"
Executing command="atmos terraform apply eks/istio/test-app -s prod"
Executing command="atmos terraform apply eks/karpenter -s prod"
Executing command="atmos terraform apply eks/karpenter-node-pool -s prod"
```
</Terminal>

<Terminal title="atmos terraform apply --stack prod --dry-run">
```shell
# Execute the `terraform apply` command on all components in the `prod` stack

> atmos terraform apply --stack prod --dry-run

Executing command="atmos terraform apply vpc -s prod"
Executing command="atmos terraform apply eks/cluster -s prod"
Executing command="atmos terraform apply eks/external-dns -s prod"
Executing command="atmos terraform apply eks/istio/base -s prod"
Executing command="atmos terraform apply eks/istio/istiod -s prod"
Executing command="atmos terraform apply eks/istio/test-app -s prod"
Executing command="atmos terraform apply eks/karpenter -s prod"
Executing command="atmos terraform apply eks/karpenter-node-pool -s prod"
```
</Terminal>

<Terminal title="atmos terraform apply --components vpc,eks/cluster --dry-run">
```shell
# Execute the `terraform apply` command on the `vpc` and `eks/cluster` components
# in all stacks.

> atmos terraform apply --components vpc,eks/cluster --dry-run

Executing command="atmos terraform apply vpc -s nonprod"
Executing command="atmos terraform apply eks/cluster -s nonprod"

Executing command="atmos terraform apply vpc -s prod"
Executing command="atmos terraform apply eks/cluster -s prod"
```
</Terminal>

<Terminal title="atmos terraform apply --stack prod --components vpc,eks/cluster --dry-run">
```shell
# Execute the `terraform apply` command on the `vpc` and `eks/cluster` components
# in the `prod` stack.

> atmos terraform apply --stack prod --components vpc,eks/cluster --dry-run

Executing command="atmos terraform apply vpc -s prod"
Executing command="atmos terraform apply eks/cluster -s prod"
```
</Terminal>

<Terminal title="atmos terraform apply --query '.vars.tags.team == &quot;eks&quot;' --dry-run">
```shell
# Execute the `terraform apply` command on the components filtered by the query expression,
# in all stacks.

> atmos terraform apply --query '.vars.tags.team == "eks"' --dry-run

Skipping the component because the query criteria not satisfied command="atmos terraform apply vpc -s nonprod" query=".vars.tags.team == \"eks\""
Executing command="atmos terraform apply eks/cluster -s nonprod"
Executing command="atmos terraform apply eks/external-dns -s nonprod"
Skipping the component because the query criteria not satisfied command="atmos terraform apply eks/istio/base -s nonprod" query=".vars.tags.team == \"eks\""
Skipping the component because the query criteria not satisfied command="atmos terraform apply eks/istio/istiod -s nonprod" query=".vars.tags.team == \"eks\""
Skipping the component because the query criteria not satisfied command="atmos terraform apply eks/istio/test-app -s nonprod" query=".vars.tags.team == \"eks\""
Executing command="atmos terraform apply eks/karpenter -s nonprod"
Executing command="atmos terraform apply eks/karpenter-node-pool -s nonprod"

Skipping the component because the query criteria not satisfied command="atmos terraform apply vpc -s prod" query=".vars.tags.team == \"eks\""
Executing command="atmos terraform apply eks/cluster -s prod"
Executing command="atmos terraform apply eks/external-dns -s prod"
Skipping the component because the query criteria not satisfied command="atmos terraform apply eks/istio/base -s prod" query=".vars.tags.team == \"eks\""
Skipping the component because the query criteria not satisfied command="atmos terraform apply eks/istio/istiod -s prod" query=".vars.tags.team == \"eks\""
Skipping the component because the query criteria not satisfied command="atmos terraform apply eks/istio/test-app -s prod" query=".vars.tags.team == \"eks\""
Executing command="atmos terraform apply eks/karpenter -s prod"
Executing command="atmos terraform apply eks/karpenter-node-pool -s prod"
```
</Terminal>

<Terminal title="atmos terraform apply --query '.vars.tags.team == &quot;eks&quot;' --stack prod --dry-run">
```shell
# Execute the `terraform apply` command on the components filtered by the query expression,
# in the `prod` stack.

> atmos terraform apply --query '.vars.tags.team == "eks"' --stack prod --dry-run

Skipping the component because the query criteria not satisfied command="atmos terraform apply vpc -s prod" query=".vars.tags.team == \"eks\""
Executing command="atmos terraform apply eks/cluster -s prod"
Executing command="atmos terraform apply eks/external-dns -s prod"
Skipping the component because the query criteria not satisfied command="atmos terraform apply eks/istio/base -s prod" query=".vars.tags.team == \"eks\""
Skipping the component because the query criteria not satisfied command="atmos terraform apply eks/istio/istiod -s prod" query=".vars.tags.team == \"eks\""
Skipping the component because the query criteria not satisfied command="atmos terraform apply eks/istio/test-app -s prod" query=".vars.tags.team == \"eks\""
Executing command="atmos terraform apply eks/karpenter -s prod"
Executing command="atmos terraform apply eks/karpenter-node-pool -s prod"
```
</Terminal>

<Terminal title="atmos terraform apply --affected --dry-run">
```shell
# Execute the `terraform apply` command on all components affected by the changes
# in the current branch, in all stacks, in dependency order.
# Assume that the components `vpc` and `eks/cluster` in all stacks are affected (e.g. just added).

> atmos terraform apply --affected --dry-run

Executing command="atmos terraform apply vpc -s nonprod"
Executing command="atmos terraform apply eks/cluster -s nonprod"

Executing command="atmos terraform apply vpc -s prod"
Executing command="atmos terraform apply eks/cluster -s prod"
```
</Terminal>

<Terminal title="atmos terraform apply --affected --stack prod --dry-run">
```shell
# Execute the `terraform apply` command on all components affected by the changes
# in the current branch, in the `prod` stack, in dependency order.
# Assume that the components `vpc` and `eks/cluster` in the `prod` stack are affected (e.g. just added).

> atmos terraform apply --affected --stack prod --dry-run

Executing command="atmos terraform apply vpc -s prod"
Executing command="atmos terraform apply eks/cluster -s prod"
```
</Terminal>

<Terminal title="atmos terraform apply --affected --include-dependents --dry-run">
```shell
# Execute the `terraform apply` command on all the components affected by the changes
# in the current branch, in all stacks.
# For each directly affected component, detect the dependent components and process
# them in dependency order, recursively.
# Dependents are components that are indirectly affected, meaning that nothing in the
# current branch modifies their code or configs, but they are configured as
# dependencies of the components that are modified.

> atmos terraform apply --affected --include-dependents --dry-run

Executing command="atmos terraform apply vpc -s nonprod"
Executing command="atmos terraform apply eks/cluster -s nonprod" dependency of component=vpc in stack=nonprod
Executing command="atmos terraform apply eks/karpenter -s nonprod" dependency of component=eks/cluster in stack=nonprod
Executing command="atmos terraform apply eks/karpenter-node-pool -s nonprod" dependency of component=eks/karpenter in stack=nonprod
Executing command="atmos terraform apply eks/external-dns -s nonprod" dependency of component=eks/cluster in stack=nonprod
Executing command="atmos terraform apply eks/istio/base -s nonprod" dependency of component=eks/cluster in stack=nonprod
Executing command="atmos terraform apply eks/istio/istiod -s nonprod" dependency of component=eks/istio/base in stack=nonprod
Executing command="atmos terraform apply eks/istio/test-app -s nonprod" dependency of component=eks/istio/istiod in stack=nonprod

Executing command="atmos terraform apply vpc -s prod"
Executing command="atmos terraform apply eks/cluster -s prod" dependency of component=vpc in stack=prod
Executing command="atmos terraform apply eks/external-dns -s prod" dependency of component=eks/cluster in stack=prod
Executing command="atmos terraform apply eks/istio/base -s prod" dependency of component=eks/cluster in stack=prod
Executing command="atmos terraform apply eks/istio/istiod -s prod" dependency of component=eks/istio/base in stack=prod
Executing command="atmos terraform apply eks/istio/test-app -s prod" dependency of component=eks/istio/istiod in stack=prod
Executing command="atmos terraform apply eks/karpenter -s prod" dependency of component=eks/cluster in stack=prod
Executing command="atmos terraform apply eks/karpenter-node-pool -s prod" dependency of component=eks/karpenter in stack=prod
```
</Terminal>

<Terminal title="atmos terraform apply --affected --stack prod --include-dependents --dry-run">
```shell
# Execute the `terraform apply` command on all the components affected by the changes
# in the current branch, in the `prod` stack.
# For each directly affected component, detect the dependent components and process
# them in dependency order, recursively.
# Dependents are components that are indirectly affected, meaning that nothing in the
# current branch modifies their code or configs, but they are configured as
# dependencies of the components that are modified.

> atmos terraform apply --affected --stack prod --include-dependents --dry-run

Executing command="atmos terraform apply vpc -s prod"
Executing command="atmos terraform apply eks/cluster -s prod" dependency of component=vpc in stack=prod
Executing command="atmos terraform apply eks/external-dns -s prod" dependency of component=eks/cluster in stack=prod
Executing command="atmos terraform apply eks/istio/base -s prod" dependency of component=eks/cluster in stack=prod
Executing command="atmos terraform apply eks/istio/istiod -s prod" dependency of component=eks/istio/base in stack=prod
Executing command="atmos terraform apply eks/istio/test-app -s prod" dependency of component=eks/istio/istiod in stack=prod
Executing command="atmos terraform apply eks/karpenter -s prod" dependency of component=eks/cluster in stack=prod
Executing command="atmos terraform apply eks/karpenter-node-pool -s prod" dependency of component=eks/karpenter in stack=prod
```
</Terminal>

## Subcommands

<DocCardList />

---

## atmos validate

import Screengrab from '@site/src/components/Screengrab'
import DocCardList from '@theme/DocCardList';
import Intro from '@site/src/components/Intro'

<Intro>
Use these subcommands to validate Atmos configurations.
</Intro>

<Screengrab title="atmos validate --help" slug="atmos-validate--help" />

## Subcommands

<DocCardList />

---

## atmos validate component

import Screengrab from '@site/src/components/Screengrab'

:::note purpose
Use this command to validate an Atmos component in a stack using JSON Schema and OPA policies.
:::

<Screengrab title="atmos validate component --help" slug="atmos-validate-component--help" />

## Usage

Execute the `validate component` command like this:

```shell
atmos validate component <component> -s <stack> [options]
```

This command validates an Atmos component in a stack using JSON Schema and OPA policies.

:::tip
Run `atmos validate component --help` to see all the available options
:::

## Examples

```shell
atmos validate component infra/vpc -s tenant1-ue2-dev
atmos validate component infra/vpc -s tenant1-ue2-dev --schema-path vpc/validate-infra-vpc-component.json --schema-type jsonschema
atmos validate component infra/vpc -s tenant1-ue2-dev --schema-path vpc/validate-infra-vpc-component.rego --schema-type opa
atmos validate component infra/vpc -s tenant1-ue2-dev --schema-path vpc/validate-infra-vpc-component.rego --schema-type opa --module-paths catalog/constants
atmos validate component infra/vpc -s tenant1-ue2-dev --schema-path vpc/validate-infra-vpc-component.rego --schema-type opa --module-paths catalog
atmos validate component infra/vpc -s tenant1-ue2-dev --timeout 15
```

## Arguments

<dl>
  <dt>`component` (required)</dt>
  <dd>Atmos component.</dd>
</dl>

## Flags

<dl>
  <dt>`--stack` / `-s` (required)</dt>
  <dd>Atmos stack.</dd>

  <dt>`--schema-path` (optional)</dt>
  <dd>Path to the schema file.Can be an absolute path or a path relative to `schemas.jsonschema.base_path`and `schemas.opa.base_path` defined in `atmos.yaml`.</dd>

  <dt>`--schema-type` (optional)</dt>
  <dd>Schema type: `jsonschema` or `opa`.</dd>

  <dt>`--module-paths` (optional)</dt>
  <dd>Comma-separated string of filesystem paths (folders or individual files) to the additional modulesfor schema validation. Each path can be an absolute path or a path relative to`schemas.opa.base_path` defined in `atmos.yaml`.</dd>

  <dt>`--timeout` (optional)</dt>
  <dd>Validation timeout in seconds. Can also be specified in `settings.validation` component config. If not provided, timeout of 20 seconds is used by default.</dd>
</dl>

---

## atmos validate editorconfig

import Screengrab from '@site/src/components/Screengrab'

:::note purpose
Use this command to validate files against the rules defined in .editorconfig file.
:::

<Screengrab title="atmos validate editorconfig --help" slug="atmos-validate-editorconfig--help" />

## Usage

Execute the `validate editorconfig` command like this:

```shell
atmos validate editorconfig
```

This command validates files against the formatting rules defined in your .editorconfig file.

:::tip
Run `atmos validate editorconfig --help` to see all the available options
:::

## Examples

```shell
atmos validate editorconfig
atmos validate editorconfig --logs-level Trace
atmos validate editorconfig --no-color
atmos validate editorconfig --dry-run
```

## Flags

<dl>
  <dt>`--config` (optional)</dt>
  <dd>Path to the configuration file (e.g., `.editorconfig`, `.editorconfig-checker.json`, `.ecrc`).</dd>

  <dt>`--disable-end-of-line` (optional)</dt>
  <dd>Disable end-of-line check (default `false`).</dd>

  <dt>`--disable-indent-size` (optional)</dt>
  <dd>Disable indent size check (default `false`).</dd>

  <dt>`--disable-indentation` (optional)</dt>
  <dd>Disable indentation check (default `false`).</dd>

  <dt>`--disable-insert-final-newline` (optional)</dt>
  <dd>Disable final newline check (default `false`).</dd>

  <dt>`--disable-max-line-length` (optional)</dt>
  <dd>Disable max line length check (default `false`).</dd>

  <dt>`--disable-trim-trailing-whitespace` (optional)</dt>
  <dd>Disable trailing whitespace check (default `false`).</dd>

  <dt>`--dry-run` (optional)</dt>
  <dd>Show which files would be checked (default `false`).</dd>

  <dt>`--exclude` (optional)</dt>
  <dd>Regex to exclude files from checking.</dd>

  <dt>`--format` (optional)</dt>
  <dd>Specify the output format: default, gcc (default `default`).</dd>

  <dt>`--help` (optional)</dt>
  <dd>help for editorconfig.</dd>

  <dt>`--ignore-defaults` (optional)</dt>
  <dd>Ignore default excludes (default `false`).</dd>

  <dt>`--init` (optional)</dt>
  <dd>Create an initial configuration (default `false`).</dd>

  <dt>`--no-color` (optional)</dt>
  <dd>Don't print colors (default `false`).</dd>

  <dt>`--version` (optional)</dt>
  <dd>Print the version number (default `false`).</dd>
</dl>

---

## atmos validate schema

import Screengrab from '@site/src/components/Screengrab'

:::note purpose
Use this command to validate files against the rules defined in your schema in atmos.yaml.
:::

<Screengrab title="atmos validate schema --help" slug="atmos-validate-schema--help" />

## Usage

Execute the `validate schema` command like this:

```shell
atmos validate schema
```

This command validates files against the formatting rules defined in your .editorconfig file.

:::tip
Run `atmos validate schema --help` to see all the available options
:::

## Examples

```shell
atmos validate schema
atmos validate schema <my_custom_key>
```

## How to set my schema validators?

You need to use the `schemas` key in atmos config.

```yaml
schemas:
    my_custom_key:
        schema: !import https://www.jsonschema.com/example.json # json to be used for validation
        matches:
            - folder/*.yaml # pattern of the file to be validated
```

## Flags

<dl>
  <dt>`--schemas-atmos-manifest` (optional)</dt>
  <dd>Specifies the path to a JSON schema file used to validate the structure and content of the Atmos manifest file.</dd>
</dl>

---

## atmos validate stacks

import Screengrab from '@site/src/components/Screengrab'
import Terminal from '@site/src/components/Terminal'
import Intro from '@site/src/components/Intro'

<Intro>
Use this command to validate Atmos stack manifest configurations.
</Intro>

<Screengrab title="atmos validate stacks --help" slug="atmos-validate-stacks--help" />

## Usage

Execute the `validate stacks` command like this:

```shell
atmos validate stacks
```

This command validates Atmos stack manifests and checks the following:

- All YAML manifest files for YAML errors and inconsistencies
  - Note: Template files (`.yaml.tmpl`, `.yml.tmpl`, `.tmpl`) are excluded from validation since they may contain template placeholders that are invalid YAML before being rendered
  - Template files are still automatically detected and processed during normal operations (imports, etc.)

- All imports: if they are configured correctly, have valid data types, and point to existing manifest files

- Schema: if all sections in all YAML manifest files are correctly configured and have valid data types

- Misconfiguration and duplication of components in stacks. If the same Atmos component in the same Atmos stack is
  defined in more than one stack manifest file, and the component configurations are different, an error message will
  be displayed similar to the following:

    <Terminal title="atmos validate stacks">
        ```console
        The Atmos component 'vpc' in the stack 'plat-ue2-dev' is defined in more than one
        top-level stack manifest file: orgs/acme/plat/dev/us-east-2-extras, orgs/acme/plat/dev/us-east-2.

        The component configurations in the stack manifests are different.

        To check and compare the component configurations in the stack manifests, run the following commands:
        - atmos describe component vpc -s orgs/acme/plat/dev/us-east-2-extras
        - atmos describe component vpc -s orgs/acme/plat/dev/us-east-2

        You can use the '--file' flag to write the results of the above commands to files
        (refer to https://atmos.tools/cli/commands/describe/component).

        You can then use the Linux 'diff' command to compare the files line by line and show the differences
        (refer to https://man7.org/linux/man-pages/man1/diff.1.html)

        When searching for the component 'vpc' in the stack 'plat-ue2-dev', Atmos can't decide which
        stack manifest file to use to get configuration for the component. This is a stack misconfiguration.

        Consider the following solutions to fix the issue:

        - Ensure that the same instance of the Atmos 'vpc' component in the stack 'plat-ue2-dev'
          is only defined once (in one YAML stack manifest file)

        - When defining multiple instances of the same component in the stack,
          ensure each has a unique name

        - Use multiple-inheritance to combine multiple configurations together
          (refer to https://atmos.tools/core-concepts/stacks/inheritance)
        ```
    </Terminal>

:::tip
Run `atmos validate stacks --help` to see all the available options
:::

## Examples

```shell
# Use the default (embedded) JSON Schema
atmos validate stacks

# Point to the JSON Schema on the local filesystem
atmos validate stacks --schemas-atmos-manifest schemas/atmos/atmos-manifest/1.0/atmos-manifest.json

# Point to the remote JSON Schema
atmos validate stacks --schemas-atmos-manifest https://atmos.tools/schemas/atmos/atmos-manifest/1.0/atmos-manifest.json
```

## Flags

<dl>
  <dt>`--schemas-atmos-manifest` (optional)</dt>
  <dd>Path to JSON Schema to validate Atmos stack manifests.Can be a URL, an absolute path,or a path relative to the `base_path` setting in `atmos.yaml`.</dd>
</dl>

## Validate Atmos Manifests using JSON Schema

Atmos uses the [Atmos Manifest JSON Schema](pathname:///schemas/atmos/atmos-manifest/1.0/atmos-manifest.json) to validate Atmos manifests, and has a default (embedded) JSON Schema.

If you don't configure the path to a JSON Schema in `atmos.yaml` and don't provide it on the command line using the `--schemas-atmos-manifest` flag,
the default (embedded) JSON Schema will be used when executing the command `atmos validate stacks`.

To override the default behavior, configure JSON Schema in `atmos.yaml`:

- Add the [Atmos Manifest JSON Schema](pathname:///schemas/atmos/atmos-manifest/1.0/atmos-manifest.json) to your repository, for example
  in  [`stacks/schemas/atmos/atmos-manifest/1.0/atmos-manifest.json`](https://github.com/cloudposse/atmos/blob/main/examples/quick-start-advanced/stacks/schemas/atmos/atmos-manifest/1.0/atmos-manifest.json)

- Configure the following section in the `atmos.yaml` [CLI config file](/cli/configuration)

  ```yaml title="atmos.yaml"
  # Validation schemas (for validating atmos stacks and components)
  schemas:
    # JSON Schema to validate Atmos manifests
    atmos:
      # Can also be set using 'ATMOS_SCHEMAS_ATMOS_MANIFEST' ENV var, or '--schemas-atmos-manifest' command-line arguments
      # Supports both absolute and relative paths (relative to the `base_path` setting in `atmos.yaml`)
      manifest: "stacks/schemas/atmos/atmos-manifest/1.0/atmos-manifest.json"
      # Also supports URLs
      # manifest: "https://atmos.tools/schemas/atmos/atmos-manifest/1.0/atmos-manifest.json"
  ```

- Instead of configuring the `schemas.atmos.manifest` section in `atmos.yaml`, you can provide the path to
  the [Atmos Manifest JSON Schema](pathname:///schemas/atmos/atmos-manifest/1.0/atmos-manifest.json) file by using the ENV variable `ATMOS_SCHEMAS_ATMOS_MANIFEST`
  or the `--schemas-atmos-manifest` command line flag:

  ```shell
  ATMOS_SCHEMAS_ATMOS_MANIFEST=stacks/schemas/atmos/atmos-manifest/1.0/atmos-manifest.json atmos validate stacks
  atmos validate stacks --schemas-atmos-manifest stacks/schemas/atmos/atmos-manifest/1.0/atmos-manifest.json
  atmos validate stacks --schemas-atmos-manifest https://atmos.tools/schemas/atmos/atmos-manifest/1.0/atmos-manifest.json
  ```

In case of any validation errors (invalid YAML syntax, Atmos manifest JSON Schema errors, invalid imports, etc.), you'll get an output from the
command similar to the following:

<Terminal title="atmos validate stacks">
```console
no matches found for the import 'globals/tenant1-globals-does-not-exist' in the
file 'catalog/invalid-yaml-and-schema/invalid-import-1.yaml'

invalid import in the file 'catalog/invalid-yaml-and-schema/invalid-import-2.yaml'
The file imports itself in 'catalog/invalid-yaml-and-schema/invalid-import-2'

invalid stack manifest 'catalog/invalid-yaml-and-schema/invalid-yaml-1.yaml'
yaml: line 15: found unknown directive name

invalid stack manifest 'catalog/invalid-yaml-and-schema/invalid-yaml-3.yaml'
yaml: line 13: did not find expected key

invalid stack manifest 'catalog/invalid-yaml-and-schema/invalid-yaml-5.yaml'
yaml: mapping values are not allowed in this context

invalid stack manifest 'catalog/invalid-yaml-and-schema/invalid-yaml-6.yaml'
yaml: line 2: block sequence entries are not allowed in this context

invalid stack manifest 'catalog/invalid-yaml-and-schema/invalid-yaml-7.yaml'
yaml: line 4: could not find expected ':'

Atmos manifest JSON Schema validation error in the
file 'catalog/invalid-yaml-and-schema/invalid-import-5.yaml':
{
  "valid": false,
  "errors": [
    {
      "keywordLocation": "",
      "absoluteKeywordLocation": "tests/fixtures/scenarios/complete/schemas/atmos/atmos-manifest/1.0/atmos-manifest.json#",
      "instanceLocation": "",
      "error": "doesn't validate with tests/fixtures/scenarios/complete/schemas/atmos/atmos-manifest/1.0/atmos-manifest.json#"
    },
    {
      "keywordLocation": "/properties/import/$ref",
      "absoluteKeywordLocation": "tests/fixtures/scenarios/complete/schemas/atmos/atmos-manifest/1.0/atmos-manifest.json#/properties/import/$ref",
      "instanceLocation": "/import",
      "error": "doesn't validate with '/definitions/import'"
    },
    {
      "keywordLocation": "/properties/import/$ref/type",
      "absoluteKeywordLocation": "tests/fixtures/scenarios/complete/schemas/atmos/atmos-manifest/1.0/atmos-manifest.json#/definitions/import/type",
      "instanceLocation": "/import",
      "error": "expected array, but got object"
    }
  ]
}

Atmos manifest JSON Schema validation error in the
file 'catalog/invalid-yaml-and-schema/invalid-schema-8.yaml':
{
  "valid": false,
  "errors": [
    {
      "keywordLocation": "",
      "absoluteKeywordLocation": "tests/fixtures/scenarios/complete/schemas/atmos/atmos-manifest/1.0/atmos-manifest.json#",
      "instanceLocation": "",
      "error": "doesn't validate with tests/fixtures/scenarios/complete/schemas/atmos/atmos-manifest/1.0/atmos-manifest.json#"
    },
    {
      "keywordLocation": "/properties/env/$ref",
      "absoluteKeywordLocation": "tests/fixtures/scenarios/complete/schemas/atmos/atmos-manifest/1.0/atmos-manifest.json#/properties/env/$ref",
      "instanceLocation": "/env",
      "error": "doesn't validate with '/definitions/env'"
    },
    {
      "keywordLocation": "/properties/env/$ref/type",
      "absoluteKeywordLocation": "tests/fixtures/scenarios/complete/schemas/atmos/atmos-manifest/1.0/atmos-manifest.json#/definitions/env/type",
      "instanceLocation": "/env",
      "error": "expected object, but got array"
    }
  ]
}
```
</Terminal>

---

## atmos vendor

import Screengrab from '@site/src/components/Screengrab'
import DocCardList from '@theme/DocCardList';
import Intro from '@site/src/components/Intro'

<Intro>
Use these subcommands to vendor Atmos components and stacks.
</Intro>

<Screengrab title="atmos vendor --help" slug="atmos-vendor--help" />

## Subcommands
<DocCardList />

---

## atmos vendor pull

import Screengrab from '@site/src/components/Screengrab'
import Intro from '@site/src/components/Intro'

<Intro>
This command implements [Atmos Vendoring](/core-concepts/vendor/). Use this command to download sources from local and remote
repositories for Terraform and Helmfile components and stacks.
</Intro>

<Screengrab title="atmos vendor pull --help" slug="atmos-vendor-pull--help" />

With Atmos vendoring, you can copy components and other artifacts from the following sources:

- Copy all files from an [OCI Registry](https://opencontainers.org) into a local folder
- Copy all files from Git, Mercurial, Amazon S3, Google GCP into a local folder
- Copy all files from an HTTP/HTTPS endpoint into a local folder
- Copy a single file from an HTTP/HTTPS endpoint to a local file
- Copy a local file into a local folder (keeping the same file name)
- Copy a local file to a local file with a different file name
- Copy a local folder (all files) into a local folder

## Usage

Execute the `vendor pull` command like this:

```shell
atmos vendor pull
atmos vendor pull --everything
atmos vendor pull --component <component> [options]
atmos vendor pull -c <component> [options]
atmos vendor pull --tags <tag1>,<tag2> [options]
```

## Description

Atmos supports two different ways of vendoring components, stacks and other artifacts:

- Using `component.yaml` vendoring manifest
- Using `vendor.yaml` vendoring manifest

The `component.yaml` vendoring manifest can be used to vendor components from remote repositories.
A `component.yaml` file placed into a component's directory is used to describe the vendoring config for one component only.
Using `component.yaml` is not recommended, and it's maintained for backwards compatibility.

The `vendor.yaml` vendoring manifest provides more functionality than using `component.yaml` files.
It's used to describe vendoring config for all components, stacks and other artifacts for the entire infrastructure.
The file is placed into the directory from which the `atmos vendor pull` command is executed. It's the recommended way to describe vendoring
configurations.

## Vendoring using `vendor.yaml` manifest

- The `vendor.yaml` vendoring manifest supports Kubernetes-style YAML config to describe vendoring configuration for components, stacks,
  and other artifacts.

- The `source` attribute supports all protocols (local files, Git, Mercurial, HTTP, HTTPS, Amazon S3, Google GCP), and all URL and
  archive formats as described in [go-getter](https://github.com/hashicorp/go-getter), and also the `oci://` scheme to download artifacts from
  [OCI registries](https://opencontainers.org).

- The `targets` in the `sources` support absolute paths and relative paths (relative to the `vendor.yaml` file). Note: if the `targets` paths
  are set as relative, and if the `vendor.yaml` file is detected by Atmos using the `base_path` setting in `atmos.yaml`, the `targets` paths
  will be considered relative to the `base_path`. Multiple targets can be specified.

- `included_paths` and `excluded_paths` support [POSIX-style greedy Globs](https://en.wikipedia.org/wiki/Glob_(programming)) for filenames/paths
  (double-star/globstar `**` is supported as well).

- The `tags` in each source specifies a list of tags to apply to the component. This allows you to only vendor the components that have the
  specified tags by executing a command `atmos vendor pull --tags <tag1>,<tag2>`

:::tip
Refer to [`Atmos Vendoring`](/core-concepts/vendor) for more details
:::

## Vendoring using `component.yaml` manifest

- The `component.yaml` vendoring manifest supports Kubernetes-style YAML config to describe component vendoring configuration.
  The file is placed into the component's folder.

- The URIs (`uri`) in `component.yaml` support all protocols (local files, Git, Mercurial, HTTP, HTTPS, Amazon S3, Google GCP), and all URL and
  archive formats as described in [go-getter](https://github.com/hashicorp/go-getter), and also the `oci://` scheme to download artifacts from
  [OCI registries](https://opencontainers.org).

- `included_paths` and `excluded_paths` in `component.yaml` support [POSIX-style greedy Globs](https://en.wikipedia.org/wiki/Glob_(programming)) for
  file names/paths (double-star/globstar `**` is supported as well).

:::tip
Refer to [`Atmos Component Vendoring`](/core-concepts/vendor/component-manifest) for more details
:::

## Vendoring from OCI Registries

The following config can be used to download the `vpc` component from an AWS public ECR registry:

  ```yaml
  apiVersion: atmos/v1
  kind: ComponentVendorConfig
  metadata:
    name: vpc-vendor-config
    description: Config for vendoring of 'vpc' component
  spec:
    source:
      # Download the component from the AWS public ECR registry (https://docs.aws.amazon.com/AmazonECR/latest/public/public-registries.html)
      uri: "oci://public.ecr.aws/cloudposse/components/terraform/stable/aws/vpc:{{.Version}}"
      version: "latest"
  ```

## Vendoring from SSH

Atmos supports SSH for accessing non-public Git repositories, which is convenient for local development. Atmos will use any installed SSH keys automatically.

:::tip
In automated systems like GitHub Actions, we recommend sticking with the `https://` scheme for vendoring. Atmos will automatically inject the `GITHUB_TOKEN`.
:::

There are two primary ways to specify an SSH source.

### SCP-style Sources

Atmos supports traditional SCP-style sources, which use a colon to separate the host from the repository, like this:

```shell
git::git@github.com:cloudposse/terraform-null-label.git?ref={{.Version}}
```

Atmos rewrites this URL to the following format:

```shell
git::ssh://git@github.com/cloudposse/terraform-null-label.git?depth=1&ref={{.Version}}
```

If no username is supplied and the host is `github.com`, Atmos automatically injects the default username `git`.

### Explicit SSH Sources

When the `ssh://` scheme is explicitly specified, the URL is used as provided, and no rewriting occurs.

For example:

```shell
git::ssh://git@github.com/cloudposse/terraform-null-label.git?ref={{ .Version }}
```

### Important Notes

- The following URL is **invalid** because `go-getter` misinterprets `github.com:` as a URL scheme (like `http:` or `git:`), causing a parsing error:
  ```shell
  github.com:cloudposse/terraform-null-label.git?ref={{ .Version }}
  ```

- When a URL has no scheme, Atmos defaults to **HTTPS** and injects credentials if available.
  ```shell
  github.com/cloudposse/terraform-null-label.git?ref={{ .Version }}
  ```

## Git over HTTPS Vendoring

Atmos supports vendoring components using **Git over HTTPS**.

For example:
```
github.com/cloudposse/terraform-null-label.git?ref={{ .Version }}
```
is automatically resolved as:
```
git::https://github.com/cloudposse/terraform-null-label.git?depth=1&ref={{ .Version }}
```

## Authentication & Token Usage for HTTPS

Atmos prioritizes authentication credentials based on predefined environment variables. The priority order for each provider is:

### GitHub
<dl>
 <dt>`ATMOS_GITHUB_TOKEN`</dt>
 <dd>Bearer token for GitHub API requests, enabling authentication for private repositories and higher rate limits.</dd>

 <dt>`GITHUB_TOKEN`</dt>
 <dd>Used as a fallback if `ATMOS_GITHUB_TOKEN` is not set.</dd>
</dl>

**Default Username for HTTPS:** `x-access-token`

### Bitbucket
<dl>
 <dt>`ATMOS_BITBUCKET_TOKEN`</dt>
 <dd>Bitbucket app password for API requests; used to avoid rate limits. When both `ATMOS_BITBUCKET_TOKEN` and `BITBUCKET_TOKEN` are defined, the former prevails.</dd>

 <dt>`BITBUCKET_TOKEN`</dt>
 <dd>Used as a fallback when `ATMOS_BITBUCKET_TOKEN` is not set.</dd>

 <dt>`ATMOS_BITBUCKET_USERNAME`</dt>
 <dd>Bitbucket username for authentication. Takes precedence over `BITBUCKET_USERNAME`.</dd>

 <dt>`BITBUCKET_USERNAME`</dt>
 <dd>Used as a fallback when `ATMOS_BITBUCKET_USERNAME` is not set. Bitbucket requires a valid username and does not accept dummy values like `x-access-token`.</dd>
</dl>

### GitLab
<dl>
 <dt>`ATMOS_GITLAB_TOKEN`</dt>
 <dd>Personal Access Token (PAT) for GitLab authentication. Takes precedence over `GITLAB_TOKEN`.</dd>

 <dt>`GITLAB_TOKEN`</dt>
 <dd>Used as a fallback if `ATMOS_GITLAB_TOKEN` is not set.</dd>
</dl>

**Default Username for HTTPS:** `"oauth2"`

## How HTTPS URLs Are Resolved

When resolving Git sources, Atmos follows these rules:

1. If a **full HTTPS URL** is provided (`git::https://github.com/...`), it is used as-is. No token data is injected, even if environment variables are set.
2. If a **repository name** is provided without a scheme (`github.com/org/repo.git`), it defaults to `https://`, and if a token is set, it is injected into the URL.
3. If a **username and repository name** are provided in SCP format (`git@github.com:org/repo.git`), it is rewritten as an SSH URL.

:::note
For more details on configuration, refer to [Atmos Configuration](/cli/configuration).

:::

:::tip
Run `atmos vendor pull --help` to see all the available options
:::

## Examples

```shell
atmos vendor pull
atmos vendor pull --everything
atmos vendor pull --component vpc
atmos vendor pull -c vpc-flow-logs-bucket
atmos vendor pull -c echo-server --type helmfile
atmos vendor pull --tags dev,test
atmos vendor pull --tags networking --dry-run
```

:::note

When executing the `atmos vendor pull` command, Atmos performs the following steps to decide which vendoring manifest to use:

- If `vendor.yaml` manifest is found (in the directory from which the command is executed), Atmos will parse the file and execute the command
  against it. If the flag `--component` is not specified, Atmos will vendor all the artifacts defined in the `vendor.yaml` manifest.
  If the flag `--component` is passed in, Atmos will vendor only that component

- If `vendor.yaml` is not found, Atmos will look for the `component.yaml` manifest in the component's folder. If `component.yaml` is not found,
  an error will be thrown. The flag `--component` is required in this case

:::

## Flags

<dl>
  <dt>`--component` / `-c` (optional)</dt>
  <dd>Atmos component to pull.</dd>

  <dt>`--everything` (optional)</dt>
  <dd>Vendor all components.</dd>

  <dt>`--tags` (optional)</dt>
  <dd>Only vendor the components that have the specified tags.`tags` is a comma-separated values (CSV) string.</dd>

  <dt>`--type` / `-t` (optional)</dt>
  <dd>Component type: `terraform` or `helmfile` (`terraform` is default).</dd>

  <dt>`--dry-run` (optional)</dt>
  <dd>Dry run.</dd>
</dl>

---

## atmos version

import Screengrab from '@site/src/components/Screengrab'
import Intro from '@site/src/components/Intro'

<Intro>
Use this command to get the Atmos CLI version
</Intro>

<Screengrab title="atmos help" slug="atmos-version" />

## Usage

Execute the `atmos version` command like this:

```shell
atmos version
```

This will show the CLI version.

From time to time, Atmos will check for updates. The frequency of these checks is configured in the `atmos.yaml` file.

Atmos supports three ways to specify the update check frequency:

1. As an integer: Specify the number of seconds between checks (for example, 3600 for hourly checks).
2. As a duration with a suffix: Use a time suffix to indicate the interval (for example, `1m` for one minute, `5h` for five hours, or `2d` for two days).
3. As one of the predefined keywords: Choose from the following options: minute, hourly, daily, weekly, monthly, and yearly. The default is daily.
The default is to check `daily`, and if any unsupported values are passed this default will be used.

It is also possible to turn off version checks in `atmos.yaml` by setting `version.check.enabled` to `false`,
or by setting the `ATMOS_VERSION_CHECK_ENABLED` environment variable to `false`, which overrides
the `version.check.enabled` settings in `atmos.yaml`.

```shell
atmos version --check
```

## Flags

<dl>
  <dt>`--check` (optional)</dt>
  <dd>Force Atmos to check for a new version, irrespective of the configuration settings.</dd>

  <dt>`--format` (optional)</dt>
  <dd>Specify the output format: `yaml` or `json`.</dd>
</dl>

:::tip
To find the latest version of Atmos, go to the [releases](https://github.com/cloudposse/atmos/releases) page on GitHub.
For help with installing the latest version of Atmos, check out our [installation](/install) page.
:::

When executing the `atmos version` command, Atmos automatically checks for the latest release
from the [Atmos releases](https://github.com/cloudposse/atmos/releases) page on GitHub and compares the current
version with the latest release.

If the installed Atmos version is out of date, the following information is presented to the user:

---

## atmos workflow

import Screengrab from '@site/src/components/Screengrab'
import Terminal from '@site/src/components/Terminal'
import Intro from '@site/src/components/Intro'

<Intro>
Use this command to perform sequential execution of `atmos` and `shell` commands defined as workflow steps.
</Intro>

<Screengrab title="atmos workflow --help" slug="atmos-workflow--help" />

An Atmos workflow is a series of steps that are run in order to achieve some outcome. Every workflow has a name and is
easily executed from the
command line by calling `atmos workflow`. Use workflows to orchestrate any number of commands. Workflows can call
any `atmos` subcommand (including
[Atmos Custom Commands](/core-concepts/custom-commands)), shell commands, and have access to the stack configurations.

:::note
You can use [Atmos Custom Commands](/core-concepts/custom-commands) in [Atmos Workflows](/core-concepts/workflows),
and [Atmos Workflows](/core-concepts/workflows)
in [Atmos Custom Commands](/core-concepts/custom-commands)
:::

## Usage

Execute the `atmos workflow` command like this:

```shell
atmos workflow <workflow_name> --file <workflow_file> [options]
```

## Screenshots

### Workflow UI

Just run `atmos workflow` to start an interactive UI to view, search and execute the configured Atmos
workflows:

```shell
atmos workflow
```

- Use the `right/left` arrow keys to navigate between the "Workflow Manifests", "Workflows" and the selected workflow
  views

- Use the `up/down` arrow keys (or the mouse wheel) to select a workflow manifest and a workflow to execute

- Use the `/` key to filter/search for the workflow manifests and workflows in the corresponding views

- Press `Enter` to execute the selected workflow from the selected workflow manifest starting with the selected step

<Terminal title="atmos workflow (interactive)">
![`atmos workflow` CLI command 4](/img/cli/workflow/atmos-workflow-command-4.png)
</Terminal>

### Execute a Workflow

<Terminal title="atmos workflow (interactive)">
![`atmos workflow` CLI command 2](/img/cli/workflow/atmos-workflow-command-2.png)
</Terminal>

### Run Any Workflow Step

Use the `Tab` key to flip the 3rd column view between the selected workflow steps and full workflow definition.
For example:
<Terminal title="atmos workflow (interactive)">
![`atmos workflow` CLI command 3](/img/cli/workflow/atmos-workflow-command-3.png)
</Terminal>

## Examples

```shell
atmos workflow
atmos workflow plan-all-vpc --file networking
atmos workflow apply-all-components -f networking --dry-run
atmos workflow test-1 -f workflow1 --from-step step2
```

:::tip
Run `atmos workflow --help` to see all the available options
:::

## Arguments

<dl>
  <dt>`workflow_name`</dt>
  <dd>Workflow name</dd>
</dl>

## Flags

<dl>
  <dt>`--file` / `-f` (required)</dt>
  <dd>File name where the workflow is defined.</dd>

  <dt>`--stack` / `-s` (optional)</dt>
  <dd>Atmos stack (if provided, will override stacks defined in the workflow or workflow steps).</dd>

  <dt>`--from-step` (optional)</dt>
  <dd>Start the workflow from the named step.</dd>

  <dt>`--dry-run` (optional)</dt>
  <dd>Dry run. Print information about the executed workflow steps without executing them.</dd>
</dl>

---

## Customize Commands

import Screengrab from '@site/src/components/Screengrab'
import Terminal from '@site/src/components/Terminal'
import File from '@site/src/components/File'
import Intro from '@site/src/components/Intro'

<Intro>
You can extend the Atmos CLI and add as many custom commands as you want. This is a great way to increase improve the DX by exposing a consistent CLI interface to developers.
</Intro>

For example, one great way to use custom commands is to tie all the miscellaneous scripts into one consistent CLI interface.
Then we can kiss those ugly, inconsistent arguments to bash scripts goodbye! Just wire up the commands in atmos to call the script.
Then, developers can just run `atmos help` and discover all available commands.

Here are some examples to play around with to get started.

<File title="atmos.yaml">
```yaml
# Custom CLI commands
commands:
  - name: tf
    description: Execute 'terraform' commands

    # subcommands
    commands:
      - name: plan
        description: This command plans terraform components
        arguments:
          - name: component
            description: Name of the component
        flags:
          - name: stack
            shorthand: s
            description: Name of the stack
            required: true
        env:
          - key: ENV_VAR_1
            value: ENV_VAR_1_value
          - key: ENV_VAR_2
            # 'valueCommand' is an external command to execute to get the value for the ENV var
            # Either 'value' or 'valueCommand' can be specified for the ENV var, but not both
            valueCommand: echo ENV_VAR_2_value
        # steps support Go templates
        steps:
          - atmos terraform plan {{ .Arguments.component }} -s {{ .Flags.stack }}

  - name: terraform
    description: Execute 'terraform' commands

    # subcommands
    commands:
      - name: provision
        description: This command provisions terraform components
        arguments:
          - name: component
            description: Name of the component

        flags:
          - name: stack
            shorthand: s
            description: Name of the stack
            required: true

        # ENV var values support Go templates
        env:
          - key: ATMOS_COMPONENT
            value: "{{ .Arguments.component }}"
          - key: ATMOS_STACK
            value: "{{ .Flags.stack }}"
        steps:
          - atmos terraform plan $ATMOS_COMPONENT -s $ATMOS_STACK
          - atmos terraform apply $ATMOS_COMPONENT -s $ATMOS_STACK

  - name: show
    description: Execute 'show' commands

    # subcommands
    commands:
      - name: component
        description: Execute 'show component' command
        arguments:
          - name: component
            description: Name of the component
        flags:
          - name: stack
            shorthand: s
            description: Name of the stack
            required: true

        # ENV var values support Go templates and have access to {{ .ComponentConfig.xxx.yyy.zzz }} Go template variables
        env:
          - key: ATMOS_COMPONENT
            value: "{{ .Arguments.component }}"
          - key: ATMOS_STACK
            value: "{{ .Flags.stack }}"
          - key: ATMOS_TENANT
            value: "{{ .ComponentConfig.vars.tenant }}"
          - key: ATMOS_STAGE
            value: "{{ .ComponentConfig.vars.stage }}"
          - key: ATMOS_ENVIRONMENT
            value: "{{ .ComponentConfig.vars.environment }}"
          - key: ATMOS_IS_PROD
            value: "{{ .ComponentConfig.settings.config.is_prod }}"

        # If a custom command defines 'component_config' section with 'component' and 'stack', 'atmos' generates the config for the component in the stack
        # and makes it available in {{ .ComponentConfig.xxx.yyy.zzz }} Go template variables,
        # exposing all the component sections (which are also shown by 'atmos describe component' command)
        component_config:
          component: "{{ .Arguments.component }}"
          stack: "{{ .Flags.stack }}"
        # Steps support using Go templates and can access all configuration settings (e.g. {{ .ComponentConfig.xxx.yyy.zzz }})
        # Steps also have access to the ENV vars defined in the 'env' section of the 'command'
        steps:
          - 'echo Atmos component from argument: "{{ .Arguments.component }}"'
          - 'echo ATMOS_COMPONENT: "$ATMOS_COMPONENT"'
          - 'echo Atmos stack: "{{ .Flags.stack }}"'
          - 'echo Terraform component: "{{ .ComponentConfig.component }}"'
          - 'echo Backend S3 bucket: "{{ .ComponentConfig.backend.bucket }}"'
          - 'echo Terraform workspace: "{{ .ComponentConfig.workspace }}"'
          - 'echo Namespace: "{{ .ComponentConfig.vars.namespace }}"'
          - 'echo Tenant: "{{ .ComponentConfig.vars.tenant }}"'
          - 'echo Environment: "{{ .ComponentConfig.vars.environment }}"'
          - 'echo Stage: "{{ .ComponentConfig.vars.stage }}"'
          - 'echo settings.spacelift.workspace_enabled: "{{ .ComponentConfig.settings.spacelift.workspace_enabled }}"'
          - 'echo Dependencies: "{{ .ComponentConfig.deps }}"'
          - 'echo settings.config.is_prod: "{{ .ComponentConfig.settings.config.is_prod }}"'
          - 'echo ATMOS_IS_PROD: "$ATMOS_IS_PROD"'

  - name: list
    description: Execute 'atmos list' commands
    # subcommands
    commands:
      - name: stacks
        description: |
          List all Atmos stacks.
        steps:
          - >
            atmos describe stacks --process-templates=false --sections none | grep -e "^\S" | sed s/://g
      - name: components
        description: |
          List all Atmos components in all stacks or in a single stack.

          Example usage:
            atmos list components
            atmos list components -s tenant1-ue1-dev
            atmos list components --stack tenant2-uw2-prod
        flags:
          - name: stack
            shorthand: s
            description: Name of the stack
            required: false
        steps:
          - >
            {{ if .Flags.stack }}
            atmos describe stacks --stack {{ .Flags.stack }} --format json --sections none | jq ".[].components.terraform" | jq -s add | jq -r "keys[]"
            {{ else }}
            atmos describe stacks --format json --sections none | jq ".[].components.terraform" | jq -s add | jq -r "keys[]"
            {{ end }}

  - name: set-eks-cluster
    description: |
      Download 'kubeconfig' and set EKS cluster.

      Example usage:
        atmos set-eks-cluster eks/cluster -s tenant1-ue1-dev -r admin
        atmos set-eks-cluster eks/cluster -s tenant2-uw2-prod --role reader
    verbose: false  # Set to `true` to see verbose outputs
    arguments:
      - name: component
        description: Name of the component
    flags:
      - name: stack
        shorthand: s
        description: Name of the stack
        required: true
      - name: role
        shorthand: r
        description: IAM role to use
        required: true
    # If a custom command defines 'component_config' section with 'component' and 'stack',
    # Atmos generates the config for the component in the stack
    # and makes it available in {{ .ComponentConfig.xxx.yyy.zzz }} Go template variables,
    # exposing all the component sections (which are also shown by 'atmos describe component' command)
    component_config:
      component: "{{ .Arguments.component }}"
      stack: "{{ .Flags.stack }}"
    env:
      - key: KUBECONFIG
        value: /dev/shm/kubecfg.{{ .Flags.stack }}-{{ .Flags.role }}
    steps:
      - >
        aws
        --profile {{ .ComponentConfig.vars.namespace }}-{{ .ComponentConfig.vars.tenant }}-gbl-{{ .ComponentConfig.vars.stage }}-{{ .Flags.role }}
        --region {{ .ComponentConfig.vars.region }}
        eks update-kubeconfig
        --name={{ .ComponentConfig.vars.namespace }}-{{ .Flags.stack }}-eks-cluster
        --kubeconfig="${KUBECONFIG}"
        > /dev/null
      - chmod 600 ${KUBECONFIG}
      - echo ${KUBECONFIG}
```
</File>

:::tip
  For more information, refer to [Atmos Custom Commands](/core-concepts/custom-commands)
:::

---

## Customize Component Behavior

import Screengrab from '@site/src/components/Screengrab'
import Terminal from '@site/src/components/Terminal'
import File from '@site/src/components/File'
import Intro from '@site/src/components/Intro'

<Intro>
In Atmos, every component is associated with a command. The command is what drives or provisions that component.
For example, [Terraform "root modules"](/core-concepts/components/terraform) can be used as components in Atmos.
To instruct Atmos how to interact with that component, we must specify the command to run and and where the code
for the component is located. Then, depending on the type of component, certain behaviors can be configured.
</Intro>

The `components` section of the `atmos.yaml` is how we do it. It defines how Atmos locates and executes your components.
Think of it as the bootstrapping configuration.  This is where we can define the the `command` to run,
the `base_path` location of the components, and so forth.

:::important
Do not confuse this configuration with [configuring components in stacks](/core-concepts/stacks/define-components).
This configuration below is defined in the `atmos.yaml` and meant for specifying default behaviors for components,
such as what command to use when running Terraform commands, the base path for Terraform, and more.
:::

## Terraform Component Behavior

For additional details on configuring Terraform components, refer to the [Terraform](/core-concepts/projects/configuration/terraform)
and [OpenTofu](/core-concepts/projects/configuration/opentofu) documentation.

:::note Disambiguation
The term “Terraform” is used in this documentation to refer to generic concepts such as providers, modules, stacks, the
HCL-based domain-specific language and its interpreter. Atmos works with [OpenTofu](/core-concepts/projects/configuration/opentofu).
:::

<File title="atmos.yaml">
```yaml
components:
  terraform:
    # Optional `command` specifies the executable to be called by Atmos when running Terraform commands
    # If not defined, `terraform` is used
    # Examples:
    # command: terraform
    # command: /usr/local/bin/terraform
    # command: /usr/local/bin/terraform-1.8
    # command: tofu
    # command: /usr/local/bin/tofu-1.7.1
    # Can also be set using 'ATMOS_COMPONENTS_TERRAFORM_COMMAND' ENV var, or '--terraform-command' command-line argument
    command: terraform

    # Can also be set using 'ATMOS_COMPONENTS_TERRAFORM_BASE_PATH' ENV var, or '--terraform-dir' command-line argument
    # Supports both absolute and relative paths
    base_path: "components/terraform"

    # Can also be set using 'ATMOS_COMPONENTS_TERRAFORM_APPLY_AUTO_APPROVE' ENV var
    apply_auto_approve: false

    # Can also be set using 'ATMOS_COMPONENTS_TERRAFORM_DEPLOY_RUN_INIT' ENV var, or '--deploy-run-init' command-line argument
    deploy_run_init: true

    # Can also be set using 'ATMOS_COMPONENTS_TERRAFORM_INIT_RUN_RECONFIGURE' ENV var, or '--init-run-reconfigure' command-line argument
    init_run_reconfigure: true

    # Can also be set using 'ATMOS_COMPONENTS_TERRAFORM_AUTO_GENERATE_BACKEND_FILE' ENV var, or '--auto-generate-backend-file' command-line argument
    auto_generate_backend_file: true

    init:
      # Can also be set using 'ATMOS_COMPONENTS_TERRAFORM_INIT_PASS_VARS' ENV var, or '--init-pass-vars' command-line argument
      pass_vars: false

    plan:
      # Can also be set using 'ATMOS_COMPONENTS_TERRAFORM_PLAN_SKIP_PLANFILE' ENV var, or '--skip-planfile' command-line argument
      skip_planfile: false
```
</File>

<dl>
    <dt>`command`</dt>
    <dd>
        Specifies the executable to be called by `atmos` when running Terraform/OpenTofu commands.
        If not defined, `terraform` is used. Can also be set using `ATMOS_COMPONENTS_TERRAFORM_COMMAND` ENV var,
        or `--terraform-command` command-line argument.

        Example values: `terraform`, `/usr/local/bin/terraform`, `tofu`, `/usr/local/bin/tofu-1.7.1`.
    </dd>

    <dt>`base_path`</dt>
    <dd>
        Base path to the Terraform/OpenTofu components.

        Example value: "components/terraform". Can also be set using `ATMOS_COMPONENTS_TERRAFORM_BASE_PATH` ENV var,
        or `--terraform-dir` command-line argument.
        Supports both absolute and relative paths.
    </dd>

    <dt>`apply_auto_approve`</dt>
    <dd>
        If set to `true`, Atmos automatically adds the `-auto-approve` option to instruct Terraform to apply the plan without
        asking for confirmation when executing `terraform apply` command
    </dd>

    <dt>`deploy_run_init`</dt>
    <dd>
        If set to `true`, Atmos runs `terraform init` before executing [`atmos terraform deploy`](/cli/commands/terraform/deploy) command
    </dd>

    <dt>`init_run_reconfigure`</dt>
    <dd>
        If set to `true`, Atmos automatically adds the `-reconfigure` option to update the backend configuration when executing `terraform init` command
    </dd>

    <dt>`auto_generate_backend_file`</dt>
    <dd>
        If set to `true`, Atmos automatically generates the Terraform backend file from the component configuration when executing `terraform plan` and `terraform apply` commands
    </dd>

    <dt>`init.pass_vars`</dt>
    <dd>
        If set to `true`, Atmos automatically passes the generated varfile to the `tofu init` command using the `--var-file` flag.
        [OpenTofu supports passing a varfile to `init`](https://opentofu.org/docs/cli/commands/init/#general-options) to dynamically configure backends
    </dd>
</dl>

## Helmfile Component Behavior

<File title="atmos.yaml">
```yaml
components:
  helmfile:
    # Optional `command` specifies the executable to be called by Atmos when running Helmfile commands
    # If not defined, `helmfile` is used
    # Examples:
    # command: helmfile
    # command: /usr/local/bin/helmfile
    # Can also be set using 'ATMOS_COMPONENTS_HELMFILE_COMMAND' ENV var, or '--helmfile-command' command-line argument
    command: helmfile

    # Can also be set using 'ATMOS_COMPONENTS_HELMFILE_BASE_PATH' ENV var, or '--helmfile-dir' command-line argument
    # Supports both absolute and relative paths
    base_path: "components/helmfile"

    # Can also be set using 'ATMOS_COMPONENTS_HELMFILE_USE_EKS' ENV var
    # If not specified, defaults to 'true'
    use_eks: true

    # Can also be set using 'ATMOS_COMPONENTS_HELMFILE_KUBECONFIG_PATH' ENV var
    kubeconfig_path: "/dev/shm"

    # Can also be set using 'ATMOS_COMPONENTS_HELMFILE_HELM_AWS_PROFILE_PATTERN' ENV var
    helm_aws_profile_pattern: "{namespace}-{tenant}-gbl-{stage}-helm"

    # Can also be set using 'ATMOS_COMPONENTS_HELMFILE_CLUSTER_NAME_PATTERN' ENV var
    cluster_name_pattern: "{namespace}-{tenant}-{environment}-{stage}-eks-cluster"
```
</File>

<dl>
  <dt>`command`</dt>
  <dd>
    Specifies the executable to be called by `atmos` when running Helmfile commands.
    If not defined, `helmfile` is used. Can also be set using `ATMOS_COMPONENTS_HELMFILE_COMMAND` ENV var,
    or `--helmfile-command` command-line argument.

    Example values: `helmfile`, `/usr/local/bin/helmfile`.
  </dd>

  <dt>`base_path`</dt>
  <dd>
    Base path to the Helmfile components.

    Example value: "components/helmfile". Can also be set using `ATMOS_COMPONENTS_HELMFILE_BASE_PATH` ENV var,
    or `--helmfile-dir` command-line argument.
    Supports both absolute and relative paths.
  </dd>

  <dt>`use_eks`</dt>
  <dd>
    If not specified, defaults to `true`.
    Can also be set using `ATMOS_COMPONENTS_HELMFILE_USE_EKS` ENV var.
  </dd>

  <dt>`kubeconfig_path`</dt>
  <dd>
    Can also be set using `ATMOS_COMPONENTS_HELMFILE_KUBECONFIG_PATH` ENV var.
    Example value: `/dev/shm`.
  </dd>

  <dt>`helm_aws_profile_pattern`</dt>
  <dd>
    Can also be set using `ATMOS_COMPONENTS_HELMFILE_HELM_AWS_PROFILE_PATTERN` ENV var.
    Example value:
    ```
    {namespace}-{tenant}-{gbl}-{stage}-helm
    ```
  </dd>

  <dt>`cluster_name_pattern`</dt>
  <dd>
    Can also be set using   ATMOS_COMPONENTS_HELMFILE_CLUSTER_NAME_PATTERN` ENV var.
    Example value:
    ```
    {namespace}-{tenant}-{environment}-{stage}-eks-cluster`
    ```
  </dd>
</dl>

---

## CLI Configuration

import Screengrab from '@site/src/components/Screengrab'
import Terminal from '@site/src/components/Terminal'
import File from '@site/src/components/File'
import Intro from '@site/src/components/Intro'
import Tabs from '@theme/Tabs'
import TabItem from '@theme/TabItem'

# CLI Configuration

<Intro>
Use the `atmos.yaml` configuration file to control the behavior of the [Atmos CLI](/cli)
</Intro>

Everything in the [Atmos CLI](/cli) is configurable. The defaults are established in the `atmos.yaml` configuration file. The CLI configuration should not
be confused with [Stack configurations](/core-concepts/stacks/), which have a different schema.

Think of this file as where you [bootstrap the settings or configuration of your project](/core-concepts/projects). If you'll be using
[terraform](/core-concepts/components/terraform), then [this is where](/cli/configuration/components#terraform-component-behavior)
you'd specify the command to run (e.g. [`opentofu`](/core-concepts/projects/configuration/opentofu)),
the base path location of the components, and so forth.

## Configuration File (`atmos.yaml`)
The --config flag allows you to specify a relative or absolute path to a valid configuration file. Only the configuration files specified by this flag will be loaded.
The --config-path flag designates a directory containing Atmos configuration files. files name should be (atmos.yaml, .atmos.yaml,atmos.yml, .atmos.yml). Only files from the specified directory will be loaded.
You can use both --config and --config-path multiple times in a single command. Configurations will be deep-merged in the order provided,
with the first specified config having the lowest priority and the last one having the highest. This allows later configurations to override settings from earlier ones.
For example, to load multiple configuration files, you would run:
  ```bash
    atmos --config /path/to/config1.yaml --config /path/to/config2.yaml --config-path /path/first/config/ -config-path /path/second/config/ ...
  ```
Configuration Load Order
If --config and --config-path not specified in command
The CLI config is loaded from the following locations (from lowest to highest priority):

- System directory (`/usr/local/etc/atmos/atmos.yaml` on Linux, `%LOCALAPPDATA%/atmos/atmos.yaml` on Windows)
- Home directory (`~/.atmos/atmos.yaml`)
- Current directory (`./atmos.yaml`)
- Environment variable `ATMOS_CLI_CONFIG_PATH` (the ENV var should point to a folder without specifying the file name)

Each configuration file discovered is deep-merged with the preceding configurations.

:::tip Pro-Tip
Atmos supports [POSIX-style greedy Globs](https://en.wikipedia.org/wiki/Glob_(programming)) for all file
names/paths (double-star/globstar `**` is supported as well)
:::

## Default CLI Configuration

If `atmos.yaml` is not found in any of the searched locations, Atmos will use the following default CLI configuration:

<File title="atmos.yaml">
```yaml
base_path: "."
vendor:
  base_path: "./vendor.yaml"
components:
  terraform:
    base_path: components/terraform
    apply_auto_approve: false
    deploy_run_init: true
    init_run_reconfigure: true
    auto_generate_backend_file: true
    init:
      pass_vars: false
  helmfile:
    base_path: components/helmfile
    use_eks: true
    kubeconfig_path: /dev/shm
    helm_aws_profile_pattern: '{namespace}-{tenant}-gbl-{stage}-helm'
    cluster_name_pattern: '{namespace}-{tenant}-{environment}-{stage}-eks-cluster'
stacks:
  base_path: stacks
  included_paths:
    - "orgs/**/*"
  excluded_paths:
    - "**/_defaults.yaml"
  # To define Atmos stack naming convention, use either `name_pattern` or `name_template`.
  # `name_template` has higher priority (if `name_template` is specified, `name_pattern` will be ignored).
  # `name_pattern` uses the predefined context tokens {namespace}, {tenant}, {environment}, {stage}.
  # `name_pattern` can also be set using 'ATMOS_STACKS_NAME_PATTERN' ENV var
  name_pattern: "{tenant}-{environment}-{stage}"
  # `name_template` is a Golang template.
  # For the template tokens, and you can use any Atmos sections and attributes that the Atmos command
  # `atmos describe component <component> -s <stack>` generates (refer to https://atmos.tools/cli/commands/describe/component).
  # `name_template` can also be set using 'ATMOS_STACKS_NAME_TEMPLATE' ENV var
  # name_template: "{{.vars.tenant}}-{{.vars.environment}}-{{.vars.stage}}"
workflows:
  base_path: stacks/workflows
logs:
  # Can also be set using 'ATMOS_LOGS_FILE' ENV var, or '--logs-file' command-line argument
  # File or standard file descriptor to write logs to
  # Logs can be written to any file or any standard file descriptor, including `/dev/stdout`, `/dev/stderr` and `/dev/null`
  file: "/dev/stderr"
  # Supported log levels: Trace, Debug, Info, Warning, Off
  # Can also be set using 'ATMOS_LOGS_LEVEL' ENV var, or '--logs-level' command-line argument
  level: Info
profiler:
  # Enable or disable the pprof profiling server
  # Can also be set using '--profiler-enabled' command-line flag
  enabled: false
  # Host to bind the profiling server to
  # Can also be set using '--profiler-host' command-line flag
  host: "localhost"
  # Port to run the profiling server on
  # Can also be set using '--profiler-port' command-line flag
  port: 6060
schemas:
  jsonschema:
    base_path: stacks/schemas/jsonschema
  opa:
    base_path: stacks/schemas/opa
# https://atmos.tools/core-concepts/stacks/templates
# https://pkg.go.dev/text/template
templates:
  settings:
    enabled: true
    # https://masterminds.github.io/sprig
    sprig:
      enabled: true
    # https://docs.gomplate.ca
    gomplate:
      enabled: true
settings:
  list_merge_strategy: replace
  terminal:
    color: true     # Enable colored output (Can be set using 'ATMOS_COLOR' or 'COLOR' ENV var)
    # no_color: false # DEPRECATED in config file - use 'color: false' instead
                      # Note: NO_COLOR and ATMOS_NO_COLOR env vars are NOT deprecated
    max_width: 120  # Maximum width for terminal output
    pager: false    # Pager disabled by default (set to true or pager command to enable)
```
</File>

If Atmos does not find an `atmos.yaml` file and the default CLI config is used, and if you set the ENV variable `ATMOS_LOGS_LEVEL` to `Debug`
(e.g. `export ATMOS_LOGS_LEVEL=Debug`) before executing Atmos commands, you'll see the following message:

<Terminal>
![Atmos CLI command mode 1](/img/cli/atmos.yaml/atmos-default-cli-config-message.png)
</Terminal>

What follows are all the sections of the `atmos.yaml` configuration file.

## YAML Functions

Atmos extends standard YAML with several custom functions that can be used in atmos configuration `atmos.yaml` file. These functions provide powerful tools for dynamic configuration:

<dl>
  <dt>`!env`</dt>
  <dd>
    Used to retrieve environment variables.
    See the [`!env` documentation](/functions/yaml/env) for more details.
  </dd>

  <dt>`!exec`</dt>
  <dd>
    Used to execute shell scripts and assign their output.
    See the [`!exec` documentation](/functions/yaml/exec) for more details.
  </dd>

  <dt>`!include`</dt>
  <dd>
    Used to include other YAML files into the current configuration.
    See the [`!include` documentation](/functions/yaml/include) for more details.
  </dd>

  <dt>`!repo-root`</dt>
  <dd>
    Used to retrieve the root directory of the Atmos repository. If the Git root is not found, it will return a default value if specified; otherwise, it returns an error.
    See the [`!repo-root` documentation](/functions/yaml/repo-root) for more details.
  </dd>
</dl>
## Imports

Additionally, Atmos supports `imports` of other CLI configurations. Use imports to break large Atmos CLI configurations into smaller ones, such as organized by top-level section. File imports are relative to the base path (if `import`  section is set in the config). All imports are processed at the time the configuration is loaded, and then deep-merged in order, so that the last file in the list supersedes settings in the preceding imports. For an example, see [`scenarios/demo-atmos-cli-imports`](https://github.com/cloudposse/atmos/tree/main/tests/fixtures/scenarios/atmos-cli-imports).

:::tip Pro-Tip
Atmos supports [POSIX-style greedy Globs](https://en.wikipedia.org/wiki/Glob_(programming)) for all file
names/paths (double-star/globstar `**` is supported as well)
:::

Imports can be any of the following:
- Remote URL
- Specific Path
- Wildcard globs (`*`), including recursive globs (`**`), can be combined (e.g., `**/*` matches all files and subfolders recursively). Only files ending in `.yml` or `.yaml` will be considered for import when using globs.

For example, we can import from multiple locations like this:

```yaml
import:
  # Load the Atmos configuration from the main branch of the 'cloudposse/atmos' repository
  - "https://raw.githubusercontent.com/cloudposse/atmos/refs/heads/main/atmos.yaml"
  # Then merge the configs
  - "configs.d/**/*"
  # Finally, override some logging settings
  - "./logs.yaml"
```
Note, templated imports of Atmos configuration are not supported (unlike stacks).

:::warning Be Careful with Remote Imports
- Always use HTTPS URLs (currently correctly demonstrated in the example).
- Verify the authenticity of remote sources.
- Consider pinning to specific commit hashes instead of branch references
:::

Each configuration file discovered is deep-merged with the preceding configurations.

## Base Path

The base path for components, stacks, workflows and validation configurations.
It can also be set using `ATMOS_BASE_PATH` environment variable, or by passing the `--base-path` command-line argument.
It supports both absolute and relative paths.

If not provided or is an empty string, `components.terraform.base_path`, `components.helmfile.base_path`, `stacks.base_path` and `workflows.base_path`
are independent settings (supporting both absolute and relative paths).

If `base_path` is provided, `components.terraform.base_path`, `components.helmfile.base_path`, `stacks.base_path`, `workflows.base_path`,
`schemas.jsonschema.base_path` and `schemas.opa.base_path` are considered paths relative to `base_path`.

```yaml
base_path: "."
```

### Windows Path Handling

When configuring paths in `atmos.yaml` on Windows, there are important considerations for how YAML interprets backslashes:

:::warning Windows Path Escaping
Backslashes (`\`) are treated as escape characters only inside double-quoted YAML scalars. Single-quoted and plain scalars treat backslashes literally. Use single quotes or plain scalars for Windows paths, or double-escape backslashes in double quotes.
:::

#### Correct Ways to Specify Windows Paths

<Tabs defaultValue="forward">
<TabItem value="forward" label="Forward Slashes (Recommended)">
```yaml
# Forward slashes work on all platforms including Windows
components:
  terraform:
    base_path: "C:/Users/username/projects/components/terraform"
```
</TabItem>

<TabItem value="escaped" label="Escaped Backslashes">
```yaml
# Double backslashes to escape them in YAML
components:
  terraform:
    base_path: "C:\\Users\\username\\projects\\components\\terraform"
```
</TabItem>

<TabItem value="single" label="Single Quotes (Backslashes Literal)">
```yaml
# Single quotes treat backslashes as literal characters
components:
  terraform:
    base_path: 'C:\Users\username\projects\components\terraform'
```
</TabItem>

<TabItem value="unquoted" label="Unquoted Paths">
```yaml
# Unquoted paths with forward slashes also work
components:
  terraform:
    base_path: C:/Users/username/projects/components/terraform
```
</TabItem>
</Tabs>

#### Incorrect Windows Path Format

```yaml
# ❌ WRONG: Single backslashes get interpreted as escape sequences
components:
  terraform:
    base_path: "C:\Users\username\projects\components\terraform"
    # This becomes: C:Usersusernameprojectscomponentsterraform (invalid)
```

:::tip Best Practice
Use forward slashes (`/`) for all paths in `atmos.yaml`. They work correctly on all operating systems including Windows, Linux, and macOS.
:::

## Settings

The `settings` section configures Atmos global settings.

<File title="atmos.yaml">
    ```yaml
    settings:
      # `list_merge_strategy` specifies how lists are merged in Atmos stack manifests.
      # Can also be set using 'ATMOS_SETTINGS_LIST_MERGE_STRATEGY' environment variable, or '--settings-list-merge-strategy' command-line argument
      # The following strategies are supported:
      # `replace`: Most recent list imported wins (the default behavior).
      # `append`:  The sequence of lists is appended in the same order as imports.
      # `merge`:   The items in the destination list are deep-merged with the items in the source list.
      #            The items in the source list take precedence.
      #            The items are processed starting from the first up to the length of the source list (the remaining items are not processed).
      #            If the source and destination lists have the same length, all items in the destination lists are
      #            deep-merged with all items in the source list.
      list_merge_strategy: replace

      # Terminal settings for displaying content
      terminal:
        max_width: 120  # Maximum width for terminal output
        pager: false    # Pager disabled by default
        color: true     # Enable colored output
      inject_github_token: true # Adds the GITHUB_TOKEN as a Bearer token for GitHub API requests.
    ```
</File>

<dl>
  <dt>`settings.list_merge_strategy`</dt>
  <dd>
    Specifies how lists are merged in Atmos stack manifests.

    The following strategies are supported:
    <dl>
      <dt>`replace`</dt>
      <dd>Most recent list imported wins (the default behavior).</dd>

      <dt>`append`</dt>
      <dd>The sequence of lists is appended in the same order as imports.</dd>

      <dt>`merge`</dt>
      <dd>The items in the destination list are deep-merged with the items in the source list. The items in the source list take precedence. The items are processed starting from the first up to the length of the source list (the remaining items are not processed). If the source and destination lists have the same length, all items in the destination lists are deep-merged with all items in the source list.</dd>
    </dl>
  </dd>

  <dt>`settings.terminal`</dt>
  <dd>
    Specifies how content is displayed in the terminal.

    The following settings are supported:
    <dl>
      <dt>`max_width`</dt>
      <dd>The maximum width for displaying content in the terminal.</dd>

      <dt>`pager`</dt>
      <dd>Configure pager behavior. Can be set to `false` (disabled, default), `true` (enabled), or a specific pager like `less` or `more`.</dd>

      <dt>`color`</dt>
      <dd>Enable or disable colored output (default: `true`). Can be overridden with `--no-color` flag or `NO_COLOR`/`ATMOS_NO_COLOR` environment variables.</dd>
    </dl>

    :::info Environment Variables for Portability
    **Configuration Deprecation**: The `no_color` field in `atmos.yaml` is deprecated. Use `color: false` instead.

    **Environment Variables Still Supported**: The `NO_COLOR` and `ATMOS_NO_COLOR` environment variables remain fully supported for portability across different environments and CI/CD systems.
    :::
  </dd>

  <dt>`settings.inject_github_token`</dt>
  <dd>
    Adds the `GITHUB_TOKEN` as a Bearer token for GitHub API requests, enabling authentication for private repositories and increased rate limits. If `ATMOS_GITHUB_TOKEN` is set, it takes precedence, overriding this behavior.
  </dd>

  <dt>`settings.docs` (Deprecated)</dt>
  <dd>
    :::warning Deprecated
    The `settings.docs` section is deprecated and will be removed in a future version. Please use `settings.terminal` instead.
    :::

    <dl>
      <dt>`max-width` (Deprecated)</dt>
      <dd>Use `settings.terminal.max_width` instead.</dd>

      <dt>`pagination` (Deprecated)</dt>
      <dd>Use `settings.terminal.pager` instead.</dd>
    </dl>
  </dd>
</dl>

## Workflows

<File title="atmos.yaml">
```yaml
workflows:
  # Can also be set using 'ATMOS_WORKFLOWS_BASE_PATH' ENV var, or '--workflows-dir' command-line argument
  # Supports both absolute and relative paths
  base_path: "stacks/workflows"
```
</File>

## Integrations

Atmos supports many native Atmos integrations. They extend the core functionality of Atmos.

<File title="atmos.yaml">
```yaml
# Integrations
integrations:

  # Atlantis integration
  # https://www.runatlantis.io/docs/repo-level-atlantis-yaml.html
  atlantis:
    # Path and name of the Atlantis config file 'atlantis.yaml'
    # Supports absolute and relative paths
    # All the intermediate folders will be created automatically (e.g. 'path: /config/atlantis/atlantis.yaml')
    # Can be overridden on the command line by using '--output-path' command-line argument in 'atmos atlantis generate repo-config' command
    # If not specified (set to an empty string/omitted here, and set to an empty string on the command line), the content of the file will be dumped to 'stdout'
    # On Linux/macOS, you can also use '--output-path=/dev/stdout' to dump the content to 'stdout' without setting it to an empty string in 'atlantis.path'
    path: "atlantis.yaml"

    # Config templates
    # Select a template by using the '--config-template <config_template>' command-line argument in 'atmos atlantis generate repo-config' command
    config_templates:
      config-1:
        version: 3
        automerge: true
        delete_source_branch_on_merge: true
        parallel_plan: true
        parallel_apply: true
        allowed_regexp_prefixes:
          - dev/
          - staging/
          - prod/

    # Project templates
    # Select a template by using the '--project-template <project_template>' command-line argument in 'atmos atlantis generate repo-config' command
    project_templates:
      project-1:
        # generate a project entry for each component in every stack
        name: "{tenant}-{environment}-{stage}-{component}"
        workspace: "{workspace}"
        dir: "{component-path}"
        terraform_version: v1.2
        delete_source_branch_on_merge: true
        autoplan:
          enabled: true
          when_modified:
            - "**/*.tf"
            - "varfiles/$PROJECT_NAME.tfvars.json"
        apply_requirements:
          - "approved"

    # Workflow templates
    # https://www.runatlantis.io/docs/custom-workflows.html#custom-init-plan-apply-commands
    # https://www.runatlantis.io/docs/custom-workflows.html#custom-run-command
    workflow_templates:
      workflow-1:
        plan:
          steps:
            - run: terraform init -input=false
            # When using workspaces, you need to select the workspace using the $WORKSPACE environment variable
            - run: terraform workspace select $WORKSPACE || terraform workspace new $WORKSPACE
            # You must output the plan using '-out $PLANFILE' because Atlantis expects plans to be in a specific location
            - run: terraform plan -input=false -refresh -out $PLANFILE -var-file varfiles/$PROJECT_NAME.tfvars.json
        apply:
          steps:
            - run: terraform apply $PLANFILE
```
</File>

:::tip
For more information, refer to Atmos Integrations.
- [GitHub Actions](/integrations/github-actions)
- [Atlantis](/integrations/atlantis)
- [Spacelift](/integrations/spacelift)
:::

## Schemas

Configure the paths where to find OPA and JSON Schema files to validate Atmos stack manifests and components.

<File title="atmos.yaml">
```yaml
# Validation schemas (for validating atmos stacks and components)
schemas:
  # https://json-schema.org
  jsonschema:
    # Can also be set using 'ATMOS_SCHEMAS_JSONSCHEMA_BASE_PATH' ENV var, or '--schemas-jsonschema-dir' command-line argument
    # Supports both absolute and relative paths
    base_path: "stacks/schemas/jsonschema"
  # https://www.openpolicyagent.org
  opa:
    # Can also be set using 'ATMOS_SCHEMAS_OPA_BASE_PATH' ENV var, or '--schemas-opa-dir' command-line argument
    # Supports both absolute and relative paths
    base_path: "stacks/schemas/opa"
  # JSON Schema to validate Atmos manifests
  # https://atmos.tools/cli/schemas/
  # https://atmos.tools/cli/commands/validate/stacks/
  # https://atmos.tools/quick-start/advanced/configure-validation/
  # https://atmos.tools/schemas/atmos/atmos-manifest/1.0/atmos-manifest.json
  # https://json-schema.org/draft/2020-12/release-notes
  atmos:
    # Can also be set using 'ATMOS_SCHEMAS_ATMOS_MANIFEST' ENV var, or '--schemas-atmos-manifest' command-line argument
    # Supports both absolute and relative paths (relative to the `base_path` setting in `atmos.yaml`)
    manifest: "stacks/schemas/atmos/atmos-manifest/1.0/atmos-manifest.json"
```
</File>

:::tip
For more information, refer to:
- [Atmos Manifests Validation](/cli/schemas)
- [Atmos Component Validation](/core-concepts/validate)
:::

## Logs

Logs are configured in the `logs` section:

<File title="atmos.yaml">
```yaml
logs:
  # Can also be set using 'ATMOS_LOGS_FILE' ENV var, or '--logs-file' command-line argument
  # File or standard file descriptor to write logs to
  # Logs can be written to any file or any standard file descriptor, including `/dev/stdout`, `/dev/stderr` and `/dev/null`
  file: "/dev/stderr"
  # Supported log levels: Trace, Debug, Info, Warning, Off
  # Can also be set using 'ATMOS_LOGS_LEVEL' ENV var, or '--logs-level' command-line argument
  level: Info
```
</File>

- `logs.file` - the file to write Atmos logs to. Logs can be written to any file or any standard file descriptor,
  including `/dev/stdout`, `/dev/stderr` and `/dev/null`. If omitted, `/dev/stdout` will be used.
  The environment variable `ATMOS_LOGS_FILE` can also be used to specify the log file

- `logs.level` - Log level. Supported log levels are `Trace`, `Debug`, `Info`, `Warning`, `Off`. If the log level is set to `Off`, Atmos will not log
  any messages (note that this does not prevent other tools like Terraform from logging).
  The environment variable `ATMOS_LOGS_LEVEL` can also be used to specify the log level

To prevent Atmos from logging any messages (except for the outputs of the executed commands), you can do one of the following:

- Set `logs.file` or the ENV variable `ATMOS_LOGS_FILE` to `/dev/null`

- Set `logs.level` or the ENV variable `ATMOS_LOGS_LEVEL` to `Off`

Note that when you set the log level to `Debug` or `Trace`, Atmos will log additional messages before printing the output
of an executed command. For example, let's consider the `atmos describe affected` command:

<File title="atmos.yaml">
```yaml
logs:
  file: "/dev/stdout"
  level: Trace
```
</File>

<Terminal title="atmos describe affected">
```console
Checking out Git ref 'refs/remotes/origin/HEAD' ...
Checked out Git ref 'refs/remotes/origin/HEAD'

Current HEAD: ffd2154e1daa32357b75460b9f45d268922b51e1 refs/heads/update-logs
BASE: f7aa382aa8b3d48be8f06cfdb27aad344b89aff4 HEAD

Changed files:

examples/quick-start-advanced/Dockerfile
examples/quick-start-advanced/atmos.yaml

Affected components and stacks:

[
   {
      "component": "vpc",
      "component_type": "terraform",
      "component_path": "examples/quick-start-advanced/components/terraform/vpc",
      "stack": "plat-uw2-prod",
      "stack_slug": "plat-uw2-prod-vpc",
      "affected": "stack.vars"
   },
   {
      "component": "vpc",
      "component_type": "terraform",
      "component_path": "examples/quick-start-advanced/components/terraform/vpc",
      "stack": "plat-ue2-prod",
      "stack_slug": "plat-ue2-prod-vpc",
      "affected": "stack.vars"
   }
]
````
</Terminal>

With `logs.level: Trace`, and `logs.file: "/dev/stdout"`, all the messages and the command's JSON output will be printed
to the console to the `/dev/stdout` standard output.

This behavior might be undesirable when you execute the command `atmos describe affected` in CI/CD (e.g. GitHub Actions).

For example, you might want to log all the Atmos messages (by setting `logs.level: Trace`) for debugging purposes,
and also want to parse the JSON output of the command (e.g. by using `jq`) for further processing. In this case, `jq`
will not be able to parse the JSON output because all the other messages make the output an invalid JSON document.

To deal with that, you can set `logs.file` to `/dev/stderr` in `atmos.yaml`:

<File title="atmos.yaml">
```yaml
logs:
  file: "/dev/stderr"
  level: Trace
```
</File>

Now when the `atmos describe affected` command is executed, the additional messages are printed to `/dev/stderr`,
but the command's JSON output is printed to `/dev/stdout`, allowing `jq` to parse it without errors.

<Terminal title="atmos describe affected">
```console
# NOTE: These messages are printed to `/dev/stderr`

Checking out Git ref 'refs/remotes/origin/HEAD' ...
Checked out Git ref 'refs/remotes/origin/HEAD'
Current HEAD: ffd2154e1daa32357b75460b9f45d268922b51e1 refs/heads/update-logs
BASE: f7aa382aa8b3d48be8f06cfdb27aad344b89aff4 HEAD

# NOTE: This JSON output is printed to `/dev/stdout`

[
   {
      "component": "vpc",
      "component_type": "terraform",
      "component_path": "examples/quick-start-advanced/components/terraform/vpc",
      "stack": "plat-uw2-prod",
      "stack_slug": "plat-uw2-prod-vpc",
      "affected": "stack.vars"
   },
   {
      "component": "vpc",
      "component_type": "terraform",
      "component_path": "examples/quick-start-advanced/components/terraform/vpc",
      "stack": "plat-ue2-prod",
      "stack_slug": "plat-ue2-prod-vpc",
      "affected": "stack.vars"
   }
]
````
</Terminal>

## Profiler

Atmos includes built-in performance profiling capabilities using Go's pprof profiler. This allows you to analyze CPU usage, memory allocations, goroutines, and other performance metrics when running Atmos commands.

The profiler is configured in the `profiler` section:

<File title="atmos.yaml">
```yaml
profiler:
  # Enable or disable the pprof profiling server
  enabled: false
  # Host to bind the profiling server to (default: localhost)
  host: "localhost"
  # Port to run the profiling server on (default: 6060)
  port: 6060
```
</File>

<dl>
  <dt>`profiler.enabled`</dt>
  <dd>
    Enable or disable the pprof profiling server. When enabled, Atmos will start an HTTP server that serves pprof endpoints for performance analysis. Can also be set using the `--profiler-enabled` command-line flag.
  </dd>

  <dt>`profiler.host`</dt>
  <dd>
    The host address to bind the profiling server to. Defaults to `localhost` for security. Can also be set using the `--profiler-host` command-line flag.
  </dd>

  <dt>`profiler.port`</dt>
  <dd>
    The port number for the profiling server. Defaults to `6060` (the standard pprof port). Can also be set using the `--profiler-port` command-line flag.
  </dd>
</dl>

### Using the Profiler

When the profiler is enabled, Atmos will start a pprof server and display the URL when any command is run:

<Terminal title="atmos terraform plan vpc -s plat-ue2-dev --profiler-enabled">
```console
pprof profiler available at: http://localhost:6060/debug/pprof/
Executing 'terraform plan' command...
```
</Terminal>

The profiler provides several endpoints for different types of analysis:

- **CPU Profile**: `http://localhost:6060/debug/pprof/profile` - 30-second CPU profile
- **Memory Profile**: `http://localhost:6060/debug/pprof/heap` - Memory heap profile
- **Goroutines**: `http://localhost:6060/debug/pprof/goroutine` - Stack traces of all current goroutines
- **Web Interface**: `http://localhost:6060/debug/pprof/` - Interactive web interface

### Analyzing Performance Data

You can use Go's pprof tool to analyze the profiling data:

<Terminal title="Analyzing CPU profile">
```console
# Capture and analyze CPU profile
go tool pprof http://localhost:6060/debug/pprof/profile

# Capture and analyze memory profile
go tool pprof http://localhost:6060/debug/pprof/heap

# Generate a web-based visualization
go tool pprof -http=:8080 http://localhost:6060/debug/pprof/profile
```
</Terminal>

### Security Considerations

:::warning Security Notice
The profiler exposes detailed runtime information about your Atmos process. Only enable it when needed for debugging or performance analysis, and ensure the host/port are not accessible from untrusted networks.
:::

By default, the profiler binds to `localhost` only, which prevents external access. If you need to access the profiler from another machine, make sure to use appropriate network security measures.

## Aliases

CLI command aliases are configured in the `aliases` section.

An alias lets you create a shortcut name for an existing CLI command. Any CLI command can be aliased, including the Atmos
native commands like `terraform apply` or `describe stacks`, as well as [Atmos Custom Commands](/core-concepts/custom-commands).

For example:

<File title="atmos.yaml">
```yaml
# CLI command aliases
aliases:
  # Aliases for Atmos native commands
  tf: terraform
  tp: terraform plan
  up: terraform apply
  down: terraform destroy
  ds: describe stacks
  dc: describe component
  # Aliases for Atmos custom commands
  ls: list stacks
  lc: list components
```
</File>

Execute an alias as you would any Atmos native or custom command:

<Terminal title="'atmos ls' command alias">
```console
> atmos ls

plat-ue2-dev
plat-ue2-prod
plat-ue2-staging
plat-uw2-dev
plat-uw2-prod
plat-uw2-staging
```
</Terminal>

The aliases configured in the `aliases` section automatically appear in Atmos help, and are shown as
`alias for '<command>'`.

For example:

<Terminal title="atmos --help">
    ![`atmos --help`](/img/cli/help/atmos-help-command-3.png)
</Terminal>

An alias automatically supports all command line arguments and flags that the aliased command accepts.

For example:

- `atmos up <component> -s <stack>` supports all the parameters from the aliased command `atmos terraform apply <component> -s <stack>`
- `atmos dc <component> -s <stack>` supports all the parameters from the aliased command `atmos describe component <component> -s <stack>`

## Templates

Atmos supports [Go templates](https://pkg.go.dev/text/template) in stack manifests, and the following template
functions and data sources:

 - [Go `text/template` functions](https://pkg.go.dev/text/template#hdr-Functions)
 - [Atmos Template Functions](/functions/template)
 - [Sprig Functions](https://masterminds.github.io/sprig/)
 - [Gomplate Functions](https://docs.gomplate.ca/functions/)
 - [Gomplate Datasources](https://docs.gomplate.ca/datasources/)

:::tip
For more details, refer to [Atmos Stack Manifest Templating](/core-concepts/stacks/templates)
:::

<File title="atmos.yaml">
```yaml
# https://pkg.go.dev/text/template
templates:
  settings:
    enabled: true
    # https://masterminds.github.io/sprig
    sprig:
      enabled: true
    # https://docs.gomplate.ca
    # https://docs.gomplate.ca/functions
    gomplate:
      enabled: true
      # Timeout in seconds to execute the datasources
      timeout: 5
      # https://docs.gomplate.ca/datasources
      datasources:
        # 'http' datasource
        # https://docs.gomplate.ca/datasources/#using-file-datasources
        ip:
          url: "https://api.ipify.org?format=json"
          # https://docs.gomplate.ca/datasources/#sending-http-headers
          # https://docs.gomplate.ca/usage/#--datasource-header-h
          headers:
            accept:
              - "application/json"
        # 'file' datasources
        # https://docs.gomplate.ca/datasources/#using-file-datasources
        config-1:
          url: "./config1.json"
        config-2:
          url: "file:///config2.json"
```
</File>

- `templates.settings.enabled` - a boolean flag to enable/disable the processing of `Go` templates in Atmos stack manifests.
  If set to `false`, Atmos will not process `Go` templates in stack manifests

- `templates.settings.sprig.enabled` - a boolean flag to enable/disable the [Sprig Functions](https://masterminds.github.io/sprig/)
   in Atmos stack manifests

- `templates.settings.gomplate.enabled` - a boolean flag to enable/disable the [Gomplate Functions](https://docs.gomplate.ca/functions/)
   and [Gomplate Datasources](https://docs.gomplate.ca/datasources) in Atmos stack manifests

- `templates.settings.gomplate.timeout` - timeout in seconds to execute [Gomplate Datasources](https://docs.gomplate.ca/datasources)

- `templates.settings.gomplate.datasources` - a map of [Gomplate Datasource](https://docs.gomplate.ca/datasources) definitions:

   - The keys of the map are the datasource names, which are used in `Go` templates in Atmos stack manifests.
     For example:

     ```yaml
      terraform:
        vars:
          tags:
            provisioned_by_ip: '{{ (datasource "ip").ip }}'
            config1_tag: '{{ (datasource "config-1").tag }}'
            config2_service_name: '{{ (datasource "config-2").service.name }}'
     ```

   - The values of the map are the datasource definitions with the following schema:

     - `url` - the [Datasource URL](https://docs.gomplate.ca/datasources/#url-format)

     - `headers` - a map of [HTTP request headers](https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers) for
        the [`http` datasource](https://docs.gomplate.ca/datasources/#sending-http-headers).
        The keys of the map are the header names. The values of the map are lists of values for the header.

        The following configuration will result in the
        [`accept: application/json`](https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Accept) HTTP header
        being sent with the HTTP request to the datasource:

          ```yaml
          headers:
            accept:
              - "application/json"
         ```

:::warning

Some functions are present in both [Sprig](https://masterminds.github.io/sprig/) and [Gomplate](https://docs.gomplate.ca/functions/).

For example, the `env` function has the same name in [Sprig](https://masterminds.github.io/sprig/os.html) and
[Gomplate](https://docs.gomplate.ca/functions/env/), but has different syntax and accept different number of arguments.

If you use the `env` function from one templating engine and enable both [Sprig](https://masterminds.github.io/sprig/)
and [Gomplate](https://docs.gomplate.ca/functions/), it will be invalid in the other templating engine, and an error will be thrown.

For this reason, you can use the `templates.settings.sprig.enabled` and `templates.settings.gomplate.enabled` settings to selectively
enable/disable the [Sprig](https://masterminds.github.io/sprig/) and [Gomplate](https://docs.gomplate.ca/functions/)
functions.

:::

## Environment Variables
### Configuration

Most YAML settings can also be defined by environment variables. This is helpful while doing local development. For example,
setting `ATMOS_STACKS_BASE_PATH` to a path in `/localhost` to your local development folder, will enable you to rapidly iterate.

| Variable                                              | YAML Path                                       | Description                                                                                                                                                                                                                                                         |
|:------------------------------------------------------|:------------------------------------------------|:--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| ATMOS_CLI_CONFIG_PATH                                 | N/A                                             | Where to find `atmos.yaml`. Path to a folder where `atmos.yaml` CLI config file is located (e.g. `/config`)                                                                                                                                                         |
| ATMOS_BASE_PATH                                       | base_path                                       | Base path to `components` and `stacks` folders                                                                                                                                                                                                                      |
| ATMOS_VENDOR_BASE_PATH                                | vendor.base_path                                | Path to vendor configuration file or directory containing vendor files. If a directory is specified, all .yaml files in the directory will be processed in lexicographical order. Supports both absolute and relative paths.                                        |
| ATMOS_COMPONENTS_TERRAFORM_COMMAND                    | components.terraform.command                    | The executable to be called by `atmos` when running Terraform commands                                                                                                                                                                                              |
| ATMOS_COMPONENTS_TERRAFORM_BASE_PATH                  | components.terraform.base_path                  | Base path to Terraform components                                                                                                                                                                                                                                   |
| ATMOS_COMPONENTS_TERRAFORM_APPLY_AUTO_APPROVE         | components.terraform.apply_auto_approve         | If set to `true`, auto-generate Terraform backend config files when executing `atmos terraform` commands                                                                                                                                                            |
| ATMOS_COMPONENTS_TERRAFORM_DEPLOY_RUN_INIT            | components.terraform.deploy_run_init            | Run `terraform init` when executing `atmos terraform deploy` command                                                                                                                                                                                                |
| ATMOS_COMPONENTS_TERRAFORM_INIT_RUN_RECONFIGURE       | components.terraform.init_run_reconfigure       | Run `terraform init -reconfigure` when executing `atmos terraform` commands                                                                                                                                                                                         |
| ATMOS_COMPONENTS_TERRAFORM_INIT_PASS_VARS             | components.terraform.init.pass_vars             | Pass the generated varfile to `terraform init` using the `--var-file` flag. [OpenTofu supports passing a varfile to `init`](https://opentofu.org/docs/cli/commands/init/#general-options) to dynamically configure backends                                         |
| ATMOS_COMPONENTS_TERRAFORM_PLAN_SKIP_PLANFILE         | components.terraform.plan.skip_planfile         | Skip writing the plan to a file by not passing the `-out` flag to Terraform when executing `terraform plan` commands. Set it to `true` when using Terraform Cloud since the `-out` flag is not supported. Terraform Cloud automatically stores plans in its backend |
| ATMOS_COMPONENTS_TERRAFORM_AUTO_GENERATE_BACKEND_FILE | components.terraform.auto_generate_backend_file | If set to `true`, auto-generate Terraform backend config files when executing `atmos terraform` commands                                                                                                                                                            |
| ATMOS_COMPONENTS_HELMFILE_COMMAND                     | components.helmfile.command                     | The executable to be called by `atmos` when running Helmfile commands                                                                                                                                                                                               |
| ATMOS_COMPONENTS_HELMFILE_BASE_PATH                   | components.helmfile.base_path                   | Path to helmfile components                                                                                                                                                                                                                                         |
| ATMOS_COMPONENTS_HELMFILE_USE_EKS                     | components.helmfile.use_eks                     | If set to `true`, download `kubeconfig` from EKS by running `aws eks update-kubeconfig` command before executing `atmos helmfile` commands                                                                                                                          |
| ATMOS_COMPONENTS_HELMFILE_KUBECONFIG_PATH             | components.helmfile.kubeconfig_path             | Path to write the `kubeconfig` file when executing `aws eks update-kubeconfig` command                                                                                                                                                                              |
| ATMOS_COMPONENTS_HELMFILE_HELM_AWS_PROFILE_PATTERN    | components.helmfile.helm_aws_profile_pattern    | Pattern for AWS profile to use when executing `atmos helmfile` commands                                                                                                                                                                                             |
| ATMOS_COMPONENTS_HELMFILE_CLUSTER_NAME_PATTERN        | components.helmfile.cluster_name_pattern        | Pattern for EKS cluster name to use when executing `atmos helmfile` commands                                                                                                                                                                                        |
| ATMOS_STACKS_BASE_PATH                                | stacks.base_path                                | Base path to Atmos stack manifests                                                                                                                                                                                                                                  |
| ATMOS_STACKS_INCLUDED_PATHS                           | stacks.included_paths                           | List of paths to use as top-level stack manifests                                                                                                                                                                                                                   |
| ATMOS_STACKS_EXCLUDED_PATHS                           | stacks.excluded_paths                           | List of paths to not consider as top-level stacks                                                                                                                                                                                                                   |
| ATMOS_STACKS_NAME_PATTERN                             | stacks.name_pattern                             | Stack name pattern to use as Atmos stack names                                                                                                                                                                                                                      |
| ATMOS_STACKS_NAME_TEMPLATE                            | stacks.name_template                            | Stack name Golang template to use as Atmos stack names                                                                                                                                                                                                              |
| ATMOS_WORKFLOWS_BASE_PATH                             | workflows.base_path                             | Base path to Atmos workflows                                                                                                                                                                                                                                        |
| ATMOS_SCHEMAS_JSONSCHEMA_BASE_PATH                    | schemas.jsonschema.base_path                    | Base path to JSON schemas for component validation                                                                                                                                                                                                                  |
| ATMOS_SCHEMAS_OPA_BASE_PATH                           | schemas.opa.base_path                           | Base path to OPA policies for component validation                                                                                                                                                                                                                  |
| ATMOS_SCHEMAS_ATMOS_MANIFEST                          | schemas.atmos.manifest                          | Path to JSON Schema to validate Atmos stack manifests. For more details, refer to [Atmos Manifest JSON Schema](/cli/schemas)                                                                                                                                        |
| ATMOS_LOGS_FILE                                       | logs.file                                       | The file to write Atmos logs to. Logs can be written to any file or any standard file descriptor, including `/dev/stdout`, `/dev/stderr` and `/dev/null`). If omitted, `/dev/stdout` will be used                                                                   |
| ATMOS_LOGS_LEVEL                                      | logs.level                                      | Logs level. Supported log levels are `Trace`, `Debug`, `Info`, `Warning`, `Off`. If the log level is set to `Off`, Atmos will not log any messages (note that this does not prevent other tools like Terraform from logging)                                        |
| ATMOS_PROFILER_ENABLED                                | profiler.enabled                                | Enable or disable the pprof HTTP profiling server. When enabled, starts an HTTP server for interactive profiling                                                                                                                                                   |
| ATMOS_PROFILER_HOST                                   | profiler.host                                   | Host address for the profiling server. Default: `localhost`. Use `0.0.0.0` to allow external connections (security consideration)                                                                                                                                  |
| ATMOS_PROFILER_PORT                                   | profiler.port                                   | Port for the profiling server. Default: `6060`                                                                                                                                                                                                                      |
| ATMOS_PROFILE_FILE                                    | profiler.file                                   | Write profiling data to the specified file (enables profiling automatically). When specified, enables file-based profiling instead of server-based                                                                                                                |
| ATMOS_PROFILE_TYPE                                    | profiler.profile_type                           | Type of profile to collect when using `ATMOS_PROFILE_FILE`. Options: `cpu`, `heap`, `allocs`, `goroutine`, `block`, `mutex`, `threadcreate`, `trace`. Default: `cpu`                                                                                              |
| ATMOS_SETTINGS_LIST_MERGE_STRATEGY                    | settings.list_merge_strategy                    | Specifies how lists are merged in Atmos stack manifests. The following strategies are supported: `replace`, `append`, `merge`                                                                                                                                       |
| ATMOS_VERSION_CHECK_ENABLED                           | version.check.enabled                           | Enable/disable Atmos version checks for updates to the newest release                                                                                                                                                                                               |
| ATMOS_GITHUB_TOKEN                                    | N/A                                             | Bearer token for GitHub API requests, enabling authentication for private repositories and higher rate limits                                                                                                                                                       |
| ATMOS_BITBUCKET_TOKEN                                 | N/A                                             | App password for Bitbucket API requests is set to avoid rate limits. Unauthenticated Requests are limited to 60 requests per hour across all API resources.                                                                                                         |
| ATMOS_BITBUCKET_USERNAME                              | N/A                                             | Username for Bitbucket authentication. Takes precedence over BITBUCKET_USERNAME.                                                                                                                                                                                    |
| ATMOS_GITLAB_TOKEN                                    | N/A                                             | Personal Access Token (PAT) for GitLab authentication. Unauthenticated users are limited to 6 requests per minute per IP address for certain endpoints, while authenticated users have higher thresholds.                                                           |

### Context

Some commands, like [`atmos terraform shell`](/cli/commands/terraform/shell),
spawn an interactive shell with certain environment variables set, in order to enable the user to use other tools
(in the case of `atmos terraform shell`, the Terraform or Tofu CLI) natively, while still being configured for a
specific component and stack. To accomplish this, and to provide visibility and context to the user regarding the
configuration, Atmos may set the following environment variables in the spawned shell:

| Variable                | Description                                                                                            |
|:------------------------|:-------------------------------------------------------------------------------------------------------|
| ATMOS_COMPONENT         | The name of the active component                                                                       |
| ATMOS_SHELL_WORKING_DIR | The directory from which native commands should be run                                                 |
| ATMOS_SHLVL             | The depth of Atmos shell nesting. When present, it indicates that the shell has been spawned by Atmos. |
| ATMOS_STACK             | The name of the active stack                                                                           |
| ATMOS_TERRAFORM_WORKSPACE | The name of the Terraform workspace in which Terraform commands should be run                        |
| PS1                     | When a custom shell prompt has been configured in Atmos, the prompt will be set via `PS1`              |
| TF_CLI_ARGS_*           | Terraform CLI arguments to be passed to Terraform commands                                             |

---

## Markdown Styling

import File from '@site/src/components/File'
import Intro from '@site/src/components/Intro'

# Markdown Styling

<Intro>
Configure how Atmos displays markdown content in the terminal.
</Intro>

## Configuration

Configure markdown styling in your `atmos.yaml` configuration file:

<File title="atmos.yaml">
```yaml
settings:
  # Terminal settings for displaying content
  terminal:
    max_width: 120  # Maximum width for terminal output
    pager: true     # Use pager for long output
    unicode: true

    # Markdown element styling
    markdown:
      document:
        color: "${colors.text}"
      heading:
        color: "${colors.primary}"
        bold: true
      code_block:
        color: "${colors.secondary}"
        margin: 1
      link:
        color: "${colors.primary}"
        underline: true
      strong:
        color: "${colors.secondary}"
        bold: true
      emph:
        color: "${colors.muted}"
        italic: true

```
</File>

## Style Properties

Each markdown element supports the following properties:

### Common Properties

| Property | Type | Description |
|----------|------|-------------|
| `color` | string | Text color in hex format (e.g., "#FFFFFF") |
| `background_color` | string | Background color in hex format |
| `bold` | boolean | Whether to make the text bold |
| `italic` | boolean | Whether to make the text italic |
| `underline` | boolean | Whether to underline the text |
| `margin` | number | Space around the element |
| `indent` | number | Indentation level |

### Element-Specific Properties

#### Document

Base styling for all text content.

Supports all common properties.

#### Headings (H1-H6)

Individual styling for each heading level (1-6).

  ```markdown
  # Heading 1
  ## Heading 2
  ### Heading 3
  etc...
  ```

**Supports:**
- H1 supports additional `background_color` property
- All heading levels support `margin` for vertical spacing

#### Code Blocks

Styling for multi-line code blocks (aka code fences).

````markdown
```
this is a codeblock
```
````

**Supports:**
- `margin` for visual separation
- Color applies to the entire block

#### Block Quotes

Styling for quoted text. Supports all common properties.

```markdown
>
> This is quoted text
>
```

**Supports:**
- `indent` property controls quote indentation

#### Links

Styling for hyperlinks.

```
[This is a link](https://example.com/)
```

**Supports:**
- `underline` property specifically for links
- Color applies to both link text and underline

## Default Styles

If no custom styles are configured, Atmos uses a built-in default theme related to the default atmos brand colors:

```yaml
# Built-in default theme
settings:
  markdown:
    document:
      color: "#FFFFFF"  # White text
    heading:
      color: "#00A3E0"  # Blue headings
      bold: true
    h1:
      color: "#FFFFFF"  # White text
      background_color: "#9B51E0"  # Purple background
      bold: true
      margin: 2
    code_block:
      color: "#00A3E0"  # Blue code
      margin: 1
    link:
      color: "#00A3E0"  # Blue links
      underline: true
```

## Terminal Compatibility

Atmos uses [termenv](https://github.com/muesli/termenv) and [glamour](https://github.com/charmbracelet/glamour) to automatically detect and adapt to your terminal's capabilities:

- **Full Color Support (24-bit)**
  - Renders exact hex colors as specified in your config
  - Detected via `$COLORTERM=truecolor` or `$TERM` containing `24bit`/`truecolor`
  - Examples: iTerm2, Terminal.app, Windows Terminal

- **256 Color Support**
  - Automatically maps hex colors to nearest ANSI 256 colors
  - Detected via `$TERM` containing `256color`
  - Examples: xterm-256color terminals

- **Basic Color Support (8/16 colors)**
  - Automatically maps to basic ANSI colors
  - Used when `$TERM` indicates basic terminal
  - Examples: xterm, vt100, basic SSH sessions

- **No Color Support**
  - Falls back to plain text with basic formatting
  - Used when `$TERM=dumb` or no color support detected
  - Examples: Basic terminals, some CI environments

The color degradation is handled automatically by termenv's color profile detection. You don't need to configure anything - your styles will work everywhere, automatically adjusting to each terminal's capabilities.

## Examples

### Error Messages
Custom styling can help distinguish different types of messages:

```yaml
settings:
  markdown:
    # General heading styles
    heading:
      color: "#00A3E0"  # Blue for standard headings
      bold: true

    # Code blocks for command examples
    code_block:
      color: "#00FFFF"  # Cyan for code examples
      margin: 1

    # Emphasized text for warnings/errors
    emph:
      color: "#FF6B6B"  # Red for emphasis in error messages
      italic: true

    # Strong text for important messages
    strong:
      color: "#FF6B6B"  # Red for important parts
      bold: true
```

### Help Text

Atmos uses the [Glamour](https://github.com/charmbracelet/glamour) library for markdown rendering and styling. The styling is handled automatically based on your terminal's capabilities and color profile.

Key features of the markdown rendering:

- **Auto-styling**: Adapts to your terminal's color scheme
- **Word wrapping**: Automatically adjusts to terminal width
- **Emoji support**: Renders emoji characters when available
- **Rich formatting**: Supports headings, code blocks, links, and other markdown elements

The styling is managed internally by Glamour and does not require manual configuration in your atmos settings.

## Best Practices

1. **Color Contrast**: Ensure sufficient contrast between text and background colors for readability.
2. **Consistent Styling**: Use a consistent color scheme across different elements.
3. **Terminal Support**: Test your styling in different terminals to ensure compatibility.
4. **Accessibility**: Consider color-blind users when choosing your color scheme.

## Troubleshooting

1. **Verify Terminal Supports True Color:**

    - **Check `$COLORTERM`:**
      ```bash
      echo $COLORTERM
      ```
      **Expected Output:** `truecolor` or `24bit`

    - **Check `$TERM`:**
      ```bash
      echo $TERM
      ```
      **Recommended Values:** `xterm-256color`, `xterm-direct`, `xterm-truecolor`

2. **Ensure Your Terminal Emulator Supports True Color:**

    - Use a terminal emulator known for true color support (e.g., Terminal.app, iTerm2, Windows Terminal, etc).

3. **Configure Environment Variables Correctly:**

    - Set `$TERM` to a value that supports true color:
      ```bash
      export TERM=xterm-256color
      ```
      Add this to your shell's configuration file (`~/.bashrc`, `~/.zshrc`, etc.) to make it permanent.

4. **Validate `atmos.yaml` Configuration:**

    - Ensure colors are in hex format, boolean values are `true`/`false` (not quoted strings), and numbers are integers.
    - Use a YAML linter to validate the syntax.
    - Try removing custom styles to see if default styles work.

## See Also

- [CLI Configuration](/cli/configuration)
- [Command Reference](/cli/commands)

---

## Customize Stack Behavior

import Screengrab from '@site/src/components/Screengrab'
import Terminal from '@site/src/components/Terminal'
import File from '@site/src/components/File'
import Intro from '@site/src/components/Intro'

<Intro>
The `stacks` section of the `atmos.yaml` defines how Atmos locates and manages your stack configurations. Think of it as the bootstrapping configuration. Here you can define the stack name pattern or template used to build the "slugs" and specify where to find stack files.
</Intro>

:::important
Do not confuse this configuration with [stack configuration](/core-concepts/stacks).
This configuration below is defined in the `atmos.yaml` and instructs atmos where to find
your stack configurations.
:::

<File title="atmos.yaml">
```yaml
stacks:
  # Can also be set using 'ATMOS_STACKS_BASE_PATH' ENV var, or '--config-dir' and '--stacks-dir' command-line arguments
  # Supports both absolute and relative paths
  base_path: "stacks"

  # Can also be set using 'ATMOS_STACKS_INCLUDED_PATHS' ENV var (comma-separated values string)
  included_paths:
    # Tell Atmos to search for the top-level stack manifests in the `orgs` folder and its sub-folders
    - "orgs/**/*"

  # Can also be set using 'ATMOS_STACKS_EXCLUDED_PATHS' ENV var (comma-separated values string)
  excluded_paths:
    # Tell Atmos that all `_defaults.yaml` files are not top-level stack manifests
    - "**/_defaults.yaml"

  # To define Atmos stack naming convention, use either `name_pattern` or `name_template`.
  # `name_template` has higher priority (if `name_template` is specified, `name_pattern` will be ignored).
  # `name_pattern` uses the predefined context tokens {namespace}, {tenant}, {environment}, {stage}.
  # `name_pattern` can also be set using 'ATMOS_STACKS_NAME_PATTERN' ENV var
  name_pattern: "{tenant}-{environment}-{stage}"
  # `name_template` is a Golang template.
  # For the template tokens, and you can use any Atmos sections and attributes that the Atmos command
  # `atmos describe component <component> -s <stack>` generates (refer to https://atmos.tools/cli/commands/describe/component).
  # `name_template` can also be set using 'ATMOS_STACKS_NAME_TEMPLATE' ENV var
  # name_template: "{{.vars.tenant}}-{{.vars.environment}}-{{.vars.stage}}"
```
</File>

- `stacks.base_path` specifies the path to the folder where **all** Atmos stack config files (stack manifests) are defined.
  If the global `base_path` is not provided or is an empty string, `stacks.base_path` is an independent setting that supports both absolute and
  relative paths. If the global `base_path` is defined, `stacks.base_path` is relative to the global `base_path`

- `stacks.included_paths` tells Atmos where to search for the top-level stack manifests

  :::note
  Atmos top-level stack manifests are configuration files that define **all** settings and components for the corresponding environment (organization,
  OU/tenant, account, region), and they are used in `atmos` CLI commands like `atmos terraform plan <component> -s <top-level-stack>` and
  `atmos terraform apply <component> -s <top-level-stack>`
  :::

- `stacks.excluded_paths` tells Atmos which paths from `stacks.included_paths` to exclude. For example, we will exclude the config files that don't
  contain the top-level stack manifests, but just define the default values that get imported into top-level stack manifests

  :::note
  The `_defaults.yaml` naming convention is the recommended way to define stack manifests with
  default configurations for organizations, OUs/tenants, accounts and regions. This is a naming convention, not an Atmos feature.
  The `_defaults.yaml` files themselves are not top-level Atmos stacks—they just contain default values
  to make the entire configuration reusable and DRY. The underscore prefix ensures these files sort to the top
  of directory listings and are visually distinct from actual stack configurations.
  :::

  :::info
  The `_defaults.yaml` stack manifests are not imported into other Atmos manifests automatically.
  You must explicitly import them using [imports](/core-concepts/stacks/imports). Atmos has no special
  knowledge of this naming pattern—these files are only excluded from stack discovery because they match
  the `**/_defaults.yaml` pattern in the `excluded_paths` configuration.

  See the [_defaults.yaml Design Pattern](/design-patterns/defaults-pattern) for a complete explanation of this convention.
  :::

- `stacks.name_pattern` configures the name pattern for the top-level Atmos stacks using the context variables `namespace`, `tenant`, `environment`
  and `stage` as the tokens. Depending on the structure of your organization, OUs, accounts and regions, set `stacks.name_pattern` to the
  following:

  - `name_pattern: {stage}` - if you use just one region and a few accounts (stages) in just one organization and one OU. In this case, the
    top-level Atmos stacks will use just the `stage` (account) in their names, and to provision the Atmos components in the top-level stacks, you will
    be executing Atmos commands like `atmos terraform apply <component> --stack dev`, `atmos terraform apply <component> --stack staging`
    and `atmos terraform apply <component> --stack prod`

  - `name_pattern: {environment}-{stage}` - if you have multiple regions and accounts (stages) in just one organization and one OU. In this case, the
    top-level Atmos stacks will use the `environment` (region) and `stage` (account) in their names, and to provision the Atmos components in the
    top-level stacks, you will be executing Atmos commands
    like `atmos terraform apply <component> --stack ue2-dev`, `atmos terraform apply <component> --stack uw2-staging`
    and `atmos terraform apply <component> --stack ue1-prod`. Note that the `name_pattern` can also be defined
    as `{stage}-{environment}`, in which case the Atmos commands will look like `atmos terraform apply <component> --stack dev-ue2`

  - `name_pattern: {tenant}-{environment}-{stage}` - if you have multiple regions, OUs (tenants) and accounts (stages) in just one organization. In
    this case, the top-level Atmos stacks will use the `tenant`, `environment` (region) and `stage` (account) in their names, and to provision the
    Atmos components in the top-level stacks, you will be executing Atmos commands
    like `atmos terraform apply <component> --stack plat-ue2-dev`, `atmos terraform apply <component> --stack core-uw2-staging`
    and `atmos terraform apply <component> --stack plat-ue1-prod`, where `plat` and `core` are the OUs/tenants in your organization

  - `name_pattern: {namespace}-{tenant}-{environment}-{stage}` - if you have a multi-org, multi-tenant, multi-account and multi-region architecture.
    In this case, the top-level Atmos stacks will use the `namespace`, `tenant`, `environment` (region) and `stage` (account) in their names, and to
    provision the Atmos components in the top-level stacks, you will be executing Atmos commands
    like `atmos terraform apply <component> --stack org1-plat-ue2-dev`, `atmos terraform apply <component> --stack org2-core-uw2-staging`
    and `atmos terraform apply <component> --stack org2-plat-ue1-prod`, where `org1` and `org2` are the organization names (defined as `namespace` in
    the corresponding `_defaults.yaml` config files for the organizations)

- `stacks.name_template` serves the same purpose as `stacks.name_pattern` (defines the naming convention for the top-level Atmos stacks), but
  provides much more functionality. Instead of using the predefined context variables as tokens, it uses [Go templates](https://pkg.go.dev/text/template).
  [Atmos Template Functions](/functions/template),
  [Sprig Functions](https://masterminds.github.io/sprig/),
  [Gomplate Functions](https://docs.gomplate.ca/functions/),
  and [Gomplate Datasources](https://docs.gomplate.ca/datasources/) are supported as well

  - For the `Go` template tokens, and you can use any Atmos sections (e.g. `vars`, `providers`, `settings`)
    that the Atmos command [`atmos describe component <component> -s <stack>`](/cli/commands/describe/component) generates
    for a component in a stack.

  - `name_template: "{{.vars.tenant}}-{{.vars.environment}}-{{.vars.stage}}"` defines the same name pattern for the top-level
    Atmos stacks as `name_pattern: "{tenant}-{environment}-{stage}"` does

  - Since `stacks.name_template` allows using any variables form the `vars` section (and other sections), you can define
    your own naming convention for your organization or for different clouds (AWS, Azure, GCP). For example, in the
    corresponding `_defaults.yaml` stack manifests, you can use the following variables:

    - `org` instead of `namespace`
    - `division` instead of `tenant`
    - `region` instead of `environment`
    - `account` instead of `stage`

    Then define the following `stacks.name_template` in `atmos.yaml`:

    ```yaml title="atmos.yaml"
    stacks:
      name_template: "{{.vars.division}}-{{.vars.account}}-{{.vars.region}}"
    ```

    You will be able to execute all Atmos commands using the newly defined naming convention:

    ```shell
    atmos terraform plan <component> -s <division-account-region>
    atmos terraform apply <component> -s <division-account-region>
    atmos describe component <component> -s <division-account-region>
    ```

    `name_template` can have complex logic and use template expressions and functions.
    The following template defines a `name_template` that builds a `stack_name` string by validating and concatenating
    several input variables in a hierarchical order.

    <File title="atmos.yaml">
        ```yaml
        name_template: |-
          {{- $ns := .vars.namespace -}}
          {{- $tenant := .vars.tenant -}}
          {{- $env := .vars.environment -}}
          {{- $stage := .vars.stage -}}
          {{- $stack_name := "" -}}

          {{- if eq $ns "" -}}
          {{- fail "Error: 'namespace' is required." -}}
          {{- end -}}

          {{- if and (ne $tenant "") (eq $ns "") -}}
          {{- fail "Error: 'tenant' requires 'namespace'." -}}
          {{- end -}}

          {{- if and (ne $env "") (or (eq $tenant "") (eq $ns "")) -}}
          {{- fail "Error: 'environment' requires 'tenant' and 'namespace'." -}}
          {{- end -}}

          {{- if and (ne $stage "") (or (eq $env "") (eq $tenant "") (eq $ns "")) -}}
          {{- fail "Error: 'stage' requires 'environment', 'tenant', and 'namespace'." -}}
          {{- end -}}

          {{- if ne $tenant "" -}}
          {{- $stack_name = $tenant -}}
          {{- end -}}

          {{- if ne $env "" -}}
          {{- $stack_name = printf "%s-%s" $stack_name $env -}}
          {{- end -}}

          {{- if ne $stage "" -}}
          {{- $stack_name = printf "%s-%s" $stack_name $stage -}}
          {{- end -}}

          {{- $stack_name -}}
        ```
    </File>

    It pulls values from the Atmos section `vars` and assigns them to local template variables:
     - `namespace`
     - `tenant`
     - `environment`
     - `stage`

    The template enforces hierarchical dependencies between variables:
     - `namespace` is required
     - If `tenant` is provided, `namespace` must also be set
     - If `environment` is provided, both `tenant` and `namespace` must be set
     - If `stage` is provided, then `environment`, `tenant`, and `namespace` must all be set

    If validations pass, it constructs the `stack_name` progressively:
     - Starts with `tenant` if it exists
     - Appends `environment` if it exists
     - Appends `stage` if it exists

    The template outputs the resulting stack name. For example, if the variables are:

    ```yaml
    namespace: acme
    tenant: plat
    environment: ue2
    stage: prod
    ```

    The resulting stack name will be `plat-ue2-prod`.

:::note
Use either `stacks.name_pattern` or `stacks.name_template` to define the naming convention for the top-level Atmos stacks.

`stacks.name_template` has higher priority.

If `stacks.name_template` is specified, `stacks.name_pattern` will be ignored.
:::

:::tip
Refer to [Atmos Design Patterns](/design-patterns) for the examples on how to configure the `stacks` section in `atmos.yaml` for different use-cases
:::

---

## Terminal Settings

import Intro from '@site/src/components/Intro';
import KeyPoints from '@site/src/components/KeyPoints';
import Note from '@site/src/components/Note';
import File from '@site/src/components/File';

<Intro>
Atmos provides configurable terminal settings that allow you to customize the output appearance, including syntax highlighting for YAML and JSON outputs. These settings can be configured in your `atmos.yaml` configuration file.
</Intro>

<KeyPoints>
- Configure syntax highlighting for terminal output
- Customize color schemes and formatting options
- Control output pagination and line wrapping
- Set display preferences for different output formats
</KeyPoints>
## General Terminal Settings

Configure general terminal behavior. These are also the default settings if not specified in your `atmos.yaml`:

```yaml
settings:
  terminal:
    max_width: 120        # Maximum width for terminal output
    pager: false          # Pager disabled by default (set to true, or pager name like 'less' to enable)
    color: true           # Enable colored output (default: true)
    unicode: true         # Use Unicode characters in output
    tab_width: 2          # Number of spaces for YAML indentation (default: 2)
```

## Configuration Precedence

Atmos follows a clear precedence order for terminal settings, with command-line flags having the highest priority:

### Pager Configuration Precedence
1. **CLI Flags** (highest priority): `--pager=false`, `--pager=less`
2. **NO_PAGER Environment Variable**: `NO_PAGER=1` (standard CLI convention)
3. **ATMOS_PAGER Environment Variable**: `ATMOS_PAGER=less`
4. **PAGER Environment Variable**: `PAGER=more` (system default)
5. **Configuration File** (lowest priority): `settings.terminal.pager: true`

### Color Configuration Precedence
1. **CLI Flags** (highest priority): `--no-color`
2. **NO_COLOR Environment Variable**: `NO_COLOR=1` (standard CLI convention)
3. **ATMOS_NO_COLOR Environment Variable**: `ATMOS_NO_COLOR=true`
4. **ATMOS_COLOR Environment Variable**: `ATMOS_COLOR=false`
5. **COLOR Environment Variable**: `COLOR=false`
6. **Configuration File** (lowest priority): `settings.terminal.color: false`

## Syntax Highlighting

You can customize the syntax highlighting behavior for terminal output using the following settings:

```yaml
settings:
  terminal:
    # Main terminal settings
    pager: true                     # Enable pager for all terminal output
    max_width: 120                  # Maximum width for terminal output
    color: true                     # Enable colored output

    # Syntax highlighting specific settings
    syntax_highlighting:
      enabled: true                 # Enable/disable syntax highlighting
      formatter: terminal           # Output formatter
      theme: dracula                # Color scheme to use
      line_numbers: false           # Show line numbers
      wrap: false                   # Wrap long lines
```

### Terminal Configuration Options

<dl>
  <dt>`max_width`</dt>
  <dd>Maximum width for terminal output (default: `120`)</dd>

  <dt>`pager`</dt>
  <dd>
    Configure pager behavior for output display.
    - `false` or empty: Pager disabled (default)
    - `true` or `on`: Enable pager with system default
    - Pager command (e.g., `less`, `more`): Use specific pager program
    - Environment variables: `NO_PAGER` (disable), `ATMOS_PAGER`, `PAGER` (system default)
    - CLI control: `--pager` global flag
  </dd>

  <dt>`color`</dt>
  <dd>
    Enable colored terminal output (default: `true`).
    - Environment variables: `NO_COLOR` (standard), `ATMOS_NO_COLOR`, `ATMOS_COLOR`, `COLOR`
    - CLI control: `--no-color` global flag
  </dd>

  <dt>`unicode`</dt>
  <dd>Use Unicode characters in output (default: `true`)</dd>

  <dt>`tab_width`</dt>
  <dd>Number of spaces for YAML indentation (default: `2`)</dd>
</dl>

### Syntax Highlighting Configuration Options

<dl>
  <dt>`enabled`</dt>
  <dd>Enable or disable syntax highlighting (default: `true`)</dd>

  <dt>`formatter`</dt>
  <dd>Output formatter (default: `terminal`)</dd>

  <dt>`theme`</dt>
  <dd>
    Color scheme for syntax highlighting. Available options include:

      `vim`
      `monokai`
      `github`
      `dracula`
      ...and many other standard themes

    You can find the full list of supported themes [here](https://xyproto.github.io/splash/docs/).
  </dd>

  <dt>`line_numbers`</dt>
  <dd>Show line numbers in output (default: `false`)</dd>

  <dt>`wrap`</dt>
  <dd>Wrap long lines (default: `false`)</dd>
</dl>

### Example Usage

The syntax highlighting is automatically applied when using commands that output YAML or JSON, such as:

```bash
# Display config in YAML format with syntax highlighting
atmos describe config -f yaml
# Display config in JSON format with syntax highlighting
atmos describe config
```

<Note>
When the output is piped to another command, syntax highlighting is automatically disabled to ensure compatibility:
```bash
# Syntax highlighting is disabled when piping
atmos describe config | grep base_path
```
</Note>

## Supported Themes

Atmos supports a wide range of themes for syntax highlighting. You can find the full list of supported themes [here](https://xyproto.github.io/splash/docs/).

---

## Global Flags

import Intro from '@site/src/components/Intro';
import Note from '@site/src/components/Note';

# Global Flags

<Intro>
    Global flags are available for all Atmos commands and control the overall behavior of the CLI. These flags take
    precedence over environment variables and configuration files.
</Intro>

## Core Global Flags

These flags are available for every Atmos command:

<dl>
    <dt>`--base-path`</dt>
    <dd>
        Base path for the Atmos project. This is the root directory where Atmos will look for configuration files,
        stacks, and components.
        - Can also use `ATMOS_BASE_PATH` environment variable
        - Supports both absolute and relative paths
    </dd>

    <dt>`--config`</dt>
    <dd>
        Path to a specific Atmos configuration file.
        - Can be used multiple times for deep merging (later files override earlier ones)
        - Example: `--config=base.yaml --config=override.yaml`
    </dd>

    <dt>`--config-path`</dt>
    <dd>
        Path to a directory containing Atmos configuration files.
        - Can be used multiple times
        - Atmos looks for `atmos.yaml`, `.atmos.yaml`, `atmos.yml`, or `.atmos.yml` in these directories
    </dd>

    <dt>`--logs-level`</dt>
    <dd>
        Set the logging level for Atmos operations.
        - Options: `Trace`, `Debug`, `Info`, `Warning`, `Off`
        - Default: `Info`
        - Can also use `ATMOS_LOGS_LEVEL` environment variable
    </dd>

    <dt>`--logs-file`</dt>
    <dd>
        File to write Atmos logs to.
        - Default: `/dev/stderr`
        - Can be any file path or standard file descriptor (`/dev/stdout`, `/dev/stderr`, `/dev/null`)
        - Can also use `ATMOS_LOGS_FILE` environment variable
    </dd>

    <dt>`--no-color`</dt>
    <dd>
        Disable colored terminal output.
        - Useful for CI/CD environments or when piping output
        - Can also use `ATMOS_NO_COLOR` or `NO_COLOR` environment variables
        - The `NO_COLOR` env var follows the standard from https://no-color.org/
    </dd>

    <dt>`--pager`</dt>
    <dd>
        Configure pager behavior for command output.
        - `--pager` (no value): Enable pager with default settings
        - `--pager=true` or `--pager=on`: Explicitly enable pager
        - `--pager=false` or `--pager=off`: Explicitly disable pager
        - Any non-boolean value is treated as a pager command (first token) with following tokens as arguments

        **Examples:**
        - `--pager='less -R'`: Use less with raw control chars
        - `--pager="less --RAW-CONTROL-CHARS"`: Alternative syntax for less with options
        - `export ATMOS_PAGER='less -R +Gg'` or `export PAGER='less -R'`: Set via environment variables

        **Environment Variables:**
        - `NO_PAGER`: Standard CLI convention to disable pager (e.g., `NO_PAGER=1`)
        - `ATMOS_PAGER`: Atmos-specific pager configuration
        - `PAGER`: System default pager (fallback)

        **Precedence**: `--pager` flag > `NO_PAGER` > `ATMOS_PAGER` > `PAGER` > config file

        **Default**: Pager is disabled unless explicitly enabled

        **Note**: Use quotes around the value to preserve spaces and prevent shell splitting when passing pager
        arguments. The `NO_PAGER` environment variable follows the standard CLI convention for disabling pagers.
    </dd>

    <dt>`--redirect-stderr`</dt>
    <dd>
        Redirect stderr to a file or file descriptor.
        - Can redirect to any file path or standard file descriptor
        - Example: `--redirect-stderr=/dev/null` to suppress error output
    </dd>
</dl>

## Command-Specific Flags

These flags are available across multiple commands but not universally:

### Processing Flags

<dl>
    <dt>`--process-templates`</dt>
    <dd>
        Enable or disable Go template processing in Atmos manifests.
        - Default: `true`
        - Available in: `describe stacks`, `list`, `validate` commands
    </dd>

    <dt>`--process-functions`</dt>
    <dd>
        Enable or disable YAML function processing in Atmos manifests.
        - Default: `true`
        - Available in: `describe stacks`, `list`, `validate` commands
    </dd>

    <dt>`--skip`</dt>
    <dd>
        Skip processing specific Atmos functions in manifests.
        - Can be used multiple times
        - Example: `--skip=terraform.output --skip=include`
        - Available in: `describe` commands
    </dd>
</dl>

### Output Flags

<dl>
    <dt>`--format` / `-f`</dt>
    <dd>
        Specify the output format.
        - Common values: `yaml`, `json`, `table`, `csv`
        - Available in: `describe`, `list`, `validate` commands
    </dd>

    <dt>`--file`</dt>
    <dd>
        Write output to a file instead of stdout.
        - Available in: `describe`, `generate` commands
    </dd>

    <dt>`--query` / `-q`</dt>
    <dd>
        Query output using JSONPath or yq expressions.
        - Example: `--query='.components.vpc.vars'`
        - Available in: `describe` commands
    </dd>
</dl>

### Profiling Flags

<dl>
    <dt>`--profiler-enabled`</dt>
    <dd>
        Enable the pprof HTTP profiling server.
        - Default: `false`
        - When enabled, starts an HTTP server for interactive profiling
        - Can also use `ATMOS_PROFILER_ENABLED` environment variable
    </dd>

    <dt>`--profiler-host`</dt>
    <dd>
        Host address for the profiling server.
        - Default: `localhost`
        - Use `0.0.0.0` to allow external connections (security consideration)
        - Can also use `ATMOS_PROFILER_HOST` environment variable
    </dd>

    <dt>`--profiler-port`</dt>
    <dd>
        Port for the profiling server.
        - Default: `6060`
        - Can also use `ATMOS_PROFILER_PORT` environment variable
    </dd>

    <dt>`--profile-file`</dt>
    <dd>
        Write profiling data to the specified file (enables profiling automatically).
        - When specified, enables file-based profiling instead of server-based
        - File extension should match profile type (e.g., `.prof` for CPU, `.out` for trace)
        - Can also use `ATMOS_PROFILE_FILE` environment variable
    </dd>

    <dt>`--profile-type`</dt>
    <dd>
        Type of profile to collect when using `--profile-file`.
        - Options: `cpu`, `heap`, `allocs`, `goroutine`, `block`, `mutex`, `threadcreate`, `trace`
        - Default: `cpu`
        - Only used with `--profile-file`, ignored for server-based profiling
        - Can also use `ATMOS_PROFILE_TYPE` environment variable
    </dd>
</dl>

## Environment Variables

All global flags can be set using environment variables. The precedence order is:

1. Command-line flags (highest priority)
2. Environment variables
3. Configuration file (`atmos.yaml`)
4. Default values (lowest priority)

### Core Environment Variables

<dl>
    <dt>`ATMOS_BASE_PATH`</dt>
    <dd>Sets the base path for the Atmos project</dd>

    <dt>`ATMOS_LOGS_LEVEL`</dt>
    <dd>Sets the logging level</dd>

    <dt>`ATMOS_LOGS_FILE`</dt>
    <dd>Sets the log file location</dd>

    <dt>`ATMOS_COLOR` / `COLOR`</dt>
    <dd>
        Enable or disable colored output.
        - Set to `true` to enable color (default)
        - Set to `false` to disable color
        - Both `ATMOS_COLOR` and `COLOR` are supported for maximum portability
    </dd>

    <dt>`ATMOS_NO_COLOR` / `NO_COLOR`</dt>
    <dd>
        Disable colored output (any non-empty value disables color).
        - `NO_COLOR` is a standard environment variable supported by many CLI tools (https://no-color.org/)
        - Maintained for portability across different systems and CI/CD environments
        - Takes precedence over `ATMOS_COLOR`/`COLOR` settings
        - Both `ATMOS_NO_COLOR` and `NO_COLOR` are fully supported
    </dd>

    <dt>`ATMOS_PAGER` / `PAGER`</dt>
    <dd>
        Configure pager settings.
        - `PAGER` is a standard Unix environment variable maintained for portability
        - Both `ATMOS_PAGER` and `PAGER` are supported to ensure compatibility across different systems
    </dd>

    <dt>`ATMOS_PROFILER_ENABLED`</dt>
    <dd>Enable the pprof HTTP profiling server</dd>

    <dt>`ATMOS_PROFILER_HOST`</dt>
    <dd>Set the host address for the profiling server</dd>

    <dt>`ATMOS_PROFILER_PORT`</dt>
    <dd>Set the port for the profiling server</dd>

    <dt>`ATMOS_PROFILE_FILE`</dt>
    <dd>Set the file path for file-based profiling</dd>

    <dt>`ATMOS_PROFILE_TYPE`</dt>
    <dd>Set the profile type for file-based profiling (cpu, heap, allocs, goroutine, block, mutex, threadcreate, trace)</dd>
</dl>

## Portability Notes

Atmos supports both standard and Atmos-prefixed environment variables to ensure maximum portability:

- **Standard Variables** (`NO_COLOR`, `COLOR`, `PAGER`): Work across many CLI tools and Unix systems
- **Atmos Variables** (`ATMOS_NO_COLOR`, `ATMOS_COLOR`, `ATMOS_PAGER`): Provide namespace isolation when needed

This dual support ensures your scripts and CI/CD pipelines work consistently across different environments without modification.

## Examples

### Basic Usage

```bash
# Disable color and pager for CI environment
atmos describe config --no-color --pager=off

# Use specific pager with custom log level
atmos describe stacks --pager=less --logs-level=Debug

# Multiple config files with base path
atmos --base-path=/infrastructure \
      --config=base.yaml \
      --config=override.yaml \
      terraform plan vpc -s prod
```

### Pager Control Examples

```bash
# Enable pager (multiple ways)
atmos describe config --pager                # Enable with default pager
atmos describe config --pager=true           # Explicitly enable
atmos describe config --pager=less           # Use specific pager
ATMOS_PAGER=true atmos describe config      # Via environment variable

# Disable pager (explicit)
atmos describe config --pager=false          # Explicitly disable
atmos describe config --pager=off            # Alternative syntax
ATMOS_PAGER=false atmos describe config     # Via environment variable

# Disable pager using NO_PAGER (standard CLI convention)
NO_PAGER=1 atmos describe config            # Standard way to disable pager
export NO_PAGER=1; atmos describe config    # Set for entire session

# Default behavior (no flag = pager disabled)
atmos describe config                        # Pager is OFF by default
```

### Color Control Examples

```bash
# Multiple ways to disable color
atmos describe config --no-color           # Using flag
NO_COLOR=1 atmos describe config          # Using NO_COLOR standard
ATMOS_NO_COLOR=1 atmos describe config    # Using ATMOS_NO_COLOR
ATMOS_COLOR=false atmos describe config   # Using ATMOS_COLOR
COLOR=false atmos describe config         # Using COLOR

# Explicitly enable color (overrides config file setting)
ATMOS_COLOR=true atmos describe config
```

### Environment Variable Usage

```bash
# Set environment variables
export ATMOS_PAGER=off
export ATMOS_COLOR=false
export ATMOS_LOGS_LEVEL=Debug

# Commands will use these settings
atmos describe config
```

### CI/CD Configuration

```bash
# Typical CI/CD settings
export ATMOS_NO_COLOR=true
export ATMOS_PAGER=off
export ATMOS_LOGS_LEVEL=Warning
export ATMOS_LOGS_FILE=/var/log/atmos.log

# Run commands without interactive features
atmos terraform apply --auto-approve
```

### Profiling Examples

```bash
# File-based CPU profiling (default profile type)
atmos terraform plan vpc -s prod --profile-file=cpu.prof

# File-based memory heap profiling
atmos terraform plan vpc -s prod --profile-file=heap.prof --profile-type=heap

# File-based execution trace profiling
atmos terraform plan vpc -s prod --profile-file=trace.out --profile-type=trace

# Server-based profiling for interactive analysis
atmos terraform apply vpc -s prod --profiler-enabled --profiler-port=8080

# Environment variable configuration
export ATMOS_PROFILE_FILE=debug.prof
export ATMOS_PROFILE_TYPE=goroutine
atmos describe stacks

# Multiple profile types for comprehensive analysis
atmos terraform plan vpc -s prod --profile-file=cpu.prof --profile-type=cpu
atmos terraform plan vpc -s prod --profile-file=heap.prof --profile-type=heap
atmos terraform plan vpc -s prod --profile-file=trace.out --profile-type=trace
```

### CI/CD Portability Example

```bash
# These environment variables work across many tools, not just Atmos
export NO_COLOR=1        # Disables color in Atmos and other NO_COLOR-compliant tools
export ATMOS_PAGER=off  # Properly disables paging in Atmos

# Run various CLI tools - all respect the same env vars
atmos describe config
terraform plan
kubectl get pods
```

<Note>
    When output is piped to another command, Atmos automatically disables color output and pager to ensure
    compatibility:

    ```bash
    # Color and pager automatically disabled
    atmos describe stacks | grep production
    ```
</Note>

## See Also

- [CLI Configuration](/cli/configuration) - Detailed configuration file reference
- [Terminal Settings](/cli/configuration/terminal) - Terminal-specific configuration options
- [Environment Variables](/cli/configuration#environment-variables) - Complete environment variable reference

---

## Atmos Manifest JSON Schema

import Terminal from '@site/src/components/Terminal'
import Intro from '@site/src/components/Intro'

<Intro>
[Atmos Manifest JSON Schema](pathname:///schemas/atmos/atmos-manifest/1.0/atmos-manifest.json) can be used to validate Atmos stack manifests and provide auto-completion.
</Intro>

### Validate and Auto-Complete Atmos Manifests in IDEs

In supported editors like [JetBrains IDEs](https://www.jetbrains.com/), [Microsoft Visual Studio](https://visualstudio.microsoft.com/)
or [Visual Studio Code](https://code.visualstudio.com/), the schema can offer auto-completion and validation to ensure that Atmos stack manifests, and
all sections in them, are
correct.

:::tip

A list of editors that support validation using [JSON Schema](https://json-schema.org/) can be
found [here](https://json-schema.org/implementations#editors).

:::

### Validate Atmos Manifests on the Command Line

Atmos can use the [Atmos Manifest JSON Schema](pathname:///schemas/atmos/atmos-manifest/1.0/atmos-manifest.json) to validate Atmos stack manifests on the
command line by executing the command [`atmos validate stacks`](/cli/commands/validate/stacks).

For this to work, configure the following:

- Add the _optional_ [Atmos Manifest JSON Schema](pathname:///schemas/atmos/atmos-manifest/1.0/atmos-manifest.json) to your repository, for example
  in `stacks/schemas/atmos/atmos-manifest/1.0/atmos-manifest.json`. If not specified, Atmos will default to the [schema](pathname:///schemas/atmos/atmos-manifest/1.0/atmos-manifest.json) corresponding to the currently installed version of Atmos.

- Configure the following section in the `atmos.yaml` [CLI config file](/cli/configuration)

  ```yaml title="atmos.yaml"
  # Validation schemas (for validating atmos stacks and components)
  schemas:
    # JSON Schema to validate Atmos manifests
    atmos:
      # Can also be set using 'ATMOS_SCHEMAS_ATMOS_MANIFEST' ENV var, or '--schemas-atmos-manifest' command-line arguments
      # Supports both absolute and relative paths (relative to the `base_path` setting in `atmos.yaml`)
      manifest: "stacks/schemas/atmos/atmos-manifest/1.0/atmos-manifest.json"
  ```

- Execute the command [`atmos validate stacks`](/cli/commands/validate/stacks)

- Instead of configuring the `schemas.atmos.manifest` section in `atmos.yaml`, you can provide the path to the Atmos Manifest JSON Schema file by
  using the ENV variable `ATMOS_SCHEMAS_ATMOS_MANIFEST` or the `--schemas-atmos-manifest` command line argument:

  ```shell
  ATMOS_SCHEMAS_ATMOS_MANIFEST=stacks/schemas/atmos/atmos-manifest/1.0/atmos-manifest.json atmos validate stacks
  atmos validate stacks --schemas-atmos-manifest stacks/schemas/atmos/atmos-manifest/1.0/atmos-manifest.json
  ```

In case of any validation errors (invalid YAML syntax, Atmos manifest JSON Schema errors, invalid imports, etc.), you'll get an output from the
command similar to the following:

<Terminal title="atmos validate stacks">
```text
Atmos manifest JSON Schema validation error in the
file 'catalog/invalid-yaml-and-schema/invalid-import-5.yaml':
{
  "valid": false,
  "errors": [
    {
      "keywordLocation": "",
      "absoluteKeywordLocation": "tests/fixtures/scenarios/complete/schemas/atmos/atmos-manifest/1.0/atmos-manifest.json#",
      "instanceLocation": "",
      "error": "doesn't validate with tests/fixtures/scenarios/complete/schemas/atmos/atmos-manifest/1.0/atmos-manifest.json#"
    },
    {
      "keywordLocation": "/properties/import/$ref",
      "absoluteKeywordLocation": "tests/fixtures/scenarios/complete/schemas/atmos/atmos-manifest/1.0/atmos-manifest.json#/properties/import/$ref",
      "instanceLocation": "/import",
      "error": "doesn't validate with '/definitions/import'"
    },
    {
      "keywordLocation": "/properties/import/$ref/type",
      "absoluteKeywordLocation": "tests/fixtures/scenarios/complete/schemas/atmos/atmos-manifest/1.0/atmos-manifest.json#/definitions/import/type",
      "instanceLocation": "/import",
      "error": "expected array, but got object"
    }
  ]
}

Atmos manifest JSON Schema validation error in the
file 'catalog/invalid-yaml-and-schema/invalid-schema-8.yaml':
{
  "valid": false,
  "errors": [
    {
      "keywordLocation": "",
      "absoluteKeywordLocation": "tests/fixtures/scenarios/complete/schemas/atmos/atmos-manifest/1.0/atmos-manifest.json#",
      "instanceLocation": "",
      "error": "doesn't validate with tests/fixtures/scenarios/complete/schemas/atmos/atmos-manifest/1.0/atmos-manifest.json#"
    },
    {
      "keywordLocation": "/properties/env/$ref",
      "absoluteKeywordLocation": "tests/fixtures/scenarios/complete/schemas/atmos/atmos-manifest/1.0/atmos-manifest.json#/properties/env/$ref",
      "instanceLocation": "/env",
      "error": "doesn't validate with '/definitions/env'"
    },
    {
      "keywordLocation": "/properties/env/$ref/type",
      "absoluteKeywordLocation": "tests/fixtures/scenarios/complete/schemas/atmos/atmos-manifest/1.0/atmos-manifest.json#/definitions/env/type",
      "instanceLocation": "/env",
      "error": "expected object, but got array"
    }
  ]
}

no matches found for the import 'globals/tenant1-globals-does-not-exist' in the
file 'catalog/invalid-yaml-and-schema/invalid-import-1.yaml'

invalid import in the file 'catalog/invalid-yaml-and-schema/invalid-import-2.yaml'
The file imports itself in 'catalog/invalid-yaml-and-schema/invalid-import-2'

invalid stack manifest 'catalog/invalid-yaml-and-schema/invalid-yaml-1.yaml'
yaml: line 15: found unknown directive name

invalid stack manifest 'catalog/invalid-yaml-and-schema/invalid-yaml-3.yaml'
yaml: line 13: did not find expected key

invalid stack manifest 'catalog/invalid-yaml-and-schema/invalid-yaml-5.yaml'
yaml: mapping values are not allowed in this context

invalid stack manifest 'catalog/invalid-yaml-and-schema/invalid-yaml-6.yaml'
yaml: line 2: block sequence entries are not allowed in this context

invalid stack manifest 'catalog/invalid-yaml-and-schema/invalid-yaml-7.yaml'
yaml: line 4: could not find expected ':'
```
</Terminal>

## References

- https://json-schema.org
- https://json-schema.org/draft/2020-12/release-notes
- https://www.schemastore.org/json
- https://github.com/SchemaStore/schemastore
- https://www.jetbrains.com/help/idea/json.html#ws_json_using_schemas
- https://code.visualstudio.com/docs/languages/json

---

## Telemetry

import Terminal from '@site/src/components/Terminal'
import Intro from '@site/src/components/Intro'
import File from '@site/src/components/File'

<Intro>
Atmos collects **anonymous telemetry** to help improve the product by understanding how it's used.
This data helps us identify which features are most valuable and where we can make improvements.
</Intro>

### What Data is Collected

Atmos collects the following anonymous information:

- **Command Usage**: Tracks which Atmos commands are executed (e.g., `atmos describe`, `atmos terraform`, `atmos helmfile`)
- **Error Reports**: Captures anonymized error messages and exit codes to help identify and fix issues
- **System Details**: Includes OS, CPU architecture, Atmos version, and other diagnostic metadata
- **CI Provider information**: If Atmos is running as part of CI workflow, CI provider name

:::info Privacy First
We **never** collect:
- Personal information (names, emails, IP addresses)
- Your actual configuration files or stack manifests
- Sensitive data or secrets
- Repository contents or code
:::

### Atmos Pro Users

If you're using [**Atmos Pro**](https://atmos-pro.com), Atmos includes your **Workspace ID** with telemetry. This allows us to associate usage with your account and better support your team by understanding adoption and usage patterns.

### How Telemetry Works

Telemetry data is collected locally and sent securely to [PostHog](https://posthog.com/) analytics service. The data is:

- **Anonymous**: No personally identifiable information is included
- **Secure**: Transmitted over HTTPS with encryption
- **Minimal**: Only essential usage data is collected
- **Transparent**: You can see exactly what's being collected

### Opting Out of Telemetry

You can disable telemetry collection in any of the following ways:

Add the following to your `atmos.yaml` configuration file:

<File title="atmos.yaml">
```yaml
settings:
  telemetry:
    enabled: false
```
</File>

or set environment variable `ATMOS_TELEMETRY_ENABLED` to `false`

### Collect your own telemetry

You can switch telemetry to your own [PostHog](https://posthog.com/) account:

Add the following to your `atmos.yaml` configuration file:

<File title="atmos.yaml">
```yaml
settings:
  telemetry:
    enabled: true
    token: {provide your posthog token}
    endpoint: {provide your posthog endpoint}
```
</File>

or set environment variables `ATMOS_TELEMETRY_TOKEN` and `ATMOS_TELEMETRY_ENDPOINT` to your own values

### Telemetry Logging Configuration

By default, PostHog internal logging messages are suppressed to prevent cluttering your terminal output. If you need to debug telemetry issues, you can enable PostHog internal logging:

<File title="atmos.yaml">
```yaml
settings:
  telemetry:
    logging: true  # Enable PostHog internal debug messages (default: false)
```
</File>

or set environment variable `ATMOS_TELEMETRY_LOGGING` to `true`

:::tip
PostHog logging is useful for debugging telemetry connection issues or when configuring your own PostHog instance. When enabled, PostHog internal messages will be routed through Atmos logging at the DEBUG level.
:::

---

## Atmos Versioning

import Intro from '@site/src/components/Intro'

<Intro>
Atmos follows the Semantic Versioning (SemVer) convention: <code>major.minor.patch.</code>
</Intro>

Incompatible changes increment the <code>major</code> version, adding backwards-compatible functionality increments the <code>minor</code> version,
and backwards-compatible bug fixes increment the <code>patch</code> version.

## Release Schedule

### Major Release

A major release will be published when there is a breaking change introduced in `atmos`.
Several release candidates will be published prior to a major release in order to get feedback before the final release.
An outline of what is changing and why will be included with the release candidates.

### Minor Release

A minor release will be published when a new feature is added or changes that are non-breaking are introduced.
We will heavily test any changes so that we are confident with the release, but with new code comes the potential for new issues.

### Patch Release

A patch release will be published when bug fixes were included, but no breaking changes were introduced.
To ensure patch releases can fix existing code without introducing new issues from the new features, patch releases will always be published prior to
a minor release.

## Changelog

To see a list of all notable changes to `atmos` please refer to
the changelog.
It contains an ordered list of all bug fixes and new features under each release.

---

## Community

import DocCardList from '@theme/DocCardList'
import Intro from '@site/src/components/Intro'

# Community Resources

<Intro>
Need help? Join the community!

Atmos has a great community of active users who are all more than willing to help each other out. So, join us!
</Intro>

Found a bug or issue? Please report it in [our issue tracker](https://github.com/cloudposse/atmos/issues)

:::tip Join us on Office Hours

We hold ["office hours" every Wednesday at 11:30am PST](/community/office-hours).

:::

Are you more into email? Sign up for [Cloud Posse's Weekly Newsletter](https://newsletter.cloudposse.com) to get the latest news about things happening in our community and other news about building Open Source infrastructure—straight into your inbox.

<DocCardList/>

---

## Office Hours

import HubspotForm from 'react-hubspot-form'

# Office Hours Registration

<HubspotForm
    portalId='2197148'
    region="na1"
    formId='bbcd46fe-0b11-43aa-9214-33f319e52a01'
    onSubmit={() => console.log('Submit!')}
    onReady={(form) => console.log('Form ready!')}
    loading={Loading...}
/>

---

## #atmos

## Join our Slack Community!

Atmos has a great community of active users who are all more than willing to help each other out. So, join us!

<iframe src="https://slack.cloudposse.com/?theme=dark&background=00000000"
        marginWidth="0"
        marginHeight="0"
        hspace="0"
        vspace="0"
        width="300"
        height="330"
        scrolling="no"
        frameBorder="0"
        allowtransparency="true"
        style={{ borderRadius: '6px', display: 'flex', marginRight: 'auto', marginLeft: 'auto'}}
        ></iframe>

---

## Code of Conduct

import Intro from '@site/src/components/Intro'

<Intro>
As contributors and maintainers of the Atmos project by [Cloud Posse](https://cloudposse.com), we pledge to respect everyone who contributes by posting issues, updating documentation, submitting pull requests, providing feedback in comments, and any other activities.
</Intro>

Communication through any of Cloud Posse's channels ([GitHub](https://github.com/cloudposse), [Slack](https://slack.cloudposse.com), [mailing lists](https://cloudposse.com/newsletter), [Twitter](https://twitter.com/cloudposse), etc.) must be constructive and never resort to personal attacks, trolling, public or private harassment, insults, or other unprofessional conduct.

We promise to extend courtesy and respect to everyone involved in this project regardless of gender, gender identity,
sexual orientation, disability, age, race, ethnicity, religion, or level of experience. We expect anyone contributing to the Atmos project to do the same.

If any member of the community violates this code of conduct, the maintainers of the Atmos project may take action,
removing issues, comments, and PRs or blocking accounts as deemed appropriate.

---

## Contributing


import DocCardList from '@theme/DocCardList'

<DocCardList/>

---

## How to Contribute

Thanks for the interest in contributing to the Atmos project!

## Contributing Etiquette

Please see the [Contributor Code of Conduct](/contribute/coc) for information on the rules of conduct.

## Creating an Issue

- It is required that you clearly describe the steps necessary to reproduce the issue you are running into. Although we would love to help our users
  as much as possible, diagnosing issues without clear reproduction steps is extremely time-consuming and simply not sustainable.

- The issue list of the [atmos](https://github.com/cloudposse/atmos) repository is exclusively for bug reports and feature requests. Non-conforming
  issues will be closed immediately.

- Issues with no clear steps to reproduce will not be triaged. If an issue is labeled with "needs: reply" and receives no further replies from the
  author of the issue for more than 14 days, it will be closed.

- If you think you have found a bug, or have a new feature idea, please start by making sure it hasn't already
  been [reported](https://github.com/cloudposse/atmos/issues?utf8=%E2%9C%93&q=is%3Aissue). You can search through existing issues to see if there is a
  similar one reported. Include closed issues as it may have been closed with a solution.

- Next, [create a new issue](https://github.com/cloudposse/atmos/issues/new/choose) that thoroughly explains the problem. Please fill out the
  populated issue form before submitting the issue.

## Creating a Good Code Reproduction

### What is a Code Reproduction?

A code reproduction is a small application that demonstrates a particular issue. The code reproduction should contain the minimum amount of code needed to reproduce the issue and should focus on a single issue.

### Why Should You Create a Reproduction?

A code reproduction of the issue you are experiencing helps us better isolate the cause of the problem. This is an important first step to getting any bug fixed!

Without a reliable code reproduction, it is unlikely we will be able to resolve the issue, leading to it being closed. In other words, creating a code reproduction of the issue helps us help you.

## Creating a Pull Request

- We appreciate you taking the time to contribute! Before submitting a pull request, we ask that you please [create an issue](#creating-an-issue) explaining the bug or feature request and let us know that you plan on making a pull request. If an issue already exists, please comment on that issue letting us know you would like to submit a pull request for it. This helps us to keep track of the pull request and make sure there isn't duplicated effort.

- Looking for an issue to fix? Make sure to look through our issues with the [help wanted](https://github.com/cloudposse/atmos/issues?q=is%3Aopen+is%3Aissue+label%3A%22help+wanted%22) label!

## License

By contributing your code to the `cloudposse/atmos` GitHub Repository, you agree to license your contribution under
the [Apache license](http://www.apache.org/licenses).

---

## Component Library

import Intro from '@site/src/components/Intro'

<Intro>
A component library is a collection of reusable "components" that are reused any number of times from within [Stacks](/core-concepts/stacks).  It's helpful to think of these "building blocks" as the essentials of infrastructure, like VPCs, clusters or databases. They should all be kept in a library.
</Intro>

:::tip
Get a head start by utilizing Cloud Posse's free [Terraform components for AWS](https://github.com/cloudposse/terraform-aws-components), available on GitHub.
:::

## Use-cases

- **Developer Productivity** Create a component library of vetted terraform root modules that should be used by teams anytime they need to spin
  up infrastructure for VPCs, clusters, and databases.
- **Compliance and Governance:** Establish a component library to enforce infrastructure standards, security policies, and compliance requirements.
  By using pre-approved modules, organizations can maintain control over their infrastructure's configuration, reducing the risk of non-compliance.
- **Rapid Prototyping and Scalability:** Utilize a component library to quickly prototype and scale applications. Pre-built modules for common
  infrastructure patterns allow teams to focus on application development rather than infrastructure setup, accelerating time-to-market and ensuring scalability from the outset.

## Filesystem Layouts

There's no "one way" to organize your components, since it's configurable based on your needs in the [CLI Configuration](/cli/configuration). However, here are some popular ways we've seen components organized.

### Simple Filesystem Layout by Toolchain

By convention, we recommend placing components in a folder organized by the tool, within the `components/` folder.
In the following example, our toolchain consists of `docker`, `helmfile` and `terraform`, so a folder is created for each one, with the code
for that component inside of it.

If using `terraform` with multiple clouds, use the [multi-cloud filesystem layout](#multi-cloud-filesystem-layout).

```console
└── components/
    ├── docker/
    │   └── Dockerfile
    ├── helmfile/
    │   └── example-app
    │       └── helmfile.yaml
    └── terraform/
        └── example/                  # This is a terraform "root" module
            ├── main.tf
            ├── outputs.tf
            ├── modules/              # You can include submodules inside the component folder,
            │   ├── bar/              # and then reference them inside the of your root module.
            │   └── foo/              # e.g.
            │       ├── main.tf       # module "foo" {
            │       ├── outputs.tf    #   source = "./modules/foo"
            │       └── variables.tf  #   ...
            └── variables.tf          # }
```

:::tip
Organizing the components on the filesystem is configurable in the [Atmos CLI configuration](/cli/configuration/#configuration-file-atmosyaml).
:::

### Multi-Cloud Filesystem Layout

One good way to organize components is by the cloud provider for multi-cloud architectures.

For example, if an architecture consists of infrastructure in AWS, GCP, and Azure, it would look like this:

```console
└── components/
    └── terraform/
        ├── aws/                   # Components for Amazon Web Services (AWS)
        │   └── example/
        │       ├── main.tf
        │       ├── outputs.tf
        │       └── variables.tf
        ├── gcp/                   # Components for Google Cloud (GCP)
        │   └── example/
        │       ├── main.tf
        │       ├── outputs.tf
        │       └── variables.tf
        └── azure/                 # Components for Microsoft Azure (Azure)
            └── example/
                ├── main.tf
                ├── outputs.tf
                └── variables.tf
```

## Terraform Conventions

For terraform, we recommend placing the terraform "root" modules in the `components/terraform` folder. If the root modules depend on other child modules that are not hosted by a registry, we recommend placing them in a subfolder called `modules/`.

Make your Terraform components small, so they are easily reusable, but not so small that they only do to provide a single resource, which results in large, complicated configurations. A good rule of thumb is they should do one thing well. For example, provision a VPC along with all the subnets, NAT gateways, Internet gateways, NACLs, etc.

Use multiple component to break infrastructure apart into smaller pieces based on how their lifecycles are connected. For example, a single component seldom provides a VPC and a Kubernetes cluster. That's because we should be able to destroy the Kubernetes cluster without destroying the VPC and all the other resources provisioned inside of the VPC (e.g. databases). The VPC, Kubernetes cluster and Databases all have different lifecycles. Similarly, we should be able to deploy a database and destroy it without also destroying all associated backups. Therefore the backups of a database should be a separate component from the database itself.

---

## Atmos Components

import Intro from '@site/src/components/Intro'

<Intro>
When you design cloud architectures with Atmos, you start by breaking them apart into pieces called components. Then, you [implement Terraform "root modules"](/core-concepts/components/terraform) for each of those components, and [compose them with Stack configurations](/core-concepts/stacks).
</Intro>

The most common use-case for Atmos is using implementing components using [Terraform "root modules"](https://developer.hashicorp.com/terraform/language/modules#the-root-module). But since Atmos was designed to be tool-agnostic, [custom commands](/core-concepts/custom-commands) can be used to implement components for any type of tooling.

Components can be as small as you'd like (but we don't recommend too small), or as large as a [Terralith](/terms/terralith) (but we don't recommend that either). See our [best practices for components](/best-practices/components) to get a sense of what we recommend.

:::tip

Typical components of an architecture are things like VPCs, clusters, databases, buckets, load balancers, and applications. Implement components using [Terraform "root" modules](https://developer.hashicorp.com/terraform/language/modules#the-root-module).

:::

## Use-cases

Components offer a multitude of applications across various business scenarios. Cloud Posse publishes its AWS components for free, so you can see some [technical use-cases for Terraform components](https://docs.cloudposse.com/components/category/aws/).

- **Accelerate Development Cycles:** By reusing components, development teams can significantly shorten the time from concept to deployment, facilitating faster product iterations and quicker responses to market changes.

- **Security policies and compliance controls** DevOps and SecOps teams implement components to uniformly apply security policies and compliance controls across all cloud environments, ensuring regulatory adherence.

- **Enhance Collaboration Across Teams:** Components foster a shared understanding and approach to infrastructure, promoting collaboration between development, operations, and security teams, leading to more cohesive and secure product development.

## Flavors of Components

Atmos natively supports two kinds of components, but using [custom commands](/core-concepts/custom-commands), the [CLI](/cli) can be extended to support anything (e.g. `docker`, `packer`, `ansible`, etc.)

1. [Terraform](/core-concepts/components/terraform): These are stand-alone "root modules" that implement some piece of your infrastructure. For example, typical components might be an
   EKS cluster, RDS cluster, EFS filesystem, S3 bucket, DynamoDB table, etc. You can find
   the [full library of SweetOps Terraform components on GitHub](https://github.com/cloudposse/terraform-aws-components). By convention, we store
   components in the `components/terraform/` directory within the infrastructure repository.

2. [Helmfiles](/core-concepts/components/helmfile): These are stand-alone applications deployed using [`helmfile`](https://github.com/helmfile) to Kubernetes. For example, typical
   helmfiles might deploy the DataDog agent, `cert-manager` controller, `nginx-ingress` controller, etc. By convention, we store these types of components in the `components/helmfile/` directory within the infrastructure repository.

## Terraform Components

One important distinction about components that is worth noting about Terraform components is they should be opinionated Terraform "root" modules that typically call other child modules. Components are the building blocks of your infrastructure. This is where you define all the business logic for provisioning some common piece of infrastructure like ECR repos (with the [ecr](https://github.com/cloudposse/terraform-aws-components/tree/main/modules/ecr) component) or EKS clusters (with the [eks/cluster](https://github.com/cloudposse/terraform-aws-components/tree/main/modules/eks/cluster) component). Our convention is to stick Terraform components in the `components/terraform/` directory.

---

## Using Helmfiles

import Intro from '@site/src/components/Intro'

<Intro>
Atmos natively supports opinionated workflows for [Helmfile](https://github.com/helmfile/helmfile). Helmfile provides a declarative specification for deploying helm charts.
</Intro>

For a complete list of supported commands, please see the Atmos [helmfile](/cli/commands/helmfile/usage) documentation.

## Example: Provision Helmfile Component

To provision a helmfile component using the `atmos` CLI, run the following commands in the container shell:

```shell
atmos helmfile diff nginx-ingress --stack=ue2-dev
atmos helmfile apply nginx-ingress --stack=ue2-dev
```

where:

- `nginx-ingress` is the helmfile component to provision (from the `components/helmfile` folder)
- `--stack=ue2-dev` is the stack to provision the component into

Short versions of the command-line arguments can be used:

```shell
atmos helmfile diff nginx-ingress -s ue2-dev
atmos helmfile apply nginx-ingress -s ue2-dev
```

## Example: Helmfile Diff

To execute `diff` and `apply` in one step, use `helmfile deploy` command:

```shell
atmos helmfile deploy nginx-ingress -s ue2-dev
```

---

## Terraform/OpenTofu Backends

import Terminal from '@site/src/components/Terminal'
import Intro from '@site/src/components/Intro'

<Intro>
Backends define where [Terraform](https://opentofu.org/docs/language/state/) and
[OpenTofu](https://opentofu.org/docs/language/state/) store its state.
</Intro>

Atmos supports all the backends supported by Terraform:

- [local](https://developer.hashicorp.com/terraform/language/settings/backends/local)
- [s3](https://developer.hashicorp.com/terraform/language/settings/backends/s3)
- [azurerm](https://developer.hashicorp.com/terraform/language/settings/backends/azurerm)
- [gcs](https://developer.hashicorp.com/terraform/language/settings/backends/gcs)
- [remote](https://developer.hashicorp.com/terraform/language/settings/backends/remote)
- [consul](https://developer.hashicorp.com/terraform/language/settings/backends/consul)
- [cos](https://developer.hashicorp.com/terraform/language/settings/backends/cos)
- [http](https://developer.hashicorp.com/terraform/language/settings/backends/http)
- [kubernetes](https://developer.hashicorp.com/terraform/language/settings/backends/kubernetes)
- [oss](https://developer.hashicorp.com/terraform/language/settings/backends/oss)
- [pg](https://developer.hashicorp.com/terraform/language/settings/backends/pg)
- [cloud](https://developer.hashicorp.com/terraform/cli/cloud/settings)

Atmos supports all the backends supported by OpenTofu:

- [local](https://opentofu.org/docs/language/settings/backends/local)
- [s3](https://opentofu.org/docs/language/settings/backends/s3)
- [azurerm](https://opentofu.org/docs/language/settings/backends/azurerm)
- [gcs](https://opentofu.org/docs/language/settings/backends/gcs)
- [remote](https://opentofu.org/docs/language/settings/backends/remote)
- [consul](https://opentofu.org/docs/language/settings/backends/consul)
- [cos](https://opentofu.org/docs/language/settings/backends/cos)
- [http](https://opentofu.org/docs/language/settings/backends/http)
- [kubernetes](https://opentofu.org/docs/language/settings/backends/kubernetes)
- [oss](https://opentofu.org/docs/language/settings/backends/oss)
- [pg](https://opentofu.org/docs/language/settings/backends/pg)

## Local Backend

By default, Terraform will use a backend called [local](https://developer.hashicorp.com/terraform/language/settings/backends/local), which stores
Terraform state on the local filesystem, locks that state using system APIs, and performs operations locally.

Terraform's local backend is designed for development and testing purposes and is generally not recommended for production use. There are several reasons why using the local backend in a production environment may not be suitable:

- **Not Suitable for Collaboration**: Local backend doesn't support easy state sharing.
- **No Concurrency and Locking**: Local backend lacks locking, leading to race conditions when multiple users modify the state.
- **Lacks Durability and Backup**: Local backend has no durability or backup. Machine failures can lead to data loss.
- **Unsuitable for CI/CD**: Local backend isn't ideal for CI/CD pipelines.

To address these concerns, it's recommended to use one of the supported remote backends, such as Amazon S3, Azure Storage, Google Cloud Storage, HashiCorp Consul, or Terraform Cloud, for production environments. Remote backends provide better scalability, collaboration support, and durability, making them more suitable for managing infrastructure at scale in production environments.

## AWS S3 Backend

Terraform's [S3](https://developer.hashicorp.com/terraform/language/settings/backends/s3) backend is a popular remote
backend for storing Terraform state files in an Amazon Simple Storage Service (S3) bucket. Using S3 as a backend offers
many advantages, particularly in production environments.

To configure Terraform to use an S3 backend, you typically provide the S3 bucket name and an optional key prefix in your Terraform configuration.
Here's a simplified example:

<Terminal>
```hcl
terraform {
    backend "s3" {
        acl            = "bucket-owner-full-control"
        bucket         = "your-s3-bucket-name"
        key            = "path/to/terraform.tfstate"
        region         = "your-aws-region"
        encrypt        = true
        dynamodb_table = "terraform_locks"
    }
}
```
</Terminal>

In the example, `terraform_locks` is a DynamoDB table used for state locking. DynamoDB is recommended for locking when using the S3 backend to ensure
safe concurrent access.

Once the S3 bucket and DynamoDB table are provisioned, you can start using them to store Terraform state for the Terraform components.
There are two ways of doing this:

- Manually create `backend.tf` file in each component's folder with the following content:

<Terminal title="components/terraform/vpc/backend.tf">
```hcl
terraform {
    backend "s3" {
        acl                  = "bucket-owner-full-control"
        bucket               = "your-s3-bucket-name"
        dynamodb_table       = "your-dynamodb-table-name"
        encrypt              = true
        key                  = "terraform.tfstate"
        region               = "your-aws-region"
        role_arn             = "arn:aws:iam::xxxxxxxx:role/IAM Role with permissions to access the Terraform backend"
        workspace_key_prefix = "component name, e.g. `vpc` or `vpc-flow-logs-bucket`"
  }
}
```
</Terminal>

- Configure Terraform S3 backend with Atmos to automatically generate a backend file for each Atmos component. This is the recommended way
of configuring Terraform state backend since it offers many advantages and will save you from manually creating a backend configuration file for
each component

Configuring Terraform S3 backend with Atmos consists of three steps:

- Set `auto_generate_backend_file` to `true` in the `atmos.yaml` CLI config file in the `components.terraform` section:

<Terminal title="atmos.yaml">
```yaml
components:
  terraform:
    # Can also be set using 'ATMOS_COMPONENTS_TERRAFORM_AUTO_GENERATE_BACKEND_FILE' ENV var, or '--auto-generate-backend-file' command-line argument
    auto_generate_backend_file: true
```
</Terminal>

- Configure the S3 backend in one of the `_defaults.yaml` manifests. You can configure it for the entire Organization, or per OU/tenant, or per
region, or per account.

:::note
The `_defaults.yaml` stack manifests contain the default settings for Organizations, Organizational Units, and accounts.
:::

:::info
The `_defaults.yaml` stack manifests are not imported into other Atmos manifests automatically.
You need to explicitly import them using [imports](/core-concepts/stacks/imports).
:::

To configure the S3 backend for the entire Organization, add the following config in `stacks/orgs/acme/_defaults.yaml`:

<Terminal title="stacks/orgs/acme/_defaults.yaml">
```yaml
terraform:
  backend_type: s3
  backend:
    s3:
      acl: "bucket-owner-full-control"
      encrypt: true
      bucket: "your-s3-bucket-name"
      dynamodb_table: "your-dynamodb-table-name"
      key: "terraform.tfstate"
      region: "your-aws-region"
      role_arn: "arn:aws:iam::xxxxxxxx:role/IAM Role with permissions to access the Terraform backend"
```
</Terminal>

- (This step is optional) For each component, you can add `workspace_key_prefix` similar to the following:

<Terminal title="stacks/catalog/vpc.yaml">
```yaml
components:
  terraform:
    # `vpc` is the Atmos component name
    vpc:
      # Optional backend configuration for the component
      backend:
        s3:
          workspace_key_prefix: vpc
      metadata:
        # Point to the Terraform component
        component: vpc
      settings: {}
      vars: {}
      env: {}
```
</Terminal>

Note that this is optional. If you don’t add `backend.s3.workspace_key_prefix` to the component manifest, the Atmos component name will be used
automatically (which is this example is `vpc`). `/` (slash) in the Atmos component name will be replaced with `-` (dash).

We usually don’t specify `workspace_key_prefix` for each component and let Atmos use the component name as `workspace_key_prefix`.

Once all the above is configured, when you run the commands `atmos terraform plan vpc -s <stack>`
or `atmos terraform apply vpc -s <stack>`, before executing the Terraform commands, Atmos will [deep-merge](#backend-inheritance)
the backend configurations from the `_defaults.yaml` manifest and from the component itself, and will generate a backend
config JSON file `backend.tf.json` in the component's folder, similar to the following example:

<Terminal title="backend.tf.json">
```json
{
  "terraform": {
    "backend": {
      "s3": {
        "acl": "bucket-owner-full-control",
        "bucket": "your-s3-bucket-name",
        "dynamodb_table": "your-dynamodb-table-name",
        "encrypt": true,
        "key": "terraform.tfstate",
        "region": "your-aws-region",
        "role_arn": "arn:aws:iam::xxxxxxxx:role/IAM Role with permissions to access the Terraform backend",
        "workspace_key_prefix": "vpc"
      }
    }
  }
}
```
</Terminal>

You can also generate the backend configuration file for a component in a stack by executing the
command [atmos terraform generate backend](/cli/commands/terraform/generate-backend). Or generate the backend configuration files for all components
by executing the command [atmos terraform generate backends](/cli/commands/terraform/generate-backends).

## Azure Blob Storage Backend

[`azurerm`](https://developer.hashicorp.com/terraform/language/settings/backends/azurerm) backend stores the state as a
Blob with the given Key within the Blob Container within the Blob Storage Account. This backend supports state locking
and consistency checking with Azure Blob Storage native capabilities.

To configure the [Azure Blob Storage backend](https://developer.hashicorp.com/terraform/language/settings/backends/azurerm)
in Atmos, add the following config to an Atmos manifest in `_defaults.yaml`:

<Terminal title="_defaults.yaml">
```yaml
terraform:
  backend_type: azurerm
  backend:
    azurerm:
      resource_group_name: "StorageAccount-ResourceGroup"
      storage_account_name: "abcd1234"
      container_name: "tfstate"
      # Other parameters
```
</Terminal>

For each component, you can optionally add the `key` parameter similar to the following:

<Terminal>
```yaml
components:
  terraform:
    my-component:
      # Optional backend configuration for the component
      backend:
        azurerm:
          key: "my-component"
```
</Terminal>

If the `key` is not specified for a component, Atmos will use the component name (`my-component` in the example above)
to auto-generate the `key` parameter in the format `<component-name>.terraform.tfstate` replacing `<component-name>`
with the Atmos component name. In `<component-name>`, all occurrences of `/` (slash) will be replaced with `-` (dash).

If `auto_generate_backend_file` is set to `true` in the `atmos.yaml` CLI config file in the `components.terraform` section,
Atmos will [deep-merge](#backend-inheritance) the backend configurations from the `_defaults.yaml` manifests and
from the component itself, and will generate a backend config JSON file `backend.tf.json` in the component's folder,
similar to the following example:

<Terminal title="backend.tf.json">
```json
{
  "terraform": {
    "backend": {
      "azurerm": {
        "resource_group_name": "StorageAccount-ResourceGroup",
        "storage_account_name": "abcd1234",
        "container_name": "tfstate",
        "key": "my-component.terraform.tfstate"
      }
    }
  }
}
```
</Terminal>

## Google Cloud Storage Backend

[`gcs`](https://developer.hashicorp.com/terraform/language/settings/backends/gcs) backend stores the state as an object
in a configurable `prefix` in a pre-existing bucket on Google Cloud Storage (GCS).
The bucket must exist prior to configuring the backend. The backend supports state locking.

To configure the [Google Cloud Storage backend](https://developer.hashicorp.com/terraform/language/settings/backends/gcs)
in Atmos, add the following config to an Atmos manifest in `_defaults.yaml`:

<Terminal title="_defaults.yaml">
```yaml
terraform:
  backend_type: gcs
  backend:
    gcs:
      bucket: "tf-state"
      # Other parameters
```
</Terminal>

For each component, you can optionally add the `prefix` parameter similar to the following:

<Terminal>
```yaml
components:
  terraform:
    my-component:
      # Optional backend configuration for the component
      backend:
        gcp:
          prefix: "my-component"
```
</Terminal>

If the `prefix` is not specified for a component, Atmos will use the component name (`my-component` in the example above)
to auto-generate the `prefix`. In the component name, all occurrences of `/` (slash) will be replaced with `-` (dash).

If `auto_generate_backend_file` is set to `true` in the `atmos.yaml` CLI config file in the `components.terraform` section,
Atmos will [deep-merge](#backend-inheritance) the backend configurations from the `_defaults.yaml` manifests and
from the component itself, and will generate a backend config JSON file `backend.tf.json` in the component's folder,
similar to the following example:

<Terminal title="backend.tf.json">
```json
{
  "terraform": {
    "backend": {
      "gcp": {
        "bucket": "tf-state",
        "prefix": "my-component"
      }
    }
  }
}
```
</Terminal>

## Terraform Cloud Backend

[Terraform Cloud](https://developer.hashicorp.com/terraform/cli/cloud/settings) backend uses a `cloud` block to specify
which organization and workspace(s) to use.

To configure the [Terraform Cloud backend](https://developer.hashicorp.com/terraform/cli/cloud/settings)
in Atmos, add the following config to an Atmos manifest in `_defaults.yaml`:

<Terminal title="_defaults.yaml">
```yaml
terraform:
  backend_type: cloud
  backend:
    cloud:
      organization: "my-org"
      hostname: "app.terraform.io"
      workspaces:
        # Parameters for workspaces
```
</Terminal>

For each component, you can optionally specify the `workspaces.name` parameter similar to the following:

<Terminal>
```yaml
components:
  terraform:
    my-component:
      # Optional backend configuration for the component
      backend:
        cloud:
          workspaces:
            name: "my-component-workspace"
```
</Terminal>

If `auto_generate_backend_file` is set to `true` in the `atmos.yaml` CLI config file in the `components.terraform` section,
Atmos will [deep-merge](#backend-inheritance) the backend configurations from the `_defaults.yaml` manifests and
from the component itself, and will generate a backend config JSON file `backend.tf.json` in the component's folder,
similar to the following example:

<Terminal title="backend.tf.json">
```json
{
  "terraform": {
    "cloud": {
      "hostname": "app.terraform.io",
      "organization": "my-org",
      "workspaces": {
        "name": "my-component-workspace"
      }
    }
  }
}
```
</Terminal>

Instead of specifying the `workspaces.name` parameter for each component in the component manifests, you can use
the `{terraform_workspace}` token in the `cloud` backend config in the `_defaults.yaml` manifest.
The token `{terraform_workspace}` will be automatically replaced by Atmos with the Terraform workspace for each component.
This will make the entire configuration DRY.

<Terminal title="_defaults.yaml">
```yaml
terraform:
  backend_type: cloud
  backend:
    cloud:
      organization: "my-org"
      hostname: "app.terraform.io"
      workspaces:
        # The token `{terraform_workspace}` will be automatically replaced with the
        # Terraform workspace for each Atmos component
        name: "{terraform_workspace}"
```
</Terminal>

:::tip
Refer to [Terraform Workspaces in Atmos](/core-concepts/components/terraform/workspaces) for more information on how
Atmos calculates Terraform workspaces for components, and how workspaces can be overridden for each component.
:::

## Backend Inheritance

Suppose that for security and audit reasons, you want to use different Terraform backends for `dev`, `staging` and `prod`.
Each account needs to have a separate S3 bucket, DynamoDB table, and IAM role with different permissions
(for example, the `development` Team should be able to access the Terraform backend only in the `dev` account, but not in `staging` and `prod`).

Atmos supports this use-case by using deep-merging of stack manifests, [Imports](/core-concepts/stacks/imports)
and [Inheritance](/core-concepts/stacks/inheritance), which makes the backend configuration reusable and DRY.

We'll split the backend config between the Organization and the accounts.

Add the following config to the Organization stack manifest in `stacks/orgs/acme/_defaults.yaml`:

<Terminal title="stacks/orgs/acme/_defaults.yaml">
```yaml
terraform:
  backend_type: s3
  backend:
    s3:
      acl: "bucket-owner-full-control"
      encrypt: true
      key: "terraform.tfstate"
      region: "your-aws-region"
```
</Terminal>

Add the following config to the `dev` stack manifest in `stacks/orgs/acme/plat/dev/_defaults.yaml`:

<Terminal title="stacks/orgs/acme/plat/dev/_defaults.yaml">
```yaml
terraform:
  backend_type: s3
  backend:
    s3:
      bucket: "your-dev-s3-bucket-name"
      dynamodb_table: "your-dev-dynamodb-table-name"
      role_arn: "IAM Role with permissions to access the 'dev' Terraform backend"
```
</Terminal>

Add the following config to the `staging` stack manifest in `stacks/orgs/acme/plat/staging/_defaults.yaml`:

<Terminal title="stacks/orgs/acme/plat/staging/_defaults.yaml">
```yaml
terraform:
  backend_type: s3
  backend:
    s3:
      bucket: "your-staging-s3-bucket-name"
      dynamodb_table: "your-staging-dynamodb-table-name"
      role_arn: "IAM Role with permissions to access the 'staging' Terraform backend"
```
</Terminal>

Add the following config to the `prod` stack manifest in `stacks/orgs/acme/plat/prod/_defaults.yaml`:

<Terminal title="stacks/orgs/acme/plat/prod/_defaults.yaml">
```yaml
terraform:
  backend_type: s3
  backend:
    s3:
      bucket: "your-prod-s3-bucket-name"
      dynamodb_table: "your-prod-dynamodb-table-name"
      role_arn: "IAM Role with permissions to access the 'prod' Terraform backend"
```
</Terminal>

When you provision the `vpc` component into the `dev` account (by executing the command `atmos terraform apply vpc -s plat-ue2-dev`), Atmos will
deep-merge the backend configuration from the Organization-level manifest with the configuration from the `dev` manifest, and will automatically
add `workspace_key_prefix` for the component, generating the following final deep-merged backend config for the `vpc` component in the `dev` account:

<Terminal>
```json
{
  "terraform": {
    "backend": {
      "s3": {
        "acl": "bucket-owner-full-control",
        "bucket": "your-dev-s3-bucket-name",
        "dynamodb_table": "your-dev-dynamodb-table-name",
        "encrypt": true,
        "key": "terraform.tfstate",
        "region": "your-aws-region",
        "role_arn": "<IAM Role with permissions to access the `dev` Terraform backend>",
        "workspace_key_prefix": "vpc"
      }
    }
  }
}
```
</Terminal>

In the same way, you can create different Terraform backends per Organizational Unit, per region, per account (or a group of accounts, e.g. `prod`
and `non-prod`), or even per component or a set of components (e.g. root-level components like `account` and IAM roles can have a separate backend),
and then configure parts of the backend config in the corresponding Atmos stack manifests. Atmos will deep-merge all the parts from the
different scopes and generate the final backend config for the components in the stacks.

## Terraform/OpenTofu Backend with Multiple Component Instances

We mentioned before that you can configure the Terraform backend for the components manually (by creating a file `backend.tf` in each Terraform
component's folder), or you can set up Atmos to generate the backend configuration for each component in the stacks automatically. While
auto-generating the backend config file is helpful and saves you from creating the backend files for each component, it becomes a requirement
when you provision multiple instances of a Terraform component into the same environment (same account and region).

You can provision more than one instance of the same Terraform component (with the same or different settings) into the same environment by defining
many Atmos components that provide configuration for the Terraform component.

:::tip
For more information on configuring and provision multiple instances of a Terraform component,
refer to [Multiple Component Instances Atmos Design Patterns](/design-patterns/multiple-component-instances)
:::

For example, the following config shows how to define two Atmos
components, `vpc/1` and `vpc/2`, which both point to the same Terraform component `vpc`:

<Terminal>
```yaml
import:
  # Import the defaults for all VPC components
  - catalog/vpc/defaults

components:
  terraform:
    # Atmos component `vpc/1`
    vpc/1:
      metadata:
        # Point to the Terraform component in `components/terraform/vpc`
        component: vpc
        # Inherit the defaults for all VPC components
        inherits:
          - vpc/defaults
      # Define variables specific to this `vpc/1` component
      vars:
        name: vpc-1
        ipv4_primary_cidr_block: 10.9.0.0/18
      # Optional backend configuration for the component
      # If not specified, the Atmos component name `vpc/1` will be used (`/` will be replaced with `-`)
      backend:
        s3:
          workspace_key_prefix: vpc-1

    # Atmos component `vpc/2`
    vpc/2:
      metadata:
        # Point to the Terraform component in `components/terraform/vpc`
        component: vpc
        # Inherit the defaults for all VPC components
        inherits:
          - vpc/defaults
      # Define variables specific to this `vpc/2` component
      vars:
        name: vpc-2
        ipv4_primary_cidr_block: 10.10.0.0/18
      # Optional backend configuration for the component
      # If not specified, the Atmos component name `vpc/2` will be used (`/` will be replaced with `-`)
      backend:
        s3:
          workspace_key_prefix: vpc-2
```
</Terminal>

If we manually create a `backend.tf` file for the `vpc` Terraform component in the `components/terraform/vpc` folder
using `workspace_key_prefix: "vpc"`, then both `vpc/1` and `vpc/2` Atmos components will use the same `workspace_key_prefix`, and they will
not function correctly.

On the other hand, if we configure Atmos to auto-generate the backend config file, then each component will have a different `workspace_key_prefix`
auto-generated by Atmos by using the Atmos component name (or you can override this behavior by specifying `workspace_key_prefix` for each component
in the component manifest in the `backend.s3.workspace_key_prefix` section).

For example, when the command `atmos terraform apply vpc/1 -s plat-ue2-dev` is executed, the following `backend.tf.json` file is generated in the
`components/terraform/vpc` folder:

<Terminal>
```json
{
  "terraform": {
    "backend": {
      "s3": {
        "acl": "bucket-owner-full-control",
        "bucket": "your-dev-s3-bucket-name",
        "dynamodb_table": "your-dev-dynamodb-table-name",
        "encrypt": true,
        "key": "terraform.tfstate",
        "region": "your-aws-region",
        "role_arn": "<IAM Role with permissions to access the `dev` Terraform backend>",
        "workspace_key_prefix": "vpc-1"
      }
    }
  }
}
```
</Terminal>

Similarly, when the command `atmos terraform apply vpc/2 -s plat-ue2-dev` is executed, the following `backend.tf.json` file is generated in the
`components/terraform/vpc` folder:

<Terminal>
```json
{
  "terraform": {
    "backend": {
      "s3": {
        "acl": "bucket-owner-full-control",
        "bucket": "your-dev-s3-bucket-name",
        "dynamodb_table": "your-dev-dynamodb-table-name",
        "encrypt": true,
        "key": "terraform.tfstate",
        "region": "your-aws-region",
        "role_arn": "<IAM Role with permissions to access the `dev` Terraform backend>",
        "workspace_key_prefix": "vpc-2"
      }
    }
  }
}
```
</Terminal>

The generated files will have different `workspace_key_prefix` attribute auto-generated by Atmos.

For this reason, configuring Atmos to auto-generate the backend configuration for the components in the stacks is recommended
for all supported backend types.

## References

- [Terraform Backend Configuration](https://developer.hashicorp.com/terraform/language/settings/backends/configuration)
- [OpenTofu Backend Configuration](https://opentofu.org/docs/language/settings/backends/configuration)
- [Terraform Cloud Settings](https://developer.hashicorp.com/terraform/cli/cloud/settings)
- [Multiple Component Instances Atmos Design Patterns](/design-patterns/multiple-component-instances)

---

## Brownfield Considerations

import Intro from '@site/src/components/Intro'

<Intro>
There are some considerations you should be aware of when adopting Atmos in a brownfield environment. Atmos works best when you adopt the [Atmos mindset](/quick-start/mindset).
</Intro>

The term "brownfield" comes from urban planning and refers to the redevelopment of land that was previously used and may need cleaning or modification. As it relates to infrastructure, [Brownfield development](https://en.wikipedia.org/wiki/Brownfield_(software_development)) describes the development and deployment of new software systems in the presence of existing (legacy) software applications/systems. Anytime this happens, new software architectures must take into account and coexist with the existing software.

Atmos is not just a tool; it is a framework that provides a set of opinionated conventions, methodologies, design patterns, and best practices to ensure teams succeed with Terraform from the start. It can be hard to shoehorn existing systems that are not designed according to the [Atmos mindset](/quick-start/mindset).

- **Decomposition**: Not only do you have challenges around how to decompose your architecture, but also the difficulty of making changes to live systems.
- **Technical Debt:** You may have significant technical debt that needs to be addressed
- **Knowledge Gaps**: There may be gaps in knowledge within the team regarding Atmos conventions and methodologies.

By understanding these challenges, teams can better prepare for a smooth transition to using Atmos effectively.

## Brownfield Development in Atmos

Atmos is easier for new organizations or "greenfield" environments because you need to architect Terraform according to
our [best practices](/best-practices/components) to get all the benefits of Atmos. For example, when using our [Terraform components](https://github.com/cloudposse/terraform-aws-components), we frequently use [Terraform Remote State](/core-concepts/share-data/remote-state) to retrieve the outputs from other components.

This works well when you use our components but less so when you operate in a "brownfield" environment, for example,
with an existing VPC, S3 bucket, or IAM role.

When you approach brownfield development with Atmos, begin by designing what your architecture could look like if you break it down into various pieces. Then devise a plan to decompose those pieces into components you implement as Terraform "root modules".

The process of configuring Atmos components and stacks for the existing, already provisioned resources, will depend on how easy or hard this decomposition will be. Working on and updating existing infrastructure rather than creating a new one from scratch, known as "greenfield" development, will always be more difficult.

The process needs to respect the existing systems' constraints while progressively introducing improvements and modern practices. This will ultimately lead to more robust, flexible, and efficient systems.

## Remote State in Brownfield Development

So what happens when infrastructure wasn't provisioned by Atmos or predates your infrastructure? Then there's no way to retrieve that state in Terraform.

For this reason, we support something we refer to as the `static` remote state backend. Using the static remote state backend, you can
populate a virtual state backend with the outputs as though it had been provisioned with Terraform. You can use this
technique anytime you want to use the remote state functionality in Atmos, but when the remote state was provisioned
elsewhere.

### Hacking Remote State with `static` Backends

Atmos supports brownfield configuration by using the remote state of type `static`.

Suppose that we need to provision
the [`vpc`](https://github.com/cloudposse/atmos/tree/main/examples/quick-start-advanced/components/terraform/vpc)
Terraform component and, instead of provisioning an S3 bucket for VPC Flow Logs, we want to use an existing bucket.

The `vpc` Terraform component needs the outputs from the `vpc-flow-logs-bucket` Terraform component to
configure [VPC Flow Logs](https://docs.aws.amazon.com/vpc/latest/userguide/flow-logs.html).

Let's redesign the example with the `vpc` and `vpc-flow-logs-bucket` components described in
[Terraform Component Remote State](/core-concepts/share-data/remote-state) and configure the `static` remote state for
the `vpc-flow-logs-bucket` component to use an existing S3 bucket.

## Examples

### Configure the `vpc-flow-logs-bucket` Component

In the `stacks/catalog/vpc-flow-logs-bucket.yaml` file, add the following configuration for
the `vpc-flow-logs-bucket/defaults` Atmos component:

```yaml title="stacks/catalog/vpc-flow-logs-bucket.yaml"
components:
  terraform:
    vpc-flow-logs-bucket/defaults:
      metadata:
        type: abstract
      # Use `static` remote state to configure the attributes for an existing
      # S3 bucket for VPC Flow Logs
      remote_state_backend_type: static
      remote_state_backend:
        static:
          # ARN of the existing S3 bucket
          # `vpc_flow_logs_bucket_arn` is used as an input for the `vpc` component
          vpc_flow_logs_bucket_arn: "arn:aws:s3:::my-vpc-flow-logs-bucket"
```

In the `stacks/ue2-dev.yaml` stack config file, add the following config for the `vpc-flow-logs-bucket-1` Atmos
component in the `ue2-dev` Atmos stack:

```yaml title="stacks/ue2-dev.yaml"
# Import the base Atmos component configuration from the `catalog`.
# `import` supports POSIX-style Globs for file names/paths (double-star `**` is supported).
# File extensions are optional (if not specified, `.yaml` is used by default).
import:
  - catalog/vpc-flow-logs-bucket

components:
  terraform:
    vpc-flow-logs-bucket-1:
      metadata:
        # Point to the Terraform component in `components/terraform` folder
        component: infra/vpc-flow-logs-bucket
        inherits:
          # Inherit all settings and variables from the
          # `vpc-flow-logs-bucket/defaults` base Atmos component
          - vpc-flow-logs-bucket/defaults
```

### Configure and Provision the `vpc` Component

In the `components/terraform/infra/vpc/remote-state.tf` file, configure the
[remote-state](https://github.com/cloudposse/terraform-yaml-stack-config/tree/main/modules/remote-state) Terraform
module to obtain the remote state for the `vpc-flow-logs-bucket-1` Atmos component:

```hcl title="components/terraform/infra/vpc/remote-state.tf"
module "vpc_flow_logs_bucket" {
  count = local.vpc_flow_logs_enabled ? 1 : 0

  source  = "cloudposse/stack-config/yaml//modules/remote-state"
  version = "1.5.0"

  # Specify the Atmos component name (defined in YAML stack config files)
  # for which to get the remote state outputs
  component = var.vpc_flow_logs_bucket_component_name

  # `context` input is a way to provide the information about the stack (using the context
  # variables `namespace`, `tenant`, `environment`, `stage` defined in the stack config)
  context = module.this.context
}
```

In the `components/terraform/infra/vpc/vpc-flow-logs.tf` file, configure the `aws_flow_log` resource for the `vpc`
Terraform component to use the remote state output `vpc_flow_logs_bucket_arn` from the `vpc-flow-logs-bucket-1` Atmos
component:

```hcl title="components/terraform/infra/vpc/vpc-flow-logs.tf"
locals {
  enabled               = module.this.enabled
  vpc_flow_logs_enabled = local.enabled && var.vpc_flow_logs_enabled
}

resource "aws_flow_log" "default" {
  count = local.vpc_flow_logs_enabled ? 1 : 0

  # Use the remote state output `vpc_flow_logs_bucket_arn` of the `vpc_flow_logs_bucket` component
  log_destination = module.vpc_flow_logs_bucket[0].outputs.vpc_flow_logs_bucket_arn

  log_destination_type = var.vpc_flow_logs_log_destination_type
  traffic_type         = var.vpc_flow_logs_traffic_type
  vpc_id               = module.vpc.vpc_id

  tags = module.this.tags
}
```

In the `stacks/catalog/vpc.yaml` file, add the following default config for the `vpc/defaults` Atmos component:

```yaml title="stacks/catalog/vpc.yaml"
components:
  terraform:
    vpc/defaults:
      metadata:
        # `metadata.type: abstract` makes the component `abstract`,
        # explicitly prohibiting the component from being deployed.
        # `atmos terraform apply` will fail with an error.
        # If `metadata.type` attribute is not specified, it defaults to `real`.
        # `real` components can be provisioned by `atmos` and CI/CD like Spacelift and Atlantis.
        type: abstract
      # Default variables, which will be inherited and can be overridden in the derived components
      vars:
        public_subnets_enabled: false
        nat_gateway_enabled: false
        nat_instance_enabled: false
        max_subnet_count: 3
        vpc_flow_logs_enabled: false
        vpc_flow_logs_log_destination_type: s3
        vpc_flow_logs_traffic_type: "ALL"
```

In the `stacks/ue2-dev.yaml` stack config file, add the following config for the `vpc/1` Atmos component in
the `ue2-dev` stack:

```yaml title="stacks/ue2-dev.yaml"
# Import the base component configuration from the `catalog`.
# `import` supports POSIX-style Globs for file names/paths (double-star `**` is supported).
# File extensions are optional (if not specified, `.yaml` is used by default).
import:
  - catalog/vpc

components:
  terraform:
    vpc/1:
      metadata:
        # Point to the Terraform component in `components/terraform` folder
        component: infra/vpc
        inherits:
          # Inherit all settings and variables from the `vpc/defaults` base Atmos component
          - vpc/defaults
      vars:
        # Define variables that are specific for this component
        # and are not set in the base component
        name: vpc-1
        ipv4_primary_cidr_block: 10.8.0.0/18
        # Override the default variables from the base component
        vpc_flow_logs_enabled: true
        vpc_flow_logs_traffic_type: "REJECT"

        # Specify the name of the Atmos component that provides configuration
        # for the `infra/vpc-flow-logs-bucket` Terraform component
        vpc_flow_logs_bucket_component_name: vpc-flow-logs-bucket-1
```

Having the stacks configured as shown above, we can now provision the `vpc/1` Atmos component in the `ue2-dev` stack
by executing the following Atmos commands:

```shell
atmos terraform plan vpc/1 -s ue2-dev
atmos terraform apply vpc/1 -s ue2-dev
```

When the commands are executed, the `vpc_flow_logs_bucket` remote-state module detects that the `vpc-flow-logs-bucket-1`
component has the `static` remote state configured, and instead of reading its remote state from the S3 state
bucket, it just returns the static values from the `remote_state_backend.static` section.
The `vpc_flow_logs_bucket_arn` is then used as an input for the `vpc` component.

---

## Terraform Providers

import File from '@site/src/components/File'
import Terminal from '@site/src/components/Terminal'
import Intro from '@site/src/components/Intro'

<Intro>
Terraform utilizes plugins known as [providers](https://developer.hashicorp.com/terraform/language/providers) for
communication with cloud providers, SaaS providers, and various APIs.
</Intro>

In order for Terraform to install these providers, the corresponding Terraform configurations need to
explicitly state what providers are required. Furthermore, certain providers require additional configuration, such as
specifying endpoint URLs or cloud regions, before they can be used.

## Provider Configuration in Terraform

When working with Terraform, you specify provider configurations in your Terraform code. This involves
declaring which providers your infrastructure requires and providing any necessary configuration parameters.
These parameters may include endpoint URLs, cloud regions, access credentials, or any other provider-specific
configuration parameters.

To declare a provider in Terraform, use a `provider` block within your Terraform configuration files,
usually in a `providers.tf` file in the component (a.k.a. root module) directory.
The `provider` block specifies the provider type and all the necessary configuration parameters.

Here's an AWS provider configuration example for a `vpc` component. The provider config is defined in
the `components/terraform/vpc/providers.tf` file:

<File title="components/terraform/vpc/providers.tf">
    ```hcl
      provider "aws" {
        region = "us-east-2"
        assume_role {
          role_arn: "IAM Role ARN"
        }
      }
   ```
</File>

In this example, the `aws` provider block includes the region and IAM role required for Terraform to communicate
with the AWS services.

By correctly defining provider configurations in your Terraform code, you ensure that Terraform can seamlessly install,
configure, and use the necessary plugins to manage your infrastructure across various cloud and services.

## Provider Configuration and Overrides in Atmos Manifests

Atmos allows you to define and override provider configurations using the `providers` section in Atmos stack manifests.
The section can be defined globally for the entire organization, OU/tenant, account, region, or per component.

For example, the `providers` section at the global scope can look like this:

<File title="stacks/orgs/acme/_defaults.yaml">
    ```yaml
    terraform:
      providers:
        aws:
          region: "us-east-2"
          assume_role:
            role_arn: "IAM Role ARN"
    ```
</File>

Similarly, it can be defined (or overridden) at the OU/tenant, account and region scopes in the corresponding
`_defaults.yaml` stack manifests.

If you want to override a provider configuration for a specific component, use the `component.terraform.<component>.providers`
section. For example, the following config can be used to override the `assume_role` parameter just for the `vpc` component:

<File title="stacks/catalog/vpc/defaults.yaml">
    ```yaml
    components:
      terraform:
        vpc:
          providers:
            aws:
              assume_role:
                role_arn: "IAM Role ARN for VPC"
    ```
</File>

You can include the `providers` sections in any Atmos stack manifest at any level of inheritance. Atmos will process,
deep-merge and override all the `providers` configurations for a component in the following order:

- Global scopes (`terraform.providers` sections for the Org, OUs, accounts and regions)
- Base component scope (`component.terraform.<base_component>.providers` section)
- Current component scope (`component.terraform.<component>.providers` section)

:::tip
Refer to [Atmos Component Inheritance](/core-concepts/stacks/inheritance) for more information on all types of component inheritance
supported by Atmos
:::

When you define the `providers` sections, Atmos processes the inheritance chain for a component and generates a
file `providers_override.tf.json` in the component's folder with the final values for all the defined providers.

For example:

<Terminal title="atmos terraform plan vpc -s plat-ue2-prod --logs-level=Trace">
    ```console
    > atmos terraform plan vpc -s plat-ue2-prod --logs-level=Trace

    Variables for the component 'vpc' in the stack 'plat-ue2-prod':
      environment: ue2
      max_subnet_count: 3
      name: common
      namespace: cp
      region: us-east-2
      stage: prod
      tenant: plat

    Writing the variables to file:
    components/terraform/vpc/plat-ue2-prod.terraform.tfvars.json

    Writing the provider overrides to file:
    components/terraform/vpc/providers_override.tf.json
    ```
</Terminal>

The generated `providers_override.tf.json` file would look like this:

<File title="providers_override.tf.json">
    ```json
    {
        "provider": {
          "aws": {
            "assume_role": {
              "role_arn": "IAM Role ARN for VPC"
            }
          }
        }
    }
    ```
</File>

Terraform then uses the values in the generated `providers_override.tf.json` to
[override](https://developer.hashicorp.com/terraform/language/files/override) the parameters for all the providers in the file.

## `alias`: Multiple Provider Configuration in Atmos Manifests

Atmos allows you to define multiple configurations for the same provider using a list of provider blocks and the
`alias` meta-argument.

The generated `providers_override.tf.json` file will have a list of provider configurations, and Terraform/OpenTofu
will use and override the providers as long as the aliased providers are defined in the Terraform component.

For example:

<File title="stacks/catalog/vpc/defaults.yaml">
    ```yaml
    components:
      terraform:
        vpc:
          providers:
            aws:
              - region: us-west-2
                assume_role:
                  role_arn: "role-1"
              - region: us-west-2
                alias: "account-2"
                assume_role:
                  role_arn: "role-2"
    ```
</File>

:::warning

The above example uses a list of configuration blocks for the `aws` provider.

Since it's a list, by default it doesn't work with deep-merging of stacks in the
[inheritance](/core-concepts/stacks/inheritance) chain since list are not deep-merged, they are replaced.

If you want to use the above configuration in the inheritance chain and allow appending or merging of lists, consider
configuring the `settings.list_merge_strategy` in the `atmos.yaml` CLI config file.

For more details, refer to [Atmos CLI Settings](/cli/configuration/#settings).

:::

## References

- [Terraform Providers](https://developer.hashicorp.com/terraform/language/providers)
- [Terraform Override Files](https://developer.hashicorp.com/terraform/language/files/override)
- [alias: Multiple Provider Configurations](https://developer.hashicorp.com/terraform/language/providers/configuration#alias-multiple-provider-configurations)

---

## State Backend Configuration

import Intro from '@site/src/components/Intro'

<Intro>
Atmos supports configuring [Terraform/OpenTofu Backends](/core-concepts/components/terraform/backends)
to define where [Terraform](https://developer.hashicorp.com/terraform/language/state) and [OpenTofu](https://opentofu.org/docs/language/state/) store its state,
and [Remote State](/core-concepts/share-data/remote-state) to get the outputs of a [Terraform/OpenTofu component](/core-concepts/components),
provisioned in the same or a different [Atmos stack](/core-concepts/stacks), and use the outputs as inputs to another Atmos component.
</Intro>

Bear in mind that Atmos is simply managing the configuration of the Backend;
provisioning the backend resources themselves is the responsibility of a Terraform/OpenTofu component.

Atmos also supports Remote State Backends (in the `remote_state_backend` section), which can be used to configure the
following:

- Override [Terraform Backend](/core-concepts/components/terraform/backends) configuration to access the
  remote state of a component (e.g. override the IAM role to assume, which in this case can be a read-only role)

- Configure a remote state of type `static` which can be used to provide configurations for
  [Brownfield development](https://en.wikipedia.org/wiki/Brownfield_(software_development))

## Override Terraform Backend Configuration to Access Remote State

Atmos supports the `remote_state_backend` section which can be used to provide configuration to access the remote state
of components.

To access the remote state of components, you can override
any [Terraform Backend](/core-concepts/components/terraform/backends)
configuration in the `backend` section using the `remote_state_backend` section. The `remote_state_backend` section
is a first-class section, and it can be defined globally at any scope (organization, tenant, account, region), or per
component, and then deep-merged using [Atmos Component Inheritance](/core-concepts/stacks/inheritance).

For example, let's suppose we have the following S3 backend configuration for the entire organization
(refer to [AWS S3 Backend](/core-concepts/components/terraform/backends#aws-s3-backend) for more details):

```yaml title="stacks/orgs/acme/_defaults.yaml"
terraform:
  backend_type: s3
  backend:
    s3:
      acl: "bucket-owner-full-control"
      encrypt: true
      bucket: "your-s3-bucket-name"
      dynamodb_table: "your-dynamodb-table-name"
      key: "terraform.tfstate"
      region: "your-aws-region"
      role_arn: "arn:aws:iam::xxxxxxxx:role/terraform-backend-read-write"
```

Let's say we also have a read-only IAM role, and we want to use it to access the remote state instead of the read-write
role, because accessing remote state is a read-only operation, and we don't want to give the role more permissions than
it requires - this is the [principle of least privilege](https://en.wikipedia.org/wiki/Principle_of_least_privilege).

We can add the `remote_state_backend` and `remote_state_backend_type` to override the required attributes from the
`backend` section:

```yaml title="stacks/orgs/acme/_defaults.yaml"
terraform:
  backend_type: s3  # s3, remote, vault, azurerm, gcs, cloud
  backend:
    s3:
      acl: "bucket-owner-full-control"
      encrypt: true
      bucket: "your-s3-bucket-name"
      dynamodb_table: "your-dynamodb-table-name"
      key: "terraform.tfstate"
      region: "your-aws-region"
      role_arn: "arn:aws:iam::xxxxxxxx:role/terraform-backend-read-write"

  remote_state_backend_type: s3 # s3, remote, vault, azurerm, gcs, cloud, static
  remote_state_backend:
    s3:
      role_arn: "arn:aws:iam::xxxxxxxx:role/terraform-backend-read-only"
      # Override the other attributes from the `backend.s3` section as needed
```

In the example above, we've overridden the `role_arn` attribute for the `s3` backend to use the read-only role when
accessing the remote state of all components. All other attributes will be taken from the `backend` section (Atmos
deep-merges the `remote_state_backend` section with the `backend` section).

When working with Terraform backends and writing/updating the state, the `terraform-backend-read-write` role will be
used. But when reading the remote state of components, the `terraform-backend-read-only` role will be used.

---

## Terraform Root Modules

import DocCardList from '@theme/DocCardList'
import KeyPoints from '@site/src/components/KeyPoints'
import Intro from '@site/src/components/Intro'

<Intro>
Use Atmos to provision your Terraform root modules and manage their configurations consistently and repeatably
by leveraging imports and inheritance for DRY configurations and reduced blast radius of changes.
</Intro>

<KeyPoints>
- Why does Terraform need additional tooling
- How does Atmos change how you write Terraform code
- How to use Terraform with Atmos
</KeyPoints>

Atmos can change how you think about the Terraform modules that you use to build your infrastructure.

When you design cloud architectures with Atmos, you will first break it apart into pieces called components.
Then, you will implement Terraform "root modules" for each of your components.
To make them highly reusable, they should serve a "single purpose" so that they are the smallest possible
unit of infrastructure managed in the software development lifecycle (SDLC).
Finally, you will connect your components together using stacks, so that everything comes together.

In the [Quick Start](/quick-start/simple) tutorial, we’ll guide you through the thought process of building Terraform "root modules" that are suitable for use as components.

## What is Terraform?

Terraform is a command-line utility or interpreter (like Perl or Ruby), that processes infrastructure configurations
written in ["HashiCorp's Configuration Language" ("HCL")](https://en.wikipedia.org/wiki/HCL) to orchestrate infrastructure provisioning.
Its chief role is to delineate and structure infrastructure definitions. Terraform by itself is not a framework.

:::note Disambiguation
The term “Terraform” is used in this documentation to refer to generic concepts such as providers, modules, stacks, the
HCL-based domain-specific language and its interpreter. Atmos works with [OpenTofu](/core-concepts/projects/configuration/opentofu).
:::

<details>
<summary>Fun Fact!</summary>

HCL is backward compatible with JSON, although it's not a strict superset of JSON.
HCL is more human-friendly and readable, while JSON is often used for machine-generated configurations.
This means you can write Terraform configurations in HCL or JSON, and Terraform will understand them.
This feature is particularly useful for programmatically generating configurations or integration with systems that already use JSON.
</details>

## How has Terraform HCL Evolved?

Terraform's HCL started strictly as a configuration language, not a markup or programming language, although it has evolved
considerably over the years.

As Terraform progressed and HCL evolved, notably from version _0.12_ onwards, HCL began incorporating features typical
of programming languages (albeit without a debugger!). This shift enriched infrastructure definitions, positioning HCL
more as a [domain-specific programming language (DSL)](https://en.wikipedia.org/wiki/Domain-specific_language) for
defining infrastructure than strictly a configuration language (aka data interchange formats like JSON). As a result,
the complexity of configuring Terraform projects has risen, while Terraform's inherent capabilities to be configured
haven't evolved at the same pace.

- **Rich Expressions:** Introduced a richer expression syntax, removing the need for interpolations.

- **For Loops and Conditionals:** Added for expressions and conditional expressions.

- **Type System:** Introduced a more explicit type system for input and output values.

## Why is additional tooling needed when using Terraform?

**Every foundational tool begins simply.**

As users grow more advanced and their ambitions expand, the need for advanced tooling emerges. These shifts demonstrate that core
technologies naturally progress, spawning more advanced constructs to tackle increased intricacies and enhance efficiency -- all
while retaining their core essence. Just as CSS, NodeJS, Docker, Helm, and many other tools have evolved to
include higher-order utilities, Terraform, too, benefits from additional orchestration tools, given the complexities and challenges
users face at different stages of adoption.

Examples of tools like these are numerous, like:

- **CSS has Sass:** Sass provides more expressive styling capabilities, variables, and functions, making stylesheets more maintainable and organized, especially for large projects.
- **NodeJS has React:** React brings component-based architecture to JavaScript, enhancing the creation of interactive UIs, improving code reusability, and better supporting the development of large-scale applications.
- **Docker has Docker Compose:** Docker Compose simplifies the management and orchestration of multi-container Docker applications, making it easier to define, run, and scale services collectively.
- **Helm charts have Helmfiles:** While Helm charts define the blueprints of Kubernetes services, Helmfiles enable better orchestration, management, and deployment of multiple charts, similar to coordinating various instruments in a symphony.
- **Kubernetes manifests have Kustomize:** Kustomize allows customization of Kubernetes manifests without changing their original form, facilitating dynamic configurations tailored to specific deployment scenarios.

**These days, no one would dream of building a modern web app without a framework. Why should Terraform be any different?**

When considering Terraform in the context of large-scale organizations or enterprises, it's clear that Terraform and its inherent language don't address all challenges. This is why teams progress through [10 stages of maturity](/introduction/why-atmos). With hundreds or even of components spread across hundreds of accounts, cloud providers and managed by a vast number of DevOps engineers and developers, the complexity becomes overwhelming and difficult to manage.

A lot of the same challenges faced by NodeJS, Docker, Helm and Kubernetes also exist in Terraform as well.

**Challenges in Terraform are centered around Root Modules:**
- **Large-Scale Architectures**: Providing better support for large-scale service-oriented architectures
- **Composition**: Making it straightforward to compose architectures of multiple "root modules"
- **Code Reusability and Maintainability**: Simplifying the definition and reuse of "root modules"
- **Ordered Dependencies**: Handling orchestration, management, and deployment of multiple loosely coupled "root modules"
- **Sharing State**: Sharing state between "root modules"
- **CI/CD Automation**: Enhancing CI/CD automation, especially in monorepos, when there are no rollback capabilities

These are not language problems. These are framework problems. Without a coherent framework, Terraform is hard to use at scale.
Ultimately, the goal is to make Terraform more scalable, maintainable, and developer-friendly, especially in complex and large-scale environments.

## Refresher on Terraform Concepts

<dl>
<dt>Child Modules</dt>
<dd>Child modules are reusable pieces of Terraform code that accept parameters (variables) for customization and emit outputs.
    Outputs can be passed between child modules and used to connect them together.
    They are stateless and can be invoked multiple times. Child modules can also call other child modules, making
    them a primary method for reducing repetition in Terraform HCL code; it's how you DRY up your HCL code.</dd>

<dt>Root Modules</dt>
<dd>Root modules in Terraform are the topmost modules that can call child modules or directly use Terraform code.
    The key distinction between root and child modules is that root modules maintain Terraform state,
    typically stored in a remote state backend like S3. Root modules cannot call other root modules,
    but they can access the outputs of any other root module using Remote State.</dd>

<dt>State Backends</dt>
<dd>State Backends are where the desired state of your infrastructure code is stored.
    It's always defined exactly once per "root module". This where the computed state of your HCL code is stored,
    and it is what `terraform apply` will execute. The most common state backend is object storage
    like S3, but there are many other types of state backends available.</dd>

<dt>Remote State</dt>
<dd>Remote state refers to the concept of retrieving the outputs from other root modules.
    Terraform natively supports passing information between "root modules" without any additional tooling,
    a capability we rely on in Atmos.</dd>
</dl>

:::info Disambiguation

- **Terraform Component** is a [Terraform Root Module](https://developer.hashicorp.com/terraform/language/modules#the-root-module) and stored typically in `components/terraform/$name` that consists of the resources defined in the `.tf` files in a working directory
  (e.g. [components/terraform/infra/vpc](https://github.com/cloudposse/atmos/tree/main/examples/quick-start-advanced/components/terraform/vpc))

- **Stack** provides configuration (variables and other settings) for a Terraform Component and is defined in one or more Atmos stack manifests
  (a.k.a. stack config files)

:::

## Example: Provision Terraform Component

To provision a Terraform component using the `atmos` CLI, run the following commands in the container shell:

```console
atmos terraform plan eks --stack=ue2-dev
atmos terraform apply eks --stack=ue2-dev
```

where:

- `eks` is the Terraform component to provision (from the `components/terraform` folder)
- `--stack=ue2-dev` is the stack to provision the component into

Short versions of all command-line arguments can be used:

```console
atmos terraform plan eks -s ue2-dev
atmos terraform apply eks -s ue2-dev
```

The `atmos terraform deploy` command executes `terraform apply -auto-approve` to provision components in stacks without
user interaction:

```console
atmos terraform deploy eks -s ue2-dev
```

## Using Submodules (Child Modules)

If your components rely on local submodules, our convention is to use a `modules/` subfolder of the component to store them.

## Terraform Usage with Atmos

Learn how to best leverage Terraform together with Atmos.

<DocCardList/>

---

## Terraform Workspaces

import File from '@site/src/components/File'
import Terminal from '@site/src/components/Terminal'
import Intro from '@site/src/components/Intro'

<Intro>
In Terraform, a [workspace](https://developer.hashicorp.com/terraform/language/state/workspaces) is a feature that allows
you to manage multiple "state" environments within a Terraform configuration. Each workspace maintains its own state,
allowing you to deploy and manage infrastructure configurations independently.
</Intro>

Workspaces are useful in several scenarios:

- **Environment Isolation**: Workspaces enable you to have separate environments within a Terraform configuration.
Each workspace can have its own set of resources and configurations.

- **Parallel Development**: Workspaces facilitate parallel development by allowing different team members to work on
different workspaces concurrently without interfering with each other's changes.

- **Testing and Experimentation**: Workspaces are helpful for testing and experimentation.
You can create temporary workspaces to test changes or new configurations without affecting the main production environment.

- **State Management**: Workspaces manage separate states for each environment.
This helps in maintaining clarity and avoiding conflicts when multiple environments are being managed.

- **Deployment Strategies**: Workspaces can be used to implement different deployment strategies.
For example, you might use separate workspaces for blue-green deployments or canary releases.

To work with workspaces in Terraform, you can use commands like `terraform workspace new`, `terraform workspace select`,
and `terraform workspace delete` to create, switch between, and delete workspaces respectively.
Atmos automatically manages Terraform workspaces for you when you provision components in a stack.

## Terraform Workspaces in Atmos

Atmos automatically calculates Terraform workspace names and uses workspaces to manage top-level stacks. By default, Atmos uses the stack
name as the Terraform workspace when provisioning components in the stack. For example, consider the following manifest
for the component `vpc` in the stack `ue2-dev`:

<File title="stacks/ue2-dev.yaml">
```yaml
vars:
  # Context variables that define the Atmos stack `ue2-dev`
  environment: ue2
  stage: dev

components:
  terraform:
    vpc:
      metadata:
        # Point to the Terraform component in `components/terraform/vpc`
        component: vpc
      # Define the variables specific to this component
      vars:
        name: my-vpc
        ipv4_primary_cidr_block: 10.9.0.0/18
```
</File>

When you provision the `vpc` component in the stack `ue2-dev` by executing the following command:

<Terminal>
    ```shell
    atmos terraform apply vpc -s ue2-dev
    ```
</Terminal>

Atmos computes the workspace name to be `ue2-dev`. Any Atmos Terraform command other than `init`, using this stack,
will cause Atmos to select this workspace, creating it if needed. (This leaves the workspace selected as a side effect
for subsequent Terraform commands run outside of Atmos. Atmos version 1.55 took away this side effect, but it was
restored in version 1.69.)

The exception to the default rule (using the stack name as Terraform workspace) is when we provision more than one
instance of the same Terraform component (with the same or different settings) into the same stack by defining multiple
Atmos components. In this case, Atmos calculates the Terraform workspace for each component by joining the stack name
with the component name.

For example, the following manifest shows how to define two Atmos components, `vpc/1` and `vpc/2`,
which both point to the same Terraform component `vpc`, in the stack `ue2-dev`:

<File title="stacks/ue2-dev.yaml">
```yaml
vars:
  # Context variables that define the Atmos stack `ue2-dev`
  environment: ue2
  stage: dev

components:
  terraform:
    # Atmos component `vpc/1`
    vpc/1:
      metadata:
        # Point to the Terraform component in `components/terraform/vpc`
        component: vpc
        # Inherit the defaults for all VPC components
        inherits:
          - vpc/defaults
      # Define/override variables specific to this `vpc/1` component
      vars:
        name: vpc-1
        ipv4_primary_cidr_block: 10.9.0.0/18

    # Atmos component `vpc/2`
    vpc/2:
      metadata:
        # Point to the Terraform component in `components/terraform/vpc`
        component: vpc
        # Inherit the defaults for all VPC components
        inherits:
          - vpc/defaults
      # Define/override variables specific to this `vpc/2` component
      vars:
        name: vpc-2
        ipv4_primary_cidr_block: 10.10.0.0/18
```
</File>

When you provision the components by executing the commands:

<Terminal>
    ```shell
    atmos terraform apply vpc/1 -s ue2-dev
    atmos terraform apply vpc/2 -s ue2-dev
    ```
</Terminal>

Atmos computes the workspace names as `ue2-dev-vpc-1` and `ue2-dev-vpc-2` respectively,
and selects the appropriate workspace for each component (again, creating it if needed).
This is done because the same Terraform component `vpc` is used as the workspace prefix
(in case of [AWS S3 backend](https://developer.hashicorp.com/terraform/language/settings/backends/s3),
folder in the S3 state bucket), and it's necessary to have different subfolders (`ue2-dev-vpc-1`
and `ue2-dev-vpc-2` instead of just `ue2-dev`) to store the Terraform state files.

## Terraform Workspace Override in Atmos

You can override Terraform workspaces for Atmos components by using `metadata.terraform_workspace` and
`metadata.terraform_workspace_pattern` attributes. For example:

<File title="stacks/ue2-dev.yaml">
```yaml
vars:
  environment: ue2
  stage: dev

components:
  terraform:
    vpc/1:
      metadata:
        component: vpc
        # Override Terraform workspace
        terraform_workspace: "vpc-1-workspace-override"

    vpc/2:
      metadata:
        component: vpc
        # Override Terraform workspace
        terraform_workspace_pattern: "{environment}-{stage}-{component}-workspace-override"
```
</File>

When you provision the components by executing the commands:

<Terminal>
    ```shell
    atmos terraform apply vpc/1 -s ue2-dev
    atmos terraform apply vpc/2 -s ue2-dev
    ```
</Terminal>

Atmos sets the Terraform workspace `vpc-1-workspace-override` for the `vpc/1` component, and
`ue2-dev-vpc-2-workspace-override` for the `vpc/2` component.

The following context tokens are supported by the `metadata.terraform_workspace_pattern` attribute:

- `{namespace}`
- `{tenant}`
- `{environment}`
- `{region}`
- `{stage}`
- `{attributes}`
- `{component}`
- `{base-component}`

:::tip
For more information on Atmos base and derived components, and to understand the `{base-component}` token,
refer to [Atmos Component Inheritance](/core-concepts/stacks/inheritance)
:::

## References

- [Terraform Workspaces](https://developer.hashicorp.com/terraform/language/state/workspaces)
- [Managing Terraform Workspaces](https://developer.hashicorp.com/terraform/cli/workspaces)
- [Terraform Environment Variables](https://developer.hashicorp.com/terraform/cli/config/environment-variables)

## Disabling Terraform Workspaces

In some cases, you may want to disable Terraform workspaces entirely, particularly when using backends that don't support workspaces. By default, Atmos automatically manages workspaces for supported backend types, but you can control this behavior using the `components.terraform.workspaces_enabled` configuration in your `atmos.yaml` file.

### HTTP Backend and Workspace Support

The [Terraform HTTP backend](https://developer.hashicorp.com/terraform/language/settings/backends/http) does not support workspaces. When Atmos detects that you're using an HTTP backend, it automatically disables workspaces for the affected components, regardless of other configuration settings. This ensures compatibility with HTTP backends while still allowing you to use the same configuration for other backend types.

For example, when you execute a Terraform command with an HTTP backend:

<Terminal>
    ```shell
    atmos terraform apply vpc -s ue2-dev
    ```
</Terminal>

Atmos will execute Terraform without attempting to create or select a workspace, using the default workspace instead.

### Explicitly Disabling Workspaces

If you need to disable workspaces for all components, regardless of backend type, you can set the `workspaces_enabled` configuration option in your `atmos.yaml` file:

<File title="atmos.yaml">
```yaml
components:
  terraform:
    # Disable workspaces for all Terraform components
    workspaces_enabled: false
    # Other Terraform configuration...
```
</File>

When workspaces are disabled:

- Atmos will not attempt to create or select workspaces before running Terraform commands
- All Terraform operations will use the default workspace
- Workspace-related variables will be empty in component configurations

:::note
Setting `workspaces_enabled: true` for an HTTP backend will be ignored with a warning message since HTTP backends don't support workspaces.
:::

### When to Disable Workspaces

Consider disabling workspaces in the following scenarios:

- When using backends that don't support workspaces (e.g., HTTP backend)
- When you need consistent behavior with other tools that don't manage workspaces
- When you prefer to manage state files without workspace isolation
- When your workflow already handles environment separation through other means

By properly configuring workspace support, you can ensure that Atmos works seamlessly with all backend types while maintaining the flexibility to adapt to different deployment strategies.

---

## Core Concepts of the Atmos Framework

import DocCardList from '@theme/DocCardList'

Atmos simplifies the process of managing and deploying your infrastructure across cloud platforms.
Dive into these core concepts of Atmos to discover how they facilitate these processes.

You're about to discover a new way to think about things...

<DocCardList/>

---

## Atmos Custom Commands

import File from '@site/src/components/File'
import Intro from '@site/src/components/Intro'

<Intro>
Atmos can be easily extended to support any number of custom CLI commands. Custom commands are exposed through the `atmos` CLI when you run `atmos help`. It's a great way to centralize the way operational tools are run in order to improve DX.
</Intro>

For example, one great way to use custom commands is to tie all the miscellaneous scripts into one consistent CLI interface. Then we can kiss those
ugly, inconsistent arguments to bash scripts goodbye! Just wire up the commands in atmos to call the script. Then developers can just run `atmos help`
and discover all available commands.

## Simple Example

Here is an example to play around with to get started.

Adding the following to `atmos.yaml` will introduce a new `hello` command.

```yaml
# Custom CLI commands
commands:
  - name: hello
    description: This command says Hello world
    steps:
      - "echo Hello world!"
```

We can run this example like this:

```shell
atmos hello
```

## Positional Arguments

Atmos also supports positional arguments. If a positional argument is required but not provided by the user,
the command will fail—unless you define a default in your config.

For the example, adding the following to `atmos.yaml` will introduce a new `greet` command that accepts one `name` argument,
but uses a default of "John Doe" if none is provided.

```yaml
# subcommands
commands:
  - name: greet
    description: This command says hello to the provided name
    arguments:
      - name: name
        description: Name to greet
        required: true
        default: John Doe
    steps:
      - "echo Hello {{ .Arguments.name }}!"
```

We can run this example like this:

```shell
atmos greet Alice
```
or defaulting to "John Doe"

```shell
atmos greet
```

## Trailing Arguments

Atmos supports **trailing arguments** after `--` (a standalone double-dash). The `--` itself is a delimiter that signals the end of Atmos-specific options. Anything after `--` is passed directly to the underlying command without being interpreted by Atmos. The value of these trailing arguments is accessible in `{{ .TrailingArgs }}`.

For the example, adding the following to `atmos.yaml` will introduce a new `echo` command that accepts one `name` argument and also uses trailingArgs

```yaml
- name: ansible run
  description: "Runs an Ansible playbook, allowing extra arguments after --."
  arguments:
    - name: playbook
      description: "The Ansible playbook to run"
      default: site.yml
      required: true
  steps:
    - "ansible-playbook {{ .Arguments.playbook }} {{ .TrailingArgs }}"
```
Output:

```bash
$ atmos ansible run -- --limit web
Running: ansible-playbook site.yml --limit web

PLAY [web] *********************************************************************

```

## Passing Flags

Passing flags works much like passing positional arguments, except for that they are passed using long or short flags.
Flags can be optional (this is configured by setting the `required` attribute to `false`).

```yaml
# subcommands
commands:
  - name: hello
    description: This command says hello to the provided name
    flags:
      - name: name
        shorthand: n
        description: Name to greet
        required: true
    steps:
      - "echo Hello {{ .Flags.name }}!"
```

We can run this example like this, using the long flag:

```shell
atmos hello --name world
```

Or, using the shorthand, we can just write:

```shell
atmos hello -n world
```

## Advanced Examples

### Define a New Terraform Command

```yaml
# Custom CLI commands
commands:
  - name: terraform
    description: Execute 'terraform' commands
    # subcommands
    commands:
      - name: provision
        description: This command provisions terraform components
        arguments:
          - name: component
            description: Name of the component
        flags:
          - name: stack
            shorthand: s
            description: Name of the stack
            required: true
        # ENV var values support Go templates
        env:
          - key: ATMOS_COMPONENT
            value: "{{ .Arguments.component }}"
          - key: ATMOS_STACK
            value: "{{ .Flags.stack }}"
        steps:
          - atmos terraform plan $ATMOS_COMPONENT -s $ATMOS_STACK
          - atmos terraform apply $ATMOS_COMPONENT -s $ATMOS_STACK
```

### Override an Existing Terraform Command

```yaml
# Custom CLI commands
commands:
  - name: terraform
    description: Execute 'terraform' commands
    # subcommands
    commands:
      - name: apply
        description: This command executes 'terraform apply -auto-approve' on terraform components
        arguments:
          - name: component
            description: Name of the component
        flags:
          - name: stack
            shorthand: s
            description: Name of the stack
            required: true
        steps:
          - atmos terraform apply {{ .Arguments.component }} -s {{ .Flags.stack }} -auto-approve
```

### Show Component Info

```yaml
# Custom CLI commands
commands:
  - name: show
    description: Execute 'show' commands
    # subcommands
    commands:
      - name: component
        description: Execute 'show component' command
        arguments:
          - name: component
            description: Name of the component
        flags:
          - name: stack
            shorthand: s
            description: Name of the stack
            required: true
        # ENV var values support Go templates and have access to {{ .ComponentConfig.xxx.yyy.zzz }} Go template variables
        env:
          - key: ATMOS_COMPONENT
            value: "{{ .Arguments.component }}"
          - key: ATMOS_STACK
            value: "{{ .Flags.stack }}"
          - key: ATMOS_TENANT
            value: "{{ .ComponentConfig.vars.tenant }}"
          - key: ATMOS_STAGE
            value: "{{ .ComponentConfig.vars.stage }}"
          - key: ATMOS_ENVIRONMENT
            value: "{{ .ComponentConfig.vars.environment }}"
        # If a custom command defines 'component_config' section with 'component' and 'stack', 'atmos' generates the config for the component in the stack
        # and makes it available in {{ .ComponentConfig.xxx.yyy.zzz }} Go template variables,
        # exposing all the component sections (which are also shown by 'atmos describe component' command)
        component_config:
          component: "{{ .Arguments.component }}"
          stack: "{{ .Flags.stack }}"
        # Steps support using Go templates and can access all configuration settings (e.g. {{ .ComponentConfig.xxx.yyy.zzz }})
        # Steps also have access to the ENV vars defined in the 'env' section of the 'command'
        steps:
          - 'echo Atmos component from argument: "{{ .Arguments.component }}"'
          - 'echo ATMOS_COMPONENT: "$ATMOS_COMPONENT"'
          - 'echo Atmos stack: "{{ .Flags.stack }}"'
          - 'echo Terraform component: "{{ .ComponentConfig.component }}"'
          - 'echo Backend S3 bucket: "{{ .ComponentConfig.backend.bucket }}"'
          - 'echo Terraform workspace: "{{ .ComponentConfig.workspace }}"'
          - 'echo Namespace: "{{ .ComponentConfig.vars.namespace }}"'
          - 'echo Tenant: "{{ .ComponentConfig.vars.tenant }}"'
          - 'echo Environment: "{{ .ComponentConfig.vars.environment }}"'
          - 'echo Stage: "{{ .ComponentConfig.vars.stage }}"'
          - 'echo Dependencies: "{{ .ComponentConfig.deps }}"'
```

### Set EKS Cluster

```yaml
# Custom CLI commands
commands:
  - name: set-eks-cluster
    description: |
      Download 'kubeconfig' and set EKS cluster.

      Example usage:
        atmos set-eks-cluster eks/cluster -s plat-ue1-dev -r admin
        atmos set-eks-cluster eks/cluster -s plat-uw2-prod --role reader
    verbose: false  # Set to `true` to see verbose outputs
    arguments:
      - name: component
        description: Name of the component
    flags:
      - name: stack
        shorthand: s
        description: Name of the stack
        required: true
      - name: role
        shorthand: r
        description: IAM role to use
        required: true
    # If a custom command defines 'component_config' section with 'component' and 'stack',
    # Atmos generates the config for the component in the stack
    # and makes it available in {{ .ComponentConfig.xxx.yyy.zzz }} Go template variables,
    # exposing all the component sections (which are also shown by 'atmos describe component' command)
    component_config:
      component: "{{ .Arguments.component }}"
      stack: "{{ .Flags.stack }}"
    env:
      - key: KUBECONFIG
        value: /dev/shm/kubecfg.{{ .Flags.stack }}-{{ .Flags.role }}
    steps:
      - >
        aws
        --profile {{ .ComponentConfig.vars.namespace }}-{{ .ComponentConfig.vars.tenant }}-gbl-{{ .ComponentConfig.vars.stage }}-{{ .Flags.role }}
        --region {{ .ComponentConfig.vars.region }}
        eks update-kubeconfig
        --name={{ .ComponentConfig.vars.namespace }}-{{ .Flags.stack }}-eks-cluster
        --kubeconfig="${KUBECONFIG}"
        > /dev/null
      - chmod 600 ${KUBECONFIG}
      - echo ${KUBECONFIG}
```

### Describe EKS Cluster Kubernetes Version Upgrade

```yaml
# Custom CLI commands
commands:
  - name: describe
    description: "Execute 'describe' commands"
    # subcommands
    commands:
      - name: eks
        description: "Execute 'describe eks' commands"
        # subcommands
        commands:
          - name: upgrade
            description: "Describe the steps on how to upgrade an EKS cluster to the next Kubernetes version. Usage: atmos describe eks upgrade <eks_component> -s <stack>"
            arguments:
              - name: component
                description: Name of the EKS component
            flags:
              - name: stack
                shorthand: s
                description: Name of the stack
                required: true
              - name: role
                shorthand: r
                description: Role to assume to connect to the cluster
                required: false
            # If a custom command defines 'component_config' section with 'component' and 'stack',
            # Atmos generates the config for the component in the stack
            # and makes it available in {{ .ComponentConfig.xxx.yyy.zzz }} Go template variables,
            # exposing all the component sections (which are also shown by 'atmos describe component' command)
            component_config:
              component: "{{ .Arguments.component }}"
              stack: "{{ .Flags.stack }}"
            env:
              - key: KUBECONFIG
                value: /dev/shm/kubecfg-eks-upgrade.{{ .Flags.stack }}
            steps:
              - |
                # Set the environment
                color_red="\u001b[31m"
                color_green="\u001b[32m"
                color_yellow="\u001b[33m"
                color_blue="\u001b[34m"
                color_magenta="\u001b[35m"
                color_cyan="\u001b[36m"
                color_black="\u001b[30m"
                color_white="\u001b[37m"
                color_reset="\u001b[0m"

                # Check the requirements
                command -v aws >/dev/null 2>&1 || { echo -e >&2 "\n${color_red}'aws' is required but it's not installed.${color_reset}"; exit 1; }
                command -v kubectl >/dev/null 2>&1 || { echo -e >&2 "\n${color_red}'kubectl' is required but it's not installed.${color_reset}"; exit 1; }
                command -v helm >/dev/null 2>&1 || { echo -e >&2 "\n${color_red}'helm' is required but it's not installed.${color_reset}"; exit 1; }
                command -v jq >/dev/null 2>&1 || { echo -e >&2 "\n${color_red}'jq' is required but it's not installed.${color_reset}"; exit 1; }
                command -v yq >/dev/null 2>&1 || { echo -e >&2 "\n${color_red}'yq' is required but it's not installed.${color_reset}"; exit 1; }
                command -v pluto >/dev/null 2>&1 || { echo -e >&2 "\n${color_red}'pluto' is required but it's not installed.${color_reset}"; exit 1; }
                command -v awk >/dev/null 2>&1 || { echo -e >&2 "\n${color_red}'awk' is required but it's not installed.${color_reset}"; exit 1; }
                command -v sed >/dev/null 2>&1 || { echo -e >&2 "\n${color_red}'sed' is required but it's not installed.${color_reset}"; exit 1; }
                command -v tr >/dev/null 2>&1 || { echo -e >&2 "\n${color_red}'tr' is required but it's not installed.${color_reset}"; exit 1; }

                # Set the role to assume to connect to the cluster
                role={{ .Flags.role }}
                if [[ -z "$role" ]]; then
                  role=admin
                fi

                # Download kubeconfig and connect to the cluster
                echo -e "\nConnecting to EKS cluster ${color_cyan}{{ .Flags.stack }}${color_reset} and downloading kubeconfig..."
                aws \
                    --profile {{ .ComponentConfig.vars.namespace }}-{{if (index .ComponentConfig.vars "tenant") }}{{ .ComponentConfig.vars.tenant }}-gbl-{{ .ComponentConfig.vars.stage }}{{else}}gbl-{{ .ComponentConfig.vars.stage }}{{end}}-${role} \
                    --region {{ .ComponentConfig.vars.region }} \
                    eks update-kubeconfig \
                    --name={{ .ComponentConfig.vars.namespace }}-{{ .Flags.stack }}-eks-cluster \
                    --kubeconfig="${KUBECONFIG}"
                chmod 600 ${KUBECONFIG}

                # Check connectivity to the cluster
                kubectl version -o json 2>&1>/dev/null
                retVal=$?
                if [ $retVal -ne 0 ]; then
                  echo -e "${color_red}\nCould not connect to the cluster.\nIf the cluster is provisioned in private subnets or only allows private access, make sure you are connected to the VPN.\n${color_reset}"
                  exit $retVal
                fi

                # Get the current Kubernetes version from the cluster
                current_k8s_version_str=$(kubectl version -o json 2>/dev/null | jq '(.serverVersion.major + "." + .serverVersion.minor)' | sed 's/[+\"]//g')
                current_k8s_version=$(echo ${current_k8s_version_str} | jq 'tonumber')
                echo -e "\nThe cluster is running Kubernetes version ${current_k8s_version}"

                # Get all the supported Kubernetes versions from AWS EKS
                supported_eks_k8s_versions=$(aws eks describe-addon-versions | jq -r '[ .addons[].addonVersions[].compatibilities[].clusterVersion ] | unique | sort')
                supported_eks_k8s_versions_csv=$(echo ${supported_eks_k8s_versions} | jq -r 'join(", ")')
                echo -e "AWS EKS currently supports Kubernetes versions ${supported_eks_k8s_versions_csv}"

                # Calculate the next Kubernetes version that the cluster can be upgraded to
                next_k8s_version=$(echo ${supported_eks_k8s_versions} | jq -r --arg current_k8s_version "${current_k8s_version}" 'map(select((. |= tonumber) > ($current_k8s_version | tonumber)))[0]')

                # Check if the cluster can be upgraded
                upgrade_needed=false
                if [[ ! -z "$next_k8s_version" ]] && (( $(echo $next_k8s_version $current_k8s_version | awk '{if ($1 > $2) print 1;}') )) ; then
                  upgrade_needed=true
                else
                fi
                if [ ${upgrade_needed} = false ]; then
                  echo -e "${color_green}\nThe cluster is running the latest supported Kubernetes version ${current_k8s_version}\n${color_reset}"
                  exit 0
                fi

                # Describe the upgrade process
                echo -e "${color_green}\nThe cluster can be upgraded to the next Kubernetes version ${next_k8s_version}${color_reset}"

                # Describe what will be checked before the upgrade
                describe_what_will_be_checked="
                  \nBefore upgrading the cluster to Kubernetes ${next_k8s_version}, we'll check the following:

                  - Pods and containers that are not ready or crashing
                      https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle

                  - Helm releases with removed Kubernetes API versions
                      https://kubernetes.io/docs/reference/using-api/deprecation-policy
                      https://helm.sh/docs/topics/kubernetes_apis

                  - EKS add-ons versions
                      https://docs.aws.amazon.com/eks/latest/userguide/eks-add-ons.html
                "
                echo -e "${describe_what_will_be_checked}"

                echo -e "${color_cyan}\nPress Enter to continue ...${color_reset}"
                read -r

                # Show all Pods that are not in 'Running' state
                echo -e "\nChecking for Pods that are not in 'Running' state...\n"
                kubectl get pods -A | grep -Ev '([0-9]+)/\1'

                # Show failed or not ready containers
                echo -e "\nChecking for failing containers..."
                failing_containers=$(kubectl get pods -A -o json | jq '[ .items[].status.containerStatuses[].state | select(has("waiting")) | .waiting ]')
                failing_containers_count=$(echo ${failing_containers} | jq  'length')
                if [[ "$failing_containers_count" > 0 ]]; then
                  echo -e "${color_red}\nThere are ${failing_containers_count} failing container(s) on the cluster:\n${color_reset}"
                  echo ${failing_containers} | jq -r 'def red: "\u001b[31m"; def reset: "\u001b[0m"; (.[] | [ red + .message + reset ]) | @tsv'
                  echo -e "\nAlthough the cluster can be upgraded to the next Kubernetes version even with the failing Pods and containers, it's recommended to fix all the issues before upgrading.\n"
                else
                  echo -e "${color_green}\nThere are no failing containers on the cluster\n${color_reset}"
                fi

                echo -e "${color_cyan}\nPress Enter to continue ...${color_reset}"
                read -r

                # Show Helm releases with removed Kubernetes API versions
                echo -e "\nChecking for Helm releases with removed Kubernetes API versions...\n"
                releases_with_removed_versions=$(pluto detect-helm --output json --only-show-removed --target-versions k8s=v${next_k8s_version} 2>/dev/null | jq 'select(has("items")) | [ .items[] ]')
                releases_with_removed_versions_count=$(echo ${releases_with_removed_versions} | jq  'length')
                if [[ -z "$releases_with_removed_versions_count" ]] || [[ "$releases_with_removed_versions_count" = 0 ]]; then
                  echo -e "${color_green}\nAll Helm releases are up to date and ready for Kubernetes ${next_k8s_version}${color_reset}"
                else
                  echo -e "${color_red}\nThere are Helm releases with API versions removed in Kubernetes ${next_k8s_version}\n${color_reset}"
                  pluto detect-helm --output wide --only-show-removed --target-versions k8s=v${next_k8s_version} 2>/dev/null
                  helm_list_filter=$(echo ${releases_with_removed_versions} | jq -r '[ (.[].name | split("/"))[0] ] | join("|")')
                  helm list -A -a -f ${helm_list_filter}

                  # Describe how to fix the Helm releases
                  describe_how_to_fix_helm_releases="
                    \nBefore upgrading the cluster to Kubernetes ${next_k8s_version}, the Helm releases need to be fixed.

                    - For the Helm releases identified, you need to check for the latest version of the Chart (which has supported API versions)
                      or update the Chart yourself. Then deploy the updated Chart

                    - If the cluster was already upgraded to a new Kubernetes version without auditing for the removed API versions, it might be already running
                      with the removed API versions. When trying to redeploy the Helm Chart, you might encounter an error similar to the following:

                          Error: UPGRADE FAILED: current release manifest contains removed kubernetes api(s)
                          for this kubernetes version and it is therefore unable to build the kubernetes
                          objects for performing the diff.
                          Error from Kubernetes: unable to recognize \"\": no matches for kind "Deployment" in version "apps/v1beta1"

                      Helm fails in this scenario because it attempts to create a diff patch between the current deployed release
                      (which contains the Kubernetes APIs that are removed) against the Chart you are passing with the updated/supported API versions.

                      To fix this, you need to edit the release manifests that are stored in the cluster to use the supported API versions.
                      You can use the Helm 'mapkubeapis' plugin to update/patch the Helm releases to supported APIs.
                      Execute the following commands to patch the releases identified above:

                          helm plugin install https://github.com/helm/helm-mapkubeapis
                          helm mapkubeapis <NAME> -n <NAMESPACE>

                    NOTE: The best practice is to upgrade Helm releases that are using deprecated API versions to supported API versions
                          prior to upgrading to a Kubernetes version that removes those APIs.

                    For more information, refer to:
                      - https://helm.sh/docs/topics/kubernetes_apis
                      - https://github.com/helm/helm-mapkubeapis
                  "
                  echo -e "${describe_how_to_fix_helm_releases}"
                fi

                echo -e "${color_cyan}\nPress Enter to continue ...${color_reset}"
                read -r

                # Check EKS add-ons versions
                echo -e "\nChecking EKS add-ons versions..."
                addons=$(atmos describe component {{ .Arguments.component }} -s {{ .Flags.stack }} --format json | jq -r '.vars.addons')
                addons_count=$(echo ${addons} | jq -r '. | keys | length')
                if [[ "$addons_count" = 0 ]]; then
                  echo -e "${color_yellow}
                      \rCould not detect the 'addons' variable for the component '{{ .Arguments.component }}' in the stack '{{ .Flags.stack }}'.
                      \rMake sure that EKS add-ons are configured and provisioned on the EKS cluster.
                      \rRefer to https://docs.aws.amazon.com/eks/latest/userguide/eks-add-ons.html for more details.
                      ${color_reset}"
                else
                  echo -e "\nThere are currently ${addons_count} add-on(s) configured for the EKS component ${color_cyan}{{ .Arguments.component }}${color_reset} in the stack ${color_cyan}{{ .Flags.stack }}${color_reset} in the variable ${color_cyan}addons${color_reset}:\n"
                  echo ${addons} | yq --prettyPrint '.'
                  echo -e "\nKubernetes ${next_k8s_version} requires the following versions of the EKS add-ons:\n"

                  # Detect the latest supported versions of the EKS add-ons
                  addons_template=$(atmos describe component {{ .Arguments.component }} -s {{ .Flags.stack }} --format json | jq -r '.vars.addons')
                  for ((i=0; i<${addons_count}; i++)); do
                    addon_name=$(echo ${addons} | jq -r '(keys)['$i']')
                    addon_version=$(aws eks describe-addon-versions --kubernetes-version ${next_k8s_version} --addon-name ${addon_name} --query 'addons[].addonVersions[?compatibilities[0].defaultVersion].addonVersion' --output text)
                    addons_template=$(jq --arg addon_name "${addon_name}" --arg addon_version "${addon_version}" '.[$addon_name].addon_version = $addon_version' <<< "${addons_template}")
                  done

                  # Print the add-ons configuration for the desired Kubernetes version
                  echo ${addons_template} | yq --prettyPrint '.'
                fi

                # Describe how to provision the EKS component with the new Kubernetes version
                echo -e "${color_cyan}\nPress Enter to continue ...${color_reset}"
                read -r
                echo -e "\nAfter the Pods, Helm releases and EKS add-ons are configured and ready, do the following:\n
                  - Set the variable ${color_cyan}kubernetes_version${color_reset} to ${color_cyan}${next_k8s_version}${color_reset} for the EKS component ${color_cyan}{{ .Arguments.component }}${color_reset} in the stack ${color_cyan}{{ .Flags.stack }}${color_reset}
                  - Run the command ${color_cyan}atmos terraform apply {{ .Arguments.component }} -s {{ .Flags.stack }}${color_reset} to provision the component
                  - Run the command ${color_cyan}kubectl get pods -A${color_reset} to check the status of all Pods after the upgrade
                  - Run the command ${color_cyan}helm list -A -a${color_reset} to check the status of all Helm releases after the upgrade
                "

```

---

## Deploy Components

import Intro from '@site/src/components/Intro';

<Intro>
    Once you're done developing your components and configuring them with stacks, you can deploy them with a single command or in a CI/CD pipeline.
</Intro>

In Atmos, when we talk about "Deployment," it refers to taking the [fully rendered and deep-merged configuration](/core-concepts/describe) of a [stack](/core-concepts/stacks) and provisioning an instance of one of the components. We call this a "component instance," and it's simply a component that has been deployed in a specific stack.

### Deployment in Atmos

Deployment in Atmos can be approached in several ways.

1. **Command Line Deployment**: You can always deploy on the command line using Atmos, which is particularly useful for local development or in environments that are less mature and do not yet have CI/CD capabilities. For more complicated deployments, you can leverage [workflows](/core-concepts/workflows) to orchestrate multiple deployments in a specific order or run other commands, including [custom commands](/core-concepts/custom-commands).

2. **CI/CD Integrations**: Atmos supports several common methods for CI/CD, with [GitHub Actions](/integrations/github-actions) being the recommended method. We maintain and invest the most time and effort into GitHub Actions. However, we also support integrations with [Spacelift](/integrations/spacelift) and [Atlantis](/integrations/atlantis).

### Configuring Dependencies Between Components

When deploying components, it's important to consider the dependencies between components. For example, a database component might depend on a network component. When this happens, it's important to ensure that the network component is deployed before the database component.

Make sure to [configure dependencies](/core-concepts/stacks/dependencies) between components using the `settings.depends_on` section.

### Managing Dependency Order Between Components

Sometimes, components have dependencies on other components. For example, a database component might depend on a network component. When this happens, it's important to ensure that the network component is deployed before the database component.

In Atmos, support of ordered dependencies is reliant on the integration and not all integrations support ordered dependencies.

All configurations in Atmos are defined in YAML. If you can write a Terraform module, you can essentially Terraform anything based on the stack configuration. It's important to be aware of dependencies between your components. Depending on the integration mechanism or deployment approach you choose, handling these dependencies can be either built-in or more manual.

- [GitHub Actions](/integrations/github-actions): Currently, our GitHub Actions do not support dependency order application.
- [Spacelift](/integrations/spacelift): Our Spacelift integrations support dependency order application.
- [Atlantis](/integrations/atlantis): By customizing the template generated for Atlantis, similar dependency handling can probably be achieved, although we do not have any documentation on this.

### Automate Cold Starts

Atmos supports [workflows](/core-concepts/workflows), which provide a convenient way to automate deployments, especially for cold starts. A cold start is when you go from zero to a full deployment, typically occurring on day zero in the life cycle of your resources.

---

## Describe Components

import Intro from '@site/src/components/Intro'
import ActionCard from '@site/src/components/ActionCard'
import PrimaryCTA from '@site/src/components/PrimaryCTA'

<Intro>
Describing components helps understand the final, fully deep-merged configuration for an [Atmos component](/core-concepts/components) in each [stack](/core-concepts/stacks).
</Intro>

The more [DRY a configuration is due to imports](/core-concepts/stacks/imports), the more [derived the configuration is due to inheritance](/core-concepts/stacks/inheritance), the harder it may be to understand what the final component configuration will be.

For example, if we wanted to understand what the final configuration looks like for a "vpc" component running in the "production" stack in the `us-east-2` AWS region, we could do that by calling the [`atmos describe component`](/cli/commands/describe/component) command and view the YAML output:

```shell
atmos describe component vpc -s ue2-prod
```

For more powerful filtering options, consider [describing stacks](/core-concepts/describe/stacks) instead.

The other helpful use-case for describing components and stacks is when developing policies for [validation](/core-concepts/validate) of
[Atmos components](/core-concepts/components) and [Atmos stacks](/core-concepts/stacks). OPA policies can enforce what is or is not permitted. Everything in the output can be validated using policies that you develop.

<ActionCard title="Want to go deeper on this topic?">
    For a deep dive into describing components, refer to the CLI command reference.
    <PrimaryCTA to="/cli/commands/describe/component">Command Reference</PrimaryCTA>
</ActionCard>

---

## Describe Configuration

import DocCardList from '@theme/DocCardList'
import Intro from '@site/src/components/Intro'

<Intro>
Atmos is a framework for defining cloud architectures in YAML. To understand what the fully-deep merged configuration will look like, you can describe it.
</Intro>

In Stacks, you define configurations for all Components, setting up small units of infrastructure like VPCs, Clusters, and Databases. Atmos lets you combine these components into reusable, nestable Stacks using Imports. You can break down everything from simple websites to full-blown multi-account/multi-subscription cloud architectures into components.

In this chapter, you’ll learn to describe all aspects of the fully-deep merged configurations so you can understand what Atmos stacks look like.

<DocCardList/>

---

## Describe Stacks

import Terminal from '@site/src/components/Terminal'
import Intro from '@site/src/components/Intro'
import ActionCard from '@site/src/components/ActionCard'
import PrimaryCTA from '@site/src/components/PrimaryCTA'

<Intro>
Describing stacks is helpful to understand what the final, fully computed and deep-merged configuration of a stack will look like. Use this to slice and dice the Stack configuration to show different information about stacks and component.
</Intro>

For example, if we wanted to understand what the final configuration looks like for the "production" stack, we could do that by calling
the [`atmos describe stacks`](/cli/commands/describe/stacks) command to view the YAML output.

The output can be written to a file by passing the `--file` command-line flag to `atmos` or even formatted as YAML or JSON by using `--format`
command-line flag.

:::tip PRO TIP

If the filtering options built-in to Atmos are not sufficient, redirect the output to [`jq`](https://stedolan.github.io/jq/) for very powerful filtering options.

:::

Since the output of a Stack might be overwhelming, and we're only interested in some particular section of the configuration, the output can be
filtered using flags to narrow the output by `stack`, `component-types`, `components`, and `sections`. The component sections can be further filtered
by `atmos_component`, `atmos_stack`, `atmos_stack_file`, `backend`, `backend_type`, `command`, `component`, `env`, `inheritance`, `metadata`,
`overrides`, `remote_state_backend`, `remote_state_backend_type`, `settings`, `vars`, `workspace`.

For example:

<Terminal title="atmos describe stacks">
```yaml
plat-ue2-dev:
  components:
    terraform:
      vpc:
        backend: {}
        backend_type: s3
        command: terraform
        component: vpc
        env: {}
        inheritance: []
        metadata:
          component: vpc
        overrides: {}
        remote_state_backend: {}
        remote_state_backend_type: ""
        settings:
          validation:
            check-vpc-component-config-with-opa-policy:
              description: Check 'vpc' component configuration using OPA policy
              disabled: false
              module_paths:
                - catalog/constants
              schema_path: vpc/validate-vpc-component.rego
              schema_type: opa
              timeout: 10
            validate-vpc-component-with-jsonschema:
              description: Validate 'vpc' component variables using JSON Schema
              schema_path: vpc/validate-vpc-component.json
              schema_type: jsonschema
        vars:
          availability_zones:
            - us-east-2a
            - us-east-2b
            - us-east-2c
          enabled: true
          environment: ue2
          map_public_ip_on_launch: true
          max_subnet_count: 3
          name: common
          namespace: acme
          nat_gateway_enabled: true
          nat_instance_enabled: false
          region: us-east-2
          stage: dev
          tenant: plat
          vpc_flow_logs_enabled: true
          vpc_flow_logs_log_destination_type: s3
          vpc_flow_logs_traffic_type: ALL
        workspace: plat-ue2-dev
      vpc-flow-logs-bucket:
        backend: {}
        backend_type: s3
        command: terraform
        component: vpc-flow-logs-bucket
        env: {}
        inheritance: []
        metadata:
          component: vpc-flow-logs-bucket
        overrides: {}
        remote_state_backend: {}
        remote_state_backend_type: ""
        settings: {}
        vars:
          enabled: true
          environment: ue2
          force_destroy: true
          lifecycle_rule_enabled: false
          name: vpc-flow-logs
          namespace: acme
          region: us-east-2
          stage: dev
          tenant: plat
          traffic_type: ALL
        workspace: plat-ue2-dev

# Other stacks here
```
</Terminal>

<Terminal title="atmos describe stacks --components vpc --sections metadata --format json">
```json
{
  "plat-ue2-dev": {
    "components": {
      "terraform": {
        "vpc": {
          "metadata": {
            "component": "vpc"
          }
        }
      }
    }
  },
  "plat-ue2-prod": {
    "components": {
      "terraform": {
        "vpc": {
          "metadata": {
            "component": "vpc"
          }
        }
      }
    }
  },
  "plat-ue2-staging": {
    "components": {
      "terraform": {
        "vpc": {
          "metadata": {
            "component": "vpc"
          }
        }
      }
    }
  },
  "plat-uw2-dev": {
    "components": {
      "terraform": {
        "vpc": {
          "metadata": {
            "component": "vpc"
          }
        }
      }
    }
  },
  "plat-uw2-prod": {
    "components": {
      "terraform": {
        "vpc": {
          "metadata": {
            "component": "vpc"
          }
        }
      }
    }
  },
  "plat-uw2-staging": {
    "components": {
      "terraform": {
        "vpc": {
          "metadata": {
            "component": "vpc"
          }
        }
      }
    }
  }
}
```
</Terminal>

<ActionCard title="Want to go deeper on this topic?">
    For a deep dive on describing stacks, refer to the CLI command reference.
    <PrimaryCTA to="/cli/commands/describe/stacks">Command Reference</PrimaryCTA>
</ActionCard>

---

## Configure Atmos CLI

import EmbedFile from '@site/src/components/EmbedFile'
import KeyPoints from '@site/src/components/KeyPoints'
import Screengrab from '@site/src/components/Screengrab'
import Intro from '@site/src/components/Intro'
import ActionCard from '@site/src/components/ActionCard'
import PrimaryCTA from '@site/src/components/PrimaryCTA'

<Intro>
The `atmos.yaml` configuration file is used to control the behavior of the `atmos` CLI for your project. This is how Atmos knows where to find your stack configurations and components. Almost everything in Atmos is configurable via this file.
</Intro>

Because this file is crucial to the configuration of the project, it should live along side of it, with your Terraform components and Atmos stacks. It's also where you can [configure integrations](/integrations), like with our [GitHub Actions](/integrations/github-actions).

<KeyPoints>
- What are the different configuration files in Atmos
- How to configure `atmos.yaml` for your project's filesystem layout
- How Atmos finds the `atmos.yaml` file
- How Atmos identifies stack configurations using context variables and naming patterns
</KeyPoints>

To configure Atmos to work with your project, we'll create a file called `atmos.yaml` to tell Atmos where to find the
Terraform components and Atmos stacks. Almost everything in Atmos is configurable via this file.

## Types of Configuration Files

In Atmos, there are some different types of configuration files to be aware of. The most important one is the `atmos.yaml` file, which is used to configure the behavior of the `atmos` CLI. This file is used to control how Atmos finds your Terraform components and Atmos stacks.

<dl>
    <dt>`atmos.yaml`</dt>
    <dd>CLI configuration for Atmos to find your Terraform components and Atmos stacks. See [vendoring](/cli/configuration).</dd>

    <dt>`vendor.yaml`</dt>
    <dd>
        Vendoring manifest for any third-party dependencies. See [vendoring](/core-concepts/vendor/vendor-manifest).
        __NOTE__: The vendor manifest can import other vendor manifests, allowing you to compose them together.
    </dd>

    <dt>`stacks/**/*.yaml`</dt>
    <dd>
        Vendoring manifest for any third-party dependencies. See [vendoring](/core-concepts/vendor/vendor-manifest).
        __NOTE__: the actual path to the stacks directory is configurable in the `atmos.yaml` file, via the `stacks.base_path` setting.
    </dd>

    <dt>`workflows/**/*.yaml`</dt>
    <dd>
        Workflow definitions. See [workflows](/core-concepts/workflows).
        __NOTE__: the actual path to the workflows directory is configurable in the `atmos.yaml` file, via the `workflows.base_path` setting.
    </dd>

    <dt>`**/components/**/component.yaml`</dt>
    <dd>
        Component manifest for vendoring individual components. See [component manifest](/core-concepts/vendor/component-manifest).
        __NOTE__: the actual path to the components directory is configurable in the `atmos.yaml` file, via the `components.<toolchain>.base_path` setting.
    </dd>

    <dt>`schemas/*.schema.json`</dt>
    <dd>
        JSON Schema for validating Atmos manifests. See [validation](/core-concepts/validate/json-schema).
        __NOTE__, the actual path to the schemas directory is configurable in the `atmos.yaml` file, via the `schemas.atmos.manifest` setting.
    </dd>

    <dt>`schemas/*.rego`</dt>
    <dd>
        OPA Policy for validating Atmos manifests. See [validation](/core-concepts/validate/opa).
    </dd>
</dl>

## Atmos CLI Configuration Schema

Below is the minimum recommended configuration for Atmos to work with Terraform and to configure [Atmos components](/core-concepts/components) and [Atmos stacks](/core-concepts/stacks). Copy this YAML config below into your `atmos.yaml` file.

<EmbedFile filePath="examples/demo-stacks/atmos.yaml" />

__NOTE:__ For a detailed description of all the sections, refer to [CLI Configuration](/cli/configuration).

### Stack Names (Slugs)

Atmos uses “slugs” to refer to stacks, so you don't need to pass multiple arguments to identify a stack or a component in a stack.

It's a deliberate design decision of Atmos to rely strictly on configuration, rather than on file names and directory locations, which can change (and would thereby change your state).

For example, with the command `atmos terraform apply myapp -s dev`, Atmos interprets the slug `dev` using the pattern `{stage}` to locate the correct stack configuration in the stacks directory.

The format of this slug, is determined by one of the following settings.

<dl>
  <dt>`stacks.name_template` (newer format, more powerful)</dt>
  <dd>
    The name template allows you to define a custom Go template to format the stack name. This is useful when you want to use a different naming convention for your stacks.
  </dd>
  <dt>`stacks.name_pattern` (old format, still supported)</dt>
  <dd>
    The name pattern relies strictly on variables (`var.namespace`, `var.tenant`, `var.environment`, `var.stage`)
    to identify the stack.  It does not support any other variables.

    You'll still see this in many of the examples, but we recommend using the newer `name_template` format.
  </dd>
</dl>

### Logging

Atmos provides some simple settings to control how it emits events to standard error. By convention, Atmos uses standard error to communicate all events related to its own processing. We reserve standard output (stdout) for the intended output of the commands that Atmos executes. By following this convention, you can safely pipe the output from Atmos into other commands as part of a pipeline.

<dl>
  <dt>`logs.level`</dt>
  <dd>
    Set to `Info` to see the most helpful logs. You can also set it to `Trace` to see all the logs, which is helpful for debugging.
    Supported options are:
    <dl>
        <dt>`Info` _default_</dt>
        <dd>Emit standard messages that describe what Atmos is doing</dd>

        <dt>`Warn`</dt>
        <dd>Show all messages with a severity of "warning" or less</dd>

        <dt>`Error`</dt>
        <dd>Show all messages with a severity of "error" or less</dd>

        <dt>`Debug`</dt>
        <dd>Emit helpful debugging information, including all other severities. This is very verbose.</dd>

        <dt>`Trace`</dt>
        <dd>Turn off all filters, and just display every single message.</dd>
    </dl>
  </dd>

  <dt>`logs.file`</dt>
  <dd>
    Set to `/dev/stderr` to send all of Atmos output to the standard error stream. This is useful when running Atmos in a CI/CD pipeline.
  </dd>
</dl>

### Command Aliases

If you get tired of typing long commands in Atmos, you can alias them using the `aliases` section. This is especially useful for commands that you run frequently, like Terraform.  Aliases you define appear in the `atmos help`, so you can see them at a glance.

```yaml
# CLI command aliases
aliases:
  # Aliases for Atmos native commands
  tf: terraform
  tp: terraform plan
  up: terraform apply
  down: terraform destroy
  ds: describe stacks
  dc: describe component
  # Aliases for Atmos custom commands
  ls: list stacks
  lc: list components
```

<ActionCard title="Want to go deeper on this topic?">
    Aliases can make Atmos easier to use by allowing you to define shortcuts for frequently used commands.
    <PrimaryCTA to="/cli/configuration#aliases">Learn Aliases</PrimaryCTA>
</ActionCard>

### Path Configuration

Well-known paths are how Atmos knows how to find all your stack configurations, components and workflows. Here are the essential paths that you need to configure:

<dl>
  <dt>`base_path`</dt>
  <dd>The base path for components, stacks, and workflows configurations. We set it to `./` so it will use the current working directory. Alternatively, we can override this behavior by setting the ENV var `ATMOS_BASE_PATH` to point to another directory location.</dd>

  <dt>`components.terraform.base_path`</dt>
  <dd>The base path to the Terraform components (Terraform root modules). As described in [Configure Repository](/quick-start/advanced/configure-repository), we've decided to put the Terraform components into the `components/terraform` directory, and this setting tells Atmos where to find them. Atmos will join the base path (set in the `ATMOS_BASE_PATH` ENV var) with `components.terraform.base_path` to calculate the final path to the Terraform components</dd>

  <dt>`stacks.base_path`</dt>
  <dd>The base path to the Atmos stacks. As described in [Configure Repository](/quick-start/advanced/configure-repository), we've decided to put the stack configurations into the `stacks` directory, and this setting tells Atmos where to find them. Atmos will join the base path (set in the `ATMOS_BASE_PATH` ENV var) with `stacks.base_path` to calculate the final path to the stacks</dd>

  <dt>`stacks.included_paths`</dt>
  <dd>List of file paths to the top-level stacks in the `stacks` directory to include in search when Atmos searches for the stack where the component is defined when executing `atmos` commands</dd>

  <dt>`stacks.excluded_paths`</dt>
  <dd>List of file paths to the top-level stacks in the `stacks` directory to exclude from search when Atmos searches for the stack where the component is defined when executing `atmos` commands</dd>

  <dt>`workflows.base_path`</dt>
  <dd>The base path to Atmos [Workflows](/core-concepts/workflows) files</dd>
</dl>

:::tip Environment variables
Everything in the `atmos.yaml` file can be overridden by environment variables. This is useful for CI/CD pipelines where you might want to control the behavior of Atmos without changing the `atmos.yaml` file.
:::

## Custom Commands

<dl>
  <dt>`commands`</dt>
  <dd>configuration for [Atmos Custom Commands](/core-concepts/custom-commands)</dd>
</dl>

See our many [practical examples](https://github.com/cloudposse/atmos/tree/main/examples) of using Custom Commands in atmos.

<ActionCard title="Want to go deeper on this topic?">
    Custom Commands are a versatile and powerful feature of Atmos. They allow you to extend Atmos’s functionality to meet your specific needs without modifying its core.
    <PrimaryCTA to="/cli/configuration/commands">Learn Custom Commands</PrimaryCTA>
</ActionCard>

## Workflows

Workflows allow you to automate routine operations, such as orchestrating the startup behavior of a series of services. Very little about workflows is configured in the `atmos.yaml`. Only the base path to the workflows is defined here. The workflows themselves are defined in the `workflows.base_path` folder.

<ActionCard title="Want to go deeper on this topic?">
    Workflows allow you to orchestrate your components or any command. Unlike Custom Commands, Workflows focus on orchestration and are reentrant, allowing you to start at any step in the workflow.
    <PrimaryCTA to="/core-concepts/workflows">Learn Workflows</PrimaryCTA>
</ActionCard>

## Schema Validation

<dl>
  <dt>`schemas`</dt>
  <dd>
  [JSON Schema](https://json-schema.org/) and [OPA Policy](https://www.openpolicyagent.org/) configurations for:
  - [Atmos Manifests Validation](/cli/schemas)
  - [Atmos Stack Validation](/core-concepts/validate)
  </dd>
</dl>

## Atmos Search Paths

Atmos searches for the `atmos.yaml` file in several locations, stopping at the first successful match. The search order (from highest to lowest priority) is:

- Environment variable `ATMOS_CLI_CONFIG_PATH`
- Current working directory
- Home dir (`~/.atmos/atmos.yaml`)
- System dir (`/usr/local/etc/atmos/atmos.yaml` on Linux, `%LOCALAPPDATA%/atmos/atmos.yaml` on Windows)

Initial Atmos configuration can be controlled by these environment variables:

<dl>
  <dt>`ATMOS_CLI_CONFIG_PATH`</dt>
  <dd>Directory that contains the `atmos.yaml` (just the folder without the file name). It's not possible to change the filename at this time.</dd>

  <dt>`ATMOS_BASE_PATH`</dt>
  <dd>Base path to the `components/` and `stacks/` folders.</dd>
</dl>

## Special Considerations for Terraform Components

If you are relying on Atmos discovering the `atmos.yaml` based on your current working directory (e.g. at the root of repository), it will work for the `atmos` CLI; however, it will **not work** for [Component Remote State](/core-concepts/share-data/remote-state) because it uses the [terraform-provider-utils](https://github.com/cloudposse/terraform-provider-utils) Terraform provider.

This is because Terraform executes provider from the component's folder (e.g. `components/terraform/vpc`), so it will no longer find the file in the root of the repository, since the working directory has changed.

Both the `atmos` CLI and [terraform-provider-utils](https://github.com/cloudposse/terraform-provider-utils) Terraform provider use the same `Go` code, which try to locate the [CLI config](/cli/configuration) `atmos.yaml` file before parsing and processing [Atmos stacks](/core-concepts/stacks).

This means that `atmos.yaml` file must be at a location in the file system where all processes can find it, such as by explicitly specifying the path in the `ATMOS_CLI_CONFIG_PATH` environment variable.

<ActionCard title="Want to go deeper on this topic?">
    For a deep-dive on configuring the Atmos CLI and all of the sections of the `atmos.yaml`, refer to CLI Configuration.
    <PrimaryCTA to="/cli/configuration">Advanced CLI Configuration</PrimaryCTA>
</ActionCard>

---

## Configure Helmfile

import Intro from '@site/src/components/Intro'
import File from '@site/src/components/File'

<Intro>
Atmos natively supports opinionated workflows for Helmfile. It's compatible with every version of helmfile and designed to work with multiple different versions of Helmfile concurrently.
</Intro>

Keep in mind that Atmos does not handle the downloading or installation of Helmfile (or its dependency Kustomize); it assumes these commands are already installed on your system. For installation instructions, refer to:
- [Helmfile Installation Guide](https://helmfile.readthedocs.io/en/latest/#installation)
- [Kustomize Installation Guide](https://kubectl.docs.kubernetes.io/installation/kustomize/)

To automate the installation process, consider creating a [Custom Command](/core-concepts/custom-commands).

Atmos provides many settings that are specific to Helmfile, which are configured in `atmos.yaml`.

## CLI Configuration

All of the following settings are defined by default in the [Atmos CLI Configuration](/cli/configuration) found in `atmos.yaml`.

:::important
At this time, these settings cannot be overridden in the [Stack](/core-concepts/stacks/#schema) configuration.
:::

The defaults for everything are defined underneath the `components.helmfile` section.

```yaml
components:
  helmfile:
    # ...
```

The following settings are available for Helmfile:
<dl>
  <dt>`components.helmfile.command`</dt>
  <dd>The executable to be called by Atmos when running Helmfile commands</dd>

  <dt>`base_path`</dt>
  <dd>The root directory where the Helmfile components and configurations are located. This path serves as the starting point for resolving any relative paths within the Helmfile setup.</dd>

  <dt>`use_eks` (default: `false`)</dt>
  <dd>A flag indicating whether the component is configured to use Amazon EKS (Elastic Kubernetes Service). When set to `true`, the component will interact with EKS for provisioning and managing Kubernetes clusters. Also, it means `cluster_name_pattern` must be defined.</dd>

  <dt>`kubeconfig_path`</dt>
  <dd>The file path to the `kubeconfig` file, which contains the necessary authentication and configuration details to interact with the Kubernetes cluster. This path is essential for managing cluster resources using Helmfile.</dd>

  <dt>`helm_aws_profile_pattern`</dt>
  <dd>A pattern that defines which AWS CLI profiles should be used by Helm when interacting with AWS services, such as EKS. This allows for dynamic selection of AWS credentials based on the environment or cluster.</dd>

  <dt>`cluster_name_pattern` (required when `use_eks=true`)</dt>
  <dd>A naming pattern used to identify and select the Kubernetes cluster within the Helmfile configuration. This pattern helps automate the management of different clusters by matching their names based on the specified criteria.</dd>

</dl>

## Example Configuration

Here is an example configuration for Helmfile that we use at Cloud Posse in our [refarch for AWS](https://docs.cloudposse.com/).

<File title="atmos.yaml">
```yaml
components:
  helmfile:
    base_path: components/helmfile
    use_eks: true
    kubeconfig_path: /dev/shm
    helm_aws_profile_pattern: '{namespace}-{tenant}-gbl-{stage}-helm'
    cluster_name_pattern: '{namespace}-{tenant}-{environment}-{stage}-eks-cluster'
```
</File>

---

## Configure OpenTofu

import useBaseUrl from '@docusaurus/useBaseUrl';
import KeyPoints from '@site/src/components/KeyPoints'
import Intro from '@site/src/components/Intro'

<Intro>
Atmos natively supports [OpenTofu](https://opentofu.org), similar to the way it supports [Terraform](/core-concepts/projects/configuration/terraform). It's compatible with every version of `opentofu` and designed to work with multiple different versions of it concurrently, and can even work alongside with [HashiCorp Terraform](/core-concepts/projects/configuration/terraform).

</Intro>

<KeyPoints>
- How to configure Atmos to use OpenTofu for Terraform components
- How to alias `terraform` to `tofu` in Atmos
- How to configure OpenTofu for only specific components
</KeyPoints>

Please see the complete configuration options for [Terraform](/core-concepts/projects/configuration/terraform), as they are the same for OpenTofu. We'll focus
only on what's different in this document, in order to utilize OpenTofu. Keep in mind that Atmos does not handle the downloading or installation
of OpenTofu; it assumes that any required binaries for the commands are already installed on your system.

Additionally, if using Spacelift together with Atmos, make sure you review the [Spacelift Integration](/integrations/spacelift) to make any necessary changes.

## CLI Configuration

All the default configuration settings to support OpenTofu are defined in the [Atmos CLI Configuration](/cli/configuration),
but can also be overridden at any level of the [Stack](/core-concepts/stacks/#schema) configuration.

```yaml
components:
  terraform:
    # The executable to be called by `atmos` when running Terraform commands
    command: "/usr/bin/tofu"  # or just `tofu`
    # Can also be set using 'ATMOS_COMPONENTS_TERRAFORM_BASE_PATH' ENV var, or '--terraform-dir' command-line argument
    # Supports both absolute and relative paths
    base_path: "components/tofu"
    # Can also be set using 'ATMOS_COMPONENTS_TERRAFORM_APPLY_AUTO_APPROVE' ENV var
    apply_auto_approve: false
    # Can also be set using 'ATMOS_COMPONENTS_TERRAFORM_DEPLOY_RUN_INIT' ENV var, or '--deploy-run-init' command-line argument
    deploy_run_init: true
    # Can also be set using 'ATMOS_COMPONENTS_TERRAFORM_INIT_RUN_RECONFIGURE' ENV var, or '--init-run-reconfigure' command-line argument
    init_run_reconfigure: true
    # Can also be set using 'ATMOS_COMPONENTS_TERRAFORM_AUTO_GENERATE_BACKEND_FILE' ENV var, or '--auto-generate-backend-file' command-line argument
    auto_generate_backend_file: false
    # Can also be set using 'ATMOS_COMPONENTS_TERRAFORM_APPEND_USER_AGENT' ENV var, or '--append-user-agent' command-line argument
    append_user_agent: "Acme/1.0 (Build 1234; arm64)"
    init:
      # Can also be set using 'ATMOS_COMPONENTS_TERRAFORM_INIT_PASS_VARS' ENV var, or '--init-pass-vars' command-line argument
      pass_vars: true
```

<dl>
    <dt>`components.terraform.command`</dt>
    <dd>The executable to be called by Atmos when running OpenTofu commands</dd>

    <dt>`components.terraform.base_path`</dt>
    <dd>The root directory where the OpenTofu components and configurations are located. This path serves as the starting point for resolving any relative paths within the OpenTofu setup.</dd>

    <dt>`components.terraform.apply_auto_approve`</dt>
    <dd>if set to `true`, Atmos automatically adds the `-auto-approve` option to instruct Terraform to apply the plan without
        asking for confirmation when executing `terraform apply` command</dd>

    <dt>`components.terraform.deploy_run_init`</dt>
    <dd>if set to `true`, Atmos runs `terraform init` before executing [`atmos terraform deploy`](/cli/commands/terraform/deploy) command</dd>

    <dt>`components.terraform.init_run_reconfigure`</dt>
    <dd>if set to `true`, Atmos automatically adds the `-reconfigure` option to update the backend configuration when executing `terraform init` command</dd>

    <dt>`components.terraform.auto_generate_backend_file`</dt>
    <dd>if set to `true`, Atmos automatically generates the Terraform backend file from the component configuration when executing `terraform plan` and `terraform apply` commands</dd>

    <dt>`components.terraform.init.pass_vars`</dt>
    <dd>
        if set to `true`, Atmos automatically passes the generated varfile to the `tofu init` command using the `--var-file` flag.
        [OpenTofu supports passing a varfile to `init`](https://opentofu.org/docs/cli/commands/init/#general-options) to dynamically configure backends
    </dd>
</dl>

To make OpenTofu the default command when running "terraform", modify [`atmos.yaml`](/cli/configuration) to configure the following global settings:

```yaml
components:
  terraform:
    # Use the `tofu` command when calling "terraform" in Atmos.
    command: "/usr/bin/tofu"  # or just `tofu`

    # Optionally, specify a different path for OpenTofu components
    base_path: "components/tofu"
```

:::important Disambiguation
Atmos consistently utilizes the `terraform` keyword across all configurations, rather than `tofu` or `opentofu`.
The term “Terraform” is used in this documentation to refer to generic concepts such as providers, modules, stacks, the
HCL-based domain-specific language and its interpreter.
:::

Additionally, if you prefer to run `atmos tofu` instead of `atmos terraform`, you can configure an alias.
Just add the following configuration somewhere in the `atmos.yaml` CLI config file:

```yaml
aliases:
  tofu: terraform
```

:::important
Creating aliases for `tofu` only changes the CLI invocation of `atmos terraform` and does not directly
influence the actual command that atmos executes when running Terraform. Atmos strictly adheres to the
specific `command` set in the Stack configurations.
:::

## Stack Configuration for Components

Settings for Terraform or OpenTofu can also be specified in stack configurations, where they are compatible with inheritance.
This feature allows projects to tailor behavior according to individual component needs.

While defaults for everything are defined in the `atmos.yaml`, the same settings, can be overridden by Stack configurations at any level:

- `terraform`
- `components.terraform`
- `components.terraform._component_`

For instance, you can modify the command executed for a specific component by overriding the `command` parameter.
This flexibility is particularly valuable for gradually transitioning to OpenTofu or managing components that are
compatible only with HashiCorp Terraform.

```yaml
components:
  terraform:
    vpc:
      command: "/usr/local/bin/tofu-1.7"
```

## Example: Provision a Terraform Component with OpenTofu

:::note
In the following examples, we'll assume that `tofu` is an Atmos alias for the `terraform` command.

```yaml
aliases:
  tofu: terraform
```

:::

Once you've configured Atmos to utilize `tofu` — either by adjusting the default `terraform.command` in the `atmos.yaml`
or by specifying the `command` for an individual component — provisioning any component follows the same procedure as
you would typically use for Terraform.

For example, to provision a Terraform component using OpenTofu, run the following commands:

```console
atmos tofu plan eks --stack=ue2-dev
atmos tofu apply eks --stack=ue2-dev
```

where:

- `eks` is the Terraform component to provision (from the `components/terraform` folder)
- `--stack=ue2-dev` is the stack to provision the component into

Short versions of all command-line arguments can be used:

```console
atmos tofu plan eks -s ue2-dev
atmos tofu apply eks -s ue2-dev
```

---

## Configure Packer

import useBaseUrl from '@docusaurus/useBaseUrl';
import KeyPoints from '@site/src/components/KeyPoints'
import Intro from '@site/src/components/Intro'
import File from '@site/src/components/File'
import Terminal from '@site/src/components/Terminal'

<Intro>
Atmos natively supports [HashiCorp Packer](https://developer.hashicorp.com/packer) and lets you create identical
machine images for multiple platforms from a single source template using the power of Atmos components,
stacks, imports, inheritance, templating and YAML functions.
It's compatible with every version of Packer and designed to work with multiple different versions of it concurrently.

</Intro>

<KeyPoints>
- How to configure Atmos to use Packer to build machine images
- Example Packer and Atmos configurations to build an AWS bastion AMI from an Amazon Linux 2023 base image
</KeyPoints>

Keep in mind that Atmos does not handle the downloading or installation
of Packer; it assumes that any required binaries for the commands are already installed on your system.

## CLI Configuration (`atmos.yaml`)

<File title="atmos.yaml">
```yaml
components:
  packer:
    # The executable to be called by Atmos when running Packer commands
    command: "packer"  # or `/usr/bin/packer`
    # Can also be set using 'ATMOS_COMPONENTS_PACKER_BASE_PATH' ENV var, or '--packer-dir' command-line argument
    # Supports both absolute and relative paths
    base_path: "components/packer"
```
</File>

<dl>
    <dt>`components.packer.command`</dt>
    <dd>The executable to be called by Atmos when running Packer commands</dd>

    <dt>`components.packer.base_path`</dt>
    <dd>The root directory where the Packer components and configurations are located. This path serves as the starting point for resolving any relative paths within the Packer setup.</dd>
</dl>

## Stack Configuration for Components

Settings for Packer can also be specified in Atmos stack configurations, where they are compatible with inheritance.
This feature allows projects to tailor behavior according to individual component needs.

While defaults for everything are defined in the `atmos.yaml`, the same settings can be overridden by Stack configurations at any level:

- `packer`
- `components.packer`
- `components.packer._component_`

For instance, you can modify the command executed for a specific component by overriding the `command` parameter.

<File title="atmos.yaml">
```yaml
components:
  packer:
    bastion:
      # Use Packer v1.14.1 to provision the `bastion` component
      command: "/usr/local/bin/packer-1.14.1"
```
</File>

## Example: Configure and Provision a Packer Component with Atmos

### Configure Packer in `atmos.yaml`

<File title="atmos.yaml">
```yaml
base_path: "./"

components:
  packer:
    # Can also be set using 'ATMOS_COMPONENTS_PACKER_COMMAND' ENV var, or '--packer-command' command-line argument
    command: packer
    # Can also be set using 'ATMOS_COMPONENTS_PACKER_BASE_PATH' ENV var, or '--packer-dir' command-line argument
    base_path: "components/packer"

stacks:
  base_path: "stacks"
  included_paths:
    - "deploy/**/*"
  excluded_paths:
    - "**/_defaults.yaml"
  name_template: "{{ .vars.stage }}"

logs:
  file: "/dev/stderr"
  level: Info

# `Go` templates in Atmos manifests
# https://atmos.tools/core-concepts/stacks/templates
templates:
  settings:
    enabled: true
    evaluations: 1
    # https://masterminds.github.io/sprig
    sprig:
      enabled: true
    # https://docs.gomplate.ca
    gomplate:
      enabled: true
      timeout: 10
      # https://docs.gomplate.ca/datasources
      datasources: {}
```
</File>

### Add Packer template (Packer component)

<File title="components/packer/aws/bastion/main.pkr.hcl">
```hcl
# https://developer.hashicorp.com/packer/docs/templates/hcl_templates/blocks/source
# https://developer.hashicorp.com/packer/integrations/hashicorp/amazon/latest/components/builder/ebs
# https://developer.hashicorp.com/packer/integrations/hashicorp/amazon
# https://developer.hashicorp.com/packer/integrations/hashicorp/amazon#authentication
# https://developer.hashicorp.com/packer/tutorials/docker-get-started/docker-get-started-post-processors
# https://developer.hashicorp.com/packer/tutorials/aws-get-started

packer {
  required_plugins {
    # https://developer.hashicorp.com/packer/integrations/hashicorp/amazon
    amazon = {
      source  = "github.com/hashicorp/amazon"
      version = "~> 1"
    }
  }
}

variable "region" {
  type        = string
  description = "AWS Region"
}

variable "stage" {
  type    = string
  default = null
}

variable "ami_org_arns" {
  type        = list(string)
  description = "List of Amazon Resource Names (ARN) of AWS Organizations that have access to launch the resulting AMI(s). By default no organizations have permission to launch the AMI"
  default     = []
}

variable "ami_ou_arns" {
  type        = list(string)
  description = "List of Amazon Resource Names (ARN) of AWS Organizations organizational units (OU) that have access to launch the resulting AMI(s). By default no organizational units have permission to launch the AMI."
  default     = []
}

variable "ami_users" {
  type        = list(string)
  description = "List of account IDs that have access to launch the resulting AMI(s). By default no additional users other than the user creating the AMI has permissions to launch it."
  default     = []
}

variable "kms_key_arn" {
  type        = string
  description = "KMS Key ARN"
}

variable "instance_type" {
  type        = string
  description = "Instance type"
}

variable "volume_size" {
  type        = number
  description = "Volume size"
}

variable "volume_type" {
  type        = string
  description = "Volume type"
}

variable "ami_name" {
  type        = string
  description = "AMI name"
}

variable "source_ami" {
  type        = string
  description = "Source AMI"
}

variable "ssh_username" {
  type        = string
  description = "Instance type"
}

variable "encrypt_boot" {
  type        = bool
  description = "Encrypt boot"
}

variable "skip_create_ami" {
  type        = bool
  description = "If true, Packer will not create the AMI. Useful for setting to true during a build test stage"
}

variable "ami_tags" {
  type = map(string)
  description = "AMI tags"
}

# https://developer.hashicorp.com/packer/integrations/hashicorp/amazon#authentication
variable "assume_role_arn" {
  type        = string
  description = "Amazon Resource Name (ARN) of the IAM Role to assume. Refer to https://developer.hashicorp.com/packer/integrations/hashicorp/amazon#authentication"
}

variable "assume_role_session_name" {
  type        = string
  description = "Assume role session name"
}

variable "assume_role_duration_seconds" {
  type        = number
  description = "Assume role duration seconds"
}

variable "manifest_file_name" {
  type        = string
  description = "Manifest file name. Refer to https://developer.hashicorp.com/packer/docs/post-processors/manifest"
}

variable "manifest_strip_path" {
  type        = bool
  description = "Manifest strip path. Refer to https://developer.hashicorp.com/packer/docs/post-processors/manifest"
}

variable "associate_public_ip_address" {
  type        = bool
  description = "If this is `true`, the new instance will get a Public IP"
}

variable "provisioner_shell_commands" {
  type        = list(string)
  description = "List of commands to execute on the machine that Packer builds"
  default     = []
}

variable "force_deregister" {
  type        = bool
  description = "Force Packer to first deregister an existing AMI if one with the same name already exists"
  default     = false
}

variable "force_delete_snapshot" {
  type        = bool
  description = "Force Packer to delete snapshots associated with AMIs, which have been deregistered by `force_deregister`"
  default     = false
}

source "amazon-ebs" "al2023" {
  ami_name      = var.ami_name
  source_ami    = var.source_ami
  instance_type = var.instance_type
  region        = var.region
  ssh_username  = var.ssh_username
  ami_org_arns  = var.ami_org_arns
  ami_ou_arns   = var.ami_ou_arns
  ami_users     = var.ami_users
  kms_key_id    = var.kms_key_arn
  encrypt_boot  = var.encrypt_boot

  force_deregister      = var.force_deregister
  force_delete_snapshot = var.force_delete_snapshot

  associate_public_ip_address = var.associate_public_ip_address

  ami_block_device_mappings {
    device_name           = "/dev/xvda"
    volume_size           = var.volume_size
    volume_type           = var.volume_type
    delete_on_termination = true
  }

  assume_role {
    role_arn         = var.assume_role_arn
    session_name     = var.assume_role_session_name
    duration_seconds = var.assume_role_duration_seconds
  }

  aws_polling {
    delay_seconds = 5
    max_attempts  = 100
  }

  tags = var.ami_tags
}

build {
  sources = ["source.amazon-ebs.al2023"]

  provisioner "shell" {
    inline = var.provisioner_shell_commands
  }

  # https://developer.hashicorp.com/packer/tutorials/docker-get-started/docker-get-started-post-processors
  # https://developer.hashicorp.com/packer/docs/post-processors
  # https://developer.hashicorp.com/packer/docs/post-processors/manifest
  post-processor "manifest" {
    output     = var.manifest_file_name
    strip_path = var.manifest_strip_path
  }
}
```
</File>

### Configure defaults for the Packer component in the `catalog`

<File title="stacks/catalog/aws/bastion/defaults.yaml">
```yaml
# yaml-language-server: $schema=https://atmos.tools/schemas/atmos/atmos-manifest/1.0/atmos-manifest.json

components:
  packer:
    aws/bastion:
      settings:
        packer:
          template: "main.pkr.hcl"
          source_ami: "ami-0013ceeff668b979b"
          source_ami_name: "al2023-ami-2023.7.20250527.1-kernel-6.12-arm64"
          source_ami_description: "Amazon Linux 2023 AMI 2023.7.20250527.1 arm64 HVM kernel-6.12"
          source_ami_owner_account_id: "137112412989"
          region: "us-east-2"
          org_id: "o-xxxxxxxxx"
          org_management_account_id: "xxxxxxxxxxxx"
      metadata:
        component: aws/bastion
      vars:
        # https://masterminds.github.io/sprig/date.html
        ami_name: "bastion-al2023-{{ now | unixEpoch }}"
        source_ami: "{{ .settings.packer.source_ami }}"
        region: "{{ .settings.packer.region }}"
        ami_org_arns:
          - "arn:aws:organizations::{{ .settings.packer.org_management_account_id }}:organization/{{ .settings.packer.org_id }}"
        ami_ou_arns: []
        ami_users: []
        kms_key_arn: null
        encrypt_boot: false
        ssh_username: "ec2-user"
        associate_public_ip_address: true
        volume_type: "gp3"
        skip_create_ami: false
        manifest_file_name: "manifest.json"
        manifest_strip_path: false
        assume_role_session_name: "atmos-packer"
        assume_role_duration_seconds: 1800
        force_deregister: false
        force_delete_snapshot: false
        # SSM Agent is pre-installed on AL2023 AMIs but should be enabled explicitly as done above.
        # `dnf clean all` removes cached metadata and packages to reduce AMI size.
        # `cloud-init clean` ensures the image will boot as a new instance on the next launch.
        provisioner_shell_commands:
          # Enable and start the SSM agent (already installed by default on AL2023)
          - "sudo systemctl enable --now amazon-ssm-agent"
          # Install packages, clean metadata and cloud-init
          - "sudo -E bash -c 'dnf install -y jq && dnf clean all && cloud-init clean'"
          # Install other packages
        ami_tags:
          SourceAMI: "{{ .settings.packer.source_ami }}"
          SourceAMIName: "{{ .settings.packer.source_ami_name }}"
          SourceAMIDescription: "{{ .settings.packer.source_ami_description }}"
          SourceAMIOwnerAccountId: "{{ .settings.packer.source_ami_owner_account_id }}"
          ScanStatus: pending
```
</File>

### Define Atmos `nonprod` and `prod` stacks

<File title="stacks/deploy/nonprod.yaml">
```yaml
# yaml-language-server: $schema=https://atmos.tools/schemas/atmos/atmos-manifest/1.0/atmos-manifest.json

vars:
  stage: nonprod

import:
  - catalog/aws/bastion/defaults

components:
  packer:
    aws/bastion:
      vars:
        # Define the variables specific to the `nonprod` account
        instance_type: "t4g.small"
        volume_size: 8
        assume_role_arn: "arn:aws:iam::NONPROD_ACCOUNT_ID:role/ROLE_NAME"
        ami_tags:
          Stage: nonprod
```
</File>

<File title="stacks/deploy/prod.yaml">
```yaml
# yaml-language-server: $schema=https://atmos.tools/schemas/atmos/atmos-manifest/1.0/atmos-manifest.json

vars:
  stage: prod

import:
  - catalog/aws/bastion/defaults

components:
  packer:
    aws/bastion:
      vars:
        # Define the variables specific to the `prod` account
        instance_type: "t4g.medium"
        volume_size: 16
        assume_role_arn: "arn:aws:iam::PROD_ACCOUNT_ID:role/ROLE_NAME"
        ami_tags:
          Stage: prod
```
</File>

### Execute Atmos Packer commands

<Terminal title="atmos packer version">
```shell
> atmos packer version

Packer v1.14.1
```
</Terminal>

<Terminal title="atmos packer validate aws/bastion -s nonprod">
```shell
# https://developer.hashicorp.com/packer/docs/commands/validate

> atmos packer validate aws/bastion -s nonprod

The configuration is valid.
```
</Terminal>

<Terminal title="atmos packer inspect aws/bastion -s nonprod">
```shell
# https://developer.hashicorp.com/packer/docs/commands/inspect

> atmos packer inspect aws/bastion -s nonprod

Packer Inspect: HCL2 mode

> input-variables:

var.ami_name: "bastion-al2023-1754457104"
var.ami_org_arns: "[\n  \"arn:aws:organizations::xxxxxxxxxxxx:organization/o-xxxxxxxxx\",\n]"
var.ami_ou_arns: "[]"
var.ami_tags: "{\n  \"ScanStatus\" = \"pending\"\n  \"SourceAMI\" = \"ami-0013ceeff668b979b\"\n  \"SourceAMIDescription\" = \"Amazon Linux 2023 AMI 2023.7.20250527.1 arm64 HVM kernel-6.12\"\n  \"SourceAMIName\" = \"al2023-ami-2023.7.20250527.1-kernel-6.12-arm64\"\n  \"SourceAMIOwnerAccountId\" = \"137112412989\"\n  \"Stage\" = \"nonprod\"\n}"
var.ami_users: "[]"
var.associate_public_ip_address: "true"
var.assume_role_arn: "null"
var.assume_role_duration_seconds: "1800"
var.assume_role_session_name: "atmos-packer"
var.encrypt_boot: "false"
var.force_delete_snapshot: "false"
var.force_deregister: "false"
var.instance_type: "t4g.small"
var.kms_key_arn: "null"
var.manifest_file_name: "manifest.json"
var.manifest_strip_path: "false"
var.provisioner_shell_commands: "[\n  \"sudo systemctl enable --now amazon-ssm-agent\",\n  \"sudo -E bash -c 'dnf install -y jq && dnf clean all && cloud-init clean'\",\n]"
var.region: "us-east-2"
var.skip_create_ami: "false"
var.source_ami: "ami-0013ceeff668b979b"
var.ssh_username: "ec2-user"
var.stage: "nonprod"
var.volume_size: "8"
var.volume_type: "gp3"

> local-variables:

> builds:

  > <0>:
    sources:
      amazon-ebs.al2023

    provisioners:
      shell

    post-processors:
      0:
        manifest
```
</Terminal>

<Terminal title="atmos packer init aws/bastion -s nonprod">
```shell
# https://developer.hashicorp.com/packer/docs/commands/init

> atmos packer init aws/bastion -s nonprod

Installed plugin github.com/hashicorp/amazon v1.3.9 in "~/.config/packer/plugins/github.com/hashicorp/amazon/packer-plugin-amazon_v1.3.9_x5.0_darwin_arm64"
```
</Terminal>

<Terminal title="atmos packer build aws/bastion -s nonprod">
```shell
# https://developer.hashicorp.com/packer/docs/commands/build

> atmos packer build aws/bastion -s nonprod

amazon-ebs.al2023:

==> amazon-ebs.al2023: Prevalidating any provided VPC information
==> amazon-ebs.al2023: Prevalidating AMI Name: bastion-al2023-1754025080
==> amazon-ebs.al2023: Found Image ID: ami-0013ceeff668b979b
==> amazon-ebs.al2023: Setting public IP address to true on instance without a subnet ID
==> amazon-ebs.al2023: No VPC ID provided, Packer will use the default VPC
==> amazon-ebs.al2023: Inferring subnet from the selected VPC "vpc-xxxxxxx"
==> amazon-ebs.al2023: Set subnet as "subnet-xxxxxxx"
==> amazon-ebs.al2023: Creating temporary keypair: packer_688c4c79-f14a-b77e-ca1e-b5b4c17b4581
==> amazon-ebs.al2023: Creating temporary security group for this instance: packer_688c4c7b-3f16-69f9-0c39-88a3fcbe94fd
==> amazon-ebs.al2023: Authorizing access to port 22 from [0.0.0.0/0] in the temporary security groups...
==> amazon-ebs.al2023: Launching a source AWS instance...
==> amazon-ebs.al2023: changing public IP address config to true for instance on subnet "subnet-xxxxxxx"
==> amazon-ebs.al2023: Instance ID: i-0b621ca091aa4c240
==> amazon-ebs.al2023: Waiting for instance (i-0b621ca091aa4c240) to become ready...
==> amazon-ebs.al2023: Using SSH communicator to connect: 18.222.63.67
==> amazon-ebs.al2023: Waiting for SSH to become available...
==> amazon-ebs.al2023: Connected to SSH!
==> amazon-ebs.al2023: Provisioning with shell script: /var/folders/rt/fqmt0tmx3fs1qfzbf3qxxq700000gn/T/packer-shell653292668
==> amazon-ebs.al2023: Waiting for process with pid 2085 to finish.
==> amazon-ebs.al2023: Amazon Linux 2023 Kernel Livepatch repository   154 kB/s |  16 kB     00:00
==> amazon-ebs.al2023: Package jq-1.7.1-49.amzn2023.0.2.aarch64 is already installed.
==> amazon-ebs.al2023: Dependencies resolved.
==> amazon-ebs.al2023: Nothing to do.
==> amazon-ebs.al2023: Complete!
==> amazon-ebs.al2023: 17 files removed
==> amazon-ebs.al2023: Stopping the source instance...
==> amazon-ebs.al2023: Stopping instance
==> amazon-ebs.al2023: Waiting for the instance to stop...
==> amazon-ebs.al2023: Creating AMI bastion-al2023-1754025080 from instance i-0b621ca091aa4c240
==> amazon-ebs.al2023: Attaching run tags to AMI...
==> amazon-ebs.al2023: AMI: ami-0b2b3b68aa3c5ada8
==> amazon-ebs.al2023: Waiting for AMI to become ready...
==> amazon-ebs.al2023: Skipping Enable AMI deprecation...
==> amazon-ebs.al2023: Skipping Enable AMI deregistration protection...
==> amazon-ebs.al2023: Modifying attributes on AMI (ami-0b2b3b68aa3c5ada8)...
==> amazon-ebs.al2023: Modifying: ami org arns
==> amazon-ebs.al2023: Modifying attributes on snapshot (snap-09ad35550e1438fb2)...
==> amazon-ebs.al2023: Adding tags to AMI (ami-0b2b3b68aa3c5ada8)...
==> amazon-ebs.al2023: Tagging snapshot: snap-09ad35550e1438fb2
==> amazon-ebs.al2023: Creating AMI tags
==> amazon-ebs.al2023: Adding tag: "Stage": "nonprod"
==> amazon-ebs.al2023: Adding tag: "ScanStatus": "pending"
==> amazon-ebs.al2023: Adding tag: "SourceAMI": "ami-0013ceeff668b979b"
==> amazon-ebs.al2023: Adding tag: "SourceAMIDescription": "Amazon Linux 2023 AMI 2023.7.20250527.1 arm64 HVM kernel-6.12"
==> amazon-ebs.al2023: Adding tag: "SourceAMIName": "al2023-ami-2023.7.20250527.1-kernel-6.12-arm64"
==> amazon-ebs.al2023: Adding tag: "SourceAMIOwnerAccountId": "137112412989"
==> amazon-ebs.al2023: Creating snapshot tags
==> amazon-ebs.al2023: Terminating the source AWS instance...
==> amazon-ebs.al2023: Cleaning up any extra volumes...
==> amazon-ebs.al2023: No volumes to clean up, skipping
==> amazon-ebs.al2023: Deleting temporary security group...
==> amazon-ebs.al2023: Deleting temporary keypair...
==> amazon-ebs.al2023: Running post-processor:  (type manifest)
Build 'amazon-ebs.al2023' finished after 3 minutes 39 seconds.

==> Wait completed after 3 minutes 39 seconds

==> Builds finished. The artifacts of successful builds are:
--> amazon-ebs.al2023: AMIs were created:
us-east-2: ami-0b2b3b68aa3c5ada8

--> amazon-ebs.al2023: AMIs were created:
us-east-2: ami-0b2b3b68aa3c5ada8
```
</Terminal>

<Terminal title="atmos packer output aws/bastion -s nonprod">
```shell
# `atmos packer output` command is specific to Atmos (Packer itself does not have an `output` command)
# The command is used to get an output from a Packer manifest
# The manifest is generated by Packer when executing a `packer build` command

> atmos packer output aws/bastion -s nonprod

builds:
  - artifact_id: us-east-2:ami-0c2ca16b7fcac7529
    build_time: 1.753281956e+09
    builder_type: amazon-ebs
    custom_data: null
    files: null
    name: al2023
    packer_run_uuid: 5114a723-92f6-060f-bae4-3ac2d0324557
  - artifact_id: us-east-2:ami-0b2b3b68aa3c5ada8
    build_time: 1.7540253e+09
    builder_type: amazon-ebs
    custom_data: null
    files: null
    name: al2023
    packer_run_uuid: a57874d1-c478-63d7-cfde-9d91e513eb9a
last_run_uuid: a57874d1-c478-63d7-cfde-9d91e513eb9a
```
</Terminal>

<Terminal title="atmos packer output aws/bastion -s nonprod --query '.builds[0].artifact_id'">
```shell
# `atmos packer output` command is specific to Atmos (Packer itself does not have an `output` command)
# The command is used to get an output from a Packer manifest
# The manifest is generated by Packer when executing a `packer build` command

# Use a YQ expression to get a specific section or attribute from the Packer manifest,
# in this case, the `artifact_id` from the first build.

> atmos packer output aws/bastion -s nonprod --query '.builds[0].artifact_id'

us-east-2:ami-0c2ca16b7fcac7529
```
</Terminal>

<Terminal title="atmos packer output aws/bastion -s nonprod -q '.builds[0].artifact_id | split(:)[1]'">
```shell
# `atmos packer output` command is specific to Atmos (Packer itself does not have an `output` command).
# The command is used to get an output from a Packer manifest.
# The manifest is generated by Packer when executing a `packer build` command.

# Use a YQ expression to get a specific section or attribute from the Packer manifest,
# in this case, the AMI (second part after the `:`) from the `artifact_id` from the first build.

> atmos packer output aws/bastion -s nonprod -q '.builds[0].artifact_id | split(":")[1]'

ami-0c2ca16b7fcac7529
```
</Terminal>

---

## Configure Stores

import Intro from '@site/src/components/Intro'

<Intro>
Atmos supports the concept of remote stores to facilitate the sharing of values between components or between
some external process and a component. In Atmos, values are saved to stores via
[hooks](/core-concepts/stacks/hooks) and are read using the [`!store`](/functions/yaml/store)
YAML function and [`atmos.Store`](/functions/template/atmos.Store) template function.
Values can also be saved to stores from outside of Atmos, for example, from a CI/CD pipeline or a script.

Currently, the following stores are supported:

- [Artifactory](https://jfrog.com/artifactory/)
- [Azure Key Vault](https://azure.microsoft.com/en-us/products/key-vault)
- [AWS SSM Parameter Store](https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-parameter-store.html)
- [Google Secret Manager](https://cloud.google.com/secret-manager)
- [Redis](https://redis.io/)
</Intro>

Atmos stores are configured in the `atmos.yaml` file and available to use in stacks via the
[store](/functions/yaml/store) YAML function.

## CLI Configuration

All of these settings should be configured in the [Atmos CLI Configuration](/cli/configuration) found in `atmos.yaml`.

### Artifactory

```yaml
stores:
  dev/artifactory:
    type: artifactory
    options:
      url: https://mydevartifactory.jfrog.io/artifactory
      repo_name: tfsharedstore

  prod/artifactory:
    type: artifactory
    options:
      url: https://myprodartifactory.jfrog.io/artifactory
      repo_name: tfsharedstore
      access_token: !env PROD_JFROG_ACCESS_TOKEN
```

<dl>
  <dt>`stores.[store_name]`</dt>
  <dd>This map key is the name of the store. It must be unique across all stores. This is how the store is referenced in the `store` function.</dd>

  <dt>`stores.[store_name].type`</dt>
  <dd>Must be set to `artifactory`</dd>

  <dt>`stores.[store_name].options`</dt>
  <dd>A map of options specific to the store type. For Artifactory, the following options are supported:</dd>

  <dt>`stores.[store_name].options.access_token (optional)`</dt>
  <dd>An access token to use for authentication. This is not recommended as it is less secure than using the
  `JFROG_ACCESS_TOKEN` or `ARTIFACTORY_ACCESS_TOKEN` environment variables. See [Authentication](#authentication) below
  for more information.</dd>

  <dt>`stores.[store_name].options.prefix (optional)`</dt>
  <dd>A prefix path that will be added to all keys stored or retrieved from SSM Parameter Store. For example if the prefix
  is `/atmos/infra-live/`, and if the stack is `plat-us2-dev`, the component is `vpc`, and the key is `vpc_id`, the full path
  would be `/atmos/infra-live/plat-us2-dev/vpc/vpc_id`.</dd>

  <dt>`stores.[store_name].options.repo_name (required)`</dt>
  <dd>The name of the Artifactory repository to use.</dd>

  <dt>`stores.[store_name].options.url (required)`</dt>
  <dd>The URL of the Artifactory instance.</dd>

  <dt>`stores.[store_name].options.stack_delimiter (optional)`</dt>
  <dd>
    The delimiter that atmos is using to delimit stacks in the key path. This defaults to `-`. This is used to build the
    key path for the store.
  </dd>
</dl>

#### Authentication

The Artifactory store supports using an access token for authentication. The access token can be set directly in the
`atmos.yaml` or via the `JFROG_ACCESS_TOKEN` or `ARTIFACTORY_ACCESS_TOKEN` environment variables.

It is also possible to specify the access token as `anonymous` to use the anonymous user to access the Artifactory
repository if the repository is configured to allow anonymous access.

**NOTE:** Storing sensitive access tokens in plain text in `atmos.yaml` is not secure and should be avoided. However, it's recommended for the `anonymous` use case or when managing multiple Artifactory stores with different access tokens. In such cases, use [`!env`](/functions/yaml/env) function to reference tokens securely.
YAML function to set the access token from an environment variable.

### Azure Key Vault

```yaml
stores:
  dev/azure-key-vault:
    type: azure-key-vault
    options:
      vault_url: https://my-keyvault.vault.azure.net/
      prefix: atmos/dev
      stack_delimiter: "-"

  prod/azure-key-vault:
    type: azure-key-vault
    options:
      vault_url: https://my-prod-keyvault.vault.azure.net/
      prefix: atmos/prod
```

<dl>
  <dt>`stores.[store_name]`</dt>
  <dd>This map key is the name of the store. It must be unique across all stores. This is how the store is referenced in the `store` function.</dd>

  <dt>`stores.[store_name].type`</dt>
  <dd>Must be set to `azure-key-vault`</dd>

  <dt>`stores.[store_name].options`</dt>
  <dd>A map of options specific to the store type. For Azure Key Vault, the following options are supported:</dd>

  <dt>`stores.[store_name].options.vault_url (required)`</dt>
  <dd>The URL of the Azure Key Vault. This should be in the format `https://{vault-name}.vault.azure.net/`.</dd>

  <dt>`stores.[store_name].options.prefix (optional)`</dt>
  <dd>A prefix path that will be added to all keys stored or retrieved from Azure Key Vault. For example if the prefix
  is `atmos/dev`, and if the stack is `plat-us2-dev`, the component is `vpc`, and the key is `vpc_id`, the full path
  would be `atmos-dev-plat-us2-dev-vpc-vpc_id` (after normalization for Azure Key Vault naming restrictions).</dd>

  <dt>`stores.[store_name].options.stack_delimiter (optional)`</dt>
  <dd>
    The delimiter that atmos is using to delimit stacks in the key path. This defaults to `-`. This is used to build the
    key path for the store.
  </dd>
</dl>

#### Authentication

Azure Key Vault supports multiple authentication methods:

1. **Default Azure Credential Chain**: By default, the Azure Key Vault store uses the DefaultAzureCredential from the Azure Identity library, which attempts authentication through multiple methods in the following order:
   - Environment variables (Azure CLI, Visual Studio, etc.)
   - Managed Identity
   - Azure CLI credentials
   - Interactive browser authentication (when running locally)

2. **Environment Variables**: Set these environment variables to authenticate:
   - `AZURE_TENANT_ID`: Your Azure Active Directory tenant ID
   - `AZURE_CLIENT_ID`: Your Azure Active Directory application ID
   - `AZURE_CLIENT_SECRET`: Your Azure Active Directory application secret

3. **Managed Identity**: When running in Azure services with managed identity enabled, authentication is automatic.

For more details, refer to the [Azure Identity Authentication Documentation](https://docs.microsoft.com/en-us/azure/developer/go/azure-sdk-authentication).

### AWS SSM Parameter Store

```yaml
stores:
  prod/ssm:
    type: aws-ssm-parameter-store
    options:
      region: us-east-2
      read_role_arn: "arn:aws:iam::123456789012:role/ssm-read-role"  # Optional role ARN for read operations
      write_role_arn: "arn:aws:iam::123456789012:role/ssm-write-role"  # Optional role ARN for write operations
```

<dl>
  <dt>`stores.[store_name]`</dt>
  <dd>This map key is the name of the store. It must be unique across all stores. This is how the store is referenced in the `store` function.</dd>

  <dt>`stores.[store_name].type`</dt>
  <dd>Must be set to `aws-ssm-parameter-store`</dd>

  <dt>`stores.[store_name].options`</dt>
  <dd>A map of options specific to the store type. For AWS SSM Parameter Store, the following options are supported:</dd>

  <dt>`stores.[store_name].options.prefix (optional)`</dt>
  <dd>A prefix path that will be added to all keys stored or retrieved from SSM Parameter Store. For example if the prefix
  is `/atmos/infra-live/`, and if the stack is `plat-us2-dev`, the component is `vpc`, and the key is `vpc_id`, the full path
  would be `/atmos/infra-live/plat-us2-dev/vpc/vpc_id`.</dd>

  <dt>`stores.[store_name].options.region (required)`</dt>
  <dd>The AWS region to use for the SSM Parameter Store.</dd>

  <dt>`stores.[store_name].options.stack_delimiter (optional)`</dt>
  <dd>
    The delimiter that atmos is using to delimit stacks in the key path. This defaults to `-`. This is used to build the
    key path for the store.
  </dd>

  <dt>`stores.[store_name].options.read_role_arn (optional)`</dt>
  <dd>The ARN of an IAM role to assume for read operations. If specified, this role will be assumed before performing any read operations.</dd>

  <dt>`stores.[store_name].options.write_role_arn (optional)`</dt>
  <dd>The ARN of an IAM role to assume for write operations. If specified, this role will be assumed before performing any write operations.</dd>
</dl>

#### Authentication

The AWS SSM Parameter Store supports the standard AWS methods for authentication and the `AWS_ACCESS_KEY_ID`,
`AWS_SECRET_ACCESS_KEY`, and `AWS_SESSION_TOKEN` environment variables. Additionally, if `read_role_arn` or `write_role_arn`
is specified, the store will assume that role before performing the respective operations.

### Google Secret Manager

```yaml
stores:
  dev/gsm:
    type: google-secret-manager
    options:
      project_id: my-project-id
      prefix: atmos/dev
      credentials: !env GOOGLE_CREDENTIALS_JSON  # Optional: JSON credentials string

  prod/gsm:
    type: gsm  # Alias for google-secret-manager
    options:
      project_id: my-prod-project
      prefix: atmos/prod
      # Uses Application Default Credentials
```

<dl>
  <dt>`stores.[store_name]`</dt>
  <dd>This map key is the name of the store. It must be unique across all stores. This is how the store is referenced in the `store` function.</dd>

  <dt>`stores.[store_name].type`</dt>
  <dd>Must be set to either `google-secret-manager` or its alias `gsm`</dd>

  <dt>`stores.[store_name].options`</dt>
  <dd>A map of options specific to the store type. For Google Secret Manager, the following options are supported:</dd>

  <dt>`stores.[store_name].options.project_id (required)`</dt>
  <dd>The Google Cloud project ID where the secrets are stored.</dd>

  <dt>`stores.[store_name].options.prefix (optional)`</dt>
  <dd>A prefix path that will be added to all keys stored or retrieved from Secret Manager. For example if the prefix
  is `atmos/infra-live/`, and if the stack is `plat-us2-dev`, the component is `vpc`, and the key is `vpc_id`, the full path
  would be `atmos/infra-live/plat-us2-dev/vpc/vpc_id`.</dd>

  <dt>`stores.[store_name].options.credentials (optional)`</dt>
  <dd>A JSON string containing Google service account credentials. If not provided, Application Default Credentials will be used.</dd>

  <dt>`stores.[store_name].options.stack_delimiter (optional)`</dt>
  <dd>
    The delimiter that atmos is using to delimit stacks in the key path. This defaults to `-`. This is used to build the
    key path for the store.
  </dd>
</dl>

#### Authentication

Google Secret Manager supports multiple authentication methods:

1. **Application Default Credentials (ADC)**: If no credentials are specified, the store will use ADC which can be set up by:
   - Running `gcloud auth application-default login` for local development
   - Using service account attached to GCP resources (like GCE instances)
   - Setting the `GOOGLE_APPLICATION_CREDENTIALS` environment variable pointing to a service account key file

2. **Direct Credentials**: You can provide service account credentials directly in the configuration using the `credentials` option.
   This is not recommended for production use. Instead, use the `!env` function to read credentials from an environment variable:
   ```yaml
   credentials: !env GOOGLE_CREDENTIALS_JSON
   ```

3. **Workload Identity**: When running in GCP, you can use Workload Identity which automatically handles authentication
   between GCP services.

### Redis

```yaml
stores:
  dev/redis:
    type: redis
    options:
      url: redis://localhost:6379

  stage/redis:
    type: redis
    options:
      url: !env ATMOS_STAGE_REDIS_URL

  prod/redis:
    type: redis
    # The ATMOS_REDIS_URL environment variable will be used if no URL is specified in the options
```

<dl>
  <dt>`stores.[store_name]`</dt>
  <dd>This map key is the name of the store. It must be unique across all stores. This is how the store is referenced in the `store` function.</dd>

  <dt>`stores.[store_name].type`</dt>
  <dd>Must be set to `redis`</dd>

  <dt>`stores.[store_name].options`</dt>
  <dd>A map of options specific to the store type. For Redis, the following options are supported:</dd>

  <dt>`stores.[store_name].options.prefix (optional)`</dt>
  <dd>A prefix path that will be added to all keys stored or retrieved from Redis. For example if the prefix
  is `/atmos/infra-live/`, and if the stack is `plat-us2-dev`, the component is `vpc`, and the key is `vpc_id`, the full path
  would be `/atmos/infra-live/plat-us2-dev/vpc/vpc_id`.</dd>

  <dt>`stores.[store_name].options.url`</dt>
  <dd>
    The URL of the Redis instance. This is optional and the `ATMOS_REDIS_URL` environment variable will be used if no
    URL is specified in the options.
  </dd>

  <dt>`stores.[store_name].options.stack_delimiter (optional)`</dt>
  <dd>
    The delimiter that atmos is using to delimit stacks in the key path. This defaults to `-`. This is used to build the
    key path for the store.
  </dd>
</dl>

#### Authentication

The Redis store supports authentication via the URL in options or via the `ATMOS_REDIS_URL` environment variable. The
URL format is described in the Redis [docs](https://redis.github.io/lettuce/user-guide/connecting-redis/).

---

## Configure Terraform

import useBaseUrl from '@docusaurus/useBaseUrl';
import Intro from '@site/src/components/Intro'

<Intro>
Atmos natively supports opinionated workflows for [Terraform](https://www.terraform.io/) and [OpenTofu](/core-concepts/projects/configuration/opentofu).
It's compatible with every version of terraform and designed to work with multiple different versions of Terraform
concurrently.

</Intro>

Keep in mind that Atmos does not handle the downloading or installation of Terraform; it assumes that any
required commands are already installed on your system. To automate this, consider creating a [Custom Command](/core-concepts/custom-commands) to install Terraform.

Atmos provides many settings that are specific to Terraform and OpenTofu.

## CLI Configuration

All of these settings are defined by default in the [Atmos CLI Configuration](/cli/configuration) found in `atmos.yaml`,
but can also be overridden at any level of the [Stack](/core-concepts/stacks/#schema) configuration.

```yaml
components:
  terraform:
    # The executable to be called by `atmos` when running Terraform commands
    command: "/usr/bin/terraform-1"
    # Can also be set using 'ATMOS_COMPONENTS_TERRAFORM_BASE_PATH' ENV var, or '--terraform-dir' command-line argument
    # Supports both absolute and relative paths
    base_path: "components/terraform"
    # Can also be set using 'ATMOS_COMPONENTS_TERRAFORM_APPLY_AUTO_APPROVE' ENV var
    apply_auto_approve: false
    # Can also be set using 'ATMOS_COMPONENTS_TERRAFORM_DEPLOY_RUN_INIT' ENV var, or '--deploy-run-init' command-line argument
    deploy_run_init: true
    # Can also be set using 'ATMOS_COMPONENTS_TERRAFORM_INIT_RUN_RECONFIGURE' ENV var, or '--init-run-reconfigure' command-line argument
    init_run_reconfigure: true
    # Can also be set using 'ATMOS_COMPONENTS_TERRAFORM_AUTO_GENERATE_BACKEND_FILE' ENV var, or '--auto-generate-backend-file' command-line argument
    auto_generate_backend_file: false
    # Can also be set using 'ATMOS_COMPONENTS_TERRAFORM_APPEND_USER_AGENT' ENV var, or '--append-user-agent' command-line argument
    append_user_agent: "Acme/1.0 (Build 1234; arm64)"
    plan:
      # Can also be set using 'ATMOS_COMPONENTS_TERRAFORM_PLAN_SKIP_PLANFILE' ENV var, or '--skip-planfile' command-line argument
      skip_planfile: false
```

<dl>
  <dt>`components.terraform.command`</dt>
  <dd>The executable to be called by Atmos when running Terraform commands</dd>

  <dt>`components.terraform.base_path`</dt>
  <dd>The root directory where the Terraform components and configurations are located. This path serves as the starting point for resolving any relative paths within the Terraform setup.</dd>

  <dt>`components.terraform.apply_auto_approve`</dt>
  <dd>if set to `true`, Atmos automatically adds the `-auto-approve` option to instruct Terraform to apply the plan without
  asking for confirmation when executing `terraform apply` command</dd>

  <dt>`components.terraform.deploy_run_init`</dt>
  <dd>if set to `true`, Atmos runs `terraform init` before executing [`atmos terraform deploy`](/cli/commands/terraform/deploy) command</dd>

  <dt>`components.terraform.init_run_reconfigure`</dt>
  <dd>if set to `true`, Atmos automatically adds the `-reconfigure` option to update the backend configuration when executing `terraform init` command</dd>

  <dt>`components.terraform.auto_generate_backend_file`</dt>
  <dd>if set to `true`, Atmos automatically generates the Terraform backend file from the component configuration when executing `terraform plan` and `terraform apply` commands</dd>

  <dt>`components.terraform.plan.skip_planfile`</dt>
  <dd>
    if set to `true`, Atmos will skip passing the `-out=FILENAME` flag when executing the `terraform plan` command.
    Set it to `true` when using Terraform Cloud since the `-out` flag is not supported.
    Terraform Cloud automatically stores plans in its backend
  </dd>
</dl>

## Configuration

The settings for terraform can be defined in multiple places and support inheritance. This ensures that projects can
override the behavior.

The defaults for everything are defined in the `atmos.yaml`.

```yaml
components:
  terraform:
    ...
```

The same settings, can be overridden by Stack configurations at any level:

- `terraform`
- `components.terraform`
- `components.terraform._component_`

For example, we can change the terraform command used by a component (useful for legacy components)

```yaml
components:
  terraform:
    vpc:
      command: "/usr/local/bin/terraform-0.13"
```

## Terraform Provider

A Terraform provider (`cloudposse/terraform-provider-utils`) implements a `data` source that can read the YAML Stack
configurations natively from
within terraform.

## Terraform Module

A Terraform module (`cloudposse/terraform-yaml-stack-config`) wraps the data source.

Here's an example of accessing the variables for a given component from within a Terraform module.

```hcl
module "vars" {
  source = "cloudposse/stack-config/yaml//modules/vars"
  # version     = "x.x.x"

  stack_config_local_path = "./stacks"
  stack                   = "my-stack"
  component_type          = "terraform"
  component               = "my-vpc"

  context = module.this.context
}
```

---

## Folder Structure

import KeyPoints from '@site/src/components/KeyPoints'
import Intro from '@site/src/components/Intro'

<Intro>
At the root of your project, you’ll typically find an `atmos.yaml` configuration file. This file defines how Atmos should discover your stack files for configuration and your Terraform root modules as components.
</Intro>

<KeyPoints>
- How to organize your project on the file system
- How to separate configuration from components
- Different ways to organize your project
</KeyPoints>

## Recommended Filesystem Layout

Atmos is fully configurable, and you can organize your project in any way that makes sense for your team by adjusting the paths in [`atmos.yaml`](/core-concepts/projects/configuration). We also provide detailed guidance on organizing your folder structure, whether it’s for a simple project or enterprise-scale architecture in our [Design Patterns](/design-patterns) section. Choose the model that best fits the stage you plan to reach when you complete the project.

Here's a simple layout, if you just have 3 deployments for things like dev, staging, and prod:
```plaintext
├── components/          # Folder containing all your components, usually organized by toolchain
│   └── terraform/       # Folder for all Terraform "root modules"
└── stacks/
    ├── deploy/          # Folder for deployable stacks
    │   ├── dev/         # Folder for development environment configurations
    │   ├── staging/     # Folder for staging environment configurations
    │   └── prod/        # Folder for production environment configurations
    ├── catalog/         # Folder for the service catalog
    ├── schemas/         # Folder for the schema validations
    └── workflows/       # Folder for workflows that operate on top of stacks
```

Alternatively, here’s a more complex layout for a larger project broken into multiple organizations, organizational units, and environments:

```plaintext
├── components/                  # Folder containing all your components, usually organized by toolchain
│   └── terraform/               # Folder for all Terraform "root modules"
└── stacks/
    ├── orgs/                    # Folder for deployable stacks
    │   └── acme/                # Folder for the Acme organization
    │       ├── core/            # OU for core services
    │       │   ├── security/    # Folder for security-related configurations
    │       │   ├── audit/       # Folder for audit-related configurations
    │       │   ├── identity/    # Folder for identity management configurations
    │       │   └── network/     # Folder for networking-related configurations
    │       └── plat/            # OU for platform environments
    │           ├── dev/         # Folder for development environment configurations
    │           ├── staging/     # Folder for staging environment configurations
    │           └── prod/        # Folder for production environment configurations
    ├── catalog/                 # Folder for the service catalog
    ├── schemas/                 # Folder for the schema validations
    └── workflows/               # Folder for workflows that operate on top of stacks
```

Note, that these are just a couple of examples.

<dl>
    <dt>`components/`</dt>
    <dd>folder containing all your components, usually organized by your toolchain</dd>

    <dt>`components/terraform`</dt>
    <dd>folder for all Terraform "root modules"</dd>

    <dt>`stacks/orgs/`</dt>
    <dd>folder for deployable stacks</dd>

    <dt>`stacks/catalog/`</dt>
    <dd>folder for the service catalog</dd>

    <dt>`stacks/workflows/`</dt>
    <dd>folder for workflows that operate on top of stacks.</dd>
</dl>

You can find some demos of how we organize projects in the Atmos GitHub repository under the [`examples/`](https://github.com/cloudposse/atmos/tree/main/examples) folder. Or check out our [Reference Architecture for AWS](https://docs.cloudposse.com/learn) for a more detailed look at how we organize our projects.

To effectively organize an Atmos project, we want to ensure you have specific locations for Atmos to find your stack configurations and components. At a minimum, we recommend the following folder structure in your project:

## Components Folder

This folder will contain all your components. Organize the components by toolchain. For example, if you have components for Terraform, place them in a Terraform subfolder (e.g. `components/terraform/vpc`).

## Stack Configurations Folder

Next, you’ll have your stacks configurations, which are organized into multiple subfolders depending on their purpose:

### Schema Validations

This folder contains the [JSON or OPA schemas used to validate the stack configurations](/core-concepts/validate).

### Catalogs

This should be a separate top-level folder containing your stack configurations. Stack configurations are divided into several parts:
- **Schemas Folder**: This folder contains the schemas used to validate the stack configurations.
- **Catalog Folder**: This includes all reusable imports, which can be organized into subfolders based on logical groupings.
- **Stacks Folder**: This contains the deployable stacks. Each stack is defined in a separate YAML file.

We follow a few conventions in our reference architecture:

### Deployments

We usually organize our stacks by organization, organizational unit, and environment. For example:

- **Orgs Folder**: Represents the AWS organizations to which you deploy. You might use a folder called deploy if you have a few simple stacks.
- **Multi-Cloud Projects**: If your project involves multiple clouds, consider additional organizational strategies.

---

## Setup Projects for Atmos

import DocCardList from '@theme/DocCardList'
import KeyPoints from '@site/src/components/KeyPoints'
import Intro from '@site/src/components/Intro'

<Intro>
Atmos is a framework, so we suggest some conventions for organizing your infrastructure using folders to separate configuration from components. This separation is key to making your components highly reusable.
</Intro>

By keeping configuration and components distinct, you can easily manage and update each part without affecting the other.

<KeyPoints>
- Where to put your terraform components
- Where to keep your configuration
- How to configure Atmos to work with Terraform
</KeyPoints>

If you're more of a hands-on learner, we also go into some of these details in our [Simple Quick Start](/quick-start/simple).

## Configuration

Learn how to best configure a project to work with Atmos. We recommend some conventions for how to organize your project into folders, then configure the Atmos CLI to use those folders.

<DocCardList/>

---

## Configure Your Editor for Atmos

import TabItem from "@theme/TabItem";
import Tabs from "@theme/Tabs";
import Intro from "@site/src/components/Intro";
import KeyPoints from "@site/src/components/KeyPoints";

<Intro>
  A properly configured editor can make working with Atmos configurations more
  intuitive and efficient. The right setup can improve readability, speed up
  your workflow, and even help you catch configuration errors as you go! Whether
  you’re setting up your editor for the first time or refining your current
  environment, we have some recommendations to get you started.
</Intro>

<KeyPoints>
  - How to configure your VS Code editor to boost productivity
  - Ensure your YAML files are validated against the Atmos schema to catch issues early and maintain compliance with best practices
  - How to format your code
  automatically
</KeyPoints>

To work effectively with Atmos, we recommend configuring your VS Code editor for the best developer experience. Alternatively, you can use a **DevContainer configuration**.

<Tabs>
    <TabItem value="vscode" label="Visual Studio Code">

        ## Configure Visual Studio Code

        You can manually configure your VS Code environment with the following settings.

        ### Recommended Visual Studio Code Extensions

        Install these extensions for enhanced productivity:
        - [Docker](https://marketplace.visualstudio.com/items?itemName=ms-azuretools.vscode-docker)
        - [GitHub Markdown Preview](https://marketplace.visualstudio.com/items?itemName=bierner.github-markdown-preview)
        - [Markdown Admonitions](https://marketplace.visualstudio.com/items?itemName=tomasdahlqvist.markdown-admonitions)
        - [Terraform](https://marketplace.visualstudio.com/items?itemName=HashiCorp.terraform)
        - [YAML](https://marketplace.visualstudio.com/items?itemName=redhat.vscode-yaml)
        - [Go Template](https://marketplace.visualstudio.com/items?itemName=casualjim.gotemplate)
        - [EditorConfig](https://marketplace.visualstudio.com/items?itemName=EditorConfig.EditorConfig)

        ### Visual Studio Code Settings

        Update your VS Code settings to optimize the experience for working with Atmos. With these configurations, your VS Code editor will be fully optimized for working with Atmos.

        Add the following to your `settings.json` for your infrastructure repository (e.g. `infra/.vscode/settings.json`)

        ```json
        {
            "git.openRepositoryInParentFolders": "always",
            "git.autofetch": true,
            "git.showProgress": true,
            "workbench.startupEditor": "readme",
            "workbench.editor.autoLockGroups": {
                "readme": "/welcome.md"
            },
            "workbench.editorAssociations": {
                "*.md": "vscode.markdown.preview.editor"
            },
            "terminal.integrated.tabs.title": "Atmos (${process})",
            "terminal.integrated.tabs.description": "${task}${separator}${local}${separator}${cwdFolder}",
            "terminal.integrated.shell.linux": "/bin/zsh",
            "terminal.integrated.allowWorkspaceConfiguration": true,
            "yaml.schemaStore.enable": true,
            "yaml.schemas": {
                "https://atmos.tools/schemas/atmos/atmos-manifest/1.0/atmos-manifest.json": [
                "**/stacks/**/*.yaml",
                "!**/stacks/workflows/**/*.yaml",
                "!**/stacks/schemas/**/*.yaml"
                ]
            }
        }
        ```

        ### Terminal Configuration

        Set your terminal to use Zsh for an improved command-line experience:

        ```json
        "terminal.integrated.shell.linux": "/bin/zsh"
        ```

        ### YAML Schema Validation

        Ensure your YAML files are validated against the Atmos schema:

        ```json
        "yaml.schemas": {
            "https://atmos.tools/schemas/atmos/atmos-manifest/1.0/atmos-manifest.json": [
                "**/stacks/**/*.yaml",
                "!**/stacks/workflows/**/*.yaml",
                "!**/stacks/schemas/**/*.yaml"
            ]
        }
        ```

    </TabItem>

    <TabItem value="devcontainer" label="DevContainer">

        ## Use DevContainers with Atmos

        When managing your infrastructure with Atmos, you can enhance your development experience by configuring your **infrastructure repository** with a [dev containers](https://containers.dev/). This ensures a consistent, isolated development environment tailored for working with Atmos and Terraform, integrated natively with your IDE.

        ## Why Use a DevContainers?

        - **Consistent Environment:** Ensures every developer uses the same tools and configurations.
        - **Pre-installed Tools:** Includes Atmos, Terraform, and any additional utilities.
        - **Simplified Setup:** Developers don’t need to manually install dependencies.

        By adding this configuration to your infrastructure repository, you'll streamline collaboration and maintain consistency across your team.

        ## Setting Up a DevContainer for Your Infrastructure Repository

        Follow these steps to configure a **DevContainer** in your repository:

        ### 1. Create a `.devcontainer` Directory

        In the root of your infrastructure repository, create a `.devcontainer` directory to store the configuration files:

        ```bash
        mkdir .devcontainer
        ```

        ### 2. Add a `devcontainer.json` File

        Inside the `.devcontainer` directory, create a `devcontainer.json` file with the following content:

        ```json
        {
            "name": "Atmos DevContainer",
            "forwardPorts": [80, 443],
            "portsAttributes": {
                "80": { "label": "Ingress" },
                "443": { "label": "Ingress (TLS)" }
            },
            "security.workspace.trust.emptyWindow": true,
            "security.workspace.trust.untrustedFiles": "prompt",
            "security.workspace.trust.domain": {
                "*.github.com": true,
                "*.app.github.dev": true,
                "localhost": true
            },
            "build": {
                "dockerfile": "Dockerfile",
                "context": "."
            },
            "hostRequirements": {
                "cpus": 4,
                "memory": "8gb",
                "storage": "16gb"
            },
            "runArgs": ["-v", "/var/run/docker.sock:/var/run/docker.sock"],
            "postCreateCommand": "/workspace/.devcontainer/post-create.sh",
            "features": {
                "ghcr.io/devcontainers/features/docker-outside-of-docker": {}
            },
            "workspaceFolder": "/workspace",
            "workspaceMount": "source=${localWorkspaceFolder},target=/workspace,type=bind",
            "customizations": {
                "vscode": {
                    "extensions": [
                        "ms-azuretools.vscode-docker",
                        "bierner.github-markdown-preview",
                        "tomasdahlqvist.markdown-admonitions",
                        "HashiCorp.terraform",
                        "redhat.vscode-yaml",
                        "casualjim.gotemplate",
                        "EditorConfig.EditorConfig"
                    ],
                    "settings": {
                        "git.openRepositoryInParentFolders": "always",
                        "git.autofetch": true,
                        "workbench.startupEditor": "readme",
                        "yaml.schemas": {
                            "https://atmos.tools/schemas/atmos/atmos-manifest/1.0/atmos-manifest.json": [
                                "**/stacks/**/*.yaml",
                                "!**/stacks/workflows/**/*.yaml",
                                "!**/stacks/schemas/**/*.yaml"
                            ]
                        }
                    }
                }
            }
        }
        ```

        ### 3. Add a `Dockerfile`

        In the `.devcontainer` directory, create a `Dockerfile` to define the environment. For Atmos and Terraform, use the following:

        ```Dockerfile
        FROM mcr.microsoft.com/devcontainers/base:ubuntu

        # Install dependencies
        RUN apt-get update && \
            apt-get install -y curl unzip git zsh && \
            curl -Lo /tmp/terraform.zip https://releases.hashicorp.com/terraform/1.5.6/terraform_1.5.6_linux_amd64.zip && \
            unzip /tmp/terraform.zip -d /usr/local/bin/ && \
            rm /tmp/terraform.zip && \
            curl -Lo /usr/local/bin/atmos https://github.com/cloudposse/atmos/releases/latest/download/atmos-linux-amd64 && \
            chmod +x /usr/local/bin/atmos

        # Install Zsh and set as default shell
        RUN chsh -s /bin/zsh
        ```

        ### 4. (Optional) Add a Post-Create Script

        If you need to run additional setup commands after creating the container, add a `post-create.sh` script:

        ```bash
        #!/bin/bash

        # Example: Install custom tools or set up environment variables
        echo "Post-create script running..."
        ```

        Make it executable:

        ```bash
        chmod +x .devcontainer/post-create.sh
        ```

        ### 5. Open Your Repository in the DevContainer

        1. Install the **Dev Containers** extension in VS Code:
           - [Dev Containers Extension](https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.remote-containers)
        2. Open the infrastructure repository in VS Code.
        3. Click on the &nbsp;<sub>&gt;</sub><sup style={{position: 'relative', left: '-7px', top: '3px'}}>&lt;</sup> icon in the bottom-left corner and a menu appears.
        4. Select **Reopen in Container**.
    </TabItem>

</Tabs>

---

## Using Remote State

import Intro from '@site/src/components/Intro'
import KeyPoints from '@site/src/components/KeyPoints'
import Note from '@site/src/components/Note'
import ActionCard from '@site/src/components/ActionCard'
import PrimaryCTA from '@site/src/components/PrimaryCTA'
import File from '@site/src/components/File'

<Intro>
Terraform natively supports the concept of remote state and there's a very easy way to access the outputs of one Terraform component in another component. We simplify this using the `remote-state` module, which is stack-aware and can be used to access the remote state of a component in the same or a different Atmos stack.
</Intro>

As your architecture grows, it helps to be more intentional about how you deconstruct and organize your components to keep your Terraform state small (see our [best practices](/best-practices/components)). By creating smaller components, your state becomes naturally more manageable. However, this introduces a new problem: there are now dependencies between your components, and the state becomes distributed. We need to find a new way for state to flow between your components and for and [a way to share configuration](/core-concepts/stacks/imports). Plus, we want to [avoid manual duplication of configurations](/core-concepts/stacks/inheritance) as much as possible because that leads to bugs, like copy-paste mistakes.

<KeyPoints>
- How to use the `remote-state` module to access the remote state of a component in the same or a different Atmos stack
- How to configure Atmos to work the `remote-state` module to access the remote state of a component
- Alternatives that might be easier for your use case
</KeyPoints>

In Atmos, this is solved by using these modules:

- [terraform-provider-utils](https://github.com/cloudposse/terraform-provider-utils) - The Cloud Posse Terraform Provider for various utilities,
  including stack configuration management

- [remote-state](https://github.com/cloudposse/terraform-yaml-stack-config/tree/main/modules/remote-state) - Terraform module that loads and processes
  stack configurations from YAML sources and returns remote state outputs for Terraform components

The [terraform-provider-utils](https://github.com/cloudposse/terraform-provider-utils) is implemented in [Go](https://go.dev/) and uses Atmos `Go` modules to work with [Atmos CLI config](/cli/configuration) and [Atmos stacks](/core-concepts/stacks). The provider processes stack configurations to get the final config for an Atmos component in an Atmos stack. The final component config is then used by the [remote-state](https://github.com/cloudposse/terraform-yaml-stack-config/tree/main/modules/remote-state) Terraform module to return the remote state for the component in the stack.

<Note>
Terraform remote state is incompatible with the `local` backend type. This is because the local backend is not recommended for production. Review the alternatives [here](/core-concepts/share-data), or consider switching to one of the other backend types.
</Note>

:::tip New & Improved Ways to Share Data
Atmos now supports new ways to share data between components using the template function `atmos.Component`
and the Atmos YAML functions `!terraform.state` and `!terraform.output` in your Stack configurations:

```shell
{{ (atmos.Component <component> <stack>).outputs.<output_name> }}

!terraform.state <component> <stack> <output_name>

!terraform.output <component> <stack> <output_name>
```

The `atmos.Component` template function allows reading any Atmos section or any attribute (not just outputs) from a section
of an Atmos component in a stack.

For more details on `atmos.Component` function, refer to [`atmos.Component`](/functions/template/atmos.Component).

The `!terraform.state` and `!terraform.output` Atmos YAML functions allow reading any output (remote state) of an Atmos component in a stack.

For more details on `!terraform.state` YAML function, refer to [`!terraform.state`](/functions/yaml/terraform.state).

For more details on `!terraform.output` YAML function, refer to [`!terraform.output`](/functions/yaml/terraform.output).
:::

## Example

Here is an example.

Suppose that we need to provision two Terraform components:

- [vpc-flow-logs-bucket](https://github.com/cloudposse/atmos/tree/main/examples/quick-start-advanced/components/terraform/vpc-flow-logs-bucket)
- [vpc](https://github.com/cloudposse/atmos/tree/main/examples/quick-start-advanced/components/terraform/vpc)

The `vpc` Terraform component needs the outputs from the `vpc-flow-logs-bucket` Terraform component to
configure [VPC Flow Logs](https://docs.aws.amazon.com/vpc/latest/userguide/flow-logs.html) and store them in the S3 bucket.

We will provision the two Terraform components in the `ue2-dev` Atmos stack (in the `dev` AWS account by setting `stage = "dev"` and in
the `us-east-2` region by setting `environment = "ue2"`).

### Configure and Provision the `vpc-flow-logs-bucket` Component

In the `stacks/catalog/vpc-flow-logs-bucket.yaml` file, add the following default configuration for the `vpc-flow-logs-bucket/defaults` Atmos component:

<File title="stacks/catalog/vpc-flow-logs-bucket.yaml">
```yaml
components:
  terraform:
    vpc-flow-logs-bucket/defaults:
      metadata:
        # `metadata.type: abstract` makes the component `abstract`,
        # explicitly prohibiting the component from being deployed.
        # `atmos terraform apply` will fail with an error.
        # If `metadata.type` attribute is not specified, it defaults to `real`.
        # `real` components can be provisioned by `atmos` and CI/CD like Spacelift and Atlantis.
        type: abstract
      # Default variables, which will be inherited and can be overridden in the derived components
      vars:
        force_destroy: false
        lifecycle_rule_enabled: false
        traffic_type: "ALL"
```
</File>

In the `stacks/ue2-dev.yaml` stack config file, add the following config for the `vpc-flow-logs-bucket-1` Atmos component in the `ue2-dev` Atmos
stack:

<File title="stacks/ue2-dev.yaml">
```yaml
# Import the base Atmos component configuration from the `catalog`.
# `import` supports POSIX-style Globs for file names/paths (double-star `**` is supported).
# File extensions are optional (if not specified, `.yaml` is used by default).
import:
  - catalog/vpc-flow-logs-bucket

components:
  terraform:
    vpc-flow-logs-bucket-1:
      metadata:
        # Point to the Terraform component in `components/terraform` folder
        component: infra/vpc-flow-logs-bucket
        inherits:
          # Inherit all settings and variables from the
          # `vpc-flow-logs-bucket/defaults` base Atmos component
          - vpc-flow-logs-bucket/defaults
      vars:
        # Define variables that are specific for this component
        # and are not set in the base component
        name: vpc-flow-logs-bucket-1
        # Override the default variables from the base component
        traffic_type: "REJECT"
```
</File>

Having the stacks configured as shown above, we can now provision the `vpc-flow-logs-bucket-1` Atmos component into the `ue2-dev` stack by executing
the following Atmos commands:

```shell
atmos terraform plan vpc-flow-logs-bucket-1 -s ue2-dev
atmos terraform apply vpc-flow-logs-bucket-1 -s ue2-dev
```

### Configure and Provision the `vpc` Component

Having the `vpc-flow-logs-bucket` Terraform component provisioned into the `ue2-dev` stack, we can now configure the `vpc` Terraform component
to obtain the outputs from the remote state of the `vpc-flow-logs-bucket-1` Atmos component.

In the `components/terraform/infra/vpc/remote-state.tf` file, configure the
[remote-state](https://github.com/cloudposse/terraform-yaml-stack-config/tree/main/modules/remote-state) Terraform module to obtain the remote state
for the `vpc-flow-logs-bucket-1` Atmos component:

<File title="components/terraform/infra/vpc/remote-state.tf">
```hcl
module "vpc_flow_logs_bucket" {
  count = local.vpc_flow_logs_enabled ? 1 : 0

  source  = "cloudposse/stack-config/yaml//modules/remote-state"
  version = "1.5.0"

  # Specify the Atmos component name (defined in YAML stack config files)
  # for which to get the remote state outputs
  component = var.vpc_flow_logs_bucket_component_name

  # Override the context variables to point to a different Atmos stack if the
  # `vpc-flow-logs-bucket-1` Atmos component is provisioned in another AWS account, OU or region
  stage       = try(coalesce(var.vpc_flow_logs_bucket_stage_name, module.this.stage), null)
  tenant      = try(coalesce(var.vpc_flow_logs_bucket_tenant_name, module.this.tenant), null)
  environment = try(coalesce(var.vpc_flow_logs_bucket_environment_name, module.this.environment), null)

  # `context` input is a way to provide the information about the stack (using the context
  # variables `namespace`, `tenant`, `environment`, `stage` defined in the stack config)
  context = module.this.context
}
```
</File>

In the `components/terraform/infra/vpc/vpc-flow-logs.tf` file, configure the `aws_flow_log` resource for the `vpc` Terraform component to use the
remote state output `vpc_flow_logs_bucket_arn` from the `vpc-flow-logs-bucket-1` Atmos component:

<File title="components/terraform/infra/vpc/vpc-flow-logs.tf">
```hcl
locals {
  enabled               = module.this.enabled
  vpc_flow_logs_enabled = local.enabled && var.vpc_flow_logs_enabled
}

resource "aws_flow_log" "default" {
  count = local.vpc_flow_logs_enabled ? 1 : 0

  # Use the remote state output `vpc_flow_logs_bucket_arn` of the `vpc_flow_logs_bucket` component
  log_destination = module.vpc_flow_logs_bucket[0].outputs.vpc_flow_logs_bucket_arn

  log_destination_type = var.vpc_flow_logs_log_destination_type
  traffic_type         = var.vpc_flow_logs_traffic_type
  vpc_id               = module.vpc.vpc_id

  tags = module.this.tags
}
```
</File>

In the `stacks/catalog/vpc.yaml` file, add the following default config for the `vpc/defaults` Atmos component:

<File title="stacks/catalog/vpc.yaml">
```yaml
components:
  terraform:
    vpc/defaults:
      metadata:
        # `metadata.type: abstract` makes the component `abstract`,
        # explicitly prohibiting the component from being deployed.
        # `atmos terraform apply` will fail with an error.
        # If `metadata.type` attribute is not specified, it defaults to `real`.
        # `real` components can be provisioned by `atmos` and CI/CD like Spacelift and Atlantis.
        type: abstract
      # Default variables, which will be inherited and can be overridden in the derived components
      vars:
        public_subnets_enabled: false
        nat_gateway_enabled: false
        nat_instance_enabled: false
        max_subnet_count: 3
        vpc_flow_logs_enabled: false
        vpc_flow_logs_log_destination_type: s3
        vpc_flow_logs_traffic_type: "ALL"
```
</File>

In the `stacks/ue2-dev.yaml` stack config file, add the following config for the `vpc/1` Atmos component in the `ue2-dev` stack:

<File title="stacks/ue2-dev.yaml">
```yaml
# Import the base component configuration from the `catalog`.
# `import` supports POSIX-style Globs for file names/paths (double-star `**` is supported).
# File extensions are optional (if not specified, `.yaml` is used by default).
import:
  - catalog/vpc

components:
  terraform:
    vpc/1:
      metadata:
        # Point to the Terraform component in `components/terraform` folder
        component: infra/vpc
        inherits:
          # Inherit all settings and variables from the `vpc/defaults` base Atmos component
          - vpc/defaults
      vars:
        # Define variables that are specific for this component
        # and are not set in the base component
        name: vpc-1
        ipv4_primary_cidr_block: 10.8.0.0/18
        # Override the default variables from the base component
        vpc_flow_logs_enabled: true
        vpc_flow_logs_traffic_type: "REJECT"

        # Specify the name of the Atmos component that provides configuration
        # for the `infra/vpc-flow-logs-bucket` Terraform component
        vpc_flow_logs_bucket_component_name: vpc-flow-logs-bucket-1

        # Override the context variables to point to a different Atmos stack if the
        # `vpc-flow-logs-bucket-1` Atmos component is provisioned in another AWS account, OU or region.

        # If the bucket is provisioned in a different AWS account,
        # set `vpc_flow_logs_bucket_stage_name`
        # vpc_flow_logs_bucket_stage_name: prod

        # If the bucket is provisioned in a different AWS OU,
        # set `vpc_flow_logs_bucket_tenant_name`
        # vpc_flow_logs_bucket_tenant_name: core

        # If the bucket is provisioned in a different AWS region,
        # set `vpc_flow_logs_bucket_environment_name`
        # vpc_flow_logs_bucket_environment_name: uw2
```
</File>

Having the stacks configured as shown above, we can now provision the `vpc/1` Atmos component into the `ue2-dev` stack by
executing the following Atmos commands:

```shell
atmos terraform plan vpc/1 -s ue2-dev
atmos terraform apply vpc/1 -s ue2-dev
```

## Atmos Configuration

Both the `atmos` [CLI](/cli) and [terraform-provider-utils](https://github.com/cloudposse/terraform-provider-utils) Terraform provider use the same `Go` code, which try to locate the [CLI config](/cli/configuration) `atmos.yaml` file before parsing and processing [Atmos stacks](/core-concepts/stacks).

This means that `atmos.yaml` file must be at a location in the file system where all the processes can find it.

While placing `atmos.yaml` at the root of the repository will work for Atmos, it will not work for the [terraform-provider-utils](https://github.com/cloudposse/terraform-provider-utils) Terraform provider because the provider gets executed from the component's directory (e.g. `components/terraform/infra/vpc`), and we don't want to replicate `atmos.yaml` into every component's folder.

:::info

`atmos.yaml` is loaded from the following locations (from lowest to highest priority):

- System dir (`/usr/local/etc/atmos/atmos.yaml` on Linux, `%LOCALAPPDATA%/atmos/atmos.yaml` on Windows)
- Home dir (`~/.atmos/atmos.yaml`)
- Current directory
- ENV variables `ATMOS_CLI_CONFIG_PATH` and `ATMOS_BASE_PATH`

:::

<Note>

  Initial Atmos configuration can be controlled by these ENV vars:

  - `ATMOS_CLI_CONFIG_PATH` - where to find `atmos.yaml`. Absolute path to a folder where the `atmos.yaml` CLI config file is located
  - `ATMOS_BASE_PATH` - absolute path to the folder containing the `components` and `stacks` folders

</Note>

### Recommended Options

For this to work for both the `atmos` CLI and the Terraform provider, we recommend doing one of the following:

- Put `atmos.yaml` at `/usr/local/etc/atmos/atmos.yaml` on local host and set the ENV var `ATMOS_BASE_PATH` to point to the absolute path of the root
  of the repo

- Put `atmos.yaml` into the home directory (`~/.atmos/atmos.yaml`) and set the ENV var `ATMOS_BASE_PATH` pointing to the absolute path of the root of
  the repo

- Put `atmos.yaml` at a location in the file system and then set the ENV var `ATMOS_CLI_CONFIG_PATH` to point to that location. The ENV var must
  point to a folder without the `atmos.yaml` file name. For example, if `atmos.yaml` is at `/atmos/config/atmos.yaml`,
  set `ATMOS_CLI_CONFIG_PATH=/atmos/config`. Then set the ENV var `ATMOS_BASE_PATH` pointing to the absolute path of the root of the repo

- When working in a Docker container, place `atmos.yaml` in the `rootfs` directory
  at [/rootfs/usr/local/etc/atmos/atmos.yaml](https://github.com/cloudposse/atmos/blob/main/examples/quick-start-advanced/rootfs/usr/local/etc/atmos/atmos.yaml)
  and then copy it into the container's file system in the [Dockerfile](https://github.com/cloudposse/atmos/blob/main/examples/quick-start-advanced/Dockerfile)
  by executing the `COPY rootfs/ /` Docker command. Then in the Dockerfile, set the ENV var `ATMOS_BASE_PATH` pointing to the absolute path of the
  root of the repo. Note that the [Atmos example](https://github.com/cloudposse/atmos/blob/main/examples/quick-start)
  uses [Geodesic](https://github.com/cloudposse/geodesic) as the base Docker image. [Geodesic](https://github.com/cloudposse/geodesic) sets the ENV
  var `ATMOS_BASE_PATH` automatically to the absolute path of the root of the repo on local host

## Summary

- Remote State for an Atmos component in an Atmos stack is obtained by using
  the [remote-state](https://github.com/cloudposse/terraform-yaml-stack-config/tree/main/modules/remote-state) Terraform module

- The module calls the [terraform-provider-utils](https://github.com/cloudposse/terraform-provider-utils) Terraform provider which processes the stack
  configs and returns the configuration for the Atmos component in the stack.
  The [terraform-provider-utils](https://github.com/cloudposse/terraform-provider-utils) Terraform provider utilizes Atmos `Go` modules to parse and
  process stack configurations

- The [remote-state](https://github.com/cloudposse/terraform-yaml-stack-config/tree/main/modules/remote-state) module accepts the `component` input as
  the Atmos component name for which to get the remote state outputs

- The module accepts the `context` input as a way to provide the information about the stack (using the context
  variables `namespace`, `tenant`, `environment`, `stage` defined in the stack manifests)

- If the Atmos component (for which we want to get the remote state outputs) is provisioned in a different Atmos stack (in a different AWS OU, or
  different AWS account, or different AWS region), we can override the context variables `tenant`, `stage` and `environment` to point the module to
  the correct stack. For example, if the component is provisioned in a different AWS region (let's say `us-west-2`), we can set `environment = "uw2"`,
  and the [remote-state](https://github.com/cloudposse/terraform-yaml-stack-config/tree/main/modules/remote-state) module will get the remote state
  outputs for the Atmos component provisioned in that region

<ActionCard title="Use Remote State in Stack Configurations">
Atmos supports alternative ways to read the outputs (remote state) of components directly in Atmos stack manifests by
using the [`!terraform.output`](/functions/yaml/terraform.output) Atmos YAML function
and the [`atmos.Component`](/functions/template/atmos.Component) Go template function instead of using
the [`remote-state`](https://github.com/cloudposse/terraform-yaml-stack-config/tree/main/modules/remote-state) module
and configuring Terraform/OpenTofu components to use the module.

<PrimaryCTA to="/functions/yaml/terraform.output">Learn how to use '!terraform.output' YAML function</PrimaryCTA>

<PrimaryCTA to="/functions/template/atmos.Component">Learn how to use 'atmos.Component' template function</PrimaryCTA>

</ActionCard>

---

## Share Data Between Components

import Intro from '@site/src/components/Intro'
import ActionCard from '@site/src/components/ActionCard'
import PrimaryCTA from '@site/src/components/PrimaryCTA'
import KeyPoints from '@site/src/components/KeyPoints'
import CollapsibleText from '@site/src/components/CollapsibleText'
import File from '@site/src/components/File'
import Note from '@site/src/components/Note'

<Intro>
Breaking up your infrastructure components into loosely coupled components is a great way to manage complexity and
reuse code. However, these smaller components often lead to a situation where you need to share data between components.
In Atmos, there are several ways you can easily share settings, configurations, and outputs among components and even
tap into external data sources and stores.
</Intro>

There are multiple ways to approach this: using native Terraform support for remote state to read outputs from other
components or using template functions in stack configurations. In this chapter, you’ll learn how to share state between
components within the same stack or even across different stacks.

<KeyPoints>
- Why you might need to share data between components
- How to share data between components using Terraform remote state
- How to use template functions to share data between components in stack configurations
</KeyPoints>

## Using YAML Functions

###  Function: `!store`

The `!store` YAML function can read data from a remote store such as SSM Parameter Store, Artifactory, or Redis.

For example, we can read the `vpc_id` output of the `vpc` component in the current stack from the SSM Parameter Store
configured in `atmos.yaml` as `ssm/prod` simply by doing:

```yaml
components:
  terraform:
    cluster:
      vars:
        vpc_id: !store ssm/prod vpc vpc_id
```

To access the configuration of a component in a different stack, you can specify the stack name as the second argument.
For example, here we're reading the `vpc_id` output of the `vpc` component in the `staging` stack:

```yaml
components:
  terraform:
    cluster:
      vars:
        vpc_id: !store ssm/prod staging vpc vpc_id
```

<ActionCard>
For more advanced examples, check out the `!store` YAML function documentation.
<PrimaryCTA to="/functions/yaml/store">Learn More</PrimaryCTA>
</ActionCard>

###  Function: `!terraform.output`

The `!terraform.output` YAML function allows reading the outputs ([remote state](/core-concepts/share-data/remote-state))
of components directly in Atmos stack manifests by internally executing a
[`terraform output`](https://developer.hashicorp.com/terraform/cli/commands/output) or
[`tofu output`](https://opentofu.org/docs/cli/commands/output/) command.

For example, we can read the `vpc_id` output of the `vpc` component in the current stack:

```yaml
components:
  terraform:
    cluster:
      vars:
        vpc_id: !terraform.output vpc vpc_id
```

To access the configuration of a component in a different stack, you can specify the stack name as the second argument.
For example, here we're reading the `vpc_id` output of the `vpc` component in the `prod` stack:

```yaml
components:
  terraform:
    cluster:
      vars:
        vpc_id: !terraform.output vpc prod vpc_id
```

<ActionCard>
For more advanced examples, check out the `!terraform.output` YAML function documentation.
<PrimaryCTA to="/functions/yaml/terraform.output">Learn More</PrimaryCTA>
</ActionCard>

###  Function: `!terraform.state`

The [`!terraform.state`](/functions/yaml/terraform.state) YAML function reads outputs **directly from the configured Terraform or OpenTofu backend**, bypassing the `terraform output` or `tofu output` pipeline — it’s **very fast**, doesn’t require provider initialization, and currently supports [S3 and local backends](/core-concepts/components/terraform/backends) for accessing [remote state](/core-concepts/share-data/remote-state).

For example, we can read the `vpc_id` output of the `vpc` component in the current stack:

```yaml
components:
  terraform:
    cluster:
      vars:
        vpc_id: !terraform.state vpc vpc_id
```

To access the configuration of a component in a different stack, you can specify the stack name as the second argument.
For example, here we're reading the `vpc_id` output of the `vpc` component in the `prod` stack:

```yaml
components:
  terraform:
    cluster:
      vars:
        vpc_id: !terraform.state vpc prod vpc_id
```

<ActionCard>
    For more advanced examples, check out the `!terraform.state` YAML function documentation.
    <PrimaryCTA to="/functions/yaml/terraform.state">Learn More</PrimaryCTA>
</ActionCard>

:::tip
The [`!terraform.state`](/functions/yaml/terraform.state) function accepts the same parameters and
produces the same result as the [`!terraform.output`](/functions/yaml/terraform.output) function,
but has significantly less impact on performance as it reads the state file directly from the configured backend without
executing Terraform/OpenTofu commands, generating varfiles and backend config files, and initializing all modules and providers.

To understand the performance implications of the `!terraform.output` and `!terraform.state` functions,
compare the [!terraform.output Execution Flow](/functions/yaml/terraform.output#terraformoutput-function-execution-flow) with the
[!terraform.state Execution Flow](/functions/yaml/terraform.state#terraformstate-function-execution-flow).
:::

## Using Template Functions

###  Function: `atmos.Store`

The `atmos.Store` template function can read data from a remote store such as SSM Parameter Store, Artifactory, or Redis.

For example, we can read the `vpc_id` output of the `vpc` component in the current stack from the SSM Parameter Store
configured in `atmos.yaml` as `ssm` simply by doing:

```yaml
components:
  terraform:
    cluster:
      vars:
        vpc_id: '{{ atmos.Store "ssm" .stack "vpc" "vpc_id" }}'
```

To access the configuration of a component in a different stack, you can specify the stack name as the second argument.
For example, here we're reading the `vpc_id` output of the `vpc` component in the `staging` stack:

```yaml
components:
  terraform:
    cluster:
      vars:
        vpc_id: '{{ atmos.Store "ssm" "staging" "vpc" "vpc_id" }}'
```

<ActionCard>
    For more advanced examples, check out the `atmos.Store` template function documentation.
    <PrimaryCTA to="/functions/template/atmos.Store">Learn More</PrimaryCTA>
</ActionCard>

###  Function: `atmos.Component`

The `atmos.Component` template function can read all configurations of any Atmos component, including its outputs.

For example, we can read the `vpc_id` output of the `vpc` component in the current `.stack`, simply by doing:

```yaml
components:
  terraform:
    cluster:
      vars:
        vpc_id: '{{ (atmos.Component "vpc" .stack).outputs.vpc_id }}'
```

The `atmos.Component` function returns the entire configuration of the component in the stack. The configuration is a map of all the sections of the component, including its outputs. You can access properties using dot (`.`) notation, and chain any number of attributes with dot (`.`) notation.

To access the configuration of a component in a different stack, you can specify the stack name as the second argument. For example, here we're reading the `vpc_id` output of the `vpc` component in the `staging` stack:

```yaml
components:
  terraform:
    cluster:
      vars:
        vpc_id: '{{ (atmos.Component "vpc" "staging").outputs.vpc_id }}'
```

<ActionCard>
For more advanced examples, check out the `atmos.Component` function documentation.
<PrimaryCTA to="/functions/template/atmos.Component">Learn More</PrimaryCTA>
</ActionCard>

### Data Sources

Data sources are incredibly powerful. They let you glue together components leveraging external data sources without modifying a line of Terraform code. This is great when you want to leave your Terraform codebase untouched, especially if you don't control the source.

Data sources allow you to fetch and use data from external sources in your stack configurations. You can use data sources to fetch data from APIs, various key/value storage systems, or even local files.

They can be fetched from any of the following schemes supported by Gomplate:

<CollapsibleText>
- **AWS Systems Manager Parameter Store** (`aws+smp://`)
- **AWS Secrets Manager** (`aws+sm://`)
- **Amazon S3** (`s3://`)
- **HashiCorp Consul** (`consul://`, `consul+http://`, `consul+https://`)
- **Environment Variables** (`env://`)
- **Files** (`file://`)
- **Git Repositories** (`git://`, `git+file://`, `git+http://`, `git+https://`, `git+ssh://`)
- **Google Cloud Storage** (`gs://`)
- **HTTP/HTTPS Endpoints** (`http://`, `https://`)
- **Merging Data Sources** (`merge://`)
- **Standard Input** (`stdin://`)
- **HashiCorp Vault** (`vault://`, `vault+http://`, `vault+https://`)
</CollapsibleText>

:::tip On-the-Fly Root Modules

When you combine data sources with [vendoring](/core-concepts/vendor), [terraform backends](/core-concepts/components/terraform/backends) and [provider](/core-concepts/components/terraform/providers) generation, you can leverage any Terraform module as a "root module" and provision it as a component with Atmos.
:::

Configure your data sources in `atmos.yaml`, then leverage them inside stack configurations.

Here we set up a data source called `ip`, which will fetch the public IP address by hitting the
`https://api.ipify.org?format=json` endpoint.

<File title="atmos.yaml">
```yaml
settings:
  templates:
    settings:
      gomplate:
        timeout: 5
        datasources:
          network_egress:
            url: "https://api.ipify.org?format=json"
            headers:
              accept:
                - "application/json"
```
</File>

Then, you can use the `network_egress` data source in your stack configurations to fetch the public `ip`. This is useful for setting a tag indicating the IP address that provisioned the resources.

<Note>This assumes the Terraform component accepts a `tags` variable and appropriately handles tags.</Note>
<File title="stack.yaml">
```yaml
terraform:
  vars:
    tags:
      provisioned_by_ip: '{{ (datasource "ip").ip }}'
```
</File>

<ActionCard>
  Use data sources to fetch data from external sources and use it in your Terraform configurations.
  <PrimaryCTA to="/core-concepts/stacks/templates/datasources">Learn More</PrimaryCTA>
</ActionCard>

## Using Terraform Remote State

Atmos provides a [`remote-state`](https://github.com/cloudposse/terraform-yaml-stack-config/tree/main/modules/remote-state) Terraform module that makes it easier to look up the remote state of other components in the stack. This module can be used to share data between components provisioned in the same stack or across different stacks, using native HCL.

Our convention is to place all remote-state dependencies in the `remote-state.tf` file. This file is responsible for fetching the remote state outputs of other components in the stack.

<File title="components/terraform/myapp/remote-state.tf">
```hcl
module "vpc" {
  source  = "cloudposse/stack-config/yaml//modules/remote-state"
  version = "1.5.0"

  # Specify the Atmos component name (defined in YAML stack config files) for which to get the remote state outputs
  component = "vpc"

  # `context` input is a way to provide the information about the stack (using the context
  # variables `namespace`, `tenant`, `environment`, `stage` defined in the stack config)
  context = module.this.context
}
```
</File>

Then we can use the `module.vpc` as easily as if it were provisioned within the `myapp` component.

This gives us the best of both worlds: the ease of use of Terraform remote state and the reduced blast radius of using smaller components.

<File title="components/terraform/myapp/main.tf">
```hcl
resource "aws_network_acl" "default" {

  vpc_id = module.vpc.vpc_id

  ingress {
    protocol   = "tcp"
    rule_no    = 100
    action     = "allow"
    cidr_block = "0.0.0.0/0"
    from_port  = 80
    to_port    = 80
  }
}
```
</File>

<ActionCard>
  Use the Terraform-native `remote-state` module to share data between components.
  <PrimaryCTA to="/core-concepts/share-data/remote-state">Learn How</PrimaryCTA>
</ActionCard>

---

## Stack Catalogs

import Intro from '@site/src/components/Intro'

<Intro>
As you start splitting your stacks apart into smaller configurations, it often makes to organize those into a catalog of reusable configurations. That way you can take advantage of imports, to reuse configuration in multiple places. Catalogs are how to logically organize all the child [Stack](/core-concepts/stacks) configurations on the filesystem for use by [imports](/core-concepts/stacks/imports).
</Intro>

There's no "right or wrong" way to do it, and Atmos does not enforce any one convention.
What we've come to realize is there's no "one way" to organize Stack configurations. The best way to organize them will come down to
how an organization wants to model its infrastructure.

:::tip See Design Patterns

We go into greater depth on this convention in our [design patterns](/design-patterns/):

- [Component Catalogs](/design-patterns/component-catalog)
- [Component Catalogs with Mixins](/design-patterns/component-catalog-with-mixins)
- [Component Catalogs with Templates](/design-patterns/component-catalog-template)
:::

Below is how we implement them at [Cloud Posse](https://cloudposse.com).

## Conventions

We provide a number of recommended conventions for your Stack catalogs. You can use all of them or some of them. These conventions have come about from our [customer engagements](https://cloudposse.com/services).

[Cloud Posse](https://cloudposse.com) typically uses `orgs` as the parent stacks, which import `teams`, `mixins` and other services from a `catalog`.

## Filesystem Layout

Here's an example of how Stack imports might be organized on disk.

```console
└── stacks/
    ├── mixins/
    │   └── region/
    │       ├── us-east-1.yaml
    │       ├── us-west-2.yaml
    │       └── eu-west-1.yaml
    │   └── stage/
    ├── teams/
    │   └── frontend/
    │       └── example-application/
    │           └── microservice/
    │               ├── prod.yaml
    │               ├── dev.yaml
    │               └── staging.yaml
    └── catalogs/
        ├── vpc/
        │   └── baseline.yaml
        └── database/
            ├── baseline.yaml
            ├── small.yaml
            ├── medium.yaml
            └── large.yaml
```

## Types of Catalogs

### Mixins

We go into more detail on using [Mixins](/core-concepts/stacks/inheritance/mixins) to manage snippets of reusable configuration. These Mixins are frequently used alongside the other conventions such as Teams and Organizations.

### Teams

When infrastructure gets very large and there's numerous teams managing it, it can be helpful to organize Stack configurations around the notion of "teams". This way it's possible to leverage [`CODEOWNERS`](https://docs.github.com/en/repositories/managing-your-repositorys-settings-and-features/customizing-your-repository/about-code-owners) together with [branch protection rules](https://docs.github.com/en/repositories/configuring-branches-and-merges-in-your-repository/defining-the-mergeability-of-pull-requests/about-protected-branches#require-pull-request-reviews-before-merging) to restrict who can merge pull requests that affect infrastructure.

Here's what that might look like:
```console
└── stacks/
    └── teams/
        └── frontend/
            └── ecom-store/
                ├── checkout/
                │    ├── prod.yaml
                │    ├── dev.yaml
                │    └── staging.yaml
                └── cart/
                    ├── prod.yaml
                    ├── dev.yaml
                    └── staging.yaml
```

In this example, there's a `frontend` team that owns an `ecom-store` application. The application consists of two microservices called `checkout` and `cart`. Each microservice has (3) stages: `dev`, `staging` and `prod`.

### Organizations

The organizational layout of Stacks is useful for modeling how infrastructure gets "physically" deployed with a given Infrastructure as a Service (IaaS) platform like AWS.

AWS infrastructure is hierarchical and can be thought of like this:

1. The top-level account is the "Organization"
2. An "Organization" can have any number of "Organizational Units" (OUs)
3. Each "OU" can have "Member Accounts"
4. Each "Member Account" has "Regions"
5. Each "Region" has "Resources" (the top-level stack)

In sticking with this theme, a good filesystem layout for infrastructure looks like this:

```text
└── stacks/
    └── orgs/
        └── acme/
            ├── ou1/
            │   ├── account1/
            │   │   ├── global-region.yaml
            │   │   └── us-east-2.yaml
            │   ├── account2/
            │   │   ├── global-region.yaml
            │   │   └── us-east-2.yaml
            │   └── account3/
            │       ├── global-region.yaml
            │       └── us-east-2.yaml
            └── ou2/
                ├── dev/
                │   ├── global-region.yaml
                │   └── us-east-2.yaml
                ├── prod/
                │   ├── global-region.yaml
                │   └── us-east-2.yaml
                └── staging/
                    ├── global-region.yaml
                    └── us-east-2.yaml
```

:::info
Cloud Posse uses the "Organizations" layout for all the "parent stacks". Parent stacks are the top-level stacks which are responsible for importing
the other child stacks (e.g. teams, mixins, etc.)
:::

What's important to point out is that all these conventions are not mutually exclusive. In fact, we like to combine them.

Here's what that might look like:

```console
└── orgs/
    └── acme/
        └── platform/
            ├── prod/
            │   ├── us-east-1/
            │   │    ├── networking.yaml
            │   │    ├── compliance.yaml
            │   │    ├── backing-services.yaml
            │   │    └── teams.yaml
            │   └── us-west-2/
            │       ├── networking.yaml
            │       ├── compliance.yaml
            │       ├── backing-services.yaml
            │       └── teams.yaml
            ├── staging/
            │   └── us-west-1/
            │       ├── networking.yaml
            │       ├── compliance.yaml
            │       ├── backing-services.yaml
            │       └── teams.yaml
            └── dev/
                └── us-west-2/
                    ├── networking.yaml
                    ├── backing-services.yaml
                    └── teams.yaml

```

In this example, there's a single organization called `acme` with an example of one organizational unit (OU) called `platform`. The OU has 3 stages: `dev`, `staging`, and `prod`. Each stage then operates in a number of regions. Each region then has a `networking` layer, a `backing-services` layer, and a `teams` layer. The `staging` and `prod` accounts have both have a `compliance` layer, which isn't needed in the `dev` stages.

The files like `networking.yaml` and `compliance.yaml` can be named anything you want. It's helpful to think about organizing Components based on their lifecycles or according to a concept of layers that stack on top of each other.

### Everything Else

For everything else, we usually have catalog that we just call `catalog/`. We place it underneath the `stacks/` folder. This is for everything else we want to define once and reuse. Use whatever convention makes sense for your company.

## Refactoring Configurations

One of the amazing things about the Atmos [Stack](/core-concepts/stacks) configurations is that the entire state of configuration is stored in the YAML configurations. The filesystem layout has no bearing on the desired state of the configuration. This means that configurations can be easily refactored at at time in the future, if you discover there's a better way to organize your Stack configurations. So long as the deep-merged configuration is the same, it will not affect any of the [Components](/core-concepts/components).

## References

- [Component Catalog Atmos Design Pattern](/design-patterns/component-catalog)
- [Component Catalog with Mixins Atmos Design Pattern](/design-patterns/component-catalog-with-mixins)
- [Component Catalog Template Atmos Design Pattern](/design-patterns/component-catalog-template)

---

## Configuring Components in Stacks

import Intro from '@site/src/components/Intro'

<Intro>
Stacks are used to compose multiple components together and provide their configuration. The schema is the same for all stacks, but the configuration can be different. Use a combination of [imports](/core-concepts/stacks/imports), [inheritance](/core-concepts/stacks/inheritance), and [catalogs](/core-concepts/stacks/catalogs) for a template-free way to reuse configuration and [override](/core-concepts/stacks/overrides) values when needed.
</Intro>

## Component Schema

A Component consists of the infrastructure as code business logic (e.g. a Terraform "root" module) as well as the configuration of that
component. The configuration of a component is stored in a Stack configuration.

To configure a Component in a [Stack](/core-concepts/stacks), you define the component in the `components` section of the Stack configuration.

:::info Disambiguation

- **Terraform Component** is a simply a [Terraform Root Module](https://developer.hashicorp.com/terraform/language/modules#the-root-module)
  that consists of the resources defined in the `.tf` files in a working directory
  (e.g. [components/terraform/infra/vpc](https://github.com/cloudposse/atmos/tree/main/tests/fixtures/scenarios/complete/components/terraform/infra/vpc))

- **Component Configuration** provides configuration (variables and other settings) for a type of component (e.g. a Terraform component)
  and is defined in one or more YAML stack config files (which are called [Atmos stacks](/core-concepts/stacks))
:::

### Terraform Schema

The schema of an Atmos Terraform Component in an Atmos Stack is as follows:

```yaml
components:
  terraform:
    # the slug of the component
    example:

      # configuration specific to atmos
      metadata:
        # Components can be of type "real" (default) or "abstract"
        type: real
        # This is the directory path of the component.
        # In this example, we're referencing a component in the `components/terraform/stable/example` folder.
        component: stable/example

        # We can leverage multiple inheritance to sequentially deep merge multiple configurations
        inherits:
          - example-defaults

        # Settings are where we store configuration related to integrations.
        # It's a freeform map; anything can be placed here.
        settings:
          spacelift: {}

      # Define the terraform variables, which will get deep-merged and exported to a `.tfvars` file by atmos.
      vars:
        enabled: true
        name: superduper
        nodes: 10
```

#### Terraform Attributes

<dl>
  <dt>`vars` (optional)</dt>
  <dd>The `vars` section is a free-form map. Use [component validation](/core-concepts/validate) to enforce policies.</dd>

  <dt>`vars.namespace` (optional)</dt>
  <dd>
  This is an *optional* [`terraform-null-label`](https://github.com/cloudposse/terraform-null-label) convention.

  The namespace of all stacks. Typically, there will be one namespace for the organization.

  Example:

  ```yaml
  vars:
    namespace: acme
  ```
  </dd>

  <dt>`vars.tenant` (optional)</dt>
  <dd>
  This is an *optional* [`terraform-null-label`](https://github.com/cloudposse/terraform-null-label) convention.

  In a multi-tenant configuration, the tenant represents a single `tenant`. By convention, we typically
  recommend that every tenant have its own Organizational Unit (OU).

  Example:

  ```yaml
  vars:
    tenant: platform
  ```
  </dd>

  <dt>`vars.stage` (optional)</dt>
  <dd>
  This is an *optional* [`terraform-null-label`](https://github.com/cloudposse/terraform-null-label) convention.

  The `stage` is where workloads run. See our [glossary](/terms) for disambiguation.

  Example:

  ```yaml
  vars:
    # Production stage
    stage: prod
  ```
  </dd>

  <dt>`vars.environment` (optional)</dt>
  <dd>
  This is an *optional* [`terraform-null-label`](https://github.com/cloudposse/terraform-null-label) convention.

  The `environment` is used for location where things run. See our [glossary](/terms) for disambiguation.

  Example:

  ```yaml

  vars:
    # us-east-1
    environment: ue1
  ```
  </dd>

  <dt>`metadata` (optional)</dt>
  <dd>The `metadata` section extends functionality of the component.</dd>

  <dt>`settings`</dt>
  <dd>The `settings` block is a free-form map used to pass configuration information to [integrations](/integrations).</dd>

</dl>

### Helmfile Schema

The schema of an Atmos Helmfile Component in an Atmos Stack is as follows:

```yaml
components:
  helmfile:
    # the slug of the component
    example:

      # configuration specific to atmos
      metadata:
        # Components can be of type "real" (default) or "abstract"
        type: real

        # This is the directory path of the component.
        # In this example, we're referencing a component in the `components/terraform/stable/example` folder.
        component: stable/example

        # We can leverage multiple inheritance to sequentially deep merge multiple configurations
        inherits:
          - example-defaults

      # Define the Helmfile variables, which will get deep-merged into the Helmfile configuration.
      vars:
        enabled: true
        release_name: my-release
        chart_version: "1.2.3"
```

#### Helmfile Attributes

<dl>

  <dt>`vars` (optional)</dt>
  <dd>The `vars` section is a free-form map. Use [component validation](/core-concepts/validate) to enforce policies.</dd>

  <dt>`vars.namespace` (optional)</dt>
  <dd>
  This is an *optional* [`terraform-null-label`](https://github.com/cloudposse/terraform-null-label) convention.

  The namespace of all stacks. Typically, there will be one namespace for the organization.

  Example:

  ```yaml
  vars:
    namespace: acme
  ```
  </dd>

  <dt>`metadata` (optional)</dt>
  <dd>The `metadata` section extends functionality of the component.</dd>

  <dt>`settings`</dt>
  <dd>The `settings` block is a free-form map used to pass configuration information to [integrations](/integrations).</dd>

</dl>

### Types of Components

In Atmos, each component configuration defines its type through the `metadata.type` parameter. This defines how the component behaves—whether it can be used directly to provision resources or serves as a base configuration for other components.

The type of component is expressed in the `metadata.type` parameter of a given component configuration.

There are two types of components:
<dl>
  <dt>`real`</dt>
  <dd>Think of a `real` component as one that can be deployed. It’s fully configured and ready to be provisioned, similar to a "concrete" class in programming. Once defined, you can use it to create resources or services directly in your infrastructure.</dd>

  <dt>`abstract`</dt>
  <dd>An `abstract` component is more like a blueprint. It can’t be deployed on its own. Instead, it’s a base configuration that needs to be extended or inherited by other components. This is similar to an ["abstract base classes"](https://en.wikipedia.org/wiki/Abstract_type) in programming—it defines reusable configurations, but it’s not complete enough to be deployed directly.</dd>
</dl>

### Disabling Components with `metadata.enabled`

The `metadata.enabled` parameter controls whether a component is included in deployment. By default, components are enabled. Setting `metadata.enabled` to `false` skips the component entirely—no workspace is created, and no Terraform commands are executed. Disabling a component does not cause deletion. It just signals that it's no longer managed by Atmos.

:::info Note
This should not be confused with [Cloud Posse's conventions and best practices](/best-practices/terraform/) of
having modules and components define a Terraform input named `enabled`.
This is a general convention and `vars.enabled` is not a special variable. Atmos does not treat it differently from any other variable.
:::

**Example**:
```yaml
# Disable a component in a specific environment
components:
  terraform:
    vpc:
      metadata:
        type: real
        enabled: false
      vars:
        name: primary-vpc
```
Using the `metadata.enabled` flag makes it easy to ensure that only the intended components are active in each environment.

### Locking Components with `metadata.locked`

The `metadata.locked` parameter prevents changes to a component while still allowing read operations. When a component is locked, operations that would modify infrastructure (like `terraform apply`) are blocked, while read-only operations (like `terraform plan`) remain available. By default, components are unlocked. Setting `metadata.locked` to `true` prevents any change operations.

:::info Note
Locking a component does not affect the Terraform state. It's intended as a way to communicate intention and prevent accidental changes to sensitive or critical infrastructure.
:::

**Example**:
```yaml
# Lock a production database component to prevent accidental changes
components:
  terraform:
    rds:
      metadata:
        locked: true
      vars:
        name: production-database
```

Using the `metadata.locked` flag helps protect critical infrastructure from unintended modifications while still allowing teams to inspect and review the configuration.

---

## Configure Dependencies Between Components

import File from '@site/src/components/File'
import Intro from '@site/src/components/Intro'
import Terminal from '@site/src/components/Terminal'

<Intro>
    Atmos supports configuring the relationships between components in the same or different stacks. You can define
    dependencies between components to ensure that components are deployed in the correct order.
</Intro>

Before deploying components, it's important to consider the dependencies between components.
For example, a database component might depend on a network component.
When this happens, it's important to ensure that the network component is deployed before the database component.

:::important Support for Dependencies
Support for dependencies is reliant on the [integration](/integrations) used and not all integrations support dependencies.

For example, GitHub Actions do not support dependency order applies, while [Spacelift does](https://docs.spacelift.io/concepts/stack/stack-dependencies).
:::

You can define component dependencies by using the `settings.depends_on` section. The section used to define all the Atmos components (in
the same or different stacks) that the current component depends on.

The `settings.depends_on` section is a map of objects. The map keys are just the descriptions of dependencies and can be strings or numbers. Provide meaningful descriptions or numbering so that people can understand what the dependencies are about.

<details>
    <summary>Why is `settings.depends_on` a map instead of a list?</summary>

    We originally implemented `settings.depends_on` as a list. However, since it’s not clear how lists should be
    deep-merged, so we decided to convert it to a map instead. In this map, the keys are lexicographically ordered, and
    based on that order, the dependencies are managed.
</details>

Each object in the `settings.depends_on` section has the following schema:

<dl>
    <dt>file (optional)</dt>
    <dd>A file on the local filesystem that the current component depends on</dd>

    <dt>folder (optional)</dt>
    <dd>A folder on the local filesystem that the current component depends on</dd>

    <dt>component (required if `file` or `folder` is not specified)</dt>
    <dd>an Atmos component that the current component depends on</dd>

    <dt>stack (optional)</dt>
    <dd>The Atmos stack where the `component` is provisioned</dd>

    <dt>namespace (optional)</dt>
    <dd>The `namespace` where the `component` is provisioned</dd>

    <dt>tenant (optional)</dt>
    <dd>The `tenant` where the `component` is provisioned</dd>

    <dt>environment (optional)</dt>
    <dd>The `environment` where the `component` is provisioned</dd>

    <dt>stage (optional)</dt>
    <dd>The `stage` where the `component` is provisioned</dd>
</dl>

One of `component`, `file` or `folder` is required.

If `component` is specified, you can provide the other context variables to define an Atmos stack other than the current stack.

For example, you can specify:

- `stack` if the `component` is from a different Atmos stack
- `namespace` if the `component` is from a different Organization
- `tenant` if the `component` is from a different Organizational Unit
- `environment` if the `component` is from a different region
- `stage` if the `component` is from a different account
- `tenant`, `environment` and `stage` if the component is from a different Atmos stack (e.g. `tenant1-ue2-dev`)

:::info
If `stack` is specified, it's processed first and the `namespace`, `tenant`, `environment` and `stage` attributes are ignored.
:::

:::tip
You can use [Atmos Stack Manifest Templating](/core-concepts/stacks/templates) in `depends_on`.
Atmos processes the templates first, and then detects all the dependencies, allowing you to provide the parameters to
`depends_on` dynamically.
:::

## Examples

In the following example, we specify that the `component1` component depends on the following:

- The `component2` component in the same Atmos stack as `component1`
- The `component3` component from the `prod` stage
- The `component4` component from the `tenant1` tenant, `ue2` environment and `staging` stage (`tenant1-ue2-staging` Atmos stack)
- The `component5` component from the `tenant1-ue2-prod` Atmos stack
- The `component6` component from the same Atmos stack as `component1`
- The `component7` component from the same tenant and stage as `component1`, but `uw2` environment

<File title="tenant1-ue1-dev.yaml">
```yaml
vars:
  tenant: "tenant1"
  environment: "ue1"
  stage: "dev"

components:
  terraform:
    component1:
      settings:
        depends_on:
          1:
            # If the context (`stack`, `namespace`, `tenant`, `environment`, `stage`) is not
            # provided, the `component` is from the same Atmos stack as `component1`
            component: "component2"
          2:
            # `component1` (in any stage) depends on `component3`
            # from the `prod` stage (in any `environment` and any `tenant`)
            component: "component3"
            stage: "prod"
          3:
            # `component1` depends on `component4`
            # from the the `tenant1` tenant, `ue2` environment and `staging` stage
            # (`tenant1-ue2-staging` Atmos stack)
            component: "component4"
            tenant: "tenant1"
            environment: "ue2"
            stage: "staging"
          4:
            # `component1` depends on `component5`
            # from the `tenant1-ue2-prod` Atmos stack
            component: "component5"
            stack: "tenant1-ue2-prod"
          5:
            # `component1` depends on `component6`
            # from the same Atmos stack
            component: "component6"
            stack: "{{ .vars.tenant }}-{{ .vars.environment }}-{{ .vars.stage }}"
          6:
            # `component1` depends on `component7`
            # from the same tenant and stage as `component1`, but `uw2` environment
            component: "component7"
            stack: "{{ .vars.tenant }}-uw2-{{ .vars.stage }}"
      vars:
        enabled: true
```
</File>

## Specifying `stack`

The `stack` attribute has higher precedence than the other context variables.
If `stack` is specified, the `namespace`, `tenant`, `environment` and `stage` attributes are ignored.

As you can see in the examples above, we can use [Atmos Stack Manifest Templating](/core-concepts/stacks/templates) in the `stack` attribute to dynamically specify the stack.

This is useful when configuring
[`stacks.name_template` in `atmos.yaml`](/core-concepts/projects/configuration/#stack-names-slugs) to define and refer to stacks.
In this case, you can't use the context variables `namespace`, `tenant`, `environment` and `stage` in `depends_on`.

For example, in `atmos.yaml`, we specify `stacks.name_template` to define Atmos stacks, and enable templating:

<File title="atmos.yaml">
```yaml
stacks:
  base_path: "stacks"
  name_template: "{{ .settings.context.tenant }}-{{ .settings.context.environment }}-{{ .settings.context.stage }}"

# `Go` templates in Atmos manifests
templates:
  settings:
    enabled: true
```
</File>

:::note
In this example, stacks are defined by the `settings.context` section, not `vars`.
:::

In the `tenant1-uw2-dev` Atmos stack, we can use the following `depends_on` configuration to define the component dependencies:

<File title="tenant1-uw2-dev.yaml">
```yaml
settings:
  context:
    tenant: "tenant1"
    environment: "uw2"
    stage: "dev"

components:
  terraform:
    vpc:
      vars:
        enabled: true

    tgw/attachment:
      settings:
        depends_on:
          1:
            # `tgw/attachment` depends on the `vpc` component
            # from the same Atmos stack (same tenant, account and region)
            component: vpc
            # NOTE: The same stack can be specified by using exactly the same template as in
            # `stacks.name_template` in `atmos.yaml`, but it's not required and not recommended.
            # If the dependent component is from the same stack, just omit the `stack` attribute completely.
            # stack: "{{ .settings.context.tenant }}-{{ .settings.context.environment }}-{{ .settings.context.stage }}"
          2:
            # `tgw/attachment` depends on the `tgw/hub` components
            # from the same tenant and account, but in `us-east-1` region (`ue1` environment)
            component: tgw/hub
            stack: "{{ .settings.context.tenant }}-ue1-{{ .settings.context.stage }}"

    tgw/cross-region-hub-connector:
      settings:
        depends_on:
          1:
            # `tgw/cross-region-hub-connector` depends on `tgw/hub` components
            # in the same tenant and account, but in `us-east-1` region (`ue1` environment)
            component: tgw/hub
            stack: "{{ .settings.context.tenant }}-ue1-{{ .settings.context.stage }}"
```
</File>

Execute the following Atmos commands to see the component dependencies:

<Terminal title="atmos describe dependents vpc -s tenant1-uw2-dev --pager off">
```shell
> atmos describe dependents vpc -s tenant1-uw2-dev --pager off
```

```json
[
  {
    "component": "tgw/attachment",
    "component_type": "terraform",
    "stack": "tenant1-uw2-dev",
    "stack_slug": "tenant1-uw2-dev-tgw-attachment"
  }
]
```
</Terminal>

<Terminal title="atmos describe dependents tgw/hub -s tenant1-ue1-dev --pager off">
```shell
> atmos describe dependents tgw/hub -s tenant1-ue1-dev --pager off
```

```json
[
  {
    "component": "tgw/attachment",
    "component_type": "terraform",
    "stack": "tenant1-uw2-dev",
    "stack_slug": "tenant1-uw2-dev-tgw-attachment"
  },
  {
    "component": "tgw/cross-region-hub-connector",
    "component_type": "terraform",
    "stack": "tenant1-uw2-dev",
    "stack_slug": "tenant1-uw2-dev-tgw-cross-region-hub-connector"
  }
]
```
</Terminal>

:::tip

For more information, refer to [`atmos describe dependents`](/cli/commands/describe/dependents)
and [`atmos describe affected`](/cli/commands/describe/affected) CLI commands.

:::

---

## Manage Lifecycle Events with Hooks

import Terminal from '@site/src/components/Terminal'
import Intro from '@site/src/components/Intro'
import File from '@site/src/components/File'

<Intro>
Atmos supports the ability to take action at various points in the lifecycle of your components. This is done by
configuring the `hooks` section in your stack manifest for the component that you want to take action on.
</Intro>

## Hooks Schema

The `hooks` section schema is as follows:

```yaml
hooks:
  store-outputs:
    events:
      - after-terraform-apply
    command: store
    name: prod/ssm
    outputs:
      vpc_id: .id
```

This schema can be specified at the top level of the stack configuration (global), within the `terraform` section,
inside individual components, or in the `overrides` section. Partial config can also be specified at various levels
to help keep the configuration [DRY](https://en.wikipedia.org/wiki/Don%27t_repeat_yourself).

#### An example demonstrating this concept is below:

At the global level, set that the store command will run after terraform apply:

```yaml
# stacks/catalog/vpc/_defaults.yaml (global)
hooks:
  store-outputs:
    events:
      - after-terraform-apply
    command: store
```
In the production account, use the `prod/ssm` store (configured in atmos.yaml):

```yaml
# stacks/orgs/acme/plat/prod/_defaults.yaml (terraform)
terraform:
  hooks:
    store-outputs:
      name: prod/ssm
```

At the component level, specify that the `id` output of the component should be stored in the store as the `vpc_id` key:

```yaml
# stacks/orgs/acme/plat/prod/us-east-2.yaml (component)
components:
  terraform:
    vpc:
      hooks:
        store-outputs:
          outputs:
            vpc_id: .id
```

## Supported Lifecycle Events

Atmos supports the following lifecycle events:

- `after-terraform-apply` (this event is triggered after the `atmos terraform apply` or `atmos terraform deploy` command is run)

## Supported Commands

## store

The `store` command is used to write data to a remote store.

<dl>
  <dt>`hooks.[hook_name]`</dt>
  <dd>This map key is the name you want to give to the hook. This must be unique for each hook in the component.</dd>

  <dt>`hooks.[hook_name].events`</dt>
  <dd>
  This is a list of [Supported Lifecycle Events](#supported-lifecycle-events) that should trigger running the command.
  </dd>

  <dt>`hooks.[hook_name].command`</dt>
  <dd>Must be set to `store`</dd>

  <dt>`hooks.[hook_name].name`</dt>
  <dd>The name of the store to use.</dd>

<dt>`hooks.[hook_name].outputs`</dt>
  <dd>
  A map of values that will be written to the store under the key for this component. The key is the name of the key in
  the store. The value is the value to write to the store. If the value begins with a dot (`.`), it will be treated as a
  [Terraform output](https://developer.hashicorp.com/terraform/language/values/outputs) and the value will be retrieved
  from the Terraform state for the current component.
  </dd>
</dl>

---

## Import Stack Configurations

import File from '@site/src/components/File'
import Intro from '@site/src/components/Intro'

<Intro>
As your stacks grow taller with more and more component configurations, it often makes sense to start splitting them apart into different files. That's why you might want to take advantage of imports. This helps you keep your stack files smaller so they are easier to understand, while reusing their configuration in multiple places.
</Intro>

Each import overlays on top of others, and gets deep merged. Then we support [inheritance](/core-concepts/stacks/inheritance) and overrides to manage configuration variations, all without relying on templating. When none of these other methods work for your use-case, we provide an "Escape Hatch" with [templating](/core-concepts/stacks/templates).

## Use cases

- **DRY Configuration:** Imports are how we reduce duplication of configurations.
- **Configuration Blueprints:** Define reusable baselines or "defaults". Think of them almost as blueprints, that you can reuse anytime you want some particular combination of components in a stack.
- **Service Catalogs:** Provide a "Service Catalog" for your team with reusable configurations that anyone can use to easily compose architectures with golden-path configurations.

:::warning Pitfalls!
Overusing imports can make configurations harder to understand. We recommend limiting import levels to maintain clarity. Review our [best practices](/best-practices/stacks) for practical guidance.
:::

Imports may be used in Stack configurations together with [inheritance](/core-concepts/stacks/inheritance)
and [mixins](/core-concepts/stacks/inheritance/mixins) to produce an exceptionally DRY configuration in a way that is logically organized and easier to maintain by your team.

## Configuration

To import any stack configuration from the `catalog/`, simply define an `import` section at the top of any [Stack](/core-concepts/stacks)
configuration. Technically, it can be placed anywhere in the file, but by convention we recommend putting it at the top.

Here are some simple examples of how to import configurations:
```yaml
import:
  - catalog/file1 # First import "file1" from the catalog
  - catalog/file2 # Second import "file2" from the catalog, deep merging on top of the first import
  - catalog/file3 # Third import "file3" from the catalog, deep merging on top of the preceding imports
```

The base path for imports is specified in the [`atmos.yaml`](/cli/configuration) in the `stacks.base_path` section.

If no file extension is used, a `.yaml` extension is automatically appended.

It's also possible to specify file extensions, although we do not recommend it.

```yaml
import:
  - catalog/file1.yml   # Explicitly load a file with a .yml extension
  - catalog/file2.yaml  # Explicitly load a file with a .yaml extension
  - catalog/file3.YAML  # Explicitly load a file with a .YAML extension
```

### Automatic Template File Detection

When importing files without specifying an extension, Atmos will now automatically search for and use template versions of the files if they exist. The search order is:

1. `.yaml`
2. `.yml`
3. `.yaml.tmpl`
4. `.yml.tmpl`

For example, if you import `catalog/file1`, Atmos will:
1. First look for `catalog/file1.yaml` or `catalog/file1.yml`
2. If found, check if a template version exists (`catalog/file1.yaml.tmpl` or `catalog/file1.yml.tmpl`)
3. Use the template version if it exists, otherwise use the regular YAML file
4. If no files are found, default to using `.yaml` extension

This feature makes it easier to work with templated configurations as you don't need to explicitly specify the template file extension - Atmos will automatically use the template version when available.

:::note Template File Validation
While template files are automatically detected and processed during normal operations (imports, etc.), they are excluded from YAML validation (`atmos validate stacks`) since they may contain template placeholders that are invalid YAML before being rendered.

This means:
- Template files are fully supported for imports and normal operations
- Template files are skipped during `atmos validate stacks` to prevent validation errors from unrendered templates
- You don't need to explicitly specify template extensions - Atmos will find them automatically
:::

## Import Path Resolution

Atmos supports two types of import paths:

### Base-Relative Paths (Default)
Most imports use paths relative to the `stacks.base_path` configured in `atmos.yaml`:
- `catalog/vpc/defaults`
- `mixins/region/us-east-2`
- `orgs/acme/_defaults`

These paths are resolved from the base stacks directory, regardless of where the importing file is located.

### File-Relative Paths
Imports starting with `.` or `..` are relative to the current file's directory:
- `./_defaults` - imports from the same directory as the current file
- `../shared/_defaults` - imports from a sibling `shared` directory

This is useful when you want to import files that are co-located with the current configuration.

## Conventions

We recommend placing all baseline "imports" in the `stacks/catalog` folder, however, they can exist anywhere.

Use [mixins](/core-concepts/stacks/inheritance/mixins) for reusable snippets of configurations that alter the behavior of Stacks in some way.

### The _defaults.yaml Pattern

Many Atmos projects use `_defaults.yaml` as a naming convention for default configurations at each level of the hierarchy. This is purely a convention—Atmos has no special handling for these files. They must be explicitly imported like any other file.

The underscore prefix ensures they:
- Sort to the top of directory listings (lexicographic sorting)
- Are visually distinct from actual stack configurations
- Are excluded from stack discovery (via `excluded_paths` configuration)

:::info
The `_defaults.yaml` pattern is a common convention, not an Atmos feature. These files are only excluded from stack discovery because they match the pattern in `excluded_paths` configuration. They must always be explicitly imported to take effect.
:::

For a complete explanation of this pattern, see the [_defaults.yaml Design Pattern](/design-patterns/defaults-pattern) documentation.

## Imports Schema

The `import` section supports two different formats, depending on whether the imported files use templates or not. One is a list of strings representing paths to the imported files, and the other is a list of objects with several feature flags.

### Imports without Templates

For a list of paths to the imported files, just provide a list of strings like this:

  ```yaml title="stacks/orgs/cp/tenant1/test1/us-east-2.yaml"
  import:
    - mixins/region/us-east-2
    - orgs/cp/tenant1/test1/_defaults
    - catalog/terraform/top-level-component1
    - catalog/terraform/test-component
    - catalog/terraform/vpc
    - catalog/helmfile/echo-server
  ```

### Imports with Templates

Sometimes you may want to import files that use Go templates. Templates can be used with or without providing a `context` - files with `.yaml.tmpl` or `.yml.tmpl` extensions are always processed as Go templates.

:::important
  Files with the `.yaml.tmpl` or `.yml.tmpl` extension are always processed as Go templates, regardless of whether `context` is provided.
  This allows you to use template functions that don't require context (like `{{ now }}`, `{{ env "VAR" }}`, `{{ uuidv4 }}`, etc.) even without providing context variables.
  If you don't want a file to be processed as a template, use the `.yaml` or `.yml` extension instead.
  The `skip_templates_processing` flag can be used to explicitly skip template processing for any imported file.
  Templating must be enabled in [`atmos.yaml`](/core-concepts/stacks/templates) for Atmos to process the imported files as Go templates.
:::

For example, here we import a file with a template and provide a `context` to passing two variables.

  ```yaml
  import:
    - path: "catalog/something.yaml.tmpl" # Path to the imported file with the required .tmpl extension for Go templates
      context:
        foo: bar
        baz: qux
      skip_templates_processing: false
      ignore_missing_template_values: false
      skip_if_missing: false
    - path: "catalog/something.yaml.tmpl"
      context: {}
      skip_templates_processing: false
      ignore_missing_template_values: true
      skip_if_missing: true
  ```

You can also use templates without providing any context variables. This is useful for including dynamic values that don't depend on context:

  ```yaml
  import:
    # This template file uses functions that don't require context
    - path: "catalog/metadata.yaml.tmpl"
      # No context needed - the template can use functions like:
      # {{ now | date "2006-01-02" }}
      # {{ env "BUILD_NUMBER" }}
      # {{ uuidv4 }}
      # {{ randAlphaNum 10 }}
  ```

Example template file (`catalog/metadata.yaml.tmpl`) without context:

  ```yaml
  metadata:
    generated_at: {{ now | date "2006-01-02T15:04:05Z07:00" }}
    build_number: {{ env "BUILD_NUMBER" | default "local" }}
    deployment_id: {{ uuidv4 }}
    version: "1.0.0"
  ```

The `import` section supports the following fields:

<dl>
  <dt>`path` - (string) **required**</dt>
  <dd>The path to the imported file</dd>

  <dt>`context` - (map)</dt>
  <dd>An optional freeform map of context variables that are applied as template variables to the imported file (if the imported file is
  a [Go template](https://pkg.go.dev/text/template))</dd>

  <dt>`skip_templates_processing` - (boolean)</dt>
  <dd>Skip template processing for the imported file. Can be used if the imported file uses `Go` templates that should not be interpreted by atmos. For example, sometimes configurations for components may pass Go template strings not intended for atmos.</dd>

  <dt>`ignore_missing_template_values` - (boolean)</dt>
  <dd>Ignore the missing template values in the imported file. Can be used if the imported file uses `Go` templates to configure external systems, e.g. Datadog. In this case, Atmos will process all template values that are provided in the `context`, and will skip the missing values in the templates for the external systems without throwing an error. The `ignore_missing_template_values` setting is different from `skip_templates_processing` in that `skip_templates_processing` skips the template processing completely in the imported file, while `ignore_missing_template_values` processes the templates using the values provided in the `context` and skips all the missing values</dd>

  <dt>`skip_if_missing` - (boolean)</dt>
  <dd>Set it to `true` to ignore the imported manifest if it does not exist, and don't throw an error. This is useful when generating Atmos manifests using other tools, but the imported files are not present yet at the generation time.</dd>

</dl>

A combination of the two formats is also supported:

  ```yaml
  import:
    - mixins/region/us-east-2
    - orgs/cp/tenant1/test1/_defaults
    - path: "<path_to_atmos_manifest1>"
    - path: "<path_to_atmos_manifest2>"
      context: {}
      skip_templates_processing: false
      ignore_missing_template_values: true
  ```

## `Go` Templates in Imports

Atmos supports all the functionality of [Go templates](https://pkg.go.dev/text/template) in imported stack configurations, including
[functions](https://pkg.go.dev/text/template#hdr-Functions) and [Sprig functions](http://masterminds.github.io/sprig/).

Stack configurations can be templatized and then reused with different settings provided via the import `context` section.

For example, we can define the following configuration for EKS Atmos components in the `catalog/terraform/eks_cluster.yaml.tmpl` template file:

```yaml title="stacks/catalog/terraform/eks_cluster.yaml.tmpl"
# Imports can also be parameterized using `Go` templates
import: []

components:
  terraform:
    "eks-{{ .flavor }}/cluster":
      metadata:
        component: "test/test-component"
      vars:
        enabled: "{{ .enabled }}"
        name: "eks-{{ .flavor }}"
        service_1_name: "{{ .service_1_name }}"
        service_2_name: "{{ .service_2_name }}"
        tags:
          flavor: "{{ .flavor }}"
```

:::note

Since `Go` processes files ending in `.yaml.tmpl` text files with templates, we can parameterize the Atmos component name `eks-{{ .flavor }}/cluster` and any values in any sections (`vars`, `settings`, `env`, `backend`, etc.), and even the `import` section in the imported file (if the file imports other configurations).

:::

Then we can import the template into a top-level stack multiple times providing different context variables to each import:

```yaml title="stacks/orgs/cp/tenant1/test1/us-west-2.yaml"
import:
  - path: "mixins/region/us-west-2"
  - path: "orgs/cp/tenant1/test1/_defaults"

  # This import with the provided context will dynamically generate
  # a new Atmos component `eks-blue/cluster` in the current stack
  - path: "catalog/terraform/eks_cluster.yaml.tmpl"
    context:
      flavor: "blue"
      enabled: true
      service_1_name: "blue-service-1"
      service_2_name: "blue-service-2"

  # This import with the provided context will dynamically generate
  # a new Atmos component `eks-green/cluster` in the current stack
  - path: "catalog/terraform/eks_cluster.yaml.tmpl"
    context:
      flavor: "green"
      enabled: false
      service_1_name: "green-service-1"
      service_2_name: "green-service-2"
```

Now we can execute the following Atmos commands to describe and provision the dynamically generated EKS components into the stack:

```shell
atmos describe component eks-blue/cluster -s tenant1-uw2-test-1
atmos describe component eks-green/cluster -s tenant1-uw2-test-1

atmos terraform apply eks-blue/cluster -s tenant1-uw2-test-1
atmos terraform apply eks-green/cluster -s tenant1-uw2-test-1
```

All the parameterized variables get their values from the `context`:

```yaml title="atmos describe component eks-blue/cluster -s tenant1-uw2-test-1"
vars:
  enabled: true
  environment: uw2
  name: eks-blue
  namespace: cp
  region: us-west-2
  service_1_name: blue-service-1
  service_2_name: blue-service-2
  stage: test-1
  tags:
    flavor: blue
  tenant: tenant1
```

```yaml title="atmos describe component eks-green/cluster -s tenant1-uw2-test-1"
vars:
  enabled: true
  environment: uw2
  name: eks-green
  namespace: cp
  region: us-west-2
  service_1_name: green-service-1
  service_2_name: green-service-2
  stage: test-1
  tags:
    flavor: green
  tenant: tenant1
```

## Hierarchical Imports with Context

Atmos supports hierarchical imports with context.
This will allow you to parameterize the entire chain of stack configurations and dynamically generate components in stacks.

For example, let's create the configuration `stacks/catalog/terraform/eks_cluster_hierarchical.yaml.tmpl` with the following content:

```yaml title="stacks/catalog/terraform/eks_cluster_hierarchical.yaml.tmpl"
import:
  # Use `region.yaml.tmpl` `Go` template and provide `context` for it.
  # This can also be done by using `Go` templates in the import path itself.
  # - path: "mixins/region/{{ .region }}"
  - path: "mixins/region/region.yaml.tmpl"
    # `Go` templates in `context`
    context:
      region: "{{ .region }}"
      environment: "{{ .environment }}"

  # `Go` templates in the import path
  - path: "orgs/cp/{{ .tenant }}/{{ .stage }}/_defaults"

components:
  terraform:
    # Parameterize Atmos component name
    "eks-{{ .flavor }}/cluster":
      metadata:
        component: "test/test-component"
      vars:
        # Parameterize variables
        enabled: "{{ .enabled }}"
        name: "eks-{{ .flavor }}"
        service_1_name: "{{ .service_1_name }}"
        service_2_name: "{{ .service_2_name }}"
        tags:
          flavor: "{{ .flavor }}"
```

Then we can import the template into a top-level stack multiple times providing different context variables to each import and to the imports for
the entire inheritance chain (which `catalog/terraform/eks_cluster_hierarchical.yaml.tmpl` imports itself):

```yaml title="stacks/orgs/cp/tenant1/test1/us-west-1.yaml"
import:

  # This import with the provided hierarchical context will dynamically generate
  # a new Atmos component `eks-blue/cluster` in the `tenant1-uw1-test1` stack
  - path: "catalog/terraform/eks_cluster_hierarchical.yaml.tmpl"
    context:
      # Context variables for the EKS component
      flavor: "blue"
      enabled: true
      service_1_name: "blue-service-1"
      service_2_name: "blue-service-2"
      # Context variables for the hierarchical imports
      # `catalog/terraform/eks_cluster_hierarchical.yaml.tmpl` imports other parameterized configurations
      tenant: "tenant1"
      region: "us-west-1"
      environment: "uw1"
      stage: "test1"

  # This import with the provided hierarchical context will dynamically generate
  # a new Atmos component `eks-green/cluster` in the `tenant1-uw1-test1` stack
  - path: "catalog/terraform/eks_cluster_hierarchical.yaml.tmpl"
    context:
      # Context variables for the EKS component
      flavor: "green"
      enabled: false
      service_1_name: "green-service-1"
      service_2_name: "green-service-2"
      # Context variables for the hierarchical imports
      # `catalog/terraform/eks_cluster_hierarchical.yaml.tmpl` imports other parameterized configurations
      tenant: "tenant1"
      region: "us-west-1"
      environment: "uw1"
      stage: "test1"
```

In the case of hierarchical imports, Atmos performs the following steps:

- Processes all the imports in the `import` section in the current configuration in the order they are specified providing the `context` to all
  imported files

- For each imported file, Atmos deep-merges the parent `context` with the current context. Note that the current `context` (in the current file) takes
  precedence over the parent `context` and will override items with the same keys. Atmos does this hierarchically for all imports in all files,
  effectively processing a graph of imports and deep-merging the contexts on all levels

For example, in the `stacks/orgs/cp/tenant1/test1/us-west-1.yaml` configuration above, we first import
the `catalog/terraform/eks_cluster_hierarchical.yaml.tmpl` and provide it with the `context` which includes the context variables for the EKS component
itself, as well as the context variables for all the hierarchical imports. Then, when processing
the `stacks/catalog/terraform/eks_cluster_hierarchical.yaml.tmpl` configuration, Atmos deep-merges the parent `context` (from
`stacks/orgs/cp/tenant1/test1/us-west-1.yaml`) with the current `context` and processes the `Go` templates.

We are now able to dynamically generate the components `eks-blue/cluster` and `eks-green/cluster` in the stack `tenant1-uw1-test1` and can
execute the following Atmos commands to provision the components into the stack:

```shell
atmos terraform apply eks-blue/cluster -s tenant1-uw1-test-1
atmos terraform apply eks-green/cluster -s tenant1-uw1-test-1
```

All the parameterized variables get their values from the hierarchical `context` settings:

```yaml title="atmos describe component eks-blue/cluster -s tenant1-uw1-test-1"
vars:
  enabled: true
  environment: uw1
  name: eks-blue
  namespace: cp
  region: us-west-1
  service_1_name: blue-service-1
  service_2_name: blue-service-2
  stage: test-1
  tags:
    flavor: blue
  tenant: tenant1
```

:::warning Handle with Care

Leveraging Go templating for Atmos stack generation grants significant power but demands equal responsibility. It can easily defy the principle of creating stack configurations that are straightforward and intuitive to read.

While templating fosters DRYer code, it comes at the expense of searchable components and introduces elements like conditionals, loops, and dynamic variables that impede understandability. It's a tool not for regular use, but for instances where code duplication becomes excessively cumbersome.

Before resorting to advanced Go templates in Atmos, rigorously evaluate the trade-off between the value added and the complexity introduced.

:::

## Advanced Examples of Templates in Atmos Configurations

Atmos supports all the functionality of [Go templates](https://pkg.go.dev/text/template), including [functions](https://pkg.go.dev/text/template#hdr-Functions) and [Sprig functions](http://masterminds.github.io/sprig/).
The Sprig library provides over 70 template functions for `Go's` template language.

The following example shows how to dynamically include a variable in the Atmos component configuration by using the `hasKey` Sprig function.
The hasKey function returns `true` if the given dictionary contains the given key.

```yaml
components:
  terraform:
    eks/iam-role/{{ .app_name }}/{{ .service_environment }}:
      metadata:
        component: eks/iam-role
      settings:
        spacelift:
          workspace_enabled: true
      vars:
        enabled: {{ .enabled }}
        tags:
          Service: {{ .app_name }}
        service_account_name: {{ .app_name }}
        service_account_namespace: {{ .service_account_namespace }}
        {{ if hasKey . "iam_managed_policy_arns" }}
        iam_managed_policy_arns:
          {{ range $i, $iam_managed_policy_arn := .iam_managed_policy_arns }}
          - '{{ $iam_managed_policy_arn }}'
          {{ end }}
        {{- end }}
        {{ if hasKey . "iam_source_policy_documents" }}
        iam_source_policy_documents:
          {{ range $i, $iam_source_policy_document := .iam_source_policy_documents }}
          - '{{ $iam_source_policy_document }}'
          {{ end }}
        {{- end }}
```

The `iam_managed_policy_arns` and `iam_source_policy_documents` variables will be included in the component configuration only if the
provided `context` object has the `iam_managed_policy_arns` and `iam_source_policy_documents` fields.

## Summary

Using imports with context (and hierarchical imports with context) with parameterized config files will help you make the configurations
extremely DRY. It's very useful in many cases, for example, when creating stacks and components
for [EKS blue-green deployment](https://aws.amazon.com/blogs/containers/kubernetes-cluster-upgrade-the-blue-green-deployment-strategy/).

## Related

- [Configure CLI](/quick-start/advanced/configure-cli)
- [Create Atmos Stacks](/quick-start/advanced/create-atmos-stacks)

---

## Inherit Configurations in Atmos Stacks

import File from '@site/src/components/File'
import Terminal from '@site/src/components/Terminal'
import PillBox from '@site/src/components/PillBox'
import Intro from '@site/src/components/Intro'

<Intro>
Inheritance provides a template-free way to customize Stack configurations. When combined with [imports](/core-concepts/stacks/imports), it provides the ability to combine multiple configurations through ordered deep-merging of configurations. Inheritance is how you manage configuration variations, without resorting to [templating](/core-concepts/stacks/templates).
</Intro>

Atmos supports the following concepts and principles of **Component-Oriented Programming (COP)**:

- [Single Inheritance](/core-concepts/stacks/inheritance#single-inheritance) - when an Atmos component inherits the configuration properties from
  another Atmos component

- [Multiple Inheritance](/core-concepts/stacks/inheritance#multiple-inheritance) - when an Atmos component inherits the configuration from more than one Atmos
  component

These concepts and principles are implemented and used in Atmos by combining two features: [`import`](/core-concepts/stacks/imports)
and [`metadata`](/core-concepts/stacks/define-components) component's configuration section.

:::info
The mechanics of mixins and inheritance apply only to the [Stack](/core-concepts/stacks) configurations. Atmos knows nothing about the underlying
components (e.g. terraform), and does not magically implement inheritance for HCL. However, by designing highly reusable components that do one thing
well, we're able to achieve many of the same benefits.
:::

Component Inheritance is implemented and used in Atmos by combining two features: [`import`](/core-concepts/stacks/imports)
and `metadata` component's configuration section.

### Definitions

<dl>
<dt>Base Component</dt>
<dd>is an Atmos component from which other Atmos components inherit all the configuration properties</dd>
<dt>Derived Component</dt>
<dd>is an Atmos component that derives the configuration properties from other Atmos components</dd>
</dl>

# Understanding Inheritance

The concept of inheritance in Atmos is implemented through deep merging. Deep merging involves taking two maps or objects and combining them in a specific order, where the values in the latter map override those in the former. This approach allows us to achieve a template-free way of defining configurations in a logical, predictable, and consistent manner.

## Single Inheritance

<PillBox>Easy</PillBox>

Single Inheritance is used when an Atmos component inherits from another base Atmos component.

In the diagram below, `ComponentA` is the base component. `ComponentB` and `ComponentC` are derived components, they inherit all the
configurations (`vars`, `settings`, `env` and other sections) from `ComponentA`, and can override the default values from `ComponentA`.

```mermaid
classDiagram
      direction TB
      ComponentA --> ComponentB
      ComponentA --> ComponentC
      ComponentA : vars
      ComponentA : settings
      ComponentA : env
      ComponentA : backend
      class ComponentB {
          vars
          settings
          env
          backend
          metadata:
          &nbsp;&nbsp;inherits:
          &nbsp;&nbsp;&nbsp;&nbsp;- ComponentA&nbsp;&nbsp;
      }
      class ComponentC {
          vars
          settings
          env
          backend
          metadata:
          &nbsp;&nbsp;inherits:
          &nbsp;&nbsp;&nbsp;&nbsp;- ComponentA&nbsp;&nbsp;
      }
```

### Single Inheritance Example

Let's say we want to provision two VPCs with different settings into the same AWS account.

In the `stacks/catalog/vpc.yaml` file, add the following config for the VPC component:

<File title="stacks/catalog/vpc.yaml">
```yaml
components:
  terraform:
    vpc-defaults:
      metadata:
        # Setting `metadata.type: abstract` makes the component `abstract`,
        # explicitly prohibiting the component from being deployed.
        # `atmos terraform apply` will fail with an error.
        # If `metadata.type` attribute is not specified, it defaults to `real`.
        # `real` components can be provisioned by `atmos` and CI/CD like Spacelift and Atlantis.
        type: abstract
      # Default variables, which will be inherited and can be overridden in the derived components
      vars:
        public_subnets_enabled: false
        nat_gateway_enabled: false
        nat_instance_enabled: false
        max_subnet_count: 3
        vpc_flow_logs_enabled: true
```
</File>

In the configuration above, the following **Component-Oriented Programming** concepts are implemented:

- **Abstract Components**: `atmos` component `vpc-defaults` is marked as abstract in `metadata.type`. This makes the component non-deployable, and it
  can be used only as a base for other components that inherit from it
- **Dynamic Polymorphism**: All the variables in the `vars` section become the default values for the derived components. This provides the ability to
  override and use the base component properties in the derived components to provision the same Terraform configuration many times but with different
  settings

<details>
<summary>Deep Dive</summary>

Component Inheritance is one of the principles of [Component-Oriented Programming (COP)](https://en.wikipedia.org/wiki/Component-based_software_engineering)
supported by Atmos.

The concept is borrowed from [Object-Oriented Programming](https://en.wikipedia.org/wiki/Inheritance_(object-oriented_programming))
to logically organize complex configurations in a way that makes conceptual sense. The side effect of this are extremely DRY and reusable
configurations.

[Component-Oriented Configuration](https://en.wikipedia.org/wiki/Component-based_software_engineering) is a reuse-based approach to defining,
implementing and composing loosely-coupled independent components into systems.

<dl>
  <dt>Dynamic Polymorphism</dt>
  <dd>Ability to use and override base component(s) properties</dd>

  <dt>Encapsulation</dt>
  <dd>Enclose a set of related configuration properties into reusable loosely-coupled modules. Encapsulation is implemented by Atmos Components which are opinionated building blocks of Infrastructure-as-Code (IAC) that solve one specific problem or use-case</dd>

  <dt>Abstraction</dt>
  <dd>

      Principle of Abstraction: In a given stack, "hide" all but the relevant information about a component configuration in order to reduce complexity and increase efficiency
      Abstract Components: If a component is marked as <code>abstract</code>, it can be used only as a base for other components and can't be provisioned using <code>atmos</code>

  </dd>
</dl>
</details>

In the `stacks/ue2-dev.yaml` stack config file, add the following config for the derived VPC components in the `ue2-dev` stack:

<File title="stacks/ue2-dev.yaml">
```yaml
# Import the base component configuration from the `catalog`.
# `import` supports POSIX-style Globs for file names/paths (double-star `**` is supported).
# File extensions are optional (if not specified, `.yaml` is used by default).
import:
  - catalog/vpc

components:
  terraform:

    vpc/1:
      metadata:
        component: infra/vpc # Point to the Terraform component in `components/terraform` folder
        inherits:
          - vpc-defaults # Inherit all settings and variables from the `vpc-defaults` base component
      vars:
        # Define variables that are specific for this component
        # and are not set in the base component
        name: vpc-1
        # Override the default variables from the base component
        public_subnets_enabled: true
        nat_gateway_enabled: true
        vpc_flow_logs_enabled: false

    vpc/2:
      metadata:
        component: infra/vpc # Point to the same Terraform component in `components/terraform` folder
        inherits:
          - vpc-defaults # Inherit all settings and variables from the `vpc-defaults` base component
      vars:
        # Define variables that are specific for this component
        # and are not set in the base component
        name: vpc-2
        # Override the default variables from the base component
        max_subnet_count: 2
        vpc_flow_logs_enabled: false
```
</File>

In the configuration above, the following **Component-Oriented Programming** concepts are implemented:

- **Component Inheritance**: In the `ue2-dev` stack (`stacks/ue2-dev.yaml` stack config file), the Atmos components `vpc/1` and `vpc/2` inherit from
  the base component `vpc-defaults`. This makes `vpc/1` and `vpc/2` derived components
- **Principle of Abstraction**: In the `ue2-dev` stack, only the relevant information about the derived components in the stack is shown. All the base
  component settings are "hidden" (in the imported `catalog`), which reduces the configuration size and complexity
- **Dynamic Polymorphism**: The derived `vpc/1` and `vpc/2` components override and use the base component properties to be able to provision the same
  Terraform configuration many times but with different settings

Having the components in the stack configured as shown above, we can now provision the `vpc/1` and `vpc/2` components into the `ue2-dev` stack by
executing the following `atmos` commands:

```shell
atmos terraform apply vpc/1 -s ue2-dev
atmos terraform apply vpc/2 -s ue2-dev
```

As we can see, using the principles of **Component-Oriented Programming (COP)**, we are able to define two (or more) components with
different settings, and provision them into the same (or different) environment (account/region) using the same Terraform code (which is
environment-agnostic). And the configurations are extremely DRY and reusable.

## Multiple Inheritance

<PillBox>Advanced</PillBox>

Multiple Inheritance is used when an Atmos component inherits from more than one Atmos component.

In the diagram below, `ComponentA` and `ComponentB` are the base components. `ComponentC` is a derived components, it inherits all the
configurations (`vars`, `settings`, `env` and other sections) from `ComponentA` and `ComponentB`, and can override the default values
from `ComponentA` and `ComponentB`.

```mermaid
classDiagram
      ComponentA --> ComponentC
      ComponentB --> ComponentC
      ComponentA : vars
      ComponentA : settings
      ComponentA : env
      ComponentA : backend
      class ComponentB {
          vars
          settings
          env
          backend
      }
      class ComponentC {
          vars
          settings
          env
          backend
          metadata:
          &nbsp;&nbsp;inherits:
          &nbsp;&nbsp;&nbsp;&nbsp;- ComponentA&nbsp;&nbsp;
          &nbsp;&nbsp;&nbsp;&nbsp;- ComponentB&nbsp;&nbsp;
      }
```

Multiple Inheritance allows a component to inherit from many base components or mixins, each base component having its own inheritance chain,
effectively making it an inheritance matrix. It uses a method similar to Method Resolution Order (MRO) using
the [C3 linearization](https://en.wikipedia.org/wiki/C3_linearization) algorithm, which is how Python supports multiple inheritance.

:::info

In **Object-Oriented Programming (OOP)**, a mixin is a class that contains methods for use by other classes without having to be the parent class of
those other classes.

In **Component-Oriented Programming (COP)** implemented in Atmos, a [mixin](/core-concepts/stacks/inheritance/mixins) is an abstract base component that is never
meant to be provisioned and does not have any physical implementation - it just contains default settings/variables/properties for use by other Atmos
components.

:::

Multiple Inheritance, similarly to Single Inheritance, is defined by the `metadata.inherits` section in the component
configuration. `metadata.inherits` is a list of component or mixins names from which the current component inherits.
In the case of multiple base components, it is processed in the order by which it was declared.

For example, in the following configuration:

```yaml
metadata:
  inherits:
    - componentA
    - componentB
```

Atmos will recursively deep-merge all the base components of `componentA` (each component overriding its base),
then all the base components of `componentB` (each component overriding its base), then the two results are deep-merged together with `componentB`
inheritance chain overriding the values from `componentA` inheritance chain.

:::caution
All the base components/mixins referenced by `metadata.inherits` must be already defined in the Stack configuration, either by using an `import`
statement or by explicitly defining them in the Stack configuration. The `metadata.inherits` statement does not imply that we are importing anything.
:::

### Multiple Inheritance Example

Here is a concrete example:

<File title="stack.yaml">
```yaml
# Import all the base components and mixins we want to inherit from.
# `import` supports POSIX-style Globs for file names/paths (double-star `**` is supported).
import:
  - catalog/terraform/test/test-component-override
  - catalog/terraform/test/test-component-override-2
  - catalog/terraform/mixins/test-*.*

components:
  terraform:
    test/test-component-override-3:
      vars: {}
      metadata:
        # `real` is implicit, you don't need to specify it.
        # `abstract` makes the component protected from being deployed.
        type: real
        # Terraform component. Must exist in `components/terraform` folder.
        # If not specified, it's assumed that this component `test/test-component-override-3`
        # is also a Terraform component in
        # `components/terraform/test/test-component-override-3` folder.
        component: "test/test-component"
        # Multiple inheritance.
        # It's a down-top/left-right matrix similar to Method Resolution Order (MRO) in Python.
        inherits:
          - "test/test-component-override"
          - "test/test-component-override-2"
          - "mixin/test-1"
          - "mixin/test-2"
```
</File>

In the configuration above, all the base components and mixins are processed and deep-merged in the order they are specified in the `inherits` list:

- `test/test-component-override-2` overrides `test/test-component-override` and its base components (all the way up its inheritance chain)

- `mixin/test-1` overrides `test/test-component-override-2` and its base components (all the way up its inheritance chain)

- `mixin/test-2` overrides `mixin/test-1` and its base components (all the way up its inheritance chain)

- The current component `test/test-component-override-3` overrides `mixin/test-2` and its base components (all the way up its inheritance chain)

When we run the following command to provision the `test/test-component-override-3` Atmos component into the stack `tenant1-ue2-dev`:

```shell
atmos terraform apply test/test-component-override-3 -s tenant1-ue2-dev
```

Atmos will process all configurations for the current component and all the base components/mixins and will show the following console output:

```text
Command info:
Atmos component: test/test-component-override-3
Terraform component: test/test-component
Terraform command: apply
Stack: tenant1-ue2-dev
Inheritance: test/test-component-override-3 -> mixin/test-2 -> mixin/test-1 ->
             test/test-component-override-2 -> test/test-component-override -> test/test-component
```

The `Inheritance` output shows the multiple inheritance steps that Atmos performed and deep-merged into the final configuration, including
the variables which are sent to the Terraform component `test/test-component` that is being provisioned.

### Multilevel Inheritance

<PillBox>Advanced</PillBox>

Multilevel Inheritance is used when an Atmos component inherits from a base Atmos component, which in turn inherits from another base Atmos component.

In the diagram below, `ComponentC` directly inherits from `ComponentB`.
`ComponentB` directly inherits from `ComponentA`.

After this Multilevel Inheritance chain gets processed by Atmos, `ComponentC` will inherit all the configurations (`vars`, `settings`, `env` and other
sections) from both `ComponentB` and `ComponentA`.

Note that `ComponentB` overrides the values from `ComponentA`.
`ComponentC` overrides the values from both `ComponentB` and `ComponentA`.

```mermaid
classDiagram
      direction LR
      ComponentA --> ComponentB
      ComponentB --> ComponentC
      ComponentA : vars
      ComponentA : settings
      ComponentA : env
      ComponentA : backend
      class ComponentB {
          vars
          settings
          env
          backend
          metadata:
          &nbsp;&nbsp;inherits:
          &nbsp;&nbsp;&nbsp;&nbsp;- ComponentA&nbsp;&nbsp;
      }
      class ComponentC {
          vars
          settings
          env
          backend
          metadata:
          &nbsp;&nbsp;inherits:
          &nbsp;&nbsp;&nbsp;&nbsp;- ComponentB&nbsp;&nbsp;
      }
```

### Hierarchical Inheritance

<PillBox>Advanced</PillBox>

Hierarchical Inheritance is a combination of Multiple Inheritance and Multilevel Inheritance.

In Hierarchical Inheritance, every component can act as a base component for one or more child (derived) components, and each derived component can
inherit from one of more base components.

```mermaid
classDiagram
      ComponentA --> ComponentB
      ComponentA --> ComponentC
      ComponentB --> ComponentD
      ComponentB --> ComponentE
      ComponentC --> ComponentF
      ComponentC --> ComponentG
      ComponentH --> ComponentE
      ComponentI --> ComponentG
      ComponentA : vars
      ComponentA : settings
      ComponentA : env
      ComponentA : backend
      class ComponentB {
          vars
          settings
          env
          backend
          metadata:
          &nbsp;&nbsp;inherits:
          &nbsp;&nbsp;&nbsp;&nbsp;- ComponentA&nbsp;&nbsp;
      }
      class ComponentC {
          vars
          settings
          env
          backend
          metadata:
          &nbsp;&nbsp;inherits:
          &nbsp;&nbsp;&nbsp;&nbsp;- ComponentA&nbsp;&nbsp;
      }
      class ComponentD {
          vars
          settings
          env
          backend
          metadata:
          &nbsp;&nbsp;inherits:
          &nbsp;&nbsp;&nbsp;&nbsp;- ComponentB&nbsp;&nbsp;
      }
      class ComponentE {
          vars
          settings
          env
          backend
          metadata:
          &nbsp;&nbsp;inherits:
          &nbsp;&nbsp;&nbsp;&nbsp;- ComponentB&nbsp;&nbsp;
          &nbsp;&nbsp;&nbsp;&nbsp;- ComponentH&nbsp;&nbsp;
      }
      class ComponentF {
          vars
          settings
          env
          backend
          metadata:
          &nbsp;&nbsp;inherits:
          &nbsp;&nbsp;&nbsp;&nbsp;- ComponentC&nbsp;&nbsp;
      }
      class ComponentG {
          vars
          settings
          env
          backend
          metadata:
          &nbsp;&nbsp;inherits:
          &nbsp;&nbsp;&nbsp;&nbsp;- ComponentI&nbsp;&nbsp;
          &nbsp;&nbsp;&nbsp;&nbsp;- ComponentC&nbsp;&nbsp;
      }
      class ComponentH {
          vars
          settings
          env
          backend
      }
      class ComponentI {
          vars
          settings
          env
          backend
      }
```

In the diagram above:

- `ComponentA` is the base component of the whole hierarchy

- `ComponentB` and `ComponentC` inherit from `ComponentA`

- `ComponentD` inherits from `ComponentB` directly, and from `ComponentA` via Multilevel Inheritance

- `ComponentE` is an example of using both Multiple Inheritance and Multilevel Inheritance.
  It inherits from `ComponentB` and `ComponentH` directly, and from `ComponentA` via Multilevel Inheritance

For `ComponentE`, the inherited components are processed and deep-merged in the order they are specified in the `inherits` list:

- `ComponentB` overrides the configuration from `ComponentA`

- `ComponentH` overrides the configuration from `ComponentB` and `ComponentA` (since it's defined after `ComponentB` in the `inherits` section)

- And finally, `ComponentE` overrides `ComponentH`, `ComponentB` and `ComponentA`

For `ComponentG`:

- `ComponentI` is processed first (since it's the first item in the `inherits` list)

- Then `ComponentA` is processed (since it's the base component for `ComponentC` which is the second item in the `inherits` list)

- Then `ComponentC` is processed, and it overrides the configuration from `ComponentA` and `ComponentI`

- And finally, `ComponentG` is processed, and it overrides `ComponentC`, `ComponentA` and `ComponentI`

#### Hierarchical Inheritance Example

Let's consider the following configuration for Atmos components `base-component-1`, `base-component-2`, `derived-component-1`
and `derived-component-2`:

<File title="stack.yaml">
```yaml
components:
  terraform:

    base-component-1:
      metadata:
        type: abstract
      vars:
        hierarchical_inheritance_test: "base-component-1"

    base-component-2:
      metadata:
        type: abstract
      vars:
        hierarchical_inheritance_test: "base-component-2"

    derived-component-1:
      metadata:
        component: "test/test-component"
        inherits:
          - base-component-1
      vars: {}

    derived-component-2:
      metadata:
        component: "test/test-component"
        inherits:
          - base-component-2
          - derived-component-1
      vars: {}
```
</File>

This configuration can be represented by the following diagram:

```mermaid
classDiagram
      `base-component-1` --> `derived-component-1`
      `derived-component-1` --> `derived-component-2`
      `base-component-2` --> `derived-component-2`
      class `base-component-1` {
          settings
          env
          backend
          vars:
          &nbsp;&nbsp;hierarchical_inheritance_test: base-component-1
      }
      class `base-component-2` {
          settings
          env
          backend
          vars:
          &nbsp;&nbsp;hierarchical_inheritance_test: base-component-2
      }
      class `derived-component-1` {
          settings
          env
          backend
          vars
          metadata:
          &nbsp;&nbsp;inherits:
          &nbsp;&nbsp;&nbsp;&nbsp;- base-component-1&nbsp;&nbsp;
      }
      class `derived-component-2` {
          settings
          env
          backend
          vars
          metadata:
          &nbsp;&nbsp;inherits:
          &nbsp;&nbsp;&nbsp;&nbsp;- base-component-2&nbsp;&nbsp;
          &nbsp;&nbsp;&nbsp;&nbsp;- derived-component-1&nbsp;&nbsp;
      }
```

In the configuration above, `derived-component-1` inherits from `base-component-1`.

`derived-component-2` inherits from `base-component-2` and `derived-component-1` via Multiple Inheritance, and from `base-component-1` via Multilevel
Inheritance.

The `derived-component-2` component is processed in the following order:

- `base-component-2` is processed first (since it's the first item in the `inherits` list)

- Then `base-component-1` is processed (since it's the base component for `derived-component-1` which is the second item in the `inherits` list), and
  it overrides the configuration from `base-component-2`

- Then `derived-component-1` is processed, and it overrides the configuration from `base-component-2` and `base-component-1`

- And finally, `derived-component-2` is processed, and it overrides `derived-component-1`, `base-component-1` and `base-component-2`

When we run the following command to provision the `derived-component-2` component, Atmos will show the following console output:

<Terminal title="atmos terraform plan derived-component-2 -s tenant1-ue2-test-1">
```console
Variables for the component 'derived-component-2' in the stack 'tenant1-ue2-test-1':
environment: ue2
hierarchical_inheritance_test: base-component-1
namespace: cp
region: us-east-2
stage: test-1
tenant: tenant1

Command info:
Terraform binary: terraform
Terraform command: plan
Component: derived-component-2
Terraform component: test/test-component
Inheritance: derived-component-2 -> derived-component-1 -> base-component-1 -> base-component-2
```
</Terminal>

Note that the `hierarchical_inheritance_test` variable was inherited from `base-component-1` because it overrode the configuration
from `base-component-2`.

If we change the order of the components in the `inherits` list for `derived-component-2`:

<File title="stack.yaml">
```yaml
components:
  terraform:

    derived-component-2:
      metadata:
        component: "test/test-component"
        inherits:
          - derived-component-1
          - base-component-2
      vars: {}
```
</File>

`base-component-2` will be processed after `base-component-1` and `derived-component-1`, and the `hierarchical_inheritance_test` variable
will be inherited from `base-component-2`:

```console
Variables for the component 'derived-component-2' in the stack 'tenant1-ue2-test-1':
environment: ue2
hierarchical_inheritance_test: base-component-2
namespace: cp
region: us-east-2
stage: test-1
tenant: tenant1

Command info:
Terraform binary: terraform
Terraform command: plan
Component: derived-component-2
Terraform component: test/test-component
Inheritance: derived-component-2 -> base-component-2 -> derived-component-1 -> base-component-1
```

## References

- [Abstract Component Atmos Design Pattern](/design-patterns/abstract-component)
- [Component Inheritance Atmos Design Pattern](/design-patterns/component-inheritance)

---

## Stack Mixins

import File from '@site/src/components/File'
import PillBox from '@site/src/components/PillBox'
import Intro from '@site/src/components/Intro'
import ActionCard from '@site/src/components/ActionCard'
import PrimaryCTA from '@site/src/components/PrimaryCTA'

<PillBox>Advanced</PillBox>

<Intro>
Mixins are reusable snippets of configurations (like regions, tags, etc) included in stack configurations to avoid repetition and enhance modularity. They allow for defining common settings, variables, or configurations once and applying them efficiently across various stacks.
</Intro>

:::important
Mixins are treated the same as all other imports in Atmos, with no special handling or technical distinction.
:::

## Use-cases

Here are some use-cases for when to use mixins.

### Mixins by Region

Mixins organized by region will make it very easy to configure where a stack is deployed by simply changing the imported mixin.

Consider naming them after the canonical region name for the cloud provider you're using.

For example, here's what it would look like for AWS. Let's name this file `mixins/region/us-east-1.yaml`.
Now, anytime we want a Parent Stack deployed in the `us-east-1` region, we just need to specify this import, and we'll automatically inherit all the settings for that region.

For example, let's define a mixin with the defaults for operating in the `us-east-1` region:

```yaml title="mixins/region/us-east-1.yaml"
vars:
  region: us-east-1   # the canonical cloud region
  availability_zones: # the designated availability zones to use in this region
  - us-east-1a
  - us-east-1b
```

Then we can use this mixin, anytime we deploy in `us-east-1` to ensure we conform to the organization's standards.

```yaml title="stacks/prod/network.yaml"
imports:
- mixins/region/us-east-1

terraform:
  components:
    vpc:
    # ...
```

### Mixins by Stage

Provide the default settings for operating in a particular stage (e.g. Dev, Staging, Prod) to enforce consistency.

For example, let's define the stage name and required tags for production in the mixin file named `mixins/stage/prod.yaml`

```yaml title="mixins/stage/prod.yaml"
vars:
  stage: prod
  tags:
    CostCenter: 12345
```

Now, anytime we want to provision a parent stack in production, we'll want to add this to the imports:

```yaml title="stacks/prod/backing-services.yaml"
imports:
- mixins/stage/prod

terraform:
  components:
    rds-cluster:
    # ...
```

:::tip Use Mixins for Naming Conventions
This simple example highlights a simple fix for one of the most common issues in enterprise organizations: naming inconsistency.
Using a mixin is a great way for organizations ensure naming conventions are followed consistently.

For example, there are many ways developers will define `production`.

- e.g. `prd`
- e.g. `prod`
- e.g. `production`
- e.g. `Production`
- e.g. `Prod`
- e.g. `PROD`
- etc
:::

To avoid this situation, use the mixin `mixins/stage/prod` and always use the appropriate naming convention.

<ActionCard>
Mixins are really just a [Design Pattern](/design-patterns/component-catalog-with-mixins) for [`imports`](/core-concepts/stacks/imports) that uses [inheritance](/core-concepts/stacks/inheritance) to alter the Stack configuration in some deliberate way.
<PrimaryCTA to="/design-patterns/component-catalog-with-mixins">Learn Design Pattern</PrimaryCTA>
</ActionCard>

---

## Override Configurations

import Terminal from '@site/src/components/Terminal'
import Intro from '@site/src/components/Intro'
import File from '@site/src/components/File'

<Intro>
Atmos supports the ability to override the behavior of [imports](/core-concepts/stacks/imports) when the order of
deep-merging interferes with what you want to express. Use the `overrides` section in Atmos stack manifests.
</Intro>

You can override the following sections in the component(s) configuration:

- `command`
- `env`
- `hooks`
- `providers`
- `settings`
- `vars`

The `overrides` section can be used in the global scope or in the Terraform and Helmfile scopes.

The [Component Overrides](/design-patterns/component-overrides) Design Pattern goes into further details on how to use this effectively.

## Overrides Schema

The `overrides` section schema at the global scope is as follows:

```yaml
overrides:
  # Override the ENV variables for the components in the current stack manifest and all its imports
  env: {}
  # Override the hooks for the components in the current stack manifest and all its imports
  hooks: {}
  # Override the settings for the components in the current stack manifest and all its imports
  settings: {}
  # Override the variables for the components in the current stack manifest and all its imports
  vars: {}
  # Override the providers configuration section for the Terraform components in the current stack manifest and all its imports
  # Note: this applies only to Terraform components in the `terraform.providers` and `component.terraform.<component>.providers` sections
  providers: {}
  # Override the command to execute for the components in the current stack manifest and all its imports
  command: "<command to execute>"
```

The `overrides` section schemas at the Terraform and Helmfile levels are as follows:

```yaml
terraform:
  overrides:
    env: {}
    hooks: {}
    settings: {}
    vars: {}
    providers: {}
    command: "<command to execute>"

helmfile:
  overrides:
    env: {}
    settings: {}
    vars: {}
    command: "<command to execute>"
```

You can include the `overrides`, `terraform.overrides` and `helmfile.overrides` sections in any Atmos stack manifest at any level of inheritance.
The scope of the `override` configuration is limited to all the Atmos components defined within the manifest and all its imports up until that point.
In other words, the `overrides` configuration defined within a stack manifest does not affect any other components defined in different stack manifests for the same top-level stack.

:::tip
Refer to [Atmos Component Inheritance](/core-concepts/stacks/inheritance) for more information on all types of component inheritance
supported by Atmos
:::

## Use-cases

### Overrides for Teams

The **overrides** pattern is used to override the components only in a particular Atmos stack manifest and all the imported
manifests. This is different from the other configuration sections (e.g. `vars`, `settings`, `env`). If we define a `vars`, `settings` or `env`
section at the global, Terraform or Helmfile levels, all the components in the top-level stack will get the updated configurations. On
the other hand, if we define an `overrides` section in a stack manifest, only the components directly defined in the manifest and its imports will get
the overridden values, not all the components in the top-level Atmos stack.

This is especially useful when you have Atmos stack manifests split per Teams. Each Team manages a set of components, and you need to define a common
configuration (or override the existing one) for the components that only a particular Team manages.

For example, we have two Teams: `devops` and `testing`.

The `devops` Team manifest is defined in `stacks/teams/devops.yaml`:

<File title="stacks/teams/devops.yaml">
```yaml
import:
  # The `devops` Team manages all the components defined in the following stack manifests:
  - catalog/terraform/top-level-component1
```
</File>

The `testing` Team manifest is defined in `stacks/teams/testing.yaml`:

<File title="stacks/teams/testing.yaml">
```yaml
import:
  # The `testing` Team manages all the components defined in the following stack manifests:
  - catalog/terraform/test-component
  - catalog/terraform/test-component-override
```
</File>

We can import the two manifests into a top-level stack manifest, e.g. `tenant1/dev/us-west-2.yaml`:

<File title="stacks/orgs/cp/tenant1/dev/us-west-2.yaml">
```yaml
import:
  - mixins/region/us-west-2
  - orgs/cp/tenant1/dev/_defaults
  # Import all components that the `devops` Team manages
  - teams/devops
  # Import all components managed by the `testing` Team
  - teams/testing
```
</File>

Suppose that we want to change some variables in the `vars` and `env` sections and some config in the `settings` section for all the components that the `testing` Team manages, but we don't want to affect any components that the `devops` Team manages.

If we added a global or Terraform level `vars`, `env` or `settings` sections to the top-level manifest `stacks/orgs/cp/tenant1/dev/us-west-2.yaml` or to the Team manifest `stacks/teams/testing.yaml`, then all the components in the `tenant1/dev/us-west-2` top-level stack would be modified, including those managed by the `devops` Team.

To solve this, we could individually modify the `vars`, `env` and `settings` sections in all the components managed by the `testing` Team, but the entire configuration would not be DRY and reusable. That's where the __overrides__ pattern comes into play. To make the configuration DRY and configured only in one place, use the `overrides` section.

For example, we want to override some values in the `env`, `vars` and `settings` sections for all the components managed by the `testing` Team:

<File title="stacks/teams/testing.yaml">
```yaml
import:
  # The `testing` Team manages all the components defined in the following stack manifests:
  - catalog/terraform/test-component
  - catalog/terraform/test-component-override

# Global overrides.
# Override the variables, env, command and settings ONLY in the components managed by the `testing` Team.
overrides:
  env:
    # This ENV variable will be added or overridden in all the components managed by the `testing` Team
    TEST_ENV_VAR1: "test-env-var1-overridden"
  settings: {}
  vars: {}

# Terraform overrides.
# Override the variables, env, command and settings ONLY in the Terraform components managed by the `testing` Team.
# The Terraform `overrides` are deep-merged with the global `overrides`
# and takes higher priority (it will override the same keys from the global `overrides`).
terraform:
  overrides:
    settings:
      spacelift:
        # All the components managed by the `testing` Team will have the Spacelift stacks auto-applied
        # if the planning phase was successful and there are no plan policy warnings
        # https://docs.spacelift.io/concepts/stack/stack-settings#autodeploy
        autodeploy: true
    vars:
      # This variable will be added or overridden in all the Terraform components managed by the `testing` Team
      test_1: 1
    # The `testing` Team uses `tofu` instead of `terraform`
    # https://opentofu.org
    # The commands `atmos terraform <sub-command> ...` will execute the `tofu` binary
    command: tofu

# Helmfile overrides.
# Override the variables, env, command and settings ONLY in the Helmfile components managed by the `testing` Team.
# The Helmfile `overrides` are deep-merged with the global `overrides`
# and takes higher priority (it will override the same keys from the global `overrides`).
helmfile:
  overrides:
    env:
      # This ENV variable will be added or overridden in all the Helmfile components managed by the `testing` Team
      TEST_ENV_VAR2: "test-env-var2-overridden"
```
</File>

In the manifest above, we configure the following:

- The global `overrides` section to override the `TEST_ENV_VAR1` ENV variable in the `env` section. All the Terraform and Helmfile components
  managed by the `testing` Team will get the ENV vars updated to `test-env-var1-overridden`.

- The Terraform-level `terraform.overrides` section to override some Spacelift configuration in the `settings` section, a variable in the `vars`
  section, and the `tofu` command to execute instead of `terraform` in the `command` section. All the Terraform components managed by the `testing`
  Team will be affected by the new values (but not the Helmfile components). The Terraform `overrides` are deep-merged with the global `overrides`
  and takes higher priority (it will override the same keys from the global `overrides`).

- The Helmfile-level `helmfile.overrides` section to override an ENV variable in the `env` section. All the Helmfile components managed by
  the `testing` Team will get the new ENV variable value (but not the Terraform components). The Helmfile `overrides` are deep-merged with the
  global `overrides` and takes higher priority (it will override the same keys from the global `overrides`).

To confirm that the components managed by the `testing` Team get the new values from the `overrides` sections, execute the following
commands:

```shell
atmos describe component test/test-component -s tenant1-uw2-dev
atmos describe component test/test-component-override -s tenant1-uw2-dev
```

You should see the following output:

<Terminal title="atmos describe component test/test-component-override -s tenant1-uw2-dev">
```yaml
# Final deep-merged `overrides` from all the global `overrides` and Terraform `overrides` sections
overrides:
  command: tofu
  env:
    TEST_ENV_VAR1: test-env-var1-overridden
  settings:
    spacelift:
      autodeploy: true
  vars:
    test_1: 1

# The `command` was overridden with the value from `terraform.overrides.command`
command: tofu

env:
  # The `TEST_ENV_VAR1` ENV variable was overridden with the value from `overrides.env.TEST_ENV_VAR1`
  TEST_ENV_VAR1: test-env-var1-overridden
  TEST_ENV_VAR2: val2

settings:
  spacelift:
    # The `autodeploy` setting was overridden with the value
    # from `terraform.overrides.settings.spacelift.autodeploy`
    autodeploy: true
    workspace_enabled: true

vars:
  environment: uw2
  namespace: cp
  region: us-west-2
  stage: dev
  tenant: tenant1
  # The `test_1` variable was overridden with the value from `terraform.overrides.vars.test_1`
  test_1: 1
```
</Terminal>

To confirm that the components managed by the `devops` Team are not affected by the `overrides` for the `testing` Team, execute the following
command:

<Terminal title="atmos describe component top-level-component1 -s tenant1-uw2-dev">
```yaml
# The `command` is not overridden
command: terraform

# The component does not get the `overrides` section since it's not defined
# for the components managed by the `devops` Team
overrides: {}

vars:
  <variables for 'top-level-component1' in the stack 'tenant1-uw2-dev'>

env:
  <ENV variables for 'top-level-component1' in the stack 'tenant1-uw2-dev'>

settings:
  <settings for 'top-level-component1' in the stack 'tenant1-uw2-dev'>
```
</Terminal>

The `top-level-component1` component managed by the `devops` Team does not get affected by the `overrides` sections for the `testing` Team,
and the sections `vars`, `env`, `settings` and `command` are not updated with the values from the `overrides` configuration.

## Importing the Overrides

To make the `overrides` configuration DRY and reusable, you can place the `overrides` sections into a separate stack manifest,
and then import it into other stacks.

For example:

Define the `overrides` sections in a separate manifest `stacks/teams/testing-overrides.yaml`:

<File title="stacks/teams/testing-overrides.yaml">
```yaml
# Global overrides
# Override the variables, env, command and settings ONLY in the components managed by the `testing` Team.
overrides:
  env:
    # This ENV variable will be added or overridden in all the components managed by the `testing` Team
    TEST_ENV_VAR1: "test-env-var1-overridden"
  settings: {}
  vars: {}

# Terraform overrides
# Override the variables, env, command and settings ONLY in the Terraform components managed by the `testing` Team.
# The Terraform `overrides` are deep-merged with the global `overrides`
# and takes higher priority (it will override the same keys from the global `overrides`).
terraform:
  overrides:
    settings:
      spacelift:
        # All the components managed by the `testing` Team will have the Spacelift stacks auto-applied
        # if the planning phase was successful and there are no plan policy warnings
        # https://docs.spacelift.io/concepts/stack/stack-settings#autodeploy
        autodeploy: true
    vars:
      # This variable will be added or overridden in all the Terraform components managed by the `testing` Team
      test_1: 1
    # The `testing` Team uses `tofu` instead of `terraform`
    # https://opentofu.org
    # The commands `atmos terraform <sub-command> ...` will execute the `tofu` binary
    command: tofu
```
</File>

Import the `stacks/teams/testing-overrides.yaml` manifest into the stack `stacks/teams/testing.yaml`:

<File title="stacks/teams/testing.yaml">
```yaml
import:
  # The `testing` Team manages all the components defined in this stack manifest and imported from the catalog
  - catalog/terraform/test-component-2
  # The `overrides` in `teams/testing-overrides` will affect all the components in this stack manifest
  # and all the components that are imported AFTER the `overrides` from `teams/testing-overrides`.
  # It will affect the components imported from `catalog/terraform/test-component-2`.
  # The `overrides` defined in this manifest will affect all the imported components, including `catalog/terraform/test-component-2`.
  - teams/testing-overrides
  - catalog/terraform/test-component
  - catalog/terraform/test-component-override

# The `overrides` in this stack manifest take precedence over the `overrides` imported from `teams/testing-overrides`

# Global overrides
# Override the variables, env, command and settings ONLY in the components managed by the `testing` Team.
overrides:
  env:
    # This ENV variable will be added or overridden in all the components managed by the `testing` Team
    TEST_ENV_VAR1: "test-env-var1-overridden-2"
  settings: {}
  vars: {}

# Terraform overrides
# Override the variables, env, command and settings ONLY in the Terraform components managed by the `testing` Team.
# The Terraform `overrides` are deep-merged with the global `overrides`
# and takes higher priority (it will override the same keys from the global `overrides`).
terraform:
  overrides:
    vars:
      # This variable will be added or overridden in all the Terraform components managed by the `testing` Team
      test_1: 2
```
</File>

:::important
- The order of the imports is important. The `overrides` in `teams/testing-overrides` will affect all the components in
  this stack manifest and all the components that are imported __after__ the `overrides` from `teams/testing-overrides`.
  In other words, the `overrides` in `teams/testing-overrides` will be applied to the `catalog/terraform/test-component`
  and `catalog/terraform/test-component-override` components, but not to `catalog/terraform/test-component-2`

- On the other hand, the `overrides` defined in this stack manifest `stacks/teams/testing.yaml` will be applied to __all__
  components defined inline in `stacks/teams/testing.yaml` and all the imported components, including `catalog/terraform/test-component-2`

- The `overrides` defined inline in the stack manifest `stacks/teams/testing.yaml` take precedence over the `overrides`
  imported from `teams/testing-overrides` (they will override the same values defined in `teams/testing-overrides`)
:::

:::tip
Refer to [`atmos describe component`](/cli/commands/describe/component) CLI command for more details
:::

---

## Atmos Stacks

import File from '@site/src/components/File'
import Intro from '@site/src/components/Intro'
import ActionCard from '@site/src/components/ActionCard'

<Intro>
When you design cloud architectures with Atmos, you break them apart into pieces called components that you implement with Terraform "root modules". Stacks are how you connect your components with configuration, so that everything comes together.
</Intro>

The power of components comes from their ability to be reused: you can compose stacks with one or more components, even reusing any component multiple times within a stack. But as your stacks grow with more and more components, it often makes sense to start splitting them into different files and that's why you might want to make use of imports. This lets you keep your Stack files easier to scan and reuse their configuration in multiple places.

Stacks define the complete configuration of an environment. Think of Stacks like an architectural "Blueprints" composed of one or more [Components](/core-concepts/components) configurations and defined using a [standardized YAML configuration](#schema).

Then by running the `atmos` command, automate the orchestrate the deployment of loosely coupled [components](/core-concepts/components), such as Terraform "root" modules. By doing this, it enables scalable infrastructure-as-code configurations, allowing environments to inherit from one or more common bases (child stacks) by importing configuration that gets deep-merged, thus minimizing config duplication and manual effort. Each stack uses a simple schema that provides a declarative description of your various environments. This approach empowers you to separate your infrastructure’s environment configuration settings from the code it manages (e.g., [Terraform components](/core-concepts/components/terraform)).

By facilitating the infrastructure configurations this way, developers achieve DRY (Don't Repeat Yourself) architectures with minimal
configuration. Stacks make infrastructure more streamlined and consistent, significantly enhancing productivity. Best of all, Stacks
can deploy vanilla Terraform "root" modules *without* any code generation, custom vendor extensions, or changes to the HCL code.

Atmos utilizes a custom YAML configuration format for stacks. YAML is ideal because it's portable across multiple toolchains and languages; every developer understands it. The Atmos [CLI](/cli), the [terraform-utils-provider](https://github.com/cloudposse/terraform-provider-utils) provider, and Spacelift via the [terraform-spacelift-cloud-infrastructure-automation](https://github.com/cloudposse/terraform-spacelift-cloud-infrastructure-automation) module all support stacks. Utilizing the Terraform provider enables native access to the entire infrastructure configuration directly from Terraform.

<ActionCard ctaLink="/core-concepts/stacks/define-components">
  Define your first component configuration using stacks.
</ActionCard>

## Use-cases

- **Rapid Environment Provisioning:** Leverage stacks to swiftly set up and replicate development, testing,
  and production environments, ensuring consistency and reducing manual setup errors. This accelerates the development
  cycle and enables businesses to respond quickly to market demands or development needs.
- **Multi-Tenant Infrastructure Management:** Utilize stacks to manage and isolate resources for different clients or projects
  within a single cloud infrastructure. This approach supports SaaS companies in providing secure, isolated environments for each
  tenant, optimizing resource utilization and simplifying the management of complex, multi-tenant architectures.
- **Compliance and Governance:** Implement stacks to enforce compliance and governance policies across all environments systematically.
  By defining standard configurations that meet regulatory requirements, businesses can ensure that every deployment is compliant,
  reducing the risk of violations and enhancing security posture.

## Conventions

The differentiation between the following two types of stacks is crucial for understanding how to organize stacks and the basis for the
various [design patterns](/design-patterns/).

### Stack Names (aka "slugs")

Every stack is uniquely identified by a name. The name is used to reference the stack in the Atmos CLI, or with stack dependencies.

These are computed from either the `name_pattern` (old way) or the more configurable
`name_template` (new way). These are configured in the `atmos.yaml` configuration file.

For example, using the slug, we can reference a stack like this when applying the `vpc` stack in the `us2-dev` environment:

```bash
atmos terraform apply vpc -s us2-dev
```

### Components vs Component instances

Components are different from Stacks.

When a component is added to a stack, we call that a "Component Instance"

### Parent Stacks vs Child Stacks

<dl>
  <dt>Parent Stacks</dt>
  <dd>These are the top-level stacks that are responsible for importing Child stacks. Components inside of Parent stacks are deployable, unlike in Child stacks.</dd>

  <dt>Child Stacks</dt>
  <dd>These are any stacks whose components cannot be deployed independently without being imported by a Parent Stack. Catalogs are typically where we keep our Child stacks.</dd>
</dl>

### Logical Stacks vs. Physical Stack Manifests

<dl>
  <dt>Logical Stacks</dt>
  <dd>
      Represent the entire environment defined by context variables and global settings in atmos.yaml.
      Logical stacks are the in-memory representation of the deep-merged configuration.
    </dd>

  <dt>Physical Stacks</dt>
  <dd>Are the raw YAML files where the specific configurations of components are defined.</dd>
</dl>

Atmos processes each physical stack file, first evaluating any templates and then processing it as YAML. After loading the YAML,
it proceeds to deep-merge the configuration with the current in-memory logical representation of the Stack, then apply any overrides.
This is done iteratively for each physical stack file in the order they are defined in the `import` section of the Stack file.

Note, the logical representation is never influenced by file paths or directories. It's only influenced by the configuration itself.

## Schema

A Stack file contains a manifest defined in YAML that follows a simple, extensible schema. In fact, every Stack file follows exactly the same schema, and every setting in the configuration is optional. Enforcing a consistent schema ensures we can easily [import and deep-merge](/core-concepts/stacks/imports) configurations and use [inheritance](/core-concepts/stacks/inheritance) to achieve DRY configuration.

<File title="stack.yaml">
```yaml
# Configurations that should get deep-merged into this one
import:
  # each import is a "Stack" file. The `.yaml` extension is optional, and we do not recommend using it.
  - ue2-globals

# Top-level variables that are inherited by every single component.
# Use these wisely. Too many global vars will pollute the variable namespace.
vars:
  # Variables can be anything you want. They can be scalars, lists, and maps. Whatever is supported by YAML.
  stage: dev

# There can then be global variables for each type of component.
# Here we set global variables for any "terraform" component.
terraform:
  vars: {}

# Here we set global variables for any "helmfile" component.
helmfile:
  vars: {}

# Components are the building blocks of reusable infrastructure.
# They can be anything. Atmos natively supports "terraform" and "helmfile".
components:
  terraform:
    vpc:
      command: "/usr/bin/terraform-0.15"
      backend:
        s3:
          workspace_key_prefix: "vpc"
      vars:
        cidr_block: "10.102.0.0/18"
    eks:
      backend:
        s3:
          workspace_key_prefix: "eks"
      vars: {}

  helmfile:
    nginx-ingress:
      vars:
        installed: true
```
</File>

### Stack Attributes

<dl>
  <dt>`components`</dt>
  <dd>
  The `components` is the list of all the building blocks.

  Example:

  ```yaml
  components:
    sometool: # "sometool" can be any tool
      somecomponent: # "somecomponent" can be the name of any "sometool" component
        vars: # etc...
  ```
  </dd>

  <dt>`components.terraform`</dt>
  <dd>
  So for `terraform`, it might look something like this:

  ```yaml
  components:
    terraform:
      vpc:
        vars: # etc...
  ```
  </dd>
</dl>

## Stack Files

Stack files can be very numerous in large cloud environments (think many dozens to hundreds of stack files). To enable the proper organization of stack files, SweetOps has established some conventions that are good to follow. However, these are just conventions, and there are no limits enforced by the tool.

By convention, we recommend storing all Stacks in a `stacks/` folder at the root of your infrastructure repository. This way it's clear where they live and helps keep the configuration separate from your code (e.g. HCL).

The filename of individual environment stacks can follow any convention, and the best one will depend on how you model environments at your organization.

### Basic Layout

A basic form of organization is to follow the naming pattern where each `$environment-$stage.yaml` is a file. This works well until you have so
many environments and stages.

For example, `$environment` might be `ue2` (for `us-east-2`) and `$stage` might be `prod` which would result in `stacks/ue2-prod.yaml`

Some resources, however, are global in scope. For example, Route53 and IAM might not make sense to tie to a region. These are what we call "global
resources". You might want to put these into a file like `stacks/global-region.yaml` to connote that they are not tied to any particular region.

### Hierarchical Layout

We recommend using a hierarchical layout that follows the way AWS thinks about infrastructure. This works very well when you may have dozens or
hundreds of accounts and regions that you operate in. Use [Catalogs](/core-concepts/stacks/catalogs) to organize your Stack configurations.

---

## Template Data Sources

import File from '@site/src/components/File'
import Terminal from '@site/src/components/Terminal'
import PillBox from '@site/src/components/PillBox'
import Intro from '@site/src/components/Intro'

<PillBox>Advanced</PillBox>

<Intro>
Data sources in Atmos refer to external locations from which Atmos can fetch configuration data.
Atmos supports all data sources supported by [Gomplate](https://docs.gomplate.ca/datasources).
For example, you can use data sources to fetch JSON metadata from API endpoints or read from various backends like S3 Buckets, AWS SSM Parameter Store, HashiCorp Vault, and many others.
</Intro>

## Data sources

Currently, Atmos supports all the [Gomplate Datasources](https://docs.gomplate.ca/datasources).
More data sources will be added in the future (and this doc will be updated).
All datasource configurations are defined in the `templates.settings.gomplate.datasources` section in `atmos.yaml` [CLI config file](/cli/configuration)
or in the `settings.templates.settings.gomplate.datasources` section of any [Atmos stack manifests](/core-concepts/stacks).

The `gomplate.datasources` section is a map of [Gomplate Datasource](https://docs.gomplate.ca/datasources) definitions.

The keys of the map are the data source names (aliases) that you will use to refer to them. For example,
if you define a data source called `foobar` which has a property called `tag`, you could refer to it like this in a
stack manifest: `{{ (datasource "foobar").tag }}`.

For example:

<File title="stack.yaml">
```yaml
terraform:
  vars:
    tags:
      provisioned_by_ip: '{{ (datasource "ip").ip }}'
      config1_tag: '{{ (datasource "config-1").tag }}'
      config2_service_name: '{{ (datasource "config-2").service.name }}'
```
</File>

The values in the map are data source definitions following this schema:

<dl>
  <dt>`url`</dt>
  <dd>
  All data sources are defined as a [URL](https://docs.gomplate.ca/datasources/#url-format).

  As a refresher, a Gomplate Data Source URL is made up of the following components:

    ```plaintext
    scheme://user@host.com:8080/path?query=string#fragment
    ```
  </dd>

  <dt>`headers`</dt>
  <dd>
    A map of [HTTP request headers](https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers) for
    the [`http` data source](https://docs.gomplate.ca/datasources/#sending-http-headers).
    The keys of the map are the header names. The values of the map are lists of values for the header.

    The following configuration will result in the
    [`accept: application/json`](https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Accept) HTTP header
    being sent with the HTTP request to the data source:

       ```yaml
       headers:
         accept:
           - "application/json"
      ```
    </dd>
</dl>

## Types of Data Sources

The following are the types of data sources are supported by Atmos via [Gomplate](https://docs.gomplate.ca/datasources/#url-format).

<dl>
    <dt>`aws+smp://`</dt>
    <dd>
    AWS Systems Manager Parameter Store is a key/value store that supports encryption and versioning.
    </dd>

    <dt>`aws+sm://`</dt>
    <dd>
    AWS Secrets Manager lets you store and retrieve secrets.
    </dd>

    <dt>`s3://`</dt>
    <dd>Amazon S3 provides object storage, which is convenient for stashing shared configurations.</dd>

    <dt>`consul://`, `consul+http://`, `consul+https://`</dt>
    <dd>Use HashiCorp Consul provides as a backend key/value store</dd>

    <dt>`env://`</dt>
    <dd>Environment variables can be used as data sources, although [template functions](/functions/template) might make more sense.</dd>

    <dt>`file://`</dt>
    <dd>Files can be read in any of the supported formats (JSON, YAML). Directories are also supported, just end the URL path with a `/`.</dd>

    <dt>`git://`, `git+file://`, `git+http://`, `git+https://`, `git+ssh://`</dt>
    <dd>
    Files can be read from a local or remote git repository, at specific branches or tags. Directory semantics are also supported.
    </dd>

    <dt>`gs://`</dt>
    <dd>
    Google Cloud Storage is the object storage service that is similar to AWS S3.
    </dd>

    <dt>`http://`, `https://`</dt>
    <dd>
    Retrieve data from HTTP/HTTPS endpoints. Custom HTTP headers can also be passed.
    </dd>

    <dt>`merge://`</dt>
    <dd>
    Merge two or more data sources together to produce the final value - useful for resolving defaults. Uses coll.Merge for merging.
    </dd>

    <dt>`stdin://`</dt>
    <dd>
    Read configuration data from standard input.
    </dd>

    <dt>`vault://`, `vault+http://`, `vault+https://`</dt>
    <dd>
    HashiCorp Vault is a popular open-source secret management platform.
    </dd>
</dl>

## Environment Variables

Some data sources might need environment variables that are different from the environment variables in Stack configuration. Environment variables may be passed to data sources when processing and executing templates by defining `env` map.
It's supported in both the `templates.settings` section in `atmos.yaml` [CLI config file](/cli/configuration) and in the
`settings.templates.settings` section in [Atmos stack manifests](/core-concepts/stacks).

For example:

<File title="atmos.yaml">
```yaml
settings:
  templates:
    settings:
      # Environment variables passed to `datasources` when evaluating templates
      # https://docs.gomplate.ca/datasources/#using-awssmp-datasources
      # https://docs.gomplate.ca/functions/aws/#configuring-aws
      # https://docs.aws.amazon.com/sdk-for-go/v1/developer-guide/configuring-sdk.html
      env:
        AWS_PROFILE: "<AWS profile>"
        AWS_TIMEOUT: 2000
```
</File>

This is useful when executing data sources that need to authenticate to cloud APIs.

For more details, refer to:

- [Configuring AWS](https://docs.gomplate.ca/functions/aws/#configuring-aws)
- [Configuring GCP](https://docs.gomplate.ca/functions/gcp/#configuring-gcp)

## Configuring Data Sources

For example, let's define the following Gomplate `datasources` in the global `settings` section (this will apply to all
components in all stacks in the infrastructure).

First, enable `Go` templates and `gomplate` datasources in the `atmos.yaml` CLI config file:

<File title="atmos.yaml">
```yaml
templates:
  settings:
    # Enable `Go` templates in Atmos stack manifests
    enabled: true
    gomplate:
      # Enable Gomplate functions and data sources in `Go` templates in Atmos stack manifests
      enabled: true
```
</File>

Then, define the following data sources in the global `settings` section in an Atmos stack manifest:

<File title="stacks/orgs/acme/_defaults.yaml">
```yaml
settings:
  templates:
    settings:
      gomplate:
        # Timeout in seconds to execute the data sources
        timeout: 5
        # https://docs.gomplate.ca/datasources
        datasources:
          # 'http' data source
          # https://docs.gomplate.ca/datasources/#using-file-datasources
          ip:
            url: "https://api.ipify.org?format=json"
            # https://docs.gomplate.ca/datasources/#sending-http-headers
            # https://docs.gomplate.ca/usage/#--datasource-header-h
            headers:
              accept:
                - "application/json"
          # 'file' data sources
          # https://docs.gomplate.ca/datasources/#using-file-datasources
          config-1:
            url: "./config1.json"
          config-2:
            url: "file:///config2.json"
          # `aws+smp` AWS Systems Manager Parameter Store data source
          # https://docs.gomplate.ca/datasources/#using-awssmp-datasources
          secret-1:
            url: "aws+smp:///path/to/secret"
          # `aws+sm` AWS Secrets Manager datasource
          # https://docs.gomplate.ca/datasources/#using-awssm-data source
          secret-2:
            url: "aws+sm:///path/to/secret"
          # `s3` datasource
          # https://docs.gomplate.ca/datasources/#using-s3-data sources
          s3-config:
            url: "s3://mybucket/config/config.json"
```
</File>

After the above data sources are defined, you can use them in Atmos stack manifests like this:

<File>
```yaml
terraform:
 vars:
   tags:
     tag1: '{{ (datasource "config-1").tag }}'
     service_name2: '{{ (datasource "config-2").service.name }}'
     service_name3: '{{ (datasource "s3-config").config.service_name }}'

components:
  terraform:
    vpc-1:
      settings:
        provisioned_by_ip: '{{ (datasource "ip").ip }}'
        secret-1: '{{ (datasource "secret-1").secret1.value }}'
      vars:
        enabled: '{{ (datasource "config-2").config.enabled }}'
```
</File>

## Using templates in the URLs of `datasources`

<PillBox>Advanced</PillBox>

Let's suppose that your company uses a centralized software catalog to consolidate all tags for tagging all the cloud
resources. The tags can include tags per account, per team, per service, billing tags, etc.

:::note
An example of such a centralized software catalog could be [Backstage](https://backstage.io).
:::

Let's also suppose that you have a service to read the tags from the centralized catalog and write them into an S3
bucket in one of your accounts. The bucket serves as a cache to not hit the external system's API with too many requests
and not to trigger rate limiting.

And finally, let's say that in the bucket, you have folders per account (`dev`, `prod`, `staging`). Each folder has a JSON
file with all the tags defined for all the cloud resources in the accounts.

We can then use the [Gomplate S3 datasource](https://docs.gomplate.ca/datasources/#using-s3-datasources) to read the JSON
file with the tags for each account and assign the tags to all cloud resources.

In `atmos.yaml`, we figure two evaluations steps of template processing:

<File title="atmos.yaml">
```yaml
templates:
  settings:
    enabled: true
    # Number of evaluations to process `Go` templates
    evaluations: 2
    gomplate:
      enabled: true
```
</File>

In an Atmos stack manifest, we define the environment variables in the `env` section (AWS profile with permissions to
access the S3 bucket), and the `s3-tags` Gomplate datasource.

In the `terraform.vars.tags` section, we define all the tags that are returned from the call to the S3 datasource.

```yaml
import:
  # Import the default configuration for all VPCs in the infrastructure
  - catalog/vpc/defaults

# Global settings
settings:
  templates:
    settings:
      # Environment variables passed to data sources when evaluating templates
      # https://docs.gomplate.ca/functions/aws/#configuring-aws
      # https://docs.aws.amazon.com/sdk-for-go/v1/developer-guide/configuring-sdk.html
      env:
        # AWS profile with permissions to access the S3 bucket
        AWS_PROFILE: "<AWS profile>"
      gomplate:
        # Timeout in seconds to execute the data sources
        timeout: 5
        # https://docs.gomplate.ca/datasources
        datasources:
          # `s3` datasource
          # https://docs.gomplate.ca/datasources/#using-s3-datasources
          s3-tags:
            # The `url` uses a `Go` template with the delimiters `${ }`,
            # which is processed as first step in the template processing pipeline
            url: "s3://mybucket/{{ .vars.stage }}/tags.json"

# Global Terraform config
terraform:
  # Global variables that are used by all Atmos components
  vars:
    tags:
      atmos_component: "{{ .atmos_component }}"
      atmos_stack: "{{ .atmos_stack }}"
      terraform_component: "{{ .component }}"
      terraform_workspace: "{{ .workspace }}"
      devops_team: '{{`{{ (datasource "s3-tags").tags.devops_team }}`}}'
      billing_team: '{{`{{ (datasource "s3-tags").tags.billing_team }}`}}'
      service: '{{`{{ (datasource "s3-tags").tags.service }}`}}'

# Atmos component configurations
components:
  terraform:
    vpc/1:
      metadata:
        component: vpc  # Point to the Terraform component in `components/terraform/vpc` folder
        inherits:
          # Inherit from the `vpc/defaults` base Atmos component, which defines the default
          # configuration for all VPCs in the infrastructure.
          # The `vpc/defaults` base component is defined in the `catalog/vpc/defaults`
          # manifest (which is imported above).
          # This inheritance makes the `vpc/1` Atmos component config DRY.
          - "vpc/defaults"
      vars:
        name: "vpc-1"
```

When executing an Atmos command like `atmos terraform apply vpc/1 -s plat-ue2-dev`, the above template will be processed
in two evaluation steps:

- Evaluation 1:

  - `datasources.s3-tags.url` is set to `s3://mybucket/dev/tags.json`

  - the tags that use the `datasource` templates are set to the following:

    ```yaml
    devops_team: '{{ (datasource "s3-tags").tags.devops_team }}'
    billing_team: '{{ (datasource "s3-tags").tags.billing_team }}'
    service: '{{ (datasource "s3-tags").tags.service }}'
    ```

- Evaluation 2:
    - all `s3-tags` datasources get executed, the JSON file `s3://mybucket/dev/tags.json` with the tags
      for the `dev` account is downloaded from the S3 bucket, and the tags are parsed and assigned in the
      `terraform.vars.tags` section

After executing the two evaluation steps, the resulting tags for the Atmos component `vpc/1` in the stack `plat-ue2-dev`
would look like this:

```yaml
atmos_component: vpc/1
atmos_stack: plat-ue2-dev
terraform_component: vpc
terraform_workspace: plat-ue2-dev-vpc-1
devops_team: dev_networking
billing_team: billing_net
service: net
```

The tags will be added to all the AWS resources provisioned by the `vpc` Terraform component in the `plat-ue2-dev` stack.

---

## Stack Manifest Templating

import File from '@site/src/components/File'
import Terminal from '@site/src/components/Terminal'
import PillBox from '@site/src/components/PillBox'
import Intro from '@site/src/components/Intro'
import ActionCard from '@site/src/components/ActionCard'
import PrimaryCTA from '@site/src/components/PrimaryCTA'
import SecondaryCTA from '@site/src/components/SecondaryCTA'

<PillBox>Advanced</PillBox>

<Intro>
    Use templates as an _escape hatch_, when standard [inheritance](/core-concepts/stacks/inheritance) or
    [Atmos Functions](/functions/yaml) are insufficient.
    Atmos supports [Go templates](https://pkg.go.dev/text/template) in stack manifests and functions to customize Stack configurations.
</Intro>

:::note Template File Validation
Template files (`.yaml.tmpl`, `.yml.tmpl`, `.tmpl`) are automatically detected and processed during normal operations (imports, etc.).
However, they are excluded from YAML validation (`atmos validate stacks`) since they may contain template placeholders that are invalid YAML before being rendered.
This ensures that template files can contain valid Go template syntax without causing validation errors.
:::

### Enable Templating

Templating in Atmos stack manifests is configured in the `atmos.yaml` [CLI config file](/cli/configuration) in the
`templates.settings` section.

<dl>
  <dt>`templates.settings`</dt>
  <dd>In the `templates.settings` section in `atmos.yaml` [CLI config file](/cli/configuration)</dd>

  <dt> `settings.templates.settings`</dt>
  <dd>
    In the `settings.templates.settings` section in [Atmos stack manifests](/core-concepts/stacks). The `settings.templates.settings` section can be defined globally per organization, tenant, account, or per component. Atmos deep-merges the configurations from all scopes into the final result using [inheritance](/core-concepts/stacks/inheritance).
  </dd>

  <dt>`templates.settings.enabled`</dt>
  <dd>A boolean flag to enable/disable the processing of `Go` templates in Atmos stack manifests. If set to `false`, Atmos will not process `Go` templates in stack manifests.</dd>
</dl>

### Configure Templating

<dl>

  <dt>`templates.settings.env`</dt>
  <dd>A map of environment variables to use when executing the templates.</dd>

  <dt>`templates.settings.evaluations`</dt>
  <dd>Number of evaluations/passes to process `Go` templates. If not defined, `evaluations` is automatically set to `1`. For more details, refer to [Template Evaluations and Template Processing Pipelines](#processing-pipelines).</dd>

  <dt>`templates.settings.delimiters`</dt>
  <dd>A list of left and right delimiters to use to process the templates. If not defined, the default `Go` template delimiters `["{{", "}}"]` will be used.</dd>

  <dt>`templates.settings.sprig.enabled`</dt>
  <dd>A boolean flag to enable/disable the [Sprig Functions](https://masterminds.github.io/sprig/) in Atmos stack manifests.</dd>

  <dt>`templates.settings.gomplate.enabled`</dt>
  <dd>A boolean flag to enable/disable the [Gomplate Functions](https://docs.gomplate.ca/functions/) and [Gomplate Datasources](https://docs.gomplate.ca/datasources) in Atmos stack manifests.</dd>

  <dt>`templates.settings.gomplate.timeout`</dt>
  <dd>Timeout in seconds to execute [Gomplate Datasources](https://docs.gomplate.ca/datasources).</dd>

</dl>

:::warning

Some functions are present in both [Sprig](https://masterminds.github.io/sprig/) and [Gomplate](https://docs.gomplate.ca/functions/).

For example, the `env` function has the same name in [Sprig](https://masterminds.github.io/sprig/os.html) and
[Gomplate](https://docs.gomplate.ca/functions/env/), but has different syntax and accept different number of arguments.

If you use the `env` function from one templating engine and enable both [Sprig](https://masterminds.github.io/sprig/)
and [Gomplate](https://docs.gomplate.ca/functions/), it will be invalid in the other templating engine, and an error will be thrown.

To be able to use the `env` function from both templating engines, you can do one of the following:

- Use the `env` function from one templating engine, and disable the other templating engine by using the
  `templates.settings.sprig.enabled` and `templates.settings,gomplate.enabled` settings

- Enable both engines and use the Gomplate's `env` function via its
  [`getenv`](https://docs.gomplate.ca/functions/env/#examples) alias

:::

#### Example Configuration

<File title="atmos.yaml">
```yaml
# https://pkg.go.dev/text/template
templates:
  settings:
    # Enable `Go` templates in Atmos stack manifests
    enabled: true
    # Number of evaluations/passes to process `Go` templates
    # If not defined, `evaluations` is automatically set to `1`
    evaluations: 2
    # Optional template delimiters
    # The `{{ }}` delimiters are the default, no need to specify/redefine them
    delimiters: ["{{", "}}"]
    # Environment variables passed to data sources when evaluating templates
    # https://docs.gomplate.ca/datasources/#using-awssmp-datasources
    # https://docs.gomplate.ca/functions/aws/#configuring-aws
    # https://docs.aws.amazon.com/sdk-for-go/v1/developer-guide/configuring-sdk.html
    env:
      AWS_PROFILE: "<AWS profile>"
      AWS_TIMEOUT: 2000
    # https://masterminds.github.io/sprig
    sprig:
      # Enable Sprig functions in `Go` templates in Atmos stack manifests
      enabled: true
    # https://docs.gomplate.ca
    # https://docs.gomplate.ca/functions
    gomplate:
      # Enable Gomplate functions and data sources in `Go` templates in Atmos stack manifests
      enabled: true
      # Timeout in seconds to execute the data sources
      timeout: 5
      datasources: {}
```
</File>

## Functions and Data Sources

Go templates by themselves are pretty basic, supporting concepts like ranges and variable interpolations. But what really makes templating powerful is the library of functions provided by Atmos to the template engine.

In `Go` templates, you can use the following functions and data sources:

 - [Go `text/template` functions](https://pkg.go.dev/text/template#hdr-Functions)
 - [Sprig Functions](https://masterminds.github.io/sprig/)
 - [Gomplate Functions](https://docs.gomplate.ca/functions/) (note, this is "Gomplate" and not "Go template")
 - [Gomplate Datasources](https://docs.gomplate.ca/datasources/)
 - [Atmos Template Functions](/functions/template)

<ActionCard>
    Functions are a crucial part of templating in Atmos stack manifests. They allow you to manipulate data and perform operations on the data to customize the stack configurations.
    <PrimaryCTA to="/functions/template">Learn About Functions</PrimaryCTA>
</ActionCard>

### Configuring Templating in Atmos Stack Manifests

Templating in Atmos can also be configured in the `settings.templates.settings` section in stack manifests.

The `settings.templates.settings` section can be defined globally per organization, tenant, account, or per component.
Atmos deep-merges the configurations from all scopes into the final result using [inheritance](/core-concepts/stacks/inheritance).

The schema is the same as `templates.settings` in the `atmos.yaml` [CLI config file](/cli/configuration),
except the following settings are not supported in the `settings.templates.settings` section:

- `settings.templates.settings.enabled`
- `settings.templates.settings.sprig.enabled`
- `settings.templates.settings.gomplate.enabled`
- `settings.templates.settings.evaluations`
- `settings.templates.settings.delimiters`

These settings are not supported for the following reasons:

- You can't disable templating in the stack manifests which are being processed by Atmos as `Go` templates

- If you define the `delimiters` in the `settings.templates.settings` section in stack manifests,
  the `Go` templating engine will think that the delimiters specify the beginning and the end of template strings, will
  try to evaluate it, which will result in an error

As an example, let's define templating configuration for the entire organization in the `stacks/orgs/acme/_defaults.yaml`
stack manifest:

<File title="stacks/orgs/acme/_defaults.yaml">
```yaml
settings:
  templates:
    settings:
      # Environment variables passed to data sources when evaluating templates
      # https://docs.gomplate.ca/datasources/#using-awssmp-datasources
      # https://docs.gomplate.ca/functions/aws/#configuring-aws
      # https://docs.aws.amazon.com/sdk-for-go/v1/developer-guide/configuring-sdk.html
      env:
        AWS_PROFILE: "<AWS profile>"
        AWS_TIMEOUT: 2000
      gomplate:
        # 7 seconds timeout to execute the data sources
        timeout: 7
        # https://docs.gomplate.ca/datasources
        datasources:
          # 'file' data sources
          # https://docs.gomplate.ca/datasources/#using-file-datasources
          config-1:
            url: "./my-config1.json"
          config-3:
            url: "file:///config3.json"
```
</File>

Atmos deep-merges the configurations from the `settings.templates.settings` section in [Atmos stack manifests](/core-concepts/stacks)
with the `templates.settings` section in `atmos.yaml` [CLI config file](/cli/configuration) using [inheritance](/core-concepts/stacks/inheritance).

The `settings.templates.settings` section in [Atmos stack manifests](/core-concepts/stacks) takes precedence over
the `templates.settings` section in `atmos.yaml` [CLI config file](/cli/configuration), allowing you to define the global
`datasources` in `atmos.yaml` and then add or override `datasources` in Atmos stack manifests for the entire organization,
tenant, account, or per component.

For example, taking into account the configurations described above in `atmos.yaml` [CLI config file](/cli/configuration)
and in the `stacks/orgs/acme/_defaults.yaml` stack manifest, the final `datasources` map will look like this:

<File title="stacks/orgs/acme/_defaults.yaml">
```yaml
gomplate:
  timeout: 7
  datasources:
    ip:
      url: "https://api.ipify.org?format=json"
      headers:
        accept:
          - "application/json"
    random:
      url: "http://www.randomnumberapi.com/api/v1.0/randomstring?min=${ .settings.random.min }&max=${ .settings.random.max }&count=1"
    secret-1:
      url: "aws+smp:///path/to/secret"
    secret-2:
      url: "aws+sm:///path/to/secret"
    s3-config:
      url: "s3://mybucket/config/config.json"
    config-1:
      url: "./my-config1.json"
    config-2:
      url: "file:///config2.json"
    config-3:
      url: "file:///config3.json"
```
</File>

Note that the `config-1` datasource from `atmos.yaml` was overridden with the `config-1` datasource from the
`stacks/orgs/acme/_defaults.yaml` stack manifest. The `timeout` attribute was overridden as well.

You can now use the `datasources` in `Go` templates in all Atmos sections that support `Go` templates.

## Atmos sections supporting `Go` templates

You can use `Go` templates in the following Atmos sections to refer to values in the same or other sections:

  - `vars`
  - `settings`
  - `env`
  - `providers`
  - `overrides`
  - `backend`
  - `backend_type`
  - `metadata`
  - `component`
  - `command`

:::tip
In the template tokens, you can refer to any value in any section that the Atmos command
[`atmos describe component <component> -s <stack>`](/cli/commands/describe/component) generates
:::

For example, let's say we have the following component configuration using `Go` templates:

<File title="stack.yaml">
```yaml
component:
  terraform:
    vpc:
      settings:
        setting1: 1
        setting2: 2
        setting3: "{{ .vars.var3 }}"
        setting4: "{{ .settings.setting1 }}"
        component: vpc
        backend_type: s3
        region: "us-east-2"
        assume_role: "<role-arn>"
      backend_type: "{{ .settings.backend_type }}"
      metadata:
        component: "{{ .settings.component }}"
      providers:
        aws:
          region: "{{ .settings.region }}"
          assume_role: "{{ .settings.assume_role }}"
      env:
        ENV1: e1
        ENV2: "{{ .settings.setting1 }}-{{ .settings.setting2 }}"
      vars:
        var1: "{{ .settings.setting1 }}"
        var2: "{{ .settings.setting2 }}"
        var3: 3
        # Add the tags to all the resources provisioned by this Atmos component
        tags:
          atmos_component: "{{ .atmos_component }}"
          atmos_stack: "{{ .atmos_stack }}"
          atmos_manifest: "{{ .atmos_stack_file }}"
          region: "{{ .vars.region }}"
          terraform_workspace: "{{ .workspace }}"
          assumed_role: "{{ .providers.aws.assume_role }}"
          description: "{{ .atmos_component }} component provisioned in {{ .atmos_stack }} stack by assuming IAM role {{ .providers.aws.assume_role }}"
          # Examples of using the Sprig and Gomplate functions and datasources
          # https://masterminds.github.io/sprig/os.html
          provisioned_by_user: '{{ env "USER" }}'
          # https://docs.gomplate.ca/functions/strings
          atmos_component_description: "{{ strings.Title .atmos_component }} component {{ .vars.name | strings.Quote }} provisioned in the stack {{ .atmos_stack | strings.Quote }}"
          # https://docs.gomplate.ca/datasources
          provisioned_by_ip: '{{ (datasource "ip").ip }}'
          config1_tag: '{{ (datasource "config-1").tag }}'
          config2_service_name: '{{ (datasource "config-2").service.name }}'
          config3_team_name: '{{ (datasource "config-3").team.name }}'
```
</File>

When executing Atmos commands like `atmos describe component` and `atmos terraform plan/apply`, Atmos processes all the template tokens
in the manifest and generates the final configuration for the component in the stack:

<Terminal title="atmos describe component vpc -s plat-ue2-dev">
```yaml
settings:
  setting1: 1
  setting2: 2
  setting3: 3
  setting4: 1
  component: vpc
  backend_type: s3
  region: us-east-2
  assume_role: <role-arn>
backend_type: s3
metadata:
  component: vpc
providers:
  aws:
    region: us-east-2
    assume_role: <role-arn>
env:
  ENV1: e1
  ENV2: 1-2
vars:
  var1: 1
  var2: 2
  var3: 3
  tags:
    assumed_role: <role-arn>
    atmos_component: vpc
    atmos_component_description: Vpc component "common" provisioned in the stack "plat-ue2-dev"
    atmos_manifest: orgs/acme/plat/dev/us-east-2
    atmos_stack: plat-ue2-dev
    config1_tag: test1
    config2_service_name: service1
    config3_team_name: my-team
    description: vpc component provisioned in plat-ue2-dev stack by assuming IAM role <role-arn>
    provisioned_by_user: <user>
    provisioned_by_ip: 167.38.132.237
    region: us-east-2
    terraform_workspace: plat-ue2-dev
```
</Terminal>

## Performance Implications

There are some performance implications of using Go Templates with Atmos Stack configurations.

Using Go templates and template functions in Atmos stack configurations is generally safe and provides powerful flexibility. However, caution is required when leveraging functions like `atmos.Component` or others that depend on remote resources or network configurations. These functions can have significant performance implications and potential impacts on availability.

:::warning Why the Caution?

Atmos processes stack configuration files in multiple stages: first as Go templates, and then as YAML. During the Go template stage, every template function must be evaluated and resolved before Atmos can load the file. This introduces a critical dependency: Atmos cannot proceed unless all referenced resources are available and accessible.
:::

1. **Performance**: Functions like [`Atmos.Component`](/functions/template/atmos.Component) may require Atmos to retrieve extensive information about other components or outputs that depend on Terraform remote state. This adds latency, especially if used extensively across your stack configurations. In the case of retrieving Terraform outputs, Atmos must initialize the Terraform component which involves downloading all Terraform providers, which is slow. Commands like [`atmos describe stacks`](/cli/commands/describe/stacks) or [`atmos describe affected`](/cli/commands/describe/affected), which rely on evaluating all templates, can become noticeably slower as the number of remote calls increases.
2. **Availability Risks:** Templated references to remote resources introduce fragility. If a referenced resource becomes unavailable—whether due to downtime, decommissioning, or network issues—Atmos commands that depend on those templates will fail. This can severely impact high availability (HA) scenarios and your ability to reliably deploy or manage infrastructure.

### Template Function Best Practices

Careful management of template dependencies is essential for optimizing the performance of Atmos while ensuring a robust and reliable infrastructure configuration process.

To avoid potential pitfalls and maximize efficiency, follow these best practices:

1. **Minimize Dependency on Remote Sources**: Avoid referencing resources in your templates that are not highly available or are prone to downtime. Where possible, use static or locally resolvable values.
2. **Use `Atmos.Component` Sparingly**: While Atmos.Component is powerful, its overuse can significantly degrade performance. Limit its use to scenarios where it is truly necessary, and consider precomputing or caching values to reduce the frequency of evaluations.
3. **Use Terraform Remote State Directly**: Instead of relying on template functions to retrieve remote state, [use Terraform's native ability to retrieve the remote state](/core-concepts/share-data/#using-terraform-remote-state) of other components.
4. **Test for Resilience**: Simulate scenarios where a remote resource becomes unavailable and observe how Atmos behaves. Design your configurations to handle failures gracefully or provide fallbacks where feasible.

## Template Evaluations

Atmos supports many different ways of configuring and using `Go` templates:

- In [Atmos Custom Commands](/core-concepts/custom-commands)
- In [Atmos Vendoring](/core-concepts/vendor)
- In [Atmos Component Vendoring](/core-concepts/vendor/vendor-manifest)
- In [Imports](/core-concepts/stacks/imports)
- In [Stack Manifests](/core-concepts/stacks)

### Phases of Template Evaluation

These templates are processed in different phases and use different context:

- `Go` templates in [Atmos Custom Commands](/core-concepts/custom-commands) are processed when the custom commands are
  executed. The execution context can be specified by using the `component_config` section. If a custom command defines
  a `component_config` section with `component` and `stack`, Atmos generates the config for the component in the stack
  and makes it available in the `{{ .ComponentConfig.xxx.yyy.zzz }}` template variables,
  exposing all the component sections that are returned by the `atmos describe component <component> -s <stack>` CLI
  command

- `Go` templates in [Atmos Vendoring](/core-concepts/vendor) and [Atmos Component Vendoring](/core-concepts/vendor/vendor-manifest)
  are processed when the CLI command [`atmos vendor pull`](/cli/commands/vendor/pull) is executed. The templates in
  the vendoring manifests support the `{{.Version}}` variable, and the execution context is provided in the `version` attribute

- [`Go` Templates in Imports](/core-concepts/stacks/imports#go-templates-in-imports) are used in imported stack
  manifests to make them DRY and reusable. The context (variables) for the `Go` templates is provided via the static
  `context` section. Atmos processes `Go` templates in imports as the **very first** phase of the stack processing pipeline.
  When executing the [CLI commands](/cli/commands), Atmos parses and executes the templates using the provided static
  `context`, processes all imports, and finds stacks and components

- `Go` templates in Atmos stack manifests, on the other hand, are processed as the **very last** phase of the stack processing
  pipeline (after all imports are processed, all stack configurations are deep-merged, and the component in the stack is found).
  For the context (template variables), it uses all the component's attributes returned from the
  [`atmos describe component`](/cli/commands/describe/component) CLI command

These mechanisms, although all using `Go` templates, serve different purposes, use different contexts, and are executed
in different phases of the stack processing pipeline.

For more details, refer to:

- [`Go` Templates in Imports](/core-concepts/stacks/imports#go-templates-in-imports)
- [Excluding templates in imports from processing by Atmos](#excluding-templates-in-stack-manifest-from-processing-by-atmos)

### Processing Pipelines

Atmos supports configuring the number of evaluations/passes for template processing in `atmos.yaml` [CLI config file](/cli/configuration).
It effectively allows you to define template processing pipelines.

For example:

<File title="atmos.yaml">
```yaml
templates:
  settings:
    # Enable `Go` templates in Atmos stack manifests
    enabled: true
    # Number of evaluations/passes to process `Go` templates
    # If not defined, `evaluations` is automatically set to `1`
    evaluations: 2
```
</File>

- `templates.settings.evaluations` - number of evaluations to process `Go` templates. If not defined, `evaluations`
  is automatically set to `1`

Template evaluations are useful in the following scenarios:

- Combining templates from different sections in Atmos stack manifests
- Using templates in the URLs of `datasources`

## Use-cases

While `Go` templates in Atmos stack manifests offer great flexibility for various use-cases, one of the obvious use-cases
is to add a standard set of tags to all the resources in the infrastructure.

For example, by adding this configuration to the `stacks/orgs/acme/_defaults.yaml` Org-level stack manifest:

```yaml title="stacks/orgs/acme/_defaults.yaml"
terraform:
  vars:
    tags:
      atmos_component: "{{ .atmos_component }}"
      atmos_stack: "{{ .atmos_stack }}"
      atmos_manifest: "{{ .atmos_stack_file }}"
      terraform_workspace: "{{ .workspace }}"
      # Examples of using the Gomplate and Sprig functions
      # https://docs.gomplate.ca/functions/strings
      atmos_component_description: "{{ strings.Title .atmos_component }} component {{ .vars.name | strings.Quote }} provisioned in the stack {{ .atmos_stack | strings.Quote }}"
      # https://masterminds.github.io/sprig/os.html
      provisioned_by_user: '{{ env "USER" }}'
```

The tags will be processed and automatically added to all the resources provisioned in the infrastructure.

## Excluding Templates in Stack Manifest from Processing by Atmos

If you need to provide `Go` templates to external systems (e.g. ArgoCD or Datadog) verbatim and prevent Atmos from
processing the templates, use **double curly braces + backtick + double curly braces** instead of just **double curly braces**:

```console
{{`{{  instead of  {{

}}`}}  instead of  }}
```

For example:

```yaml
components:
  terraform:

    eks/argocd:
      metadata:
        component: "eks/argocd"
      vars:
        enabled: true
        name: "argocd"
        chart_repository: "https://argoproj.github.io/argo-helm"
        chart_version: 5.46.0

        chart_values:
          template-github-commit-status:
            message: |
              Application {{`{{ .app.metadata.name }}`}} is now running new version.
            webhook:
              github-commit-status:
                method: POST
                path: "/repos/{{`{{ call .repo.FullNameByRepoURL .app.metadata.annotations.app_repository }}`}}/statuses/{{`{{ .app.metadata.annotations.app_commit }}`}}"
                body: |
                  {
                    {{`{{ if eq .app.status.operationState.phase "Running" }}`}} "state": "pending"{{`{{end}}`}}
                    {{`{{ if eq .app.status.operationState.phase "Succeeded" }}`}} "state": "success"{{`{{end}}`}}
                    {{`{{ if eq .app.status.operationState.phase "Error" }}`}} "state": "error"{{`{{end}}`}}
                    {{`{{ if eq .app.status.operationState.phase "Failed" }}`}} "state": "error"{{`{{end}}`}},
                    "description": "ArgoCD",
                    "target_url": "{{`{{ .context.argocdUrl }}`}}/applications/{{`{{ .app.metadata.name }}`}}",
                    "context": "continuous-delivery/{{`{{ .app.metadata.name }}`}}"
                  }
```

When Atmos processes the templates in the manifest shown above, it renders them as raw strings allowing sending
the templates to the external system for processing:

```yaml
chart_values:
  template-github-commit-status:
    message: |
      Application {{ .app.metadata.name }} is now running new version.
    webhook:
      github-commit-status:
        method: POST
        path: "/repos/{{ call .repo.FullNameByRepoURL .app.metadata.annotations.app_repository }}/statuses/{{ .app.metadata.annotations.app_commit }}"
        body: |
          {
            {{ if eq .app.status.operationState.phase "Running" }} "state": "pending"{{end}}
            {{ if eq .app.status.operationState.phase "Succeeded" }} "state": "success"{{end}}
            {{ if eq .app.status.operationState.phase "Error" }} "state": "error"{{end}}
            {{ if eq .app.status.operationState.phase "Failed" }} "state": "error"{{end}},
            "description": "ArgoCD",
            "target_url": "{{ .context.argocdUrl }}/applications/{{ .app.metadata.name }}",
            "context": "continuous-delivery/{{ .app.metadata.name }}"
          }
```

The `printf` template function is also supported and can be used instead of **double curly braces + backtick + double curly braces**.

The following examples produce the same result:

```yaml
chart_values:
  template-github-commit-status:
    message: >-
      Application {{`{{ .app.metadata.name }}`}} is now running new version.
```

```yaml
chart_values:
  template-github-commit-status:
    message: "Application {{`{{ .app.metadata.name }}`}} is now running new version."
```

```yaml
chart_values:
  template-github-commit-status:
    message: >-
      {{ printf "Application {{ .app.metadata.name }} is now running new version." }}
```

```yaml
chart_values:
  template-github-commit-status:
    message: '{{ printf "Application {{ .app.metadata.name }} is now running new version." }}'
```

## Excluding Templates in Imports

If you are using [`Go` Templates in Imports](/core-concepts/stacks/imports#go-templates-in-imports) and `Go` templates
in stack manifests in the same Atmos manifest, take into account that in this case Atmos will do `Go`
template processing two times (two passes):

  - When importing the manifest and processing the template tokens using the variables from the provided `context` object
  - After finding the component in the stack as the final step in the processing pipeline

For example, we can define the following configuration in the `stacks/catalog/eks/eks_cluster.tmpl` template file:

```yaml title="stacks/catalog/eks/eks_cluster.tmpl"
components:
  terraform:
    eks/cluster:
      metadata:
        component: eks/cluster
      vars:
        enabled: "{{ .enabled }}"
        name: "{{ .name }}"
        tags:
          atmos_component: "{{ .atmos_component }}"
          atmos_stack: "{{ .atmos_stack }}"
          terraform_workspace: "{{ .workspace }}"
```

Then we import the template into a top-level stack providing the context variables for the import in the `context` object:

```yaml title="stacks/orgs/acme/plat/prod/us-east-2.yaml"
import:
  - path: "catalog/eks/eks_cluster.tmpl"
    context:
      enabled: true
      name: prod-eks
```

Atmos will process the import and replace the template tokens using the variables from the `context`.
Since the `context` does not provide the variables for the template tokens in `tags`, the following manifest will be
generated:

```yaml
components:
  terraform:
    eks/cluster:
      metadata:
        component: eks/cluster
      vars:
        enabled: true
        name: prod-eks
        tags:
          atmos_component: <no value>
          atmos_stack: <no value>
          terraform_workspace: <no value>
```

The second pass of template processing will not replace the tokens in `tags` because they are already processed in the
first pass (importing) and the values `<no value>` are generated.

To deal with this, use **double curly braces + backtick + double curly braces** instead of just **double curly braces**
in `tags` to prevent Atmos from processing the templates in the first pass and instead process them in the second pass:

```yaml title="stacks/catalog/eks/eks_cluster.tmpl"
components:
  terraform:
    eks/cluster:
      metadata:
        component: eks/cluster
      vars:
        enabled: "{{ .enabled }}"
        name: "{{ .name }}"
        tags:
          atmos_component: "{{`{{ .atmos_component }}`}}"
          atmos_stack: "{{`{{ .atmos_stack }}`}}"
          terraform_workspace: "{{`{{ .workspace }}`}}"
```

Atmos will first process the import and replace the template tokens using the variables from the `context`.
Then in the second pass the tokens in `tags` will be replaced with the correct values.

It will generate the following manifest:

```yaml
components:
  terraform:
    eks/cluster:
      metadata:
        component: eks/cluster
      vars:
        enabled: true
        name: prod-eks
        tags:
          atmos_component: eks/cluster
          atmos_stack: plat-ue2-prod
          terraform_workspace: plat-ue2-prod
```

## Combining templates from different sections in Atmos stack manifests

You can define more than one step/pass of template processing to use and combine the results from each step.

For example:

<File title="atmos.yaml">
```yaml
templates:
  settings:
    enabled: true
    # Number of evaluations to process `Go` templates
    evaluations: 3
```
</File>

```yaml
settings:
  test: "{{ .atmos_component }}"
  test2: "{{ .settings.test }}"

components:
  terraform:
    vpc:
      vars:
        tags:
          tag1: "{{ .settings.test }}-{{ .settings.test2 }}"
          tag2: "{{\"{{`{{ .atmos_component }}`}}\"}}"
```

When executing an Atmos command like `atmos terraform plan vpc -s <stack>`, the above template will be processed
in three phases:

- Evaluation 1

  - `settings.test` is set to `vpc`
  - `settings.test2` is set to `{{ .atmos_component }}`
  - `vpc.vars.tags.tag1` is set to `{{ .atmos_component }}-{{ .settings.test }}`
  - `vpc.vars.tags.tag2` is set to `{{<backtick>{{ .atmos_component }}<backtick>}}`

- Evaluation 2

  - `settings.test` is `vpc`
  - `settings.test2` is set to `vpc`
  - `vpc.vars.tags.tag1` is set to `vpc-vpc`
  - `vpc.vars.tags.tag2` is set to `{{ .atmos_component }}`

- Evaluation 3

  - `settings.test` is `vpc`
  - `settings.test2` is `vpc`
  - `vpc.vars.tags.tag1` is `vpc-vpc`
  - `vpc.vars.tags.tag2` is set to `vpc`

:::warning

The above example shows the supported functionality in Atmos templating.
You can use it for some use-cases, but it does not mean that you **should** use it just for the sake of using, since
it's not easy to read and understand what data we have after each evaluation step.

The [Using Templates in the URLs of Datasources](/core-concepts/stacks/templates/datasources#using-templates-in-the-urls-of-datasources)
document describes a practical approach to using evaluation steps in Atmos templates to work
with data sources.

:::

---

## EditorConfig Validation

import Terminal from '@site/src/components/Terminal'
import File from '@site/src/components/File'
import Intro from '@site/src/components/Intro'

<Intro>
Atmos supports validation of EditorConfigs to check the formatting of your configuration files. By enforcing the canonical rules specified in your `.editorconfig` file, it helps ensure consistent formatting across your project.
</Intro>

## Example

<Terminal>
```shell
# Validate all files in the current project using EditorConfig
atmos validate editorconfig
```
</Terminal>

### Configuration

To use the `atmos validate editorconfig` command, ensure that your project contains a properly configured `.editorconfig` file at the root level or in relevant directories. This file defines the coding styles for the project, such as indentation, line endings, and character encodings.

<File title=".editorconfig">
```ini
# EditorConfig is awesome: https://editorconfig.org
root = true

[*]
indent_style = space
indent_size = 4
end_of_line = lf
charset = utf-8
trim_trailing_whitespace = true
insert_final_newline = true

[*.md]
trim_trailing_whitespace = false
```
</File>

### Output

The `atmos validate editorconfig` command will provide detailed output indicating whether the files comply with the `.editorconfig` rules or if there are any violations. For example:

<Terminal>
```console
scenarios/complete/modules/label/context.tf:
        267: Wrong amount of left-padding spaces(want multiple of 2)
        268: Wrong amount of left-padding spaces(want multiple of 2)

2 errors found
```
</Terminal>

### Troubleshooting

If validation fails, review your `.editorconfig` file and ensure the rules align with your project's requirements. You can also run the command with verbose output for more details:

<Terminal>
```shell
atmos validate editorconfig --logs-level trace
```
</Terminal>

---

## JSON Schema Validation

import Terminal from '@site/src/components/Terminal'
import File from '@site/src/components/File'
import EmbedFile from '@site/src/components/EmbedFile'
import Intro from '@site/src/components/Intro'

<Intro>
Atmos supports [JSON Schema](https://json-schema.org/) validation, which can validate the schema of configurations such as stacks, workflows, and vendoring manifests.
JSON Schema is an industry standard and provides a vocabulary to annotate and validate JSON documents for correctness.
</Intro>

## Example

<Terminal>
```shell
# Validate 'vpc' component using JSON Schema in the 'plat-ue2-prod' stack
atmos validate component vpc -s plat-ue2-prod --schema-path vpc/validate-vpc-component.json --schema-type jsonschema
```
</Terminal>

### Configure Component Validation

In [`atmos.yaml`](https://github.com/cloudposse/atmos/blob/main/examples/quick-start-advanced/rootfs/usr/local/etc/atmos/atmos.yaml), add the `schemas`
section:

<File title="atmos.yaml">
```yaml
# Validation schemas (for validating atmos stacks and components)
schemas:
  # https://json-schema.org
  jsonschema:
    # Can also be set using `ATMOS_SCHEMAS_JSONSCHEMA_BASE_PATH` ENV var, or `--schemas-jsonschema-dir` command-line arguments
    # Supports both absolute and relative paths
    base_path: "stacks/schemas/jsonschema"
```
</File>

In the component [manifest](https://github.com/cloudposse/atmos/blob/main/examples/quick-start-advanced/stacks/catalog/vpc/defaults.yaml), add
the `settings.validation` section:

<EmbedFile filePath="examples/quick-start-advanced/stacks/catalog/vpc/defaults.yaml"/>

Add the following JSON Schema in the
file [`stacks/schemas/jsonschema/vpc/validate-vpc-component.json`](https://github.com/cloudposse/atmos/blob/main/examples/quick-start-advanced/stacks/schemas/jsonschema/vpc/validate-vpc-component.json):

<EmbedFile filePath="examples/quick-start-advanced/stacks/schemas/jsonschema/vpc/validate-vpc-component.json"/>

---

## Open Policy Agent (OPA) Validation

import Terminal from '@site/src/components/Terminal'
import File from '@site/src/components/File'
import EmbedFile from '@site/src/components/EmbedFile'
import Intro from '@site/src/components/Intro'

<Intro>
The [Open Policy Agent](https://www.openpolicyagent.org/docs/latest/) (OPA) is the open-source industry standard for policy-as-code validation. It provides a general-purpose policy engine to unify policy enforcement across your stacks.
</Intro>

The OPA (pronounced “oh-pa”) language (Rego) is a high-level declarative language for specifying policy as code. Atmos has native support for the OPA decision-making engine to enforce policies across all the components in your stacks (e.g. for microservice configurations).

This is powerful stuff: because you can define many policies, it's possible to apply different policies depending on where a component is defined in the stacks. For example, it could validate differently based on environments or teams.

## Use Cases

Use Open Policy Agent (OPA) policies to validate Atmos stacks and component configurations.

* Validate component config (`vars`, `settings`, `backend`, `env`, `overrides` and other sections) using JSON Schema

* Check if the component config (including relations between different component variables) is correct to allow or deny component provisioning using
  OPA/Rego policies

## Usage

Atmos `validate component` command supports `--schema-path`, `--schema-type` and `--module-paths` command line arguments.
If the arguments are not provided, Atmos will try to find and use the `settings.validation` section defined in the component's YAML config.

:::tip

Refer to [atmos validate component](/cli/commands/validate/component) CLI command for more information

:::

<Terminal>
```shell

# Validate 'vpc' component using OPA policy in the 'plat-ue2-prod' stack
atmos validate component vpc -s plat-ue2-prod --schema-path vpc/validate-vpc-component.rego --schema-type opa

# Validate 'vpc' component using OPA policy in the 'plat-ue2-dev' stack with additional module paths 'catalog/constants'
atmos validate component vpc -s plat-ue2-dev --schema-path vpc/validate-vpc-component.rego --schema-type opa --module-paths catalog/constants

# Validate 'vpc' component using OPA policy in the 'plat-ue2-dev' stack with additional module paths 'catalog'
atmos validate component vpc -s plat-ue2-dev --schema-path vpc/validate-vpc-component.rego --schema-type opa --module-paths catalog

# Validate 'vpc' component in the 'plat-ue2-prod' stack
atmos validate component vpc -s plat-ue2-prod

# Validate 'vpc' component in the 'plat-ue2-dev' stack
atmos validate component vpc -s plat-ue2-dev

# Validate 'vpc' component in the 'plat-ue2-dev' stack with a timeout of 15 seconds
atmos validate component vpc -s plat-ue2-dev --timeout 15
```
</Terminal>

### Configure Component Validation

In [`atmos.yaml`](https://github.com/cloudposse/atmos/blob/main/examples/quick-start-advanced/rootfs/usr/local/etc/atmos/atmos.yaml), add the `schemas`
section:

<File title="atmos.yaml">
```yaml
# Validation schemas for OPA for validating atmos stacks and components
schemas:
  # https://www.openpolicyagent.org
  opa:
    # Can also be set using `ATMOS_SCHEMAS_OPA_BASE_PATH` ENV var, or `--schemas-opa-dir` command-line arguments
    # Supports both absolute and relative paths
    base_path: "stacks/schemas/opa"
```
</File>

In the component [manifest](https://github.com/cloudposse/atmos/blob/main/examples/quick-start-advanced/stacks/catalog/vpc/defaults.yaml), add
the `settings.validation` section:

<EmbedFile filePath="examples/quick-start-advanced/stacks/catalog/vpc/defaults.yaml" />

Add the following Rego package in the file [`stacks/schemas/opa/catalog/constants/constants.rego`](https://github.com/cloudposse/atmos/blob/main/examples/quick-start-advanced/stacks/schemas/opa/catalog/constants/constants.rego):

<EmbedFile filePath="examples/quick-start-advanced/stacks/schemas/opa/catalog/constants/constants.rego" />

Add the following OPA policy in the file [`stacks/schemas/opa/vpc/validate-vpc-component.rego`](https://github.com/cloudposse/atmos/blob/main/examples/quick-start-advanced/stacks/schemas/opa/vpc/validate-vpc-component.rego):

<EmbedFile filePath="examples/quick-start-advanced/stacks/schemas/opa/vpc/validate-vpc-component.rego" />

### Use One Policy File or Many

Atmos supports OPA policies for components validation in a single Rego file and in multiple Rego files.

As shown in the example above, you can define some Rego constants, modules and helper functions in a separate
file `stacks/schemas/opa/catalog/constants/constants.rego`, and then import them into the main policy
file `stacks/schemas/opa/vpc/validate-vpc-component.rego`.

You also need to specify the `module_paths` attribute in the component's `settings.validation` section.
The `module_paths` attribute is an array of filesystem paths (folders or individual files) to the additional modules for schema validation.
Each path can be an absolute path or a path relative to `schemas.opa.base_path` defined in `atmos.yaml`.
If a folder is specified in `module_paths`, Atmos will recursively process the folder and all its sub-folders and load all Rego files into the OPA
engine.

This allows you to separate the common OPA modules, constants and helper functions into a catalog of reusable Rego modules,
and to structure your OPA policies to make them DRY.

## Examples

### Validate VPC Component in Stacks

Run the following commands to validate the component in the stacks:

<Terminal title="atmos validate component vpc -s plat-ue2-prod">
```console
Mapping public IPs on launch is not allowed in 'prod'. Set 'map_public_ip_on_launch' variable to 'false'

exit status 1
```
</Terminal>

<Terminal title="atmos validate component vpc -s plat-ue2-dev">
```console
In 'dev', only 2 Availability Zones are allowed
VPC name must be a valid string from 2 to 20 alphanumeric chars

exit status 1
```
</Terminal>

### Validate Before Provisioning

Try to run the following commands to provision the component in the stacks:

<Terminal>
```bash
atmos terraform apply vpc -s plat-ue2-prod
atmos terraform apply vpc -s plat-ue2-dev
```
</Terminal>

Since the OPA validation policies don't pass, Atmos does not allow provisioning the component in the stacks:

<Terminal title="atmos validate vpc --stack=plat-ue2-prod">
![atmos-validate-vpc-in-plat-ue2-prod](/img/atmos-validate-infra-vpc-in-tenant1-ue2-dev.png)
</Terminal>

<Terminal title="atmos validate vpc --stack=plat-ue2-dev">
![atmos-validate-vpc-in-plat-ue2-dev](/img/atmos-validate-infra-vpc-in-tenant1-ue2-dev.png)
</Terminal>

### Advanced Policy Examples

<EmbedFile filePath="examples/quick-start-advanced/stacks/schemas/opa/vpc/validate-vpc-component.rego" />

:::note

- If a regex pattern in the 're_match' function contains a backslash to escape special chars (e.g. '\.' or '\-'),
  it must be escaped with another backslash when represented as a regular Go string ('\\.', '\\-').

- The reason is that backslash is also used to escape special characters in Go strings like newline (\n).

- If you want to match the backslash character itself, you'll need four slashes.

:::

## Policy Execution Context

Atmos allows enforcing custom governance rules based on metadata about Atmos commands and provides a powerful
policy evaluation mechanism by passing structured metadata to OPA policies at runtime.

This metadata enables fine-grained control over when certain actions (like `terraform apply`) are allowed or denied,
based on the context in which they're executed.

### Policy Metadata

When Atmos runs a command, it supplies an input object to OPA policies that contains detailed contextual information, such as:

- `process_env`: a map of the environment variables in the current process
- `cli_args`: a list of the command line arguments and flags (e.g., executing the `atmos terraform apply` command will generate the `["terraform", "apply"]` list)
- `tf_cli_vars`: a map of variables with proper type conversion from the command-line `-var` arguments
- `env_tf_cli_args`: a list of arguments from the [`TF_CLI_ARGS`](https://developer.hashicorp.com/terraform/cli/config/environment-variables#tf_cli_args-and-tf_cli_args_name) environment variable
- `env_tf_cli_vars`: a map of variables with proper type conversion from the [`TF_CLI_ARGS`](https://developer.hashicorp.com/terraform/cli/config/environment-variables#tf_cli_args-and-tf_cli_args_name) environment variable
- `vars`: a map of variables passed to the command, either via the stack config files or [CLI flags](/core-concepts/validate/terraform-variables)
- other contextual attributes that are returned from the [`atmos describe component`](/cli/commands/describe/component) command for a component in a stack

### Policy Execution Context Example

Below is an OPA policy rule to enforce infrastructure governance during command execution.
Specifically, this rule blocks the execution of `atmos terraform apply` if the variable `foo` is set to the string `"foo"`.

<File title="validate-component.rego">
```rego
# 'package atmos' is required in all Atmos OPA policies
package atmos

# Atmos looks for the 'errors' (array of strings) output from all OPA policies
# If the 'errors' output contains one or more error messages, Atmos considers the policy failed

# Don't allow `terraform apply` if the `foo` variable is set to `foo`
# The `input` map contains the `cli_args` attribute (a list of the command line arguments and flags)
errors[message] {
  count(input.cli_args) >= 2
  input.cli_args[0] == "terraform"
  input.cli_args[1] == "apply"
  input.vars.foo == "foo"
  message = "the component can't be applied if the 'foo' variable is set to 'foo'"
}
```
</File>

The rule checks if:
 - The `cli_args` list has at least two items
 - The command (first item in the `cli_args` list) is `terraform`
 - The subcommand (second item in the `cli_args` list) is `apply`
 - The variable `foo` is set to `"foo"`

If all conditions are true, the rule generates an error message.

The generated error message is added to the `errors` array.
Atmos interprets the presence of any messages in `errors` as a policy violation and blocks the operation with the
following error:

<Terminal title="atmos terraform apply component-1 -s nonprod">
```console
the component can't be applied if the 'foo' variable is set to 'foo'

exit status 1
```
</Terminal>

### Environment and Process Context Examples

The following examples demonstrate how to use the process environment and Terraform CLI context in OPA policies for advanced governance scenarios.

#### Process Environment Variables (`process_env`)

Access environment variables from the current process to enforce security and compliance policies.

<File title="validate-environment.rego">
```rego
package atmos

# Block operations if running in production without proper approval
errors[message] {
  input.process_env.ENVIRONMENT == "production"
  not input.process_env.DEPLOYMENT_APPROVED
  message = "Production deployments require DEPLOYMENT_APPROVED environment variable"
}

# Ensure required environment variables are set
errors[message] {
  required_vars := ["AWS_REGION", "AWS_PROFILE"]
  missing_var := required_vars[_]
  not input.process_env[missing_var]
  message = sprintf("Required environment variable '%s' is not set", [missing_var])
}

# Validate AWS region restrictions
errors[message] {
  input.process_env.AWS_REGION
  not input.process_env.AWS_REGION in ["us-east-1", "us-west-2", "eu-west-1"]
  message = sprintf("AWS region '%s' is not allowed. Use: us-east-1, us-west-2, or eu-west-1", [input.process_env.AWS_REGION])
}
```
</File>

#### Terraform CLI Variables (`tf_cli_vars`)

Validate variables passed via command-line `-var` arguments with proper type handling and JSON parsing.

<File title="validate-cli-vars.rego">
```rego
package atmos

# Validate instance types passed via CLI
errors[message] {
  input.tf_cli_vars.instance_type
  not input.tf_cli_vars.instance_type in ["t3.micro", "t3.small", "t3.medium"]
  message = sprintf("Instance type '%s' not allowed via CLI. Use t3.micro, t3.small, or t3.medium", [input.tf_cli_vars.instance_type])
}

# Validate JSON configuration passed via CLI
errors[message] {
  input.tf_cli_vars.config
  is_object(input.tf_cli_vars.config)
  input.tf_cli_vars.config.encryption_enabled != true
  message = "Configuration passed via CLI must have encryption_enabled set to true"
}

# Ensure sensitive variables are not passed via CLI
errors[message] {
  sensitive_vars := ["password", "secret", "api_key", "token"]
  cli_var := sensitive_vars[_]
  input.tf_cli_vars[cli_var]
  message = sprintf("Sensitive variable '%s' should not be passed via command line", [cli_var])
}

# Validate numeric ranges for CLI variables
errors[message] {
  input.tf_cli_vars.max_instances
  is_number(input.tf_cli_vars.max_instances)
  input.tf_cli_vars.max_instances > 10
  message = sprintf("max_instances cannot exceed 10, got %d", [input.tf_cli_vars.max_instances])
}
```
</File>

#### TF_CLI_ARGS Environment (`env_tf_cli_args`)

Parse and validate arguments from the `TF_CLI_ARGS` environment variable.

<File title="validate-tf-cli-args.rego">
```rego
package atmos

# Block dangerous flags in TF_CLI_ARGS
errors[message] {
  dangerous_flags := ["-auto-approve", "-force", "-lock=false"]
  flag := dangerous_flags[_]
  flag in input.env_tf_cli_args
  input.process_env.ENVIRONMENT == "production"
  message = sprintf("Flag '%s' is not allowed in production via TF_CLI_ARGS", [flag])
}

# Require planfile for apply (positional, not a flag)
errors[message] {
  some i
  input.cli_args[i] == "apply"
  # next token exists and is not a flag -> planfile path
  i+1 < count(input.cli_args)
  not startswith(input.cli_args[i+1], "-")
  # Optionally, enforce a prefix/dir policy for plan files
  not allowed_planfile(input.cli_args[i+1])
  message = "Apply must use an approved plan file generated by 'terraform plan -out=...'"
}

allowed_planfile(p) {
  startswith(p, "plans/")
}

# Validate parallelism settings
errors[message] {
  some i
  # equals form: -parallelism=50
  startswith(input.env_tf_cli_args[i], "-parallelism=")
  parallelism := to_number(replace(input.env_tf_cli_args[i], "-parallelism=", ""))
  parallelism > 20
  message = sprintf("Parallelism cannot exceed 20, got %d", [parallelism])
}

errors[message] {
  some i
  # space form: -parallelism 50
  input.env_tf_cli_args[i] == "-parallelism"
  i + 1 < count(input.env_tf_cli_args)
  parallelism := to_number(input.env_tf_cli_args[i+1])
  parallelism > 20
  message = sprintf("Parallelism cannot exceed 20, got %d", [parallelism])
}
```
</File>

#### TF_CLI_ARGS Variables (`env_tf_cli_vars`)

Access and validate variables extracted from `TF_CLI_ARGS` with JSON type conversion.

<File title="validate-env-cli-vars.rego">
```rego
package atmos

# Validate environment-specific constraints
errors[message] {
  input.env_tf_cli_vars.environment == "production"
  input.env_tf_cli_vars.instance_count
  is_number(input.env_tf_cli_vars.instance_count)
  input.env_tf_cli_vars.instance_count < 2
  message = "Production environment requires at least 2 instances"
}

# Validate complex JSON configurations from TF_CLI_ARGS
errors[message] {
  input.env_tf_cli_vars.networking_config
  is_object(input.env_tf_cli_vars.networking_config)
  not input.env_tf_cli_vars.networking_config.vpc_id
  message = "Networking configuration must include vpc_id"
}

# Cross-validate CLI args and environment variables
errors[message] {
  input.env_tf_cli_vars.region
  input.process_env.AWS_REGION
  input.env_tf_cli_vars.region != input.process_env.AWS_REGION
  message = sprintf("Region mismatch: TF_CLI_ARGS region '%s' != AWS_REGION '%s'", [
    input.env_tf_cli_vars.region,
    input.process_env.AWS_REGION
  ])
}

# Validate resource naming conventions from environment variables
errors[message] {
  input.env_tf_cli_vars.resource_name
  not regex.match("^[a-z][a-z0-9-]*[a-z0-9]$", input.env_tf_cli_vars.resource_name)
  message = sprintf("Resource name '%s' must be lowercase alphanumeric with hyphens", [input.env_tf_cli_vars.resource_name])
}

# Ensure cost controls are in place
errors[message] {
  input.env_tf_cli_vars.instance_type
  expensive_types := ["m5.large", "m5.xlarge", "c5.large", "c5.xlarge"]
  input.env_tf_cli_vars.instance_type in expensive_types
  not input.env_tf_cli_vars.cost_center
  message = sprintf("Expensive instance type '%s' requires cost_center to be specified", [input.env_tf_cli_vars.instance_type])
}
```
</File>

### Combined Context Validation

Leverage multiple context sources for comprehensive governance policies.

<File title="validate-comprehensive.rego">
```rego
package atmos

# Comprehensive validation combining all context sources
errors[message] {
  # Check if this is a production apply operation
  "apply" in input.cli_args
  (input.process_env.ENVIRONMENT == "production" or
   input.vars.environment == "production" or
   input.tf_cli_vars.environment == "production" or
   input.env_tf_cli_vars.environment == "production")

  # Ensure proper approval workflow
  not production_approved
  message = "Production deployments require proper approval workflow"
}

# Helper rule for production approval
production_approved {
  input.process_env.DEPLOYMENT_APPROVED == "true"
  input.process_env.APPROVED_BY
  input.process_env.APPROVAL_TICKET
}

# Validate consistency across all variable sources
errors[message] {
  sources := [
    object.get(input.vars, "environment", null),
    object.get(input.tf_cli_vars, "environment", null),
    object.get(input.env_tf_cli_vars, "environment", null),
    object.get(input.process_env, "ATMOS_ENVIRONMENT", null)
  ]

  # Remove null/undefined values
  defined_envs := [env | env := sources[_]; env != null; env != ""]

  # Check if all defined environments match
  count(defined_envs) > 1
  not all_equal(defined_envs)
  message = sprintf("Environment mismatch across sources: %v", [defined_envs])
}

# Helper function to check if all elements in array are equal
all_equal(arr) {
  count(arr) <= 1
}

all_equal(arr) {
  count(arr) > 1
  first := arr[0]
  all_match := [x | x := arr[_]; x == first]
  count(all_match) == count(arr)
}

# Validate resource limits based on environment context
errors[message] {
  environment := get_environment
  environment == "development"
  total_instances := get_total_instances
  total_instances > 5
  message = sprintf("Development environment limited to 5 instances, requested %d", [total_instances])
}

# Helper to get environment from any source
get_environment := env {
  env := input.vars.environment
  env != null
  env != ""
}

get_environment := env {
  env := input.tf_cli_vars.environment
  env != null
  env != ""
}

get_environment := env {
  env := input.env_tf_cli_vars.environment
  env != null
  env != ""
}

get_environment := env {
  env := input.process_env.ATMOS_ENVIRONMENT
  env != null
  env != ""
}

# Helper to calculate total instances from all sources
get_total_instances := total {
  instance_counts := [
    object.get(input.vars, "instance_count", null),
    object.get(input.tf_cli_vars, "instance_count", null),
    object.get(input.env_tf_cli_vars, "instance_count", null)
  ]
  valid_counts := [n | n := instance_counts[_]; is_number(n)]
  total := sum(valid_counts)
}
```
</File>

### Best Practices for Context-Aware Policies

1. **Environment Consistency**: Always validate that environment settings are consistent across all input sources
2. **Security First**: Use `process_env` to enforce security requirements like required credentials and approval workflows
3. **Type Safety**: Leverage Rego's type checking functions (`is_number`, `is_object`, etc.) when working with parsed JSON from CLI variables
4. **Graceful Handling**: Check for null/undefined values before processing to avoid policy evaluation errors
5. **Clear Messages**: Provide specific error messages that indicate which context source triggered the violation
6. **Separation of Concerns**: Create focused policies for different aspects (security, compliance, cost control) rather than monolithic rules

---

## Terraform Input Variables Validation

import Terminal from '@site/src/components/Terminal'
import File from '@site/src/components/File'
import EmbedFile from '@site/src/components/EmbedFile'
import Intro from '@site/src/components/Intro'

<Intro>
Use [Open Policy Agent](https://www.openpolicyagent.org/docs/latest/) (OPA) policies to validate Terraform input variables.
</Intro>

## Introduction

When executing `atmos terraform <sub-command>` commands, you can provide
[Terraform input variables](https://developer.hashicorp.com/terraform/language/values/variables) on the command line
using the `-var` flag. These variables will override the variables configured in Atmos stack manifests.

For example:

<Terminal>
```shell
atmos terraform apply <component> -s <stack> -- -var name=api

atmos terraform apply <component> -s <stack> -- -var name=api -var 'tags={"Team":"api", "Group":"web"}'
```
</Terminal>

:::tip
Use double-dash `--` to signify the end of the options for Atmos and the start
of the additional native arguments and flags for the Terraform commands.

Refer to [Terraform CLI commands usage](/cli/commands/terraform/usage) for more details.

:::

:::info
Terraform processes variables in the following order of precedence (from highest to lowest):

- Explicit `-var` flags: these variables have the highest priority and will override any other variable values, including those specified in `--var-file`.

- Variables in `--var-file`: values in a variable file override default values set in the Terraform configuration.
  Atmos generates varfiles from stack configurations and provides it to Terraform using the `--var-file` flag.

- Environment variables: variables set as environment variables using the `TF_VAR_` prefix.

- Default values in the Terraform configuration files: these have the lowest priority.
:::

When log level `Trace` is used, Atmos prints the Terraform variables specified on the command line in the "CLI variables" output.
For example:

<Terminal>
```console
ATMOS_LOGS_LEVEL=Trace /
atmos terraform apply my-component -s plat-ue2-dev -- -var name=api -var 'tags={"Team":"api", "Group":"web"}'

Variables for the component 'my-component' in the stack 'plat-ue2-dev':
environment: ue2
namespace: cp
region: us-east-2
stage: dev
tenant: plat

Writing the variables to file:
components/terraform/my-component/plat-ue2-dev-my-component.terraform.tfvars.json

CLI variables (will override the variables defined in the stack manifests):
name: api
tags:
    Team: api
    Group: web
```
</Terminal>

Atmos exposes the Terraform variables passed on the command line in the `tf_cli_vars` section, and also provides access to
the variables from the [`TF_CLI_ARGS`](https://developer.hashicorp.com/terraform/cli/config/environment-variables#tf_cli_args-and-tf_cli_args_name)
environment variable in the `env_tf_cli_vars` section. Both can be used in OPA policies for validation.

## Terraform Variables Validation using OPA Policies

In `atmos.yaml`, configure the `schemas.opa` section:

<File title="atmos.yaml">
```yaml
# Validation schemas
schemas:
  # https://www.openpolicyagent.org
  opa:
    # Can also be set using `ATMOS_SCHEMAS_OPA_BASE_PATH` ENV var, or `--schemas-opa-dir` command-line arguments
    # Supports both absolute and relative paths
    base_path: "stacks/schemas/opa"
```
</File>

In the component manifest, add the `settings.validation` section to point to the OPA policy file:

<File title="stack.yaml">
```yaml
components:
  terraform:
    my-component:
      settings:
        # All validation steps must succeed to allow the component to be provisioned
        validation:
          check-template-functions-test-component-with-opa-policy:
            schema_type: opa
            # 'schema_path' can be an absolute path or a path relative to 'schemas.opa.base_path' defined in `atmos.yaml`
            schema_path: "my-component/validate-my-component.rego"
            description: Check 'my-component' component using OPA policy
            # Validation timeout in seconds
            timeout: 5
```
</File>

### Require a Terraform variable to be specified on the command line

If you need to enforce that a Terraform variable must be specified on the command line (and not in Atmos stack manifests),
add the following OPA policy in the file `stacks/schemas/opa/my-component/validate-my-component.rego`

<File title="stacks/schemas/opa/my-component/validate-my-component.rego">
```rego
# 'package atmos' is required in all `atmos` OPA policies
package atmos

# Atmos looks for the 'errors' (array of strings) output from all OPA policies.
# If the 'errors' output contains one or more error messages, Atmos considers the policy failed.

errors["for the 'my-component' component, the variable 'name' must be provided on the command line using the '-var' flag"] {
    not input.tf_cli_vars.name
}
```
</File>

When executing the following command (and not passing the `name` variable on the command line), Atmos will validate
the component using the OPA policy, which will fail and prevent the component from being provisioned:

<Terminal>
```console
atmos terraform apply my-component -s plat-ue2-dev

Validating the component 'my-component' using OPA file 'my-component/validate-my-component.rego'

for the 'my-component' component, the variable 'name' must be provided on the command line using the '-var' flag
```
</Terminal>

On the other hand, when passing the `name` variable on the command line using the `-var name=api` flag, the command will succeed:

<Terminal>
```shell
atmos terraform apply my-component -s plat-ue2-dev -- -var name=api
```
</Terminal>

### Restrict a Terraform variable from being provided on the command line

If you need to prevent a Terraform variable from being passed (and overridden) on the command line,
add the following OPA policy in the file `stacks/schemas/opa/my-component/validate-my-component.rego`

<File title="stacks/schemas/opa/my-component/validate-my-component.rego">
```rego
package atmos

errors["for the 'my-component' component, the variable 'name' cannot be overridden on the command line using the '-var' flag"] {
    input.tf_cli_vars.name
}
```
</File>

When executing the following command, Atmos will validate the component using the OPA policy, which will fail and prevent
the component from being provisioned:

<Terminal>
```console
atmos terraform apply my-component -s plat-ue2-dev -- -var name=api

Validating the component 'my-component' using OPA file 'my-component/validate-my-component.rego'

for the 'my-component' component, the variable 'name' cannot be overridden on the command line using the '-var' flag
```
</Terminal>

This command will pass the validation and succeed:

<Terminal>
```shell
atmos terraform apply my-component -s plat-ue2-dev
```
</Terminal>

## Environment Variables Validation using OPA Policies

In addition to `tf_cli_vars` (which contains variables passed via `-var` flags on the command line),
Atmos also provides access to the variables through the `env_tf_cli_vars` section passed via the `TF_CLI_ARGS` environment variable.

### Require a variable to be set via the `TF_CLI_ARGS` environment variable

If you need to enforce that a specific Terraform variable must be set, add the following OPA policy:

<File title="stacks/schemas/opa/my-component/validate-my-component.rego">
```rego
package atmos

errors["for the 'my-component' component, 'environment' must be set in the 'TF_CLI_ARGS' environment variable"] {
    not input.env_tf_cli_vars.environment
}
```
</File>

This policy will fail if the `'environment'` variable is not set in the 'TF_CLI_ARGS' environment variable.

<Terminal>
```console
# This will fail validation
atmos terraform apply my-component -s plat-ue2-dev

# This will pass validation
TF_CLI_ARGS="-var environment=production" atmos terraform apply my-component -s plat-ue2-dev
```
</Terminal>

### Validate environment variable values

You can also validate the actual values of variables passed via `TF_CLI_ARGS`. For example, to ensure that the `environment` variable is set to one of the allowed values:

<File title="stacks/schemas/opa/my-component/validate-my-component.rego">
```rego
package atmos

# Define allowed environment values
allowed_environments := ["development", "staging", "production"]

errors["for the 'my-component' component, 'environment' variable in TF_CLI_ARGS must be one of: development, staging, production"] {
    input.env_tf_cli_vars.environment
    not input.env_tf_cli_vars.environment in allowed_environments
}
```
</File>

### Combine command-line and environment variable validation

You can create policies that validate both command-line variables (`tf_cli_vars`) and environment variables (`env_tf_cli_vars`) together:

<File title="stacks/schemas/opa/my-component/validate-my-component.rego">
```rego
package atmos

# Ensure that if a variable is set via TF_CLI_ARGS, it cannot be overridden via command line
errors["for the 'my-component' component, when 'environment' is set in TF_CLI_ARGS, it cannot be overridden with -var"] {
    input.env_tf_cli_vars.environment
    input.tf_cli_vars.environment
}

# Require either TF_CLI_ARGS variable OR command-line variable, but not both
errors["for the 'my-component' component, 'environment' must be specified either via TF_CLI_ARGS or -var, but not both"] {
    input.env_tf_cli_vars.environment
    input.tf_cli_vars.environment
}

# Require at least one method of setting the environment
errors["for the 'my-component' component, 'environment' must be specified via either TF_CLI_ARGS or -var flag"] {
    not input.env_tf_cli_vars.environment
    not input.tf_cli_vars.environment
}
```
</File>

### Complex validation with type checking

Variables passed via `TF_CLI_ARGS` are automatically parsed and converted to their appropriate types when possible, so you can validate their format and values:

<File title="stacks/schemas/opa/my-component/validate-my-component.rego">
```rego
package atmos

import rego.v1

# Validate that instance_count is a valid number
errors["for the 'my-component' component, 'instance_count' in TF_CLI_ARGS must be a valid positive integer"] {
    input.env_tf_cli_vars.instance_count
    input.env_tf_cli_vars.instance_count <= 0
}

# Validate that tags is a valid object
errors["for the 'my-component' component, 'tags' in TF_CLI_ARGS must be a valid object"] {
    input.env_tf_cli_vars.tags
    not is_object(input.env_tf_cli_vars.tags)
}

# Validate specific object structure
errors["for the 'my-component' component, 'tags' in TF_CLI_ARGS must contain 'Environment' key"] {
    input.env_tf_cli_vars.tags
    is_object(input.env_tf_cli_vars.tags)
    not input.env_tf_cli_vars.tags.Environment
}

errors["for the 'my-component' component, 'tags' in TF_CLI_ARGS must contain 'Team' key"] {
    input.env_tf_cli_vars.tags
    is_object(input.env_tf_cli_vars.tags)
    not input.env_tf_cli_vars.tags.Team
}
```
</File>

:::tip
Variables in `env_tf_cli_vars` are automatically parsed and converted to their appropriate types when possible. For example:
- `TF_CLI_ARGS="-var count=5"` becomes `input.env_tf_cli_vars.count` with integer value `5`
- `TF_CLI_ARGS="-var enabled=true"` becomes `input.env_tf_cli_vars.enabled` with boolean value `true`
- `TF_CLI_ARGS='-var tags={"env":"prod"}'` becomes `input.env_tf_cli_vars.tags` with object value `{"env":"prod"}`

This makes it easier to write OPA policies that work with the actual data types rather than just strings.
:::

:::info
The `env_tf_cli_vars` section provides a way to validate and control variables passed via the `TF_CLI_ARGS` environment variable, complementing the `tf_cli_vars` section which handles command-line variables.
Together, they give you complete control over how variables are passed to Terraform.
:::

---

## Validating Stack Configurations

import Terminal from '@site/src/components/Terminal'
import File from '@site/src/components/File'
import Intro from '@site/src/components/Intro'

<Intro>
Validation is essential for ensuring clean and correct configurations, especially in environments where multiple teams contribute
to the development and deployment processes.
Atmos enhances this validation process in three significant ways with [JSON Schema](https://json-schema.org/), [OPA](https://www.openpolicyagent.org/) policies, and the [EditorConfig Checker](https://github.com/editorconfig-checker/editorconfig-checker).
</Intro>

## Types of Validation

Atmos supports three types of native validation.

### JSON Schema

Atmos supports [JSON Schema](https://json-schema.org/) validation, which can validate the schema of configurations such as stacks, workflows, and vendoring manifests.
JSON Schema is an industry standard and provides a vocabulary to annotate and validate JSON documents for correctness.

### Open Policy Agent (OPA)

The [Open Policy Agent](https://www.openpolicyagent.org/docs/latest/) (OPA, pronounced “oh-pa”) is another open-source industry standard that provides
a general-purpose policy engine to unify policy enforcement across your stacks.
The OPA language (Rego) is a high-level declarative language for specifying policy as code.
Atmos has native support for the OPA decision-making engine to enforce policies across all the components in your stacks (e.g., for microservice configurations).

This is powerful stuff: because you can define many policies, it's possible to validate components differently for different environments or teams.

### EditorConfig Checker

The [EditorConfig Checker](https://github.com/editorconfig-checker/editorconfig-checker) is a tool that ensures adherence to the rules defined in your `.editorconfig` file. This ensures consistency in coding styles across teams, which is particularly important in collaborative environments. Atmos supports running the EditorConfig Checker to validate the configurations in your project.

## Validate Your Configurations

### Validate Components

To validate an Atmos component in a stack, execute the `validate component` command:

```shell
atmos validate component <component> --stack <stack>
```

:::tip

Refer to [atmos validate component](/cli/commands/validate/component) CLI command for more information on how to validate Atmos components

:::

### Check Your Stacks

To validate all Stack configurations and YAML syntax, execute the `validate stacks` command:

```shell
atmos validate stacks
```

The command checks and validates the following:

- All YAML manifest files for YAML errors and inconsistencies

- All imports: if they are configured correctly, have valid data types, and point to existing manifest files

- Schema: if all sections in all YAML manifest files are correctly configured and have valid data types

- Misconfiguration and duplication of components in stacks. If the same Atmos component in the same Atmos stack is
  defined in more than one stack manifest file, and the component configurations are different, an error message will
  be displayed similar to the following:

  <Terminal title="atmos validate stacks">
    ```console
    The Atmos component 'vpc' in the stack 'plat-ue2-dev' is defined in more than one
    top-level stack manifest file: orgs/acme/plat/dev/us-east-2-extras, orgs/acme/plat/dev/us-east-2.

    The component configurations in the stack manifest are different.

    To check and compare the component configurations in the stack manifests, run the following commands:
    - atmos describe component vpc -s orgs/acme/plat/dev/us-east-2-extras
    - atmos describe component vpc -s orgs/acme/plat/dev/us-east-2

    You can use the '--file' flag to write the results of the above commands to files
    (refer to https://atmos.tools/cli/commands/describe/component).

    You can then use the Linux 'diff' command to compare the files line by line and show the differences
    (refer to https://man7.org/linux/man-pages/man1/diff.1.html)

    When searching for the component 'vpc' in the stack 'plat-ue2-dev', Atmos can't decide which
    stack manifest file to use to get the configuration for the component. This is a stack misconfiguration.

    Consider the following solutions to fix the issue:

    - Ensure that the same instance of the Atmos 'vpc' component in the stack 'plat-ue2-dev'
      is only defined once (in one YAML stack manifest file)

    - When defining multiple instances of the same component in the stack,
      ensure each has a unique name

    - Use multiple-inheritance to combine multiple configurations together
      (refer to https://atmos.tools/core-concepts/stacks/inheritance)
    ```
  </Terminal>

## Validate Atmos Manifests using JSON Schema

Atmos uses the [Atmos Manifest JSON Schema](pathname:///schemas/atmos/atmos-manifest/1.0/atmos-manifest.json) to validate Atmos manifests, and has a default (embedded) JSON Schema.

If you don't configure the path to a JSON Schema in `atmos.yaml` and don't provide it on the command line using the `--schemas-atmos-manifest` flag,
the default (embedded) JSON Schema will be used when executing the command `atmos validate stacks`.

To override the default behavior, configure JSON Schema in `atmos.yaml`:

- Add the [Atmos Manifest JSON Schema](pathname:///schemas/atmos/atmos-manifest/1.0/atmos-manifest.json) to your repository, for example
in  [`stacks/schemas/atmos/atmos-manifest/1.0/atmos-manifest.json`](https://github.com/cloudposse/atmos/blob/main/examples/quick-start-advanced/stacks/schemas/atmos/atmos-manifest/1.0/atmos-manifest.json)

- Configure the following section in the `atmos.yaml` [CLI config file](/cli/configuration)

```yaml title="atmos.yaml"
# Validation schemas (for validating atmos stacks and components)
schemas:
  # JSON Schema to validate Atmos manifests
  atmos:
    # Can also be set using 'ATMOS_SCHEMAS_ATMOS_MANIFEST' ENV var, or '--schemas-atmos-manifest' command-line arguments
    # Supports both absolute and relative paths (relative to the `base_path` setting in `atmos.yaml`)
    manifest: "stacks/schemas/atmos/atmos-manifest/1.0/atmos-manifest.json"
    # Also supports URLs
    # manifest: "https://atmos.tools/schemas/atmos/atmos-manifest/1.0/atmos-manifest.json"
```

- Instead of configuring the `schemas.atmos.manifest` section in `atmos.yaml`, you can provide the path to
the [Atmos Manifest JSON Schema](pathname:///schemas/atmos/atmos-manifest/1.0/atmos-manifest.json) file by using the ENV variable `ATMOS_SCHEMAS_ATMOS_MANIFEST`
or the `--schemas-atmos-manifest` command line flag:

```shell
ATMOS_SCHEMAS_ATMOS_MANIFEST=stacks/schemas/atmos/atmos-manifest/1.0/atmos-manifest.json atmos validate stacks
atmos validate stacks --schemas-atmos-manifest stacks/schemas/atmos/atmos-manifest/1.0/atmos-manifest.json
atmos validate stacks --schemas-atmos-manifest https://atmos.tools/schemas/atmos/atmos-manifest/1.0/atmos-manifest.json
```

:::tip
For more details, refer to [`atmos validate stacks`](/cli/commands/validate/stacks) CLI command
:::

---

## Component Manifest

import File from '@site/src/components/File'
import Terminal from '@site/src/components/Terminal'
import Intro from '@site/src/components/Intro'

<Intro>
Atmos natively supports the concept of "vendoring" individual components by defining a `component.yaml` inside of the component directory, which is making a copy of 3rd-party components or other dependencies in your own repo.
</Intro>

## Examples

### Vendoring using `component.yaml` manifest

After defining the `component.yaml` vendoring manifest, the remote component can be downloaded by running the following command:

```shell
atmos vendor pull -c components/terraform/vpc
```

:::tip
Refer to [`atmos vendor pull`](/cli/commands/vendor/pull) CLI command for more details
:::

### Vendoring Components from a Monorepo

To vendor a component, create a `component.yaml` file stored inside the `components/_type_/_name_/` folder (e.g. `components/terraform/vpc/`).

The schema of a `component.yaml` file is as follows:

```yaml
apiVersion: atmos/v1
kind: ComponentVendorConfig
metadata:
  name: vpc-flow-logs-bucket-vendor-config
  description: Source and mixins config for vendoring of 'vpc-flow-logs-bucket' component
spec:
  source:
    # Source 'uri' supports the following protocols: OCI (https://opencontainers.org), Git, Mercurial, HTTP, HTTPS, Amazon S3, Google GCP,
    # and all URL and archive formats as described in https://github.com/hashicorp/go-getter
    # In 'uri', Golang templates are supported  https://pkg.go.dev/text/template
    # If 'version' is provided, '{{.Version}}' will be replaced with the 'version' value before pulling the files from 'uri'
    # To vendor a module from a Git repo, use the following format: 'github.com/cloudposse/terraform-aws-ec2-instance.git///?ref={{.Version}}
    uri: github.com/cloudposse/terraform-aws-components.git//modules/vpc-flow-logs-bucket?ref={{.Version}}
    version: 1.398.0

    # Only include the files that match the 'included_paths' patterns
    # If 'included_paths' is not specified, all files will be matched except those that match the patterns from 'excluded_paths'
    # 'included_paths' support POSIX-style Globs for file names/paths (double-star/globstar `**` is supported)
    # https://en.wikipedia.org/wiki/Glob_(programming)
    # https://github.com/bmatcuk/doublestar#patterns
    included_paths:
      - "**/*.tf"
      - "**/*.tfvars"
      - "**/*.md"

    # Exclude the files that match any of the 'excluded_paths' patterns
    # Note that we are excluding 'context.tf' since a newer version of it will be downloaded using 'mixins'
    # 'excluded_paths' support POSIX-style Globs for file names/paths (double-star/globstar `**` is supported)
    excluded_paths:
      - "**/context.tf"

  # Mixins override files from 'source' with the same 'filename' (e.g. 'context.tf' will override 'context.tf' from the 'source')
  # All mixins are processed in the order they are declared in the list.
  mixins:
    # https://github.com/hashicorp/go-getter/issues/98
    - uri: https://raw.githubusercontent.com/cloudposse/terraform-null-label/0.25.0/exports/context.tf
      filename: context.tf
    - uri: https://raw.githubusercontent.com/cloudposse/terraform-aws-components/{{.Version}}/modules/datadog-agent/introspection.mixin.tf
      version: 1.398.0
      filename: introspection.mixin.tf
```

:::warning

The `glob` library that Atmos uses to download remote artifacts does not treat the double-star `**` as including sub-folders.
If the component's folder has sub-folders, and you need to vendor them, they have to be explicitly defined as in the following example.

:::

```yaml title="component.yaml"
spec:
  source:
    uri: github.com/cloudposse/terraform-aws-components.git//modules/vpc-flow-logs-bucket?ref={{.Version}}
    version: 1.398.0
    included_paths:
      - "**/**"
      # If the component's folder has the `modules` sub-folder, it needs to be explicitly defined
      - "**/modules/**"
```

### Vendoring Modules as Components

Any terraform module can also be used as a component, provided that Atmos backend
generation ([`auto_generate_backend_file` is `true`](/cli/configuration/components)) is enabled. Use this strategy when you want to use the module
directly, without needing to wrap it in a component to add additional functionality. This is essentially treating a terraform child module as a root
module.

To vendor a module as a component, simply create a `component.yaml` file stored inside the `components/_type_/_name_/` folder
(e.g. `components/terraform/ec2-instance/component.yaml`).

The schema of a `component.yaml` file for a module is as follows.
Note the usage of the `///` in the `uri`, which is to vendor from the root of the remote repository.

```yaml
apiVersion: atmos/v1
kind: ComponentVendorConfig
metadata:
  name: ec2-instance
  description: Source for vendoring of 'ec2-instance' module as a component
spec:
  source:
    # To vendor a module from a Git repo, use the following format: 'github.com/cloudposse/terraform-aws-ec2-instance.git///?ref={{.Version}}
    uri: github.com/cloudposse/terraform-aws-ec2-instance.git///?ref={{.Version}}
    version: 0.47.1

    # Only include the files that match the 'included_paths' patterns
    # 'included_paths' support POSIX-style Globs for file names/paths (double-star/globstar `**` is supported)
    included_paths:
      - "**/*.tf"
      - "**/*.tfvars"
      - "**/*.md"
```

### Vendoring Components from OCI Registries

Atmos supports vendoring components from [OCI registries](https://opencontainers.org).

To specify a repository in an OCI registry, use the `oci://<registry>/<repository>:tag` scheme in the `sources` and `mixins`.

Components from OCI repositories are downloaded as Docker image tarballs, then all the layers are processed, un-tarred and un-compressed,
and the component's source files are written into the component's directory.

For example, to vendor the `vpc` component from the `public.ecr.aws/cloudposse/components/terraform/stable/aws/vpc`
[AWS public ECR registry](https://docs.aws.amazon.com/AmazonECR/latest/public/public-registries.html), use the following `uri`:

```yaml
uri: "oci://public.ecr.aws/cloudposse/components/terraform/stable/aws/vpc:latest"
```

The schema of a `component.yaml` file is as follows:

```yaml
# This is an example of how to download a Terraform component from an OCI registry (https://opencontainers.org), e.g. AWS Public ECR

# 'component.yaml' in the component folder is processed by the 'atmos' commands:
# 'atmos vendor pull -c infra/vpc' or 'atmos vendor pull --component infra/vpc'

apiVersion: atmos/v1
kind: ComponentVendorConfig
metadata:
  name: stable/aws/vpc
  description: Config for vendoring of the 'stable/aws/vpc' component
spec:
  source:
    # Source 'uri' supports the following protocols: OCI (https://opencontainers.org), Git, Mercurial, HTTP, HTTPS, Amazon S3, Google GCP,
    # and all URL and archive formats as described in https://github.com/hashicorp/go-getter
    # In 'uri', Golang templates are supported  https://pkg.go.dev/text/template
    # If 'version' is provided, '{{.Version}}' will be replaced with the 'version' value before pulling the files from 'uri'
    # Download the component from the AWS public ECR registry (https://docs.aws.amazon.com/AmazonECR/latest/public/public-registries.html)
    uri: "oci://public.ecr.aws/cloudposse/components/terraform/stable/aws/vpc:{{.Version}}"
    version: "latest"
    # Only include the files that match the 'included_paths' patterns
    # If 'included_paths' is not specified, all files will be matched except those that match the patterns from 'excluded_paths'
    # 'included_paths' support POSIX-style Globs for file names/paths (double-star `**` is supported)
    # https://en.wikipedia.org/wiki/Glob_(programming)
    # https://github.com/bmatcuk/doublestar#patterns
    included_paths:
      - "**/*.*"
```

---

## Vendor Manifest

import File from '@site/src/components/File'
import Terminal from '@site/src/components/Terminal'
import Intro from '@site/src/components/Intro'
import CollapsibleText from '@site/src/components/CollapsibleText'

<Intro>
The vendoring configuration is defined in the `vendor.yaml` manifest (vendor config file). The vendoring manifest is used to make copies of 3rd-party components, stacks, and other artifacts in your own repository.
</Intro>

It functions a little bit like the `packages.json` file in Node.js or the `go.mod` file in Go, but for infrastructure code.

## How it works

Atmos searches for the vendoring manifest in the following locations and uses the first one found:

- In the directory from which the [`atmos vendor pull`](/cli/commands/vendor/pull) command is executed, usually in the root of the infrastructure repo

- In the directory pointed to by the [`base_path`](/cli/configuration/#base-path) setting in the [`atmos.yaml`](/cli/configuration) CLI config file

After defining the `vendor.yaml` manifest, all the remote artifacts can be downloaded by running the following command:

```shell
atmos vendor pull
```

To vendor a particular component or other artifact, execute the following command:

```shell
atmos vendor pull -c <component>
```

To vendor components and artifacts tagged with specific tags, execute the following command:

```shell
atmos vendor pull --tags <tag1>,<tag2>
```

:::tip
Refer to [`atmos vendor pull`](/cli/commands/vendor/pull) CLI command for more details
:::

## Vendoring Manifest

To vendor remote artifacts, create a `vendor.yaml` file similar to the example below:

<CollapsibleText type="tall">
  <File title="vendor.yaml">
  ```yaml
  apiVersion: atmos/v1
  kind: AtmosVendorConfig
  metadata:
    name: example-vendor-config
    description: Atmos vendoring manifest
  spec:
    # `imports` or `sources` (or both) must be defined in a vendoring manifest
    imports:
      - "vendor/vendor2"
      - "vendor/vendor3.yaml"

    sources:
      # `source` supports the following protocols: local paths (absolute and relative), OCI (https://opencontainers.org),
      # Git, Mercurial, HTTP, HTTPS, Amazon S3, Google GCP,
      # and all URL and archive formats as described in https://github.com/hashicorp/go-getter.
      # In 'source' and 'targets', Golang templates are supported  https://pkg.go.dev/text/template.
      # Currently the fields '{{.Component}}' and '{{.Version}}' are supported.
      # Download the component from the AWS public ECR registry (https://docs.aws.amazon.com/AmazonECR/latest/public/public-registries.html).
      - component: "vpc"
        source: "oci://public.ecr.aws/cloudposse/components/terraform/stable/aws/vpc:{{.Version}}"
        version: "latest"
        targets:
          - "components/terraform/infra/vpc3"
        # Only include the files that match the 'included_paths' patterns.
        # If 'included_paths' is not specified, all files will be matched except those that match the patterns from 'excluded_paths'.
        # 'included_paths' support POSIX-style Globs for file names/paths (double-star `**` is supported).
        # https://en.wikipedia.org/wiki/Glob_(programming)
        # https://github.com/bmatcuk/doublestar#patterns
        included_paths:
          - "**/*.tf"
          - "**/*.tfvars"
          - "**/*.md"
        # Tags can be used to vendor component that have the specific tags
        # `atmos vendor pull --tags test`
        # Refer to https://atmos.tools/cli/commands/vendor/pull
        tags:
          - test
          - networking
      - component: "vpc-flow-logs-bucket"
        source: "github.com/cloudposse/terraform-aws-components.git//modules/vpc-flow-logs-bucket?ref={{.Version}}"
        version: "1.323.0"
        targets:
          - "components/terraform/infra/{{.Component}}/{{.Version}}"
        excluded_paths:
          - "**/*.yaml"
          - "**/*.yml"
        # Tags can be used to vendor component that have the specific tags
        # `atmos vendor pull --tags networking,storage`
        # Refer to https://atmos.tools/cli/commands/vendor/pull
        tags:
          - test
          - storage
      - component: "vpc-mixin-1"
        source: "https://raw.githubusercontent.com/cloudposse/terraform-null-label/0.25.0/exports/context.tf"
        targets:
          - "components/terraform/infra/vpc3"
        # Tags can be used to vendor component that have the specific tags
        # `atmos vendor pull --tags test`
        # Refer to https://atmos.tools/cli/commands/vendor/pull
        tags:
          - test
      - component: "vpc-mixin-2"
        # Copy a local file into a local folder (keeping the same file name)
        # This `source` is relative to the current folder
        source: "components/terraform/mixins/context.tf"
        targets:
          - "components/terraform/infra/vpc3"
        # Tags can be used to vendor component that have the specific tags
        # `atmos vendor pull --tags test`
        # Refer to https://atmos.tools/cli/commands/vendor/pull
        tags:
          - test
      - component: "vpc-mixin-3"
        # Copy a local folder into a local folder
        # This `source` is relative to the current folder
        source: "components/terraform/mixins"
        targets:
          - "components/terraform/infra/vpc3"
        # Tags can be used to vendor component that have the specific tags
        # `atmos vendor pull --tags test`
        # Refer to https://atmos.tools/cli/commands/vendor/pull
        tags:
          - test
      - component: "vpc-mixin-4"
        # Copy a local file into a local file with a different file name
        # This `source` is relative to the current folder
        source: "components/terraform/mixins/context.tf"
        targets:
          - "components/terraform/infra/vpc3/context-copy.tf"
        # Tags can be used to vendor component that have the specific tags
        # `atmos vendor pull --tags test`
        # Refer to https://atmos.tools/cli/commands/vendor/pull
        tags:
          - test
  ```
  </File>
</CollapsibleText>

With this configuration, it would be possible to run the following commands:
<Terminal>
```shell
# atmos vendor pull
# atmos vendor pull --everything
# atmos vendor pull --component vpc-mixin-1
# atmos vendor pull -c vpc-mixin-2
# atmos vendor pull -c vpc-mixin-3
# atmos vendor pull -c vpc-mixin-4
# atmos vendor pull --tags test
# atmos vendor pull --tags networking,storage
```
</Terminal>

## Vendoring Manifest Schema

The `vendor.yaml` vendoring manifest supports Kubernetes-style YAML config to describe vendoring configuration for components, stacks,
  and other artifacts. The file is placed into the directory from which the `atmos vendor pull` command is executed (usually the root of the repo).

<dl>
  <dt>`version`</dt>
  <dd>
    The `version` attribute is used to specify the version of the artifact to download. The `version` attribute is used in the `source` and `targets` attributes as a template parameter using `{{ .Version }}`.
  </dd>

  <dt>`source`</dt>
  <dd>
    The `source` attribute supports all protocols (local files, Git, Mercurial, HTTP, HTTPS, Amazon S3, Google GCP), and all the URL and archive formats as described in [go-getter](https://github.com/hashicorp/go-getter), and also the `oci://` scheme to download artifacts from [OCI registries](https://opencontainers.org).

    **IMPORTANT:** Include the `{{ .Version }}` parameter in your `source` URI to ensure the correct version of the artifact is downloaded.

    For example, for `http` and `https` sources, use the following format:

    ```yaml
    source: "github.com/cloudposse/terraform-aws-components.git//modules/vpc-flow-logs-bucket?ref={{.Version}}"
    ```

    For private Git repositories, prepend the URI with `git::` and use the following format to pass a environment variable with the GitHub token:

    ```yaml
    source: "git::https://{{env "GITHUB_TOKEN"}}@github.com/some-org/some-private-repo/terraform/vpc.git?ref={{.Version}}"
    ```
    Note, that `GITHUB_TOKEN` provided by GitHub Actions are only valid for the current repository, or repositories marked as `internal` within GitHub Enterprise organizations. For cross-repository access, make sure you provision a [fine grained token](https://docs.github.com/en/rest/authentication/permissions-required-for-fine-grained-personal-access-tokens?apiVersion=2022-11-28) with the necessary permissions.

    <dl>
      <dt>`ref`</dt>
      <dd>
        Pass the `ref` as a query string with either the tag, branch, or commit hash to download the correct version of the artifact. e.g. `?ref={{.Version}}` will pass the `version` attribute to the `ref` query string.
      </dd>
      <dt>`depth`</dt>
      <dd>
        Pass the `depth` as a query string to download only the specified number of commits from the repository. e.g. `?depth=1` will download only the latest commit.
      </dd>
    </dl>

  </dd>

  <dt>`targets`</dt>
  <dd>
  The `targets` in each source supports absolute paths and relative paths (relative to the `vendor.yaml` file). Note: if the `targets` paths
    are set as relative, and if the `vendor.yaml` file is detected by Atmos using the `base_path` setting in `atmos.yaml`, the `targets` paths
    will be considered relative to the `base_path`. Multiple targets can be specified.
  </dd>

  <dt>`included_paths` and `excluded_paths`</dt>
  <dd>
  `included_paths` and `excluded_paths` support [POSIX-style greedy Globs](https://en.wikipedia.org/wiki/Glob_(programming)) for filenames/paths (double-star/globstar `**` is supported as well). For more details, see [Vendoring with Globs](#vendoring-with-globs).
  </dd>

  <dt>`component`</dt>
  <dd>
  The `component` attribute in each source is optional. It's used in the `atmos vendor pull -- component <component>` command if the component is passed in. In this case, Atmos will vendor only the specified component instead of vendoring all the artifacts configured in the `vendor.yaml` manifest.
  </dd>

  <dt>`source` and `targets` templates</dt>
  <dd>
  The `source` and `targets` attributes support [Go templates](https://pkg.go.dev/text/template) and [Sprig Functions](http://masterminds.github.io/sprig/). This can be used to templatise the `source` and `targets` paths with the component name specified in the `component` attribute and artifact versions specified in the `version` attribute.

    Here's an advanced example showcasing how templates and Sprig functions can be used together with `targets`:

    ```yaml
    targets:
      # Vendor a component into a major-minor versioned folder like 1.2
      - "components/terraform/infra/vpc-flow-logs-bucket/{{ (first 2 (splitList \".\" .Version)) | join \".\" }}"
    ```
  </dd>

  <dt>`tags`</dt>
  <dd>
  The `tags` in each source specifies a list of tags to apply to the component. This allows you to only vendor the components that have the specified tags by executing a command `atmos vendor pull --tags <tag1>,<tag2>`
  </dd>

  <dt>`imports`</dt>
  <dd>
  The `imports` section defines the additional vendoring manifests that are merged into the main manifest. Hierarchical imports are supported at many levels (one vendoring manifest can import another, which in turn can import other manifests, etc.). Atmos processes all imports and all sources in the imported manifests in the order they are defined.

  :::note
  The imported file extensions are optional. Imports that do not include file extensions will default to the `.yaml` extension.
  :::

  <File title="vendor.yaml">
  ```yaml title="vendor.yaml"
  spec:
    sources:
      - component: "vpc-flow-logs-bucket"
        source: "github.com/cloudposse/terraform-aws-components.git//modules/vpc-flow-logs-bucket?ref={{.Version}}"
        version: "1.323.0"
        targets:
          - "components/terraform/vpc-flow-logs-bucket"
        included_paths:
          - "**/**"
          # If the component's folder has the `modules` sub-folder, it needs to be explicitly defined
          - "**/modules/**"
  ```
  </File>

  :::warning

  The `glob` library that Atmos uses to download remote artifacts does not treat the double-star `**` as including sub-folders.
  If the component's folder has sub-folders, and you need to vendor them, they have to be explicitly defined as in the following example.

  :::
  </dd>
</dl>

## Template Parameters

The vendor manifest supports basic template parameters, which is useful for versioning and other dynamic values. The following template parameters are supported:

<dl>
  <dt>`{{ .Component }}`</dt>
  <dd>
    Refers to the `component` attribute in the current section. The `component` attribute is used to specify the component name. This is useful to vendor components into folders by the same name.
    ```yaml
    targets:
      - "components/terraform/{{ .Component }}"
    ```
  </dd>
  <dt>`{{ .Version }}`</dt>
  <dd>
    Refers to the `version` attribute the current section. The `version` attribute is used to specify the version of the artifact to download. This is useful to version components into different folders.
    ```yaml
    targets:
      - "components/terraform/{{ .Component }}/{{ .Version }}"
    ```
     When stacks need to pin to different versions of the same component, the `{{ .Version }}` template parameter can be used to ensure the components are vendored into different folders.
  </dd>
</dl>

You can also use any of the [hundreds of go-template functions](/functions/template). For example, to extract the major and minor version from the `{{ .Version }}` attribute, use the following template:

```yaml
targets:
  - "components/terraform/{{ .Component }}/{{ (first 2 (splitList \".\" .Version)) | join \".\" }}"
```

Or to access an environment variable in the `source` attribute, use the following template:

```yaml
source: "git::https://{{env "GITHUB_TOKEN"}}@github.com/some-org/some-private-repo/terraform/{{ .Component }}/{{ .Version }}.git?ref={{.Version}}"
```
This will enable vendoring to download the component into a versioned folder from a private repository, by reading the GitHub token from the `GITHUB_TOKEN` environment variable.

## Hierarchical Imports in Vendoring Manifests

Use `imports` to split the main `vendor.yaml` manifest into smaller files for maintainability, or by their roles in the infrastructure.

For example, import separate manifests for networking, security, data management, CI/CD, and other layers:

```yaml
imports:
  - "layers/networking"
  - "layers/security"
  - "layers/data"
  - "layers/analytics"
  - "layers/firewalls"
  - "layers/cicd"
```

Hierarchical imports are supported at many levels. For example, consider the following vendoring configurations:

<File title="vendor.yaml">
```yaml
apiVersion: atmos/v1
kind: AtmosVendorConfig
metadata:
  name: example-vendor-config
  description: Atmos vendoring manifest
spec:
  imports:
    - "vendor/vendor2"
    - "vendor/vendor3"

  sources:
    - component: "vpc"
      source: "oci://public.ecr.aws/cloudposse/components/terraform/stable/aws/vpc:{{.Version}}"
      version: "latest"
      targets:
        - "components/terraform/infra/vpc3"
    - component: "vpc-flow-logs-bucket"
      source: "github.com/cloudposse/terraform-aws-components.git//modules/vpc-flow-logs-bucket?ref={{.Version}}"
      version: "1.323.0"
      targets:
        - "components/terraform/infra/vpc-flow-logs-bucket/{{.Version}}"
```
</File>

<File title="vendor/vendor2.yaml">
```yaml
apiVersion: atmos/v1
kind: AtmosVendorConfig
metadata:
  name: example-vendor-config-2
  description: Atmos vendoring manifest
spec:
  imports:
    - "vendor/vendor4"

  sources:
    - component: "my-vpc1"
      source: "oci://public.ecr.aws/cloudposse/components/terraform/stable/aws/vpc:{{.Version}}"
      version: "1.0.2"
      targets:
        - "components/terraform/infra/my-vpc1"
```
</File>

<File title="vendor/vendor4.yaml">
```yaml
apiVersion: atmos/v1
kind: AtmosVendorConfig
metadata:
  name: example-vendor-config-4
  description: Atmos vendoring manifest
spec:
  imports:
    - "vendor/vendor5"

  sources:
    - component: "my-vpc4"
      source: "github.com/cloudposse/terraform-aws-components.git//modules/vpc?ref={{.Version}}"
      version: "1.319.0"
      targets:
        - "components/terraform/infra/my-vpc4"
```
</File>

When you execute the `atmos vendor pull` command, Atmos processes the import chain and the sources in the imported manifests in the order they
are defined:

- First, the main `vendor.yaml` file is read based on search paths
- The `vendor/vendor2` and `vendor/vendor3` manifests (defined in the main `vendor.yaml` file) are imported
- The `vendor/vendor2` file is processed, and the `vendor/vendor4` manifest is imported
- The `vendor/vendor4` file is processed, and the `vendor/vendor5` manifest is imported
- Etc.
- Then all the sources from all the imported manifests are processed and the artifacts are downloaded into the paths defined by the `targets`

<Terminal>
```shell
> atmos vendor pull

Processing vendor config file 'vendor.yaml'
Pulling sources for the component 'my-vpc6' from 'github.com/cloudposse/terraform-aws-components.git//modules/vpc?ref=1.315.0' into 'components/terraform/infra/my-vpc6'
Pulling sources for the component 'my-vpc5' from 'github.com/cloudposse/terraform-aws-components.git//modules/vpc?ref=1.317.0' into 'components/terraform/infra/my-vpc5'
Pulling sources for the component 'my-vpc4' from 'github.com/cloudposse/terraform-aws-components.git//modules/vpc?ref=1.319.0' into 'components/terraform/infra/my-vpc4'
Pulling sources for the component 'my-vpc1' from 'public.ecr.aws/cloudposse/components/terraform/stable/aws/vpc:1.0.2' into 'components/terraform/infra/my-vpc1'
Pulling sources for the component 'my-vpc2' from 'github.com/cloudposse/terraform-aws-components.git//modules/vpc?ref=1.320.0' into 'components/terraform/infra/my-vpc2'
Pulling sources for the component 'vpc' from 'public.ecr.aws/cloudposse/components/terraform/stable/aws/vpc:latest' into 'components/terraform/infra/vpc3'
Pulling sources for the component 'vpc-flow-logs-bucket' from 'github.com/cloudposse/terraform-aws-components.git//modules/vpc-flow-logs-bucket?ref=1.323.0' into 'components/terraform/infra/vpc-flow-logs-bucket/1.323.0'
```
</Terminal>

## Vendoring Multiple Versions of Components

Atmos supports vendoring multiple versions of the same component. This is useful when you need to pin stacks to different versions of the same component.

When vendoring multiple versions of the same component, use the `{{ .Version }}` template parameter in the `targets` attribute to ensure the components are vendored into different folders. Then update the stack configuration to point to the correct version of the component, and ensure the `backend.s3.workspace_key_prefix` is defined _without the version_ to ensure you can seamlessly upgrade between versions of a component without losing the state. By default the `workspace_key_prefix` incorporates the `component` relative path, which will include the version if it's included in the path.

```
components:
  terraform:
    # `vpc` is the Atmos component name
    vpc:
      # Backend configuration for the component
      backend:
        s3:
          # Ensure the path in the bucket is stable across versions
          # IMPORTANT: If not explicitly set, the `workspace_key_prefix` will include the version
          #            This will cause the state to be lost when upgrading between versions.
          workspace_key_prefix: vpc
      metadata:
        # Point to the Terraform component on the filesystem
        component: vpc/1.2.3
```

:::important
If not using the S3 backend, use the appropriate parameter for your backend to ensure the workspace is stable across versions of the component deployed.
:::

## Vendoring from OCI Registries

Atmos supports vendoring from [OCI registries](https://opencontainers.org).

To specify a repository in an OCI registry, use the `oci://<registry>/<repository>:tag` scheme.

Artifacts from OCI repositories are downloaded as Docker image tarballs, then all the layers are processed, un-tarred and un-compressed,
and the files are written into the directories specified by the `targets` attribute of each `source`.

For example, to vendor the `vpc` component from the `public.ecr.aws/cloudposse/components/terraform/stable/aws/vpc`
[AWS public ECR registry](https://docs.aws.amazon.com/AmazonECR/latest/public/public-registries.html), use the following `source`:

<File title="vendor.yaml">
```yaml
# This is an example of how to download a Terraform component from an OCI registry (https://opencontainers.org), e.g. AWS Public ECR

apiVersion: atmos/v1
kind: AtmosVendorConfig
metadata:
  name: example-vendor-config
  description: Atmos vendoring manifest
spec:
  sources:
    - component: "vpc"
      source: "oci://public.ecr.aws/cloudposse/components/terraform/stable/aws/{{ .Component }}:{{ .Version }}"
      version: "latest"
      targets:
        - "components/terraform/{{ .Component }}"
      included_paths:
        - "**/*.tf"
        - "**/*.tfvars"
        - "**/*.md"
      excluded_paths: []
```
</File>

To vendor the `vpc` component, execute the following command:

<Terminal>
```bash
atmos vendor pull -c vpc
```
</Terminal>

## Vendoring with Globs

In Atmos, **glob patterns** define which files and directories are included or excluded during vendoring. These patterns go beyond simple wildcard characters like `*`—they follow specific rules that dictate how paths are matched. Understanding the difference between **greedy** (`**`) and **non-greedy** (`*`) patterns, along with other advanced glob syntax, ensures precise control over vendoring behavior.

### Understanding Wildcards, Ranges, and Recursion

Glob patterns in Atmos provide flexible and powerful matching, that's simpler to understand than regular expressions:

<dl>
  <dt>`*` (single asterisk)</dt>
  <dd>Matches any sequence of characters within a single path segment.</dd>
  <dd>Example: `vendor/*.yaml` matches `vendor/config.yaml` but not `vendor/subdir/config.yaml`.</dd>

  <dt>`**` (double asterisk, also known as a "greedy glob")</dt>
  <dd>Matches across multiple path segments recursively.</dd>
  <dd>Example: `vendor/**/*.yaml` matches `vendor/config.yaml`, `vendor/subdir/config.yaml`, and `vendor/deep/nested/config.yaml`.</dd>

  <dt>`?` (question mark)</dt>
  <dd>Matches exactly one character in a path segment.</dd>
  <dd>Example: `file?.txt` matches `file1.txt` and `fileA.txt` but not `file10.txt`.</dd>

  <dt>`[abc]` (character class)</dt>
  <dd>Matches any single character inside the brackets.</dd>
  <dd>Example: `file[123].txt` matches `file1.txt`, `file2.txt`, and `file3.txt`, but not `file4.txt` or `file12.txt`.</dd>

  <dt>`[a-z]` (character range)</dt>
  <dd>Matches any single character within the specified range.</dd>
  <dd>Example: `file[a-c].txt` matches `filea.txt`, `fileb.txt`, and `filec.txt`.</dd>

  <dt>`{a,b,c}` (brace expansion)</dt>
  <dd>Matches any of the comma-separated patterns.</dd>
  <dd>Example: `*.{jpg,png,gif}` matches `image.jpg`, `image.png`, and `image.gif`.</dd>
</dl>

This distinction is important when excluding specific directories or files while vendoring.

#### Example: Excluding a Subdirectory

Consider the following configuration:

```yaml
included_paths:
  - "**/demo-library/**"
excluded_paths:
  - "**/demo-library/**/stargazers/**"
```

How it works:
- The `included_paths` rule `**/demo-library/**` ensures all files inside `demo-library` (at any depth) are vendored.
- The `excluded_paths` rule `**/demo-library/**/stargazers/**` prevents any files inside `stargazers` subdirectories from being vendored.

This means:
- All files within `demo-library` except those inside any `stargazers` subdirectory are vendored.
- Any other files outside `stargazers` are unaffected by this exclusion.

#### Example: A Non-Recursive Pattern That Doesn't Work

```yaml
included_paths:
  - "**/demo-library/*"
excluded_paths:
  - "**/demo-library/**/stargazers/**"
```

In this case:
- `**/demo-library/*` only matches immediate children of `demo-library`, not nested files or subdirectories.
- This means `stargazers/` itself could be matched, but its contents might not be explicitly excluded.
- To correctly capture all subdirectories and files while still excluding stargazers, use `**/demo-library/**/*`.

Using `{...}` for Multiple Extensions or Patterns

Curly braces `{...}` allow for expanding multiple patterns into separate glob matches. This is useful when selecting multiple file types or directories within a single glob pattern.

#### Example: Matching Multiple File Extensions

```yaml
included_paths:
  - "**/demo-library/**/*.{tf,md}"
```

This is equivalent to writing:

```yaml
included_paths:
  - "**/demo-library/**/*.tf"
  - "**/demo-library/**/*.md"
```

The `{tf,md}` part expands to both `*.tf` and `*.md`, making the rule more concise.

#### Example: Excluding Multiple Directories

```yaml
excluded_paths:
  - "**/demo-library/**/{stargazers,archive}/**"
```

This excludes both:
- `**/demo-library/**/stargazers/**`
- `**/demo-library/**/archive/**`

Using `{...}` here prevents the need to write two separate exclusion rules.

## Key Takeaways

1. Use `**/` for recursive matching to include everything inside a directory.
2. Use `*` for single-segment matches, which won't include deeper subdirectories.
3. Use `{...}` to match multiple extensions or directories within a single pattern.
4. Exclusion rules must match nested paths explicitly when trying to exclude deep directories.

By carefully combining `included_paths`, `excluded_paths`, and `{...}` expansion, you can precisely control which files are vendored while ensuring unwanted directories are omitted.

---

## Vendoring

import File from '@site/src/components/File'
import Terminal from '@site/src/components/Terminal'
import Intro from '@site/src/components/Intro'

<Intro>
Atmos natively supports "vendoring," a practice that involves replicating 3rd-party components, stacks, and artifacts within your own repository. This feature is particularly beneficial for managing dependencies in software like Terraform, which do not support pulling root modules remotely by configuration.
</Intro>

Vendoring standardizes dependency management, encourages enterprise component reuse, and ensures compliance standards adherence. Furthermore, it allows teams to customize and independently manage their vendored components according to their specific requirements.

## Use-cases

Atmos vendoring streamlines component sharing and version control across an enterprise, enhancing efficiency and collaboration while offering the flexibility to customize and manage multiple versions of dependencies, ensuring best practices in DevOps environments.

- **Sharing Components Across an Enterprise**: Utilize Atmos vendoring to access a centralized component library, promoting code reuse and
  efficiency across teams (or business units) while enabling customization and independent version control post-vendoring. This approach enhances collaboration without sacrificing the flexibility for teams to tailor components to their specific needs or update them at their preferred pace.
- **Managing Multiple Versions of Dependencies:** Use Atmos vendoring to manage multiple versions of remote dependencies,
  effectively implementing version pinning through locally controlled artifacts. By configuring a stacks component directory (e.g., `vpc/v1` or `vpc/v2`), vendoring provides maximum flexibility while still aligning with best practices in DevOps environments.
- **Reinforce Immutable Infrastructure**: Employ Atmos vendoring to store immutable infrastructure artifacts, guaranteeing that once a committed,
  it remains unaltered throughout its lifecycle, ensuring stability and reliability in deployments.

## Types of Vendoring

Atmos supports two different ways of vendoring components:

- [**Vendor Manifest**](/core-concepts/vendor/vendor-manifest) Using `vendor.yaml` vendoring manifest file containing a list of all dependencies.
- [**Component Manifest**](/core-concepts/vendor/vendor-manifest) Using `component.yaml` manifest file inside of a component directory. See below.

The `vendor.yaml` vendoring manifest describes the vendoring config for all components, stacks and other artifacts for the entire infrastructure.
The file is placed into the directory from which the `atmos vendor pull` command is executed. It's the recommended way to describe vendoring
configurations.

:::tip
Refer to [`Atmos Vendoring`](/core-concepts/vendor) for more details
:::

The `component.yaml` vendoring manifest is used to vendor components from remote repositories.
A `component.yaml` file placed into a component's directory is used to describe the vendoring config for one component only.

:::tip Pro Tip! Use GitOps
Vendoring plays nicely with GitOps practices, especially when leveraging [GitHub Actions](/integrations/github-actions/).
Use a workflow that automatically updates the vendor manifest and opens a pull request (PR) with all the changes.
This allows you to inspect and precisely assess the impact of any upgrades before merging by reviewing the job summary of the PR.
:::

## Features

With Atmos vendoring, you can copy components and other artifacts from the following sources:

- Copy all files from an [OCI Registry](https://opencontainers.org) into a local folder
- Copy all files from Git, Mercurial, Amazon S3, Google GCP into a local folder
- Copy all files from an HTTP/HTTPS endpoint into a local folder
- Copy a single file from an HTTP/HTTPS endpoint to a local file
- Copy a local file into a local folder (keeping the same file name)
- Copy a local file to a local file with a different file name
- Copy a local folder (all files) into a local folder

Our implementation is primarily inspired by the excellent tool by VMware Tanzu, called [`vendir`](https://github.com/vmware-tanzu/carvel-vendir).
While Atmos does not call `vendir`, it functions and supports a subset of the configuration that is very similar.

---

## Workflows

import File from '@site/src/components/File'
import Terminal from '@site/src/components/Terminal'
import Intro from '@site/src/components/Intro'

<Intro>
Workflows are a way of combining multiple commands into one executable unit of work.
</Intro>

You can use [Atmos Custom Commands](/core-concepts/custom-commands) in Atmos Workflows, and Atmos Workflows in [Atmos Custom Commands](/core-concepts/custom-commands)

## Simple Example

Here's an example workflow called `eks-up` which runs a few commands that will bring up the EKS cluster:

```yaml title=stacks/workflows/workflow1.yaml
workflows:
  eks-up:
    description: |
      Bring up the EKS cluster.
    steps:
      - command: terraform apply vpc -auto-approve
      - command: terraform apply eks/cluster -auto-approve
      - command: terraform apply eks/alb-controller -auto-approve
```

## Retry Configuration

The `command` section of the workflow schema has been updated to allow for retrying of failed commands. The `retry` section
accepts the following parameters:

* `max_attempts`: The maximum number of times the command will be retried. The default is `1`.
* `delay`: The amount of time to delay between retries. The default is `5s`.
* `backoff_strategy`: The backoff strategy to use. The default is `constant`. The other options are `exponential` and `linear`.
* `initial_delay`: The initial delay to use when retrying. The default is `5s`.
* `random_jitter`: the random jitter number to the delay between retries. The default is `0.0`.
* `multiplier`: The multiplier to use when retrying. The default is `2`.
* `max_elapsed_time`: The maximum time allocated to the command before the retry fails. The default is 30 minutes.

Here is an example of a workflow with retry configuration:
```
workflows:
  eks-up:
    description: Bring up the eks cluster
    steps:
      - command: terraform apply vpc -auto-approve
        retry:
          max_attempts: 3
          backoff_strategy: exponential # could be exponential, constant and linear
          initial_delay: 3s # the time used by the backoff_strategy
          random_jitter: 0.0 # the random jitter to be added while retrying.
          multiplier: 2 # used during exponential strategy as multiplier that would be raised by configuration
          max_elapsed_time: 4m # Default (30 min for all commands). This sets the total duration alloated to the command before the retry fails.
      - command: terraform apply eks/cluster -auto-approve
      - command: terraform apply eks/alb-controller -auto-approve
```

:::note

The workflow name can be anything you want, and the workflow can also accept command-line parameters (e.g. stack name)

:::

If you define this workflow in the file `workflow1.yaml`, it can we executed like this to provision
the `vpc`, `eks/cluster` and `eks/alb-controller` [Atmos Components](/core-concepts/components) into
the `tenant1-ue2-dev` [Atmos Stack](/core-concepts/stacks):

```shell
atmos workflow eks-up -f workflow1 --stack tenant1-ue2-dev
```

:::tip

Refer to [`atmos workflow`](/cli/commands/workflow) for the complete description of the CLI command

:::

## Configuration

To configure and execute Atmos workflows, follow these steps:

- Configure workflows in [`atmos.yaml` CLI config file](/cli/configuration)
- Create workflow files and define workflows using the workflow schema

### Configure Workflows in `atmos.yaml`

In `atmos.yaml` CLI config file, add the following sections related to Atmos workflows:

```yaml
# Base path for components, stacks and workflows configurations.
# Can also be set using 'ATMOS_BASE_PATH' ENV var, or '--base-path' command-line argument.
# Supports both absolute and relative paths.
# If not provided or is an empty string, 'components.terraform.base_path', 'components.helmfile.base_path', 'stacks.base_path'
# and 'workflows.base_path' are independent settings (supporting both absolute and relative paths).
# If 'base_path' is provided, 'components.terraform.base_path', 'components.helmfile.base_path', 'stacks.base_path'
# and 'workflows.base_path' are considered paths relative to 'base_path'.
base_path: ""

workflows:
  # Can also be set using 'ATMOS_WORKFLOWS_BASE_PATH' ENV var, or '--workflows-dir' command-line arguments
  # Supports both absolute and relative paths
  base_path: "stacks/workflows"
```

where:

<dl>
  <dt>`base_path`</dt>
  <dd>The base path for components, stacks and workflows configurations</dd>

  <dt>`workflows.base_path`</dt>
  <dd>The base path to Atmos workflow files</dd>
</dl>

### Create Workflow Files

In `atmos.yaml`, we set `workflows.base_path` to `stacks/workflows`. The folder is relative to the root of the repository.

Refer to [networking.yaml](https://github.com/cloudposse/atmos/tree/main/examples/quick-start-advanced/stacks/workflows/networking.yaml) for an example.

We put the workflow files into the folder. The workflow file names can be anything you want, but we recommend naming them according to the functions
they perform, e.g. create separate workflow files per environment, account, team, or service.

For example, you can have a workflow file `stacks/workflows/workflows-eks.yaml` to define all EKS-related workflows.

Or, you can have a workflow file `stacks/workflows/workflows-dev.yaml` to define all workflows to provision resources into the `dev` account.
Similarly, you can create a workflow file `stacks/workflows/workflows-prod.yaml` to define all workflows to provision resources into the `prod`
account.

You can segregate the workflow files even further, e.g. per account and service. For example, in the workflow
file `stacks/workflows/workflows-dev-eks.yaml` you can define all EKS-related workflows for the `dev` account.

### Use Workflow Schema

Workflow files must confirm to the following schema:

```yaml
workflows:

  workflow-1:
    description: "Description of Workflow #1"
    steps: []

  workflow-2:
    description: "Description of Workflow #2"
    steps: []
```

Each workflow file must have the `workflows:` top-level section with a map of workflow definitions.

Each workflow definition must confirm to the following schema:

```yaml
  workflow-1:
    description: "Description of Workflow #1"
    stack: <Atmos stack> # optional
    steps:
      - command: <Atmos command to execute>
        name: <step name>>  # optional
        type: atmos  # optional
        stack: <Atmos stack> # optional
      - command: <Atmos command to execute>
        name: <step name>>  # optional
        stack: <Atmos stack> # optional
      - command: <shell script>
        name: <step name>>  # optional
        type: shell  # required for the steps of type `shell`
```

### Schema Definitions

<dl>
  <dt>`description`</dt>
  <dd>The workflow description</dd>

  <dt>`stack`</dt>
  <dd>Workflow-level Atmos stack (optional). If specified, all workflow steps of type `atmos` will be executed for this Atmos stack. It can be overridden in each step or on the command line by using the `--stack` flag (`-s` for shorthand)</dd>

  <dt>`steps`</dt>
  <dd>A list of workflow steps which are executed sequentially in the order they are specified</dd>

  <dt>`command`</dt>
  <dd>The command to execute. Can be either an Atmos [CLI command](/cli/commands) (without the `atmos` binary name in front of it, for example `command: terraform apply vpc`), or a shell script. The type of the command is specified by the `type` attribute</dd>

  <dt>`name`</dt>
  <dd>Step name (optional). It's used to find the first step from which to start executing the workflow when the command-line flag `--from-step` is specified. If the `name` is omitted, a friendly name will be generated for you consisting of a prefix of `step` and followed by the index of the step (the index starts with 1, so the first generated step name would be `step1`).</dd>

  <dt>`type`</dt>
  <dd>The type of the command. Can be either `atmos` or `shell`. Type `atmos` is implicit, you don't have to specify it if the `command` is an Atmos [CLI command](/cli/commands). Type `shell` is required if the command is a shell script. When executing a step of type `atmos`, Atmos prepends the `atmos` binary name to the provided command before executing it</dd>

  <dt>`stack`</dt>
  <dd>Step-level Atmos stack (optional). If specified, the `command` will be executed for this Atmos stack. It overrides the workflow-level `stack` attribute, and can itself be overridden on the command line by using the `--stack` flag (`-s` for shorthand)</dd>
</dl>

:::note

A workflow command of type `shell` can be any simple or complex shell command or script.
You can use [YAML Multiline Strings](https://yaml-multiline.info/) to create complex multi-line shell scripts.

:::

## Executing Workflow from a Named Step

Each workflow step can be given an arbitrary name (step's identifier) using the `name` attribute. For example:

```yaml title=stacks/workflows/workflow1.yaml
workflows:
  test-1:
    description: "Test workflow"
    steps:
      - command: echo Command 1
        name: step1
        type: shell
      - command: echo Command 2
        name: step2
        type: shell
      - command: echo Command 3
        name: step3
        type: shell
      - command: echo Command 4
        type: shell
```

The step's name can be used in the `--from-step` command-line flag to start the workflow execution from the step.

For example, the following command will skip the first two steps and will start executing the workflow from `step3`:

```console
atmos workflow test-1 -f workflow1 --from-step step3
```

This is useful when you want to restart the workflow from a particular step.

For example:

- You run the workflow first time with the command `atmos workflow test-1 -f workflow1`
- `step1` and `step2` succeed, but `step3` fails
- You fix the issue with the `step3` command
- You don't want to execute `step1` and `step2` again (to not spend time on it, or if they are
  not [idempotent](https://en.wikipedia.org/wiki/Idempotence))
- You run the command `atmos workflow test-1 -f workflow1 --from-step step3` to restart the workflow from `step3`

If the `name` attribute in a workflow step is omitted, a friendly name will be generated for you consisting of a prefix of `step` and followed by the
index of the step. The index starts with 1, so the first generated step name would be `step1`.

The `test-1` workflow defined above does not have the `name` attribute for the last workflow step.
When we execute the `atmos workflow` commands, Atmos automatically generates the names for the steps where the `name` attribute is omitted.
In this case, the generated name for the last step will be `step4`.

For example:

<Terminal title="atmos workflow test-1 -f workflow1 --from-step step4">
```console
Executing the workflow 'test-1' from 'stacks/workflows/workflow1.yaml'

description: Test workflow
steps:
- name: step1
  command: echo Command 1
  type: shell
- name: step2
  command: echo Command 2
  type: shell
- name: step3
  command: echo Command 3
  type: shell
- name: step4
  command: echo Command 4
  type: shell

Executing workflow step: echo Command 4

Executing command: echo Command 4
Command 4

Executing workflow step: echo Command 5

Executing command: echo Command 5
Command 5
```
</Terminal>

## Workflow Examples

The following workflow defines four steps of type `atmos` (implicit type) without specifying the workflow-level or step-level `stack` attribute.
Since the workflow does not specify the stack, it's generic and can be executed for any Atmos stack.
In this case, the stack needs to be provided on the command line.

```yaml title=stacks/workflows/workflow1.yaml
workflows:
  terraform-plan-all-test-components:
    description: |
      Run 'terraform plan' on 'test/test-component' and all its derived components.
      The stack must be provided on the command line:
      `atmos workflow terraform-plan-all-test-components -f workflow1 -s <stack>`
    steps:
      # Inline scripts are also supported
      # Refer to https://yaml-multiline.info for more details
      - type: shell
        command: |
          echo "Starting the workflow execution..."
          read -p "Press any key to continue... " -n1 -s
      - command: terraform plan test/test-component
      - command: terraform plan test/test-component-override
      - command: terraform plan test/test-component-override-2
      - command: terraform plan test/test-component-override-3
      - type: shell
        command: >-
          echo "All done!"
```

To run this workflow for the `tenant1-ue2-dev` stack, execute the following command:

```console
atmos workflow terraform-plan-all-test-components -f workflow1 -s tenant1-ue2-dev
```

The following workflow executes `terraform plan` on `test/test-component-override-2` component in all stacks.
In this case, the stack is specified inline for each workflow command.

```yaml title=stacks/workflows/workflow1.yaml
workflows:
  terraform-plan-test-component-override-2-all-stacks:
    description: Run 'terraform plan' on 'test/test-component-override-2' component in all stacks
    steps:
      - command: terraform plan test/test-component-override-2 -s tenant1-ue2-dev
      - command: terraform plan test/test-component-override-2 -s tenant1-ue2-staging
      - command: terraform plan test/test-component-override-2 -s tenant1-ue2-prod
      - command: terraform plan test/test-component-override-2 -s tenant2-ue2-dev
      - command: terraform plan test/test-component-override-2 -s tenant2-ue2-staging
      - command: terraform plan test/test-component-override-2 -s tenant2-ue2-prod
      - type: shell
        command: echo "All done!"
```

To run this workflow, execute the following command:

```console
atmos workflow terraform-plan-test-component-override-2-all-stacks -f workflow1
```

The following workflow is similar to the above, but the stack for each command is specified in the step-level `stack` attribute.

```yaml title=stacks/workflows/workflow1.yaml
workflows:
  terraform-plan-test-component-override-3-all-stacks:
    description: Run 'terraform plan' on 'test/test-component-override-3' component in all stacks
    steps:
      - command: terraform plan test/test-component-override-3
        stack: tenant1-ue2-dev
      - command: terraform plan test/test-component-override-3
        stack: tenant1-ue2-staging
      - command: terraform plan test/test-component-override-3
        stack: tenant1-ue2-prod
      - command: terraform plan test/test-component-override-3
        stack: tenant2-ue2-dev
      - command: terraform plan test/test-component-override-3
        stack: tenant2-ue2-staging
      - command: terraform plan test/test-component-override-3
        stack: tenant2-ue2-prod
```

To run this workflow, execute the following command:

```console
atmos workflow terraform-plan-test-component-override-3-all-stacks -f workflow1
```

## Working with Atmos Stacks in Workflows

The Atmos stack used by the workflow commands of type `atmos` can be specified in four different ways:

### Inline in the command itself
  ```yaml
  steps:
    - command: terraform plan test/test-component-override-2 -s tenant1-ue2-dev
  ```

### In the workflow-level `stack` attribute
  ```yaml
  workflows:
    my-workflow:
      stack: tenant1-ue2-dev
      steps:
        - command: terraform plan test/test-component
  ```

### In the step-level `stack` attribute
  ```yaml
  steps:
    - command: terraform plan test/test-component
      stack: tenant1-ue2-dev
  ```

### On the command line
  ```console
  atmos workflow my-workflow -f workflow1 -s tenant1-ue2-dev
  ```

## Workflow Failure Handling and Resuming

When a workflow step fails, Atmos will:
1. Display which step failed
2. Show the exact command that failed
3. Provide a ready-to-use command to resume the workflow from the failed step

Given this workflow:

```yaml title="stacks/workflows/networking.yaml"
workflows:
  provision-vpcs:
    description: "Deploy vpc components"
    steps:
      - command: terraform plan vpc -s plat-ue2-dev
        name: step-1
      - command: terraform plan vpc -s plat-ue2-staging
        name: step-2
      - command: terraform plan vpc -s plat-ue2-prod
        name: step-3
```

If step-2 fails, you'll see:

```console
Step 'step-2' failed!

Command failed:
terraform plan vpc -s plat-ue2-staging

To resume the workflow from this step, run:
atmos workflow provision-vpcs -f networking --from-step step-2
```

### Stack Precedence

The stack defined inline in the command itself has the lowest priority, it can and will be overridden by any other stack definition.
The step-level stack will override the workflow-level stack. The command line `--stack` option will override all other stacks defined in the workflow
itself. You can also use any combinations of the above (e.g. specify the stack at the workflow level, then override it at the step level for some
commands, etc.).

While this provides a great flexibility in defining the stack for workflow commands, we recommend creating generic workflows without defining
stacks in the workflow itself (the stack should be provided on the command line). This way, the workflow can be executed for any stack without any
modifications and without dealing with multiple workflows that are similar but differ only by the environment where the resources are provisioned.

---

## Abstract Component

import File from '@site/src/components/File'
import Intro from '@site/src/components/Intro'
import PillBox from '@site/src/components/PillBox'

<PillBox>Atmos Design Pattern</PillBox>

<Intro>
The **Abstract Component** Design Pattern describes the mechanism of deriving Atmos components from one or more **abstract** base components (blueprints), allowing reusing the base components' configurations and prohibiting the abstract base component from being provisioned.
</Intro>

Atmos supports two types of components:

<dl>
<dt>`real`</dt>
<dd>is a ["concrete"](https://en.wikipedia.org/wiki/Concrete_class) component instance that can be provisioned</dd>

<dt>`abstract`</dt>
<dd>a component configuration, which cannot be instantiated directly. The concept is borrowed
  from ["abstract base classes"](https://en.wikipedia.org/wiki/Abstract_type) of Object-Oriented Programming</dd>
</dl>

The type of component is expressed in the `metadata.type` parameter of a given component configuration.

:::note
For more details, refer to:

- [Atmos Components](/core-concepts/components)
- [Glossary](/terms)

:::

## Use-cases

Use the **Abstract Component** pattern when:

- You need to have reusable base components that serve as blueprints for the derived Atmos components

- You need to prevent the abstract base components from being provisioned

- You need to keep the configuration of all components [DRY](https://en.wikipedia.org/wiki/Don%27t_repeat_yourself)

## Benefits

The **Abstract Component** pattern provides the following benefits:

- Allows creating very DRY and reusable configurations that are built upon existing abstract base components (blueprints)

- Prevents the abstract base components from being provisioned

- The `metadata.type: abstract` attribute serves as a guard against accidentally deploying the components that are not meant to be deployed

## Example

The following example shows the Atmos stack and component configurations to provision the `vpc` component into
a multi-account, multi-region environment. In the `catalog/vpc` folder, we have the `defaults.yaml` manifest that configures the base **abstract**
component `vpc/defaults` to be inherited by all the derived VPC components in all stacks. By being **abstract**, the base component `vpc/defaults`
is prohibited from being provisioned.

```console
   │   # Centralized stacks configuration (stack manifests)
   ├── stacks
   │   ├── catalog  # component-specific defaults
   │   │   └── vpc
   │   │       └── defaults.yaml
   │   ├── mixins
   │   │   ├── tenant  # tenant-specific defaults
   │   │   │   └── plat.yaml
   │   │   ├── region  # region-specific defaults
   │   │   │   ├── us-east-2.yaml
   │   │   │   └── us-west-2.yaml
   │   │   └── stage  # stage-specific defaults
   │   │       ├── dev.yaml
   │   │       ├── staging.yaml
   │   │       └── prod.yaml
   │   └── orgs  # Organizations
   │       └── acme
   │           ├── _defaults.yaml
   │           └── plat  # 'plat' represents the "Platform" OU (a.k.a tenant)
   │               ├── _defaults.yaml
   │               ├── dev
   │               │   ├── _defaults.yaml
   │               │   ├── us-east-2.yaml
   │               │   └── us-west-2.yaml
   │               ├── staging
   │               │   ├── _defaults.yaml
   │               │   ├── us-east-2.yaml
   │               │   └── us-west-2.yaml
   │               └── prod
   │                   ├── _defaults.yaml
   │                   ├── us-east-2.yaml
   │                   └── us-west-2.yaml
   │   # Centralized components configuration
   └── components
       └── terraform  # Terraform components (a.k.a Terraform "root" modules)
           └── vpc
```

Add the following minimal configuration to `atmos.yaml` [CLI config file](/cli/configuration) :

```yaml title="atmos.yaml"
components:
  terraform:
    base_path: "components/terraform"

stacks:
  base_path: "stacks"
  name_pattern: "{tenant}-{environment}-{stage}"
  included_paths:
    # Tell Atmos to search for the top-level stack manifests in the `orgs` folder and its sub-folders
    - "orgs/**/*"
  excluded_paths:
    # Tell Atmos that the `defaults` folder and all sub-folders don't contain top-level stack manifests
    - "defaults/**/*"

schemas:
  jsonschema:
    base_path: "stacks/schemas/jsonschema"
  opa:
    base_path: "stacks/schemas/opa"
  atmos:
    manifest: "stacks/schemas/atmos/atmos-manifest/1.0/atmos-manifest.json"
```

Add the following configuration for the abstract base component `vpc/defaults` to the `stacks/catalog/vpc/defaults.yaml` manifest:

```yaml title="stacks/catalog/vpc/defaults.yaml"
components:
  terraform:
    vpc/defaults:
      metadata:
        # Abstract components can't be provisioned, they just serve as base components (blueprints) for real components
        # `metadata.type` can be `abstract` or `real`
        # `real` is the default and can be omitted
        type: abstract
      settings:
        # All validation steps must succeed to allow the component to be provisioned
        validation:
          validate-vpc-component-with-jsonschema:
            schema_type: jsonschema
            schema_path: "vpc/validate-vpc-component.json"
            description: Validate 'vpc' component variables using JSON Schema
          check-vpc-component-config-with-opa-policy:
            schema_type: opa
            schema_path: "vpc/validate-vpc-component.rego"
            module_paths:
              - "catalog/constants"
            description: Check 'vpc' component configuration using OPA policy
      vars:
        enabled: true
        name: "common"
        max_subnet_count: 3
        map_public_ip_on_launch: true
        assign_generated_ipv6_cidr_block: false
        nat_gateway_enabled: true
        nat_instance_enabled: false
        vpc_flow_logs_enabled: true
        vpc_flow_logs_traffic_type: "ALL"
        vpc_flow_logs_log_destination_type: "s3"
        nat_eip_aws_shield_protection_enabled: false
        subnet_type_tag_key: "acme/subnet/type"
        ipv4_primary_cidr_block: 10.0.0.0/18
```

Configure the `vpc` Atmos component in the `stacks/orgs/acme/plat/prod/us-east-2.yaml` top-level stack. The `vpc` component inherits from
the `vpc/defaults` abstract base component:

```yaml title="stacks/orgs/acme/plat/prod/us-east-2.yaml"
import:
  import:
    - orgs/acme/plat/prod/_defaults
    - mixins/region/us-east-2
    # Import the `vpc/defaults` component from the `catalog/vpc/defaults.yaml` manifest
    - catalog/vpc/defaults

components:
  terraform:
    # Atmos component `vpc`
    vpc:
      metadata:
        # `metadata.type` can be `abstract` or `real`
        # `real` is the default and can be omitted
        # Real components can be provisioned
        type: real
        # Point to the Terraform component in `components/terraform/vpc`
        component: vpc
        # Inherit from the `vpc/defaults` Atmos base component
        # This is Single Inheritance: the Atmos component inherits from one base Atmos component
        inherits:
          - vpc/defaults
      # Define/override variables specific to this `vpc` component
      vars:
        name: my-vpc
        vpc_flow_logs_enabled: false
        ipv4_primary_cidr_block: 10.9.0.0/18
```

To provision the `vpc` component into the `plat-ue2-prod` top-level stack, execute the following command:

```shell
atmos terraform apply vpc -s plat-ue2-prod
```

If you try to execute the following commands to provision the `vpc/defaults` abstract base component:

```shell
atmos terraform plan vpc/defaults -s plat-ue2-prod
atmos terraform apply vpc/defaults -s plat-ue2-prod
```

the following error will be thrown:

```console
abstract component 'vpc/defaults' cannot be provisioned since it's explicitly prohibited from
being deployed by 'metadata.type: abstract' attribute
```

## Related Patterns

- [Component Inheritance](/design-patterns/component-inheritance)
- [Multiple Component Instances](/design-patterns/multiple-component-instances)
- [Component Catalog](/design-patterns/component-catalog)
- [Component Catalog with Mixins](/design-patterns/component-catalog-with-mixins)
- [Component Catalog Template](/design-patterns/component-catalog-template)
- [Inline Component Configuration](/design-patterns/inline-component-configuration)
- [Inline Component Customization](/design-patterns/inline-component-customization)

## References

- [Component-Oriented Programming](/core-concepts/stacks/inheritance)
- [Component Inheritance](/core-concepts/stacks/inheritance)

---

## Component Catalog Template

import File from '@site/src/components/File'
import Intro from '@site/src/components/Intro'
import PillBox from '@site/src/components/PillBox'

<PillBox>Atmos Design Pattern</PillBox>

<Intro>
The **Component Catalog Template** Design Pattern is used when you have an unbounded number of a component's instances provisioned in one environment (the same organization, OU/tenant, account and region). New instances with different settings can be configured and provisioned anytime. The old instances must be kept unchanged and never destroyed.
</Intro>

This is achieved by using [`Go` Templates in Imports](/core-concepts/stacks/imports#go-templates-in-imports) and
[Hierarchical Imports with Context](/core-concepts/stacks/imports#hierarchical-imports-with-context).

## Use-cases

Use the **Component Catalog Template** pattern when:

- You have an unbounded number of a component's instances provisioned in one environment (the same organization, OU/tenant, account and region)

- New instances of the component with different settings can be configured and provisioned anytime

- The old instances of the component must be kept unchanged and never destroyed

- You want to keep the configurations [DRY](https://en.wikipedia.org/wiki/Don%27t_repeat_yourself)

## Benefits

The **Component Catalog Template** pattern provides the following benefits:

- All settings for a component are defined in just one place (in the component's template) making the entire
  configuration [DRY](https://en.wikipedia.org/wiki/Don%27t_repeat_yourself)

- Many instances of the component can be provisioned without repeating all the configuration values

- New Atmos components are generated dynamically

## Design Pattern

The **Component Catalog Template** pattern recommends the following:

- In the component's catalog folder, create a [`Go` template](https://pkg.go.dev/text/template) manifest with all the configurations for the
  component (refer to [`Go` Templates in Imports](/core-concepts/stacks/imports#go-templates-in-imports) for more details)

- Import the `Go` template manifest into a top-level stack many times to configure the component's instances
  using [Hierarchical Imports with Context](/core-concepts/stacks/imports#hierarchical-imports-with-context) and providing different template values
  for each import

## Example

Suppose that we have an EKS cluster provisioned in one of the accounts and regions.
The cluster is running many different applications, each one requires
an [IAM role for service accounts](https://docs.aws.amazon.com/eks/latest/userguide/iam-roles-for-service-accounts.html) (IRSA) with permissions
to access various AWS resources.

The Development team can create a new application anytime, and we need to provision a new IRSA in the EKS cluster.
We'll use the **Component Catalog Template** Design Pattern to configure the IAM roles with different settings for each application.

```console
   │   # Centralized stacks configuration (stack manifests)
   ├── stacks
   │   └── catalog  # component-specific defaults
   │       └── eks
   │           └── iam-role
   │               └── defaults.tmpl
   │   # Centralized components configuration
   └── components
       └── terraform  # Terraform components (a.k.a Terraform "root" modules)
           └── eks
               └── iam-role
```

Add the following minimal configuration to `atmos.yaml` [CLI config file](/cli/configuration) :

```yaml title="atmos.yaml"
components:
  terraform:
    base_path: "components/terraform"

stacks:
  base_path: "stacks"
  name_pattern: "{tenant}-{environment}-{stage}"
  included_paths:
    # Tell Atmos to search for the top-level stack manifests in the `orgs` folder and its sub-folders
    - "orgs/**/*"
  excluded_paths:
    # Tell Atmos that the `defaults` folder and all sub-folders don't contain top-level stack manifests
    - "defaults/**/*"

schemas:
  jsonschema:
    base_path: "stacks/schemas/jsonschema"
  opa:
    base_path: "stacks/schemas/opa"
  atmos:
    manifest: "stacks/schemas/atmos/atmos-manifest/1.0/atmos-manifest.json"
```

Add the following configuration to the `stacks/catalog/eks/iam-role/defaults.tmpl` `Go` template manifest:

```yaml title="stacks/catalog/eks/iam-role/defaults.tmpl"
components:
  terraform:
    eks/iam-role/{{ .app_name }}:
      metadata:
        # Point to the Terraform component
        component: eks/iam-role
      vars:
        enabled: true
        tags:
          Service: {{ .app_name }}
        service_account_name: {{ .service_account_name }}
        service_account_namespace: {{ .service_account_namespace }}
        # Example of using the Sprig functions in `Go` templates.
        # Refer to https://masterminds.github.io/sprig for more details.
        {{ if hasKey . "iam_managed_policy_arns" }}
        iam_managed_policy_arns:
          {{ range $i, $iam_managed_policy_arn := .iam_managed_policy_arns }}
          - '{{ $iam_managed_policy_arn }}'
          {{ end }}
        {{ - end }}
```

Import the `stacks/catalog/eks/iam-role/defaults.tmpl` manifest template into a top-level stack,
for example `stacks/orgs/acme/plat/prod/us-east-2.yaml`, and provide the configuration for each application in the `context` object:

```yaml title="stacks/orgs/acme/plat/prod/us-east-2.yaml"
import:
  - orgs/acme/plat/prod/_defaults
  - mixins/region/us-east-2

  # This import will dynamically generate a new Atmos component `eks/iam-role/admin-ui`
  - path: catalog/eks/iam-role/defaults.tmpl
    context:
      app_name: "admin-ui"
      service_account_name: "admin-ui"
      service_account_namespace: "admin"
      iam_managed_policy_arns: [ "<arn1>", "<arn2>" ]

  # This import will dynamically generate a new Atmos component `eks/iam-role/auth`
  - path: catalog/eks/iam-role/defaults.tmpl
    context:
      app_name: "auth"
      service_account_name: "auth"
      service_account_namespace: "auth"
      iam_managed_policy_arns: [ "<arn3>" ]

  # This import will dynamically generate a new Atmos component `eks/iam-role/payment-processing`
  - path: catalog/eks/iam-role/defaults.tmpl
    context:
      app_name: "payment-processing"
      service_account_name: "payment-processing"
      service_account_namespace: "payments"
      iam_managed_policy_arns: [ "<arn4>", "<arn5>" ]

  # Add new application configurations here
```

To provision the Atmos components in the stack, execute the following commands:

```shell
atmos terraform apply eks/iam-role/admin-ui --stack plat-ue2-prod
atmos terraform apply eks/iam-role/auth --stack plat-ue2-prod
atmos terraform apply eks/iam-role/payment-processing --stack plat-ue2-prod
```

## Limitations

The **Component Catalog Template** pattern has the following limitations and drawbacks:

- Since new Atmos components are generated dynamically, sometimes it's not easy to know the names of the Atmos components that need to be provisioned
  without looking at the `Go` template and figuring out all the Atmos component names

- With great power comes great responsibility. The Go templating engine leads to infinite possibilities, which makes the stack configs more
  challenging to maintain and reduces consistency. Try to leverage the inheritance model as much as possible, over templated stack configs.

:::note

To address the limitations of the **Component Catalog Template** Design Pattern, consider using the following patterns:

- [Component Catalog](/design-patterns/component-catalog)
- [Component Catalog with Mixins](/design-patterns/component-catalog-with-mixins)

:::

## Related Patterns

- [Component Catalog](/design-patterns/component-catalog)
- [Component Catalog with Mixins](/design-patterns/component-catalog-with-mixins)
- [Component Inheritance](/design-patterns/component-inheritance)
- [Inline Component Configuration](/design-patterns/inline-component-configuration)
- [Inline Component Customization](/design-patterns/inline-component-customization)
- [Organizational Structure Configuration](/design-patterns/organizational-structure-configuration)

## References

- [Catalogs](/core-concepts/stacks/catalogs)

---

## Component Catalog with Mixins

import File from '@site/src/components/File'
import Intro from '@site/src/components/Intro'
import PillBox from '@site/src/components/PillBox'

<PillBox>Atmos Design Pattern</PillBox>

<Intro>
The **Component Catalog with Mixins** Design Pattern is a variation of the [Component Catalog](/design-patterns/component-catalog) pattern, with the difference being that we first create parts of a component's configuration related to different environments (e.g. in `mixins` folder), then assemble environment-specific manifests from the parts, and then import the environment-specific manifests themselves into the top-level stacks.
</Intro>

It's similar to how [Helm](https://helm.sh/) and [helmfile](https://helmfile.readthedocs.io/en/latest/#environment) handle environments.

## Use-cases

Use the **Component Catalog** pattern when:

- You have many components that are provisioned in multiple stacks (many OUs, accounts, regions) with different configurations for each stack

- You need to make the component configurations reusable across different environments

- You want to keep the configurations [DRY](https://en.wikipedia.org/wiki/Don%27t_repeat_yourself)

## Benefits

The **Component Catalog with Mixins** pattern provides the following benefits:

- Easy to see where the configuration for each environment is defined

- Easy to manage different variations of the configurations

- The defaults for the components are defined in just one place (in the catalog) making the entire
  configuration [DRY](https://en.wikipedia.org/wiki/Don%27t_repeat_yourself)

- The defaults for the components are reusable across many environments by using hierarchical [imports](/core-concepts/stacks/imports)

## Design Pattern

The **Component Catalog with Mixins** Design Pattern prescribes the following:

- For a Terraform component, create a folder with the same name in `stacks/catalog` to make it symmetrical and easy to find.
  For example, the `stacks/catalog/vpc` folder should mirror the `components/terraform/vpc` folder.

- In the component's catalog folder, in the `mixins` sub-folder, add manifests with component configurations for specific environments (organizations,
  tenants, regions, accounts). For example:

  | File Path                                      | Description                                                |
  | ----------------------------------------------| ----------------------------------------------------------- |
  | `stacks/catalog/vpc/mixins/defaults.yaml`      | Component manifest with default values                     |
  | `stacks/catalog/vpc/mixins/dev.yaml`           | Component manifest with settings for `dev` account         |
  | `stacks/catalog/vpc/mixins/prod.yaml`          | Component manifest with settings for `prod` account        |
  | `stacks/catalog/vpc/mixins/staging.yaml`       | Component manifest with settings for `staging` account     |
  | `stacks/catalog/vpc/mixins/ue2.yaml`           | Component manifest with settings for `us-east-2` region    |
  | `stacks/catalog/vpc/mixins/uw2.yaml`           | Component manifest with settings for `us-west-2` region    |
  | `stacks/catalog/vpc/mixins/core.yaml`          | Component manifest with settings for `core` tenant         |
  | `stacks/catalog/vpc/mixins/plat.yaml`          | Component manifest with settings for `plat` tenant         |
  | `stacks/catalog/vpc/mixins/org1.yaml`          | Component manifest with settings for `org1` organization   |
  | `stacks/catalog/vpc/mixins/org2.yaml`          | Component manifest with settings for `org2` organization   |

  :::note
  Having the environment-specific manifests in the component's catalog makes the most sense for multi-Org, multi-OU and/or
  multi-region architectures, such that there will be multiple dev/staging/prod or region configurations, which get imported
  into multiple Org/OU top-level stack manifests.
  :::

- In the component's catalog folder, add manifests for specific environments by assembling the corresponding mixins together (using imports). For
  example:

  | File Path                                       | Description                                                                          |
  | ----------------------------------------------- | ------------------------------------------------------------------------------------ |
  | `stacks/catalog/vpc/org1-plat-ue2-dev.yaml`     | Manifest for the `org1` organization, `plat` tenant, `ue2` region, `dev` account     |
  | `stacks/catalog/vpc/org1-plat-ue2-prod.yaml`    | Manifest for the `org1` organization, `plat` tenant, `ue2` region, `prod` account    |
  | `stacks/catalog/vpc/org1-plat-ue2-staging.yaml` | Manifest for the `org1` organization, `plat` tenant, `ue2` region, `staging` account |
  | `stacks/catalog/vpc/org1-plat-uw2-dev.yaml`     | Manifest for the `org1` organization, `plat` tenant, `uw2` region, `dev` account     |
  | `stacks/catalog/vpc/org1-plat-uw2-prod.yaml`    | Manifest for the `org1` organization, `plat` tenant, `uw2` region, `prod` account    |
  | `stacks/catalog/vpc/org1-plat-uw2-staging.yaml` | Manifest for the `org1` organization, `plat` tenant, `uw2` region, `staging` account |
  | `stacks/catalog/vpc/org2-plat-ue2-dev.yaml`     | Manifest for the `org2` organization, `plat` tenant, `ue2` region, `dev` account     |
  | `stacks/catalog/vpc/org2-plat-ue2-prod.yaml`    | Manifest for the `org2` organization, `plat` tenant, `ue2` region, `prod` account    |
  | `stacks/catalog/vpc/org2-plat-ue2-staging.yaml` | Manifest for the `org2` organization, `plat` tenant, `ue2` region, `staging` account |
  | `stacks/catalog/vpc/org2-plat-uw2-dev.yaml`     | Manifest for the `org2` organization, `plat` tenant, `uw2` region, `dev` account     |
  | `stacks/catalog/vpc/org2-plat-uw2-prod.yaml`    | Manifest for the `org2` organization, `plat` tenant, `uw2` region, `prod` account    |
  | `stacks/catalog/vpc/org2-plat-uw2-staging.yaml` | Manifest for the `org2` organization, `plat` tenant, `uw2` region, `staging` account |

- Import the environment manifests into the top-level stacks. For example:

  | Action                                                               | Top-Level Stack                                     |
  | -------------------------------------------------------------------- | --------------------------------------------------- |
  | Import the `stacks/catalog/vpc/org1-plat-ue2-dev.yaml` manifest      | `stacks/orgs/org1/plat/dev/us-east-2.yaml`          |
  | Import the `stacks/catalog/vpc/org1-plat-ue2-prod.yaml` manifest     | `stacks/orgs/org1/plat/prod/us-east-2.yaml`         |
  | Import the `stacks/catalog/vpc/org1-plat-uw2-staging.yaml` manifest  | `stacks/orgs/org1/plat/staging/us-west-2.yaml`      |
  | Import the `stacks/catalog/vpc/org2-plat-ue2-dev.yaml` manifest      | `stacks/orgs/org2/plat/dev/us-east-2.yaml`          |
  | etc.                                                                 |                                                     |

## Example

The following example shows the Atmos stack and component configurations to provision the `vpc` component into
a multi-org, multi-tenant, multi-account, multi-region environment. The component's configuration for each organization, tenant, account and region
are defined as mixins in the component's catalog. The mixins then combined into the environment manifests, and the environment manifests are imported
into the top-level Atmos stacks.

```console
   │   # Centralized stacks configuration (stack manifests)
   ├── stacks
   │   └── catalog  # component-specific defaults
   │       └── vpc
   │           ├── mixins
   │           │   ├── defaults.yaml
   │           │   ├── dev.yaml
   │           │   ├── prod.yaml
   │           │   ├── staging.yaml
   │           │   ├── ue2.yaml
   │           │   ├── uw2.yaml
   │           │   ├── core.yaml
   │           │   ├── plat.yaml
   │           │   ├── org1.yaml
   │           │   └── org2.yaml
   │           ├── org1-plat-ue2-dev.yaml
   │           ├── org1-plat-ue2-prod.yaml
   │           ├── org1-plat-ue2-staging.yaml
   │           ├── org1-plat-uw2-dev.yaml
   │           ├── org1-plat-uw2-prod.yaml
   │           ├── org1-plat-uw2-staging.yaml
   │           ├── org2-plat-ue2-dev.yaml
   │           ├── org2-plat-ue2-prod.yaml
   │           ├── org2-plat-ue2-staging.yaml
   │           ├── org2-plat-uw2-dev.yaml
   │           ├── org2-plat-uw2-prod.yaml
   │           └── org2-plat-uw2-staging.yaml
   │   # Centralized components configuration
   └── components
       └── terraform  # Terraform components (a.k.a Terraform "root" modules)
           └── vpc
```

Add the following minimal configuration to `atmos.yaml` [CLI config file](/cli/configuration) :

```yaml title="atmos.yaml"
components:
  terraform:
    base_path: "components/terraform"

stacks:
  base_path: "stacks"
  included_paths:
    # Tell Atmos to search for the top-level stack manifests in the `orgs` folder and its sub-folders
    - "orgs/**/*"
  excluded_paths:
    # Tell Atmos that all `_defaults.yaml` files are not top-level stack manifests
    - "**/_defaults.yaml"
  # If you are using multiple organizations (namespaces), use the following `name_pattern`:
  name_pattern: "{namespace}-{tenant}-{environment}-{stage}"
  # If you are using a single organization (namespace), use the following `name_pattern`:
  # name_pattern: "{tenant}-{environment}-{stage}"

schemas:
  jsonschema:
    base_path: "stacks/schemas/jsonschema"
  opa:
    base_path: "stacks/schemas/opa"
  atmos:
    manifest: "stacks/schemas/atmos/atmos-manifest/1.0/atmos-manifest.json"
```

Add the following default configuration to the `stacks/catalog/vpc/mixins/defaults.yaml` manifest:

```yaml title="stacks/catalog/vpc/mixins/defaults.yaml"
components:
  terraform:
    vpc:
      metadata:
        # Point to the Terraform component in `components/terraform/vpc`
        component: vpc
      vars:
        enabled: true
        name: "common"
        max_subnet_count: 3
        map_public_ip_on_launch: true
        assign_generated_ipv6_cidr_block: false
        nat_gateway_enabled: true
        nat_instance_enabled: false
        vpc_flow_logs_enabled: true
        vpc_flow_logs_traffic_type: "ALL"
        vpc_flow_logs_log_destination_type: "s3"
        nat_eip_aws_shield_protection_enabled: false
        subnet_type_tag_key: "acme/subnet/type"
        ipv4_primary_cidr_block: 10.9.0.0/18
```

Add the following configuration to the `stacks/catalog/vpc/mixins/ue2.yaml` manifest:

```yaml title="stacks/catalog/vpc/mixins/ue2.yaml"
components:
  terraform:
    vpc:
      vars:
        availability_zones:
          - us-east-2a
          - us-east-2b
          - us-east-2c
```

Add the following configuration to the `stacks/catalog/vpc/mixins/uw2.yaml` manifest:

```yaml title="stacks/catalog/vpc/mixins/uw2.yaml"
components:
  terraform:
    vpc:
      vars:
        availability_zones:
          - us-west-2a
          - us-west-2b
          - us-west-2c
```

Add the following configuration to the `stacks/catalog/vpc/mixins/dev.yaml` manifest:

```yaml title="stacks/catalog/vpc/mixins/dev.yaml"
components:
  terraform:
    vpc:
      vars:
        # Override `ipv4_primary_cidr_block`, `max_subnet_count` and `vpc_flow_logs_enabled` from the defaults
        ipv4_primary_cidr_block: 10.7.0.0/18
        # In `dev`, use only 2 subnets
        max_subnet_count: 2
        # In `dev`, disable the VPC flow logs
        vpc_flow_logs_enabled: false
```

Add the following configuration to the `stacks/catalog/vpc/mixins/prod.yaml` manifest:

```yaml title="stacks/catalog/vpc/mixins/prod.yaml"
components:
  terraform:
    vpc:
      vars:
        # Override `ipv4_primary_cidr_block`, `map_public_ip_on_launch` and `assign_generated_ipv6_cidr_block` from the defaults
        ipv4_primary_cidr_block: 10.8.0.0/18
        # In `prod`, don't map public IPs on launch
        map_public_ip_on_launch: false
        # In `prod`, use IPv6
        assign_generated_ipv6_cidr_block: true
```

Add the following configuration to the `stacks/catalog/vpc/mixins/staging.yaml` manifest:

```yaml title="stacks/catalog/vpc/mixins/staging.yaml"
components:
  terraform:
    vpc:
      vars:
        # Override `ipv4_primary_cidr_block`, `max_subnet_count` and `map_public_ip_on_launch` from the defaults
        ipv4_primary_cidr_block: 10.9.0.0/18
        # In `staging`, use only 2 subnets
        max_subnet_count: 2
        # In `staging`, don't map public IPs on launch
        map_public_ip_on_launch: false
```

Add the following configuration to the `stacks/catalog/vpc/mixins/core.yaml` manifest:

```yaml title="stacks/catalog/vpc/mixins/core.yaml"
components:
  terraform:
    vpc:
      vars:
        # Override `vpc_flow_logs_traffic_type` from the defaults
        # In `core` tenant, set VPC Flow Logs traffic type to `REJECT`
        vpc_flow_logs_traffic_type: "REJECT"
```

Add the following configuration to the `stacks/catalog/vpc/mixins/plat.yaml` manifest:

```yaml title="stacks/catalog/vpc/mixins/plat.yaml"
components:
  terraform:
    vpc:
      vars:
        # Override `nat_eip_aws_shield_protection_enabled` from the defaults
        # In `plat` tenant, enable NAT EIP shield protection
        nat_eip_aws_shield_protection_enabled: true
```

Add the following configuration to the `stacks/catalog/vpc/mixins/org1.yaml` manifest:

```yaml title="stacks/catalog/vpc/mixins/org1.yaml"
components:
  terraform:
    vpc:
      vars:
        # Override `subnet_type_tag_key` from the defaults
        subnet_type_tag_key: "org1/subnet/type"
```

Add the following configuration to the `stacks/catalog/vpc/mixins/org2.yaml` manifest:

```yaml title="stacks/catalog/vpc/mixins/org2.yaml"
components:
  terraform:
    vpc:
      vars:
        # Override `subnet_type_tag_key` from the defaults
        subnet_type_tag_key: "org2/subnet/type"
```

Assemble the `stacks/catalog/vpc/org1-plat-ue2-dev.yaml` environment manifest from the corresponding mixins:

```yaml title="stacks/catalog/vpc/org1-plat-ue2-dev.yaml"
import:
  # The imports are processed in the order they are defined.
  # The next imported manifest will override the configurations from the previously imported manifests
  - catalog/vpc/mixins/defaults
  - catalog/vpc/mixins/org1
  - catalog/vpc/mixins/plat
  - catalog/vpc/mixins/ue2
  - catalog/vpc/mixins/dev
```

Assemble the `stacks/catalog/vpc/org1-plat-ue2-prod.yaml` environment manifest from the corresponding mixins:

```yaml title="stacks/catalog/vpc/org1-plat-ue2-prod.yaml"
import:
  # The imports are processed in the order they are defined.
  # The next imported manifest will override the configurations from the previously imported manifests
  - catalog/vpc/mixins/defaults
  - catalog/vpc/mixins/org1
  - catalog/vpc/mixins/plat
  - catalog/vpc/mixins/ue2
  - catalog/vpc/mixins/prod
```

Assemble the `stacks/catalog/vpc/org1-plat-uw2-staging.yaml` environment manifest from the corresponding mixins:

```yaml title="stacks/catalog/vpc/org1-plat-uw2-staging.yaml"
import:
  # The imports are processed in the order they are defined.
  # The next imported manifest will override the configurations from the previously imported manifests
  - catalog/vpc/mixins/defaults
  - catalog/vpc/mixins/org1
  - catalog/vpc/mixins/plat
  - catalog/vpc/mixins/uw2
  - catalog/vpc/mixins/staging
```

Assemble the `stacks/catalog/vpc/org2-core-ue2-dev.yaml` environment manifest from the corresponding mixins:

```yaml title="stacks/catalog/vpc/org2-core-ue2-dev.yaml"
import:
  # The imports are processed in the order they are defined.
  # The next imported manifest will override the configurations from the previously imported manifests
  - catalog/vpc/mixins/defaults
  - catalog/vpc/mixins/org2
  - catalog/vpc/mixins/core
  - catalog/vpc/mixins/ue2
  - catalog/vpc/mixins/dev
```

Similarly, assemble the mixins for the other environments.

Import the `stacks/catalog/vpc/org1-plat-ue2-dev.yaml` environment manifest into the `stacks/orgs/org1/plat/dev/us-east-2.yaml` top-level stack:

```yaml title="stacks/orgs/org1/plat/dev/us-east-2.yaml"
import:
  - orgs/org1/plat/dev/_defaults
  - mixins/region/us-east-2
  - catalog/vpc/org1-plat-ue2-dev
```

Import the `stacks/catalog/vpc/org1-plat-ue2-prod.yaml` environment manifest into the `stacks/orgs/org1/plat/prod/us-east-2.yaml` top-level stack:

```yaml title="stacks/orgs/org1/plat/prod/us-east-2.yaml"
import:
  - orgs/org1/plat/prod/_defaults
  - mixins/region/us-east-2
  - catalog/vpc/org1-plat-ue2-prod
```

Import the `stacks/catalog/vpc/org1-plat-uw2-staging.yaml` environment manifest into the `stacks/orgs/org1/plat/staging/us-west-2.yaml` top-level
stack:

```yaml title="stacks/orgs/org1/plat/staging/us-west-2.yaml"
import:
  - orgs/org1/plat/staging/_defaults
  - mixins/region/us-west-2
  - catalog/vpc/org1-plat-uw2-staging
```

Import the `stacks/catalog/vpc/org2-core-ue2-dev.yaml` environment manifest into the `stacks/orgs/org2/core/dev/us-east-2.yaml` top-level
stack:

```yaml title="stacks/orgs/org2/core/dev/us-east-2.yaml"
import:
  - orgs/org2/core/dev/_defaults
  - mixins/region/us-east-2
  - catalog/vpc/org2-core-ue2-dev
```

Similarly, import the other environment mixins into the corresponding top-level stacks.

## Limitations

The **Component Catalog with Mixins** pattern has the following limitations and drawbacks:

- The structure described by the pattern can be complex for basic infrastructures, e.g. for a very simple organizational structure (one organization
  and OU), and just a few components deployed into a few accounts and regions

:::note

To address the limitations of the **Component Catalog with Mixins** Design Pattern when you are provisioning a very basic infrastructure, consider
using the following patterns:

- [Inline Component Configuration](/design-patterns/inline-component-configuration)
- [Inline Component Customization](/design-patterns/inline-component-customization)
- [Component Catalog](/design-patterns/component-catalog)

:::

## Related Patterns

- [Component Catalog](/design-patterns/component-catalog)
- [Component Catalog Template](/design-patterns/component-catalog-template)
- [Component Inheritance](/design-patterns/component-inheritance)
- [Inline Component Configuration](/design-patterns/inline-component-configuration)
- [Inline Component Customization](/design-patterns/inline-component-customization)
- [Organizational Structure Configuration](/design-patterns/organizational-structure-configuration)

## References

- [Catalogs](/core-concepts/stacks/catalogs)
- [Mixins](/core-concepts/stacks/inheritance/mixins)

---

## Component Catalog

import File from '@site/src/components/File'
import Intro from '@site/src/components/Intro'
import PillBox from '@site/src/components/PillBox'

<PillBox>Atmos Design Pattern</PillBox>
<Intro>
The **Component Catalog** is a powerful pattern for organizing and managing the configuration of components across multiple stacks, enabling easy reuse of default configurations. Use this pattern to maintain a DRY (Don't Repeat Yourself) configuration approach, ensuring consistent and efficient deployments across various environments and regions.
</Intro>

## Use-cases

Use the **Component Catalog** pattern when:

- You have many components that are provisioned in multiple stacks (many OUs, accounts, regions) with different configurations for each stack

- You need to make the components' default configurations reusable across different stacks

- You want the component catalog folders structures to mirror the Terraform components folder structure to make it easy to find and manage

- You want to keep the configurations [DRY](https://en.wikipedia.org/wiki/Don%27t_repeat_yourself)

## Benefits

The **Component Catalog** pattern provides the following benefits:

- The defaults for the components are defined in just one place (in the catalog) making the entire
  configuration [DRY](https://en.wikipedia.org/wiki/Don%27t_repeat_yourself)

- The defaults for the components are reusable across many environments by using hierarchical [imports](/core-concepts/stacks/imports)

- It's easy to add a new manifest in the component's catalog to enable a new component's feature, then import the manifest into the corresponding
  stacks where the feature is required

## Design Pattern

The **Component Catalog** pattern prescribes the following:

- For each Terraform component, create a folder with the same name in `stacks/catalog` to make it symmetrical and easy to find.
  For example, the `stacks/catalog/vpc` folder should mirror the `components/terraform/vpc` folder.

- In the component's catalog folder, create `defaults.yaml` manifest with all the default values for the component (the defaults that can be reused
  across multiple environments). Define all the required Atmos sections, e.g. `metadata`, `settings`, `vars`, `env`.

- In the component's catalog folder, add other manifests for different combinations of component configurations.
  We refer to them as archetype manifests. Each archetype can import the `defaults.yaml` file to reuse the default values and make the entire config
  DRY. For example:

  - `stacks/catalog/vpc/disabled.yaml` - component manifest with the component disabled (`vars.enabled: false`)
  - `stacks/catalog/vpc/dev.yaml` - component manifest with the settings related to the `dev` account
  - `stacks/catalog/vpc/staging.yaml` - component manifest with the settings related to the `staging` account
  - `stacks/catalog/vpc/prod.yaml` - component manifest with the settings related to the `prod` account
  - `stacks/catalog/vpc/ue2.yaml` - component manifest with the settings for `us-east-2` region
  - `stacks/catalog/vpc/uw2.yaml` - component manifest with the settings for `us-west-2` region
  - `stacks/catalog/vpc/feature-1.yaml` - component manifest with `feature-1` setting enabled

:::note
Having the `dev`, `staging`, `prod`, `ue2` and `uw2` manifests in the component's catalog makes the most sense for multi-org, multi-OU and/or
multi-region architectures, such that there will be multiple dev/staging/prod or region configurations, which get imported into multiple Org/OU
top-level stack manifests.
:::

- After we have defined the manifests for different use-cases, we import them into different top-level stacks depending on a particular use-case.
  For example:

  - import the `catalog/vpc/ue2.yaml` manifest into the `stacks/mixins/region/us-east-2.yaml` mixin since we need the `vpc`
    component with the `us-east-2` region-related config provisioned in the region
  - import the `catalog/vpc/uw2.yaml` manifest into the `stacks/mixins/region/us-west-2.yaml` mixin since we need the `vpc`
    component with the `us-west-2` region-related config provisioned in the region
  - import the `catalog/vpc/dev.yaml` manifest into the `stacks/orgs/acme/plat/dev/us-east-2.yaml` top-level stack since we need the `vpc`
    component with the dev-related config provisioned in the stack
  - import the `catalog/vpc/prod.yaml` manifest into the `stacks/orgs/acme/plat/prod/us-east-2.yaml` top-level stack since we need the `vpc`
    component with the prod-related config provisioned in the stack
  - import the `catalog/vpc/staging.yaml` manifest into the `stacks/orgs/acme/plat/staging/us-east-2.yaml` top-level stack since we need the `vpc`
    component with the dev-related config provisioned in the stack
  - import the `catalog/vpc/disabled.yaml` manifest into a stack where we want the `vpc` component to be disabled (e.g. temporarily until it's needed)
  - etc.

## Example

The following example shows the Atmos stack and component configurations to provision the `vpc` and `vpc-flow-logs-bucket` components into
a multi-account, multi-region environment. The components' configurations for each account and region are defined in the components' catalog.

```console
   │   # Centralized stacks configuration (stack manifests)
   ├── stacks
   │   └── catalog  # component-specific defaults
   │       ├── vpc
   │       │   ├── defaults.yaml
   │       │   ├── disabled.yaml
   │       │   ├── dev.yaml
   │       │   ├── prod.yaml
   │       │   ├── staging.yaml
   │       │   ├── ue2.yaml
   │       │   └── uw2.yaml
   │       └── vpc-flow-logs-bucket
   │           ├── defaults.yaml
   │           └── disabled.yaml
   │   # Centralized components configuration
   └── components
       └── terraform  # Terraform components (a.k.a Terraform "root" modules)
           ├── vpc
           └── vpc-flow-logs-bucket
```

Add the following minimal configuration to `atmos.yaml` [CLI config file](/cli/configuration) :

```yaml title="atmos.yaml"
components:
  terraform:
    base_path: "components/terraform"

stacks:
  base_path: "stacks"
  name_pattern: "{tenant}-{environment}-{stage}"
  included_paths:
    # Tell Atmos to search for the top-level stack manifests in the `orgs` folder and its sub-folders
    - "orgs/**/*"
  excluded_paths:
    # Tell Atmos that the `defaults` folder and all sub-folders don't contain top-level stack manifests
    - "defaults/**/*"

schemas:
  jsonschema:
    base_path: "stacks/schemas/jsonschema"
  opa:
    base_path: "stacks/schemas/opa"
  atmos:
    manifest: "stacks/schemas/atmos/atmos-manifest/1.0/atmos-manifest.json"
```

Add the following configuration to the `stacks/catalog/vpc-flow-logs-bucket/defaults.yaml` manifest:

```yaml title="stacks/catalog/vpc-flow-logs-bucket/defaults.yaml"
components:
  terraform:
    vpc-flow-logs-bucket:
      metadata:
        # Point to the Terraform component
        component: vpc-flow-logs-bucket
      vars:
        enabled: true
        name: "vpc-flow-logs"
        traffic_type: "ALL"
        force_destroy: true
        lifecycle_rule_enabled: false
```

Add the following default configuration to the `stacks/catalog/vpc/defaults.yaml` manifest:

```yaml title="stacks/catalog/vpc/defaults.yaml"
components:
  terraform:
    vpc:
      metadata:
        # Point to the Terraform component
        component: vpc
      settings:
        # All validation steps must succeed to allow the component to be provisioned
        validation:
          validate-vpc-component-with-jsonschema:
            schema_type: jsonschema
            schema_path: "vpc/validate-vpc-component.json"
            description: Validate 'vpc' component variables using JSON Schema
          check-vpc-component-config-with-opa-policy:
            schema_type: opa
            schema_path: "vpc/validate-vpc-component.rego"
            module_paths:
              - "catalog/constants"
            description: Check 'vpc' component configuration using OPA policy
      vars:
        enabled: true
        name: "common"
        max_subnet_count: 3
        map_public_ip_on_launch: true
        assign_generated_ipv6_cidr_block: false
        nat_gateway_enabled: true
        nat_instance_enabled: false
        vpc_flow_logs_enabled: true
        vpc_flow_logs_traffic_type: "ALL"
        vpc_flow_logs_log_destination_type: "s3"
        nat_eip_aws_shield_protection_enabled: false
        subnet_type_tag_key: "acme/subnet/type"
        ipv4_primary_cidr_block: 10.9.0.0/18
```

Add the following default configuration to the `stacks/catalog/vpc/ue2.yaml` manifest:

```yaml title="stacks/catalog/vpc/ue2.yaml"
import:
  - catalog/vpc/defaults

components:
  terraform:
    vpc:
      vars:
        availability_zones:
          - us-east-2a
          - us-east-2b
          - us-east-2c
```

Add the following default configuration to the `stacks/catalog/vpc/uw2.yaml` manifest:

```yaml title="stacks/catalog/vpc/uw2.yaml"
import:
  - catalog/vpc/defaults

components:
  terraform:
    vpc:
      vars:
        availability_zones:
          - us-west-2a
          - us-west-2b
          - us-west-2c
```

Add the following default configuration to the `stacks/catalog/vpc/dev.yaml` manifest:

```yaml title="stacks/catalog/vpc/dev.yaml"
components:
  terraform:
    vpc:
      vars:
        ipv4_primary_cidr_block: 10.7.0.0/18
```

Add the following default configuration to the `stacks/catalog/vpc/staging.yaml` manifest:

```yaml title="stacks/catalog/vpc/staging.yaml"
components:
  terraform:
    vpc:
      vars:
        ipv4_primary_cidr_block: 10.9.0.0/18
```

Add the following default configuration to the `stacks/catalog/vpc/prod.yaml` manifest:

```yaml title="stacks/catalog/vpc/prod.yaml"
components:
  terraform:
    vpc:
      vars:
        ipv4_primary_cidr_block: 10.8.0.0/18
        # In `prod`, don't map public IPs on launch
        # Override `map_public_ip_on_launch` from the defaults
        map_public_ip_on_launch: false
```

Import `stacks/catalog/vpc/ue2.yaml` into the `stacks/mixins/region/us-east-2.yaml` manifest:

```yaml title="stacks/mixins/region/us-east-2.yaml"
import:
  # Import the `ue2` manifest with `vpc` configuration for `us-east-2` region
  - catalog/vpc/ue2
  # All accounts (stages) in `us-east-2` region will have the `vpc-flow-logs-bucket` component
  - catalog/vpc-flow-logs-bucket/defaults

vars:
  region: us-east-2
  environment: ue2

# Other defaults for the `us-east-2` region
```

Import `stacks/catalog/vpc/uw2.yaml` into the `stacks/mixins/region/us-west-2.yaml` manifest:

```yaml title="stacks/mixins/region/us-west-2.yaml"
import:
  # Import the `uw2` manifest with `vpc` configuration for `us-west-2` region
  - catalog/vpc/uw2
  # All accounts (stages) in `us-west-2` region will have the `vpc-flow-logs-bucket` component
  - catalog/vpc-flow-logs-bucket/defaults

vars:
  region: us-west-2
  environment: uw2

# Other defaults for the `us-west-2` region
```

Import `mixins/region/us-east-2` and `stacks/catalog/vpc/dev.yaml` into the `stacks/orgs/acme/plat/dev/us-east-2.yaml` top-level stack:

```yaml title="stacks/orgs/acme/plat/dev/us-east-2.yaml"
import:
  - orgs/acme/plat/dev/_defaults
  - mixins/region/us-east-2
  # Override the `vpc` component configuration for `dev` by importing the `catalog/vpc/dev` manifest
  - catalog/vpc/dev
```

Import `mixins/region/us-west-2` and `stacks/catalog/vpc/dev.yaml` into the `stacks/orgs/acme/plat/dev/us-west-2.yaml` top-level stack:

```yaml title="stacks/orgs/acme/plat/dev/us-west-2.yaml"
import:
  - orgs/acme/plat/dev/_defaults
  - mixins/region/us-west-2
  # Override the `vpc` component configuration for `dev` by importing the `catalog/vpc/dev` manifest
  - catalog/vpc/dev
```

Import `mixins/region/us-east-2` and `stacks/catalog/vpc/staging.yaml` into the `stacks/orgs/acme/plat/staging/us-east-2.yaml` top-level stack:

```yaml title="stacks/orgs/acme/plat/staging/us-east-2.yaml"
import:
  - orgs/acme/plat/staging/_defaults
  - mixins/region/us-east-2
  # Override the `vpc` component configuration for `staging` by importing the `catalog/vpc/staging` manifest
  - catalog/vpc/staging
```

Import `mixins/region/us-west-2` and `stacks/catalog/vpc/staging.yaml` into the `stacks/orgs/acme/plat/staging/us-west-2.yaml` top-level stack:

```yaml title="stacks/orgs/acme/plat/staging/us-west-2.yaml"
import:
  - orgs/acme/plat/staging/_defaults
  - mixins/region/us-west-2
  # Override the `vpc` component configuration for `staging` by importing the `catalog/vpc/staging` manifest
  - catalog/vpc/staging
```

Import `mixins/region/us-east-2` and `stacks/catalog/vpc/prod.yaml` into the `stacks/orgs/acme/plat/prod/us-east-2.yaml` top-level stack:

```yaml title="stacks/orgs/acme/plat/prod/us-east-2.yaml"
import:
  - orgs/acme/plat/prod/_defaults
  - mixins/region/us-east-2
  # Override the `vpc` component configuration for `prod` by importing the `catalog/vpc/prod` manifest
  - catalog/vpc/prod
```

Import `mixins/region/us-west-2` and `stacks/catalog/vpc/prod.yaml` into the `stacks/orgs/acme/plat/prod/us-west-2.yaml` top-level stack:

```yaml title="stacks/orgs/acme/plat/prod/us-west-2.yaml"
import:
  - orgs/acme/plat/prod/_defaults
  - mixins/region/us-west-2
  # Override the `vpc` component configuration for `prod` by importing the `catalog/vpc/prod` manifest
  - catalog/vpc/prod
```

## Limitations

The **Component Catalog** pattern has the following limitations and drawbacks:

- Although it's always recommended to use, the catalog structure described by the pattern can be complex for basic infrastructures,
  e.g. for a very simple organizational structure (one organization and OU), and just a few components deployed into a few accounts and regions

:::note

To address the limitations of the **Component Catalog** Design Pattern when you are provisioning a very basic infrastructure, use the following
patterns:

- [Inline Component Configuration](/design-patterns/inline-component-configuration)
- [Inline Component Customization](/design-patterns/inline-component-customization)

:::

## Related Patterns

- [Component Catalog with Mixins](/design-patterns/component-catalog-with-mixins)
- [Component Catalog Template](/design-patterns/component-catalog-template)
- [Component Inheritance](/design-patterns/component-inheritance)
- [Inline Component Configuration](/design-patterns/inline-component-configuration)
- [Inline Component Customization](/design-patterns/inline-component-customization)
- [Organizational Structure Configuration](/design-patterns/organizational-structure-configuration)

## References

- [Catalogs](/core-concepts/stacks/catalogs)
- [Mixins](/core-concepts/stacks/inheritance/mixins)

---

## Component Inheritance

import File from '@site/src/components/File'
import PillBox from '@site/src/components/PillBox'
import Intro from '@site/src/components/Intro'

<PillBox>Atmos Design Pattern</PillBox>
<Intro>
The **Component Inheritance** Design Pattern describes the mechanism of deriving Atmos components from one or more base components, allowing reusing the base components' configurations.
</Intro>

In Atmos, **Component Inheritance** is the mechanism of deriving a component from one or more base components, inheriting all the
properties of the base component(s) and overriding only some fields specific to the derived component. The derived component acquires all the
properties of the base component(s), allowing creating very DRY configurations that are built upon existing components.

:::note
Atmos supports many different types on component inheritance. Refer to [Component Inheritance](/core-concepts/stacks/inheritance) for
more details.
:::

## Use-cases

Use the **Component Inheritance** pattern when:

- You need to have reusable base components that serve as blueprints for the derived Atmos components

- You need to keep the configuration of all components [DRY](https://en.wikipedia.org/wiki/Don%27t_repeat_yourself)

## Benefits

The **Component Inheritance** pattern provides the following benefits:

- Enables very DRY, consistent, and reusable configurations built upon existing components

- Any Atmos component can serve as a building block for other Atmos components

## Example

The following example shows the Atmos stack and component configurations to provision the `vpc` component into
a multi-account, multi-region environment. In the `catalog/vpc` folder, we have the `defaults.yaml` manifest that configures the base
component `vpc/defaults` to be inherited by all the derived VPC components in all stacks.

```console
   │   # Centralized stacks configuration (stack manifests)
   ├── stacks
   │   ├── catalog  # component-specific defaults
   │   │   └── vpc
   │   │       └── defaults.yaml
   │   ├── mixins
   │   │   ├── tenant  # tenant-specific defaults
   │   │   │   └── plat.yaml
   │   │   ├── region  # region-specific defaults
   │   │   │   ├── us-east-2.yaml
   │   │   │   └── us-west-2.yaml
   │   │   └── stage  # stage-specific defaults
   │   │       ├── dev.yaml
   │   │       ├── staging.yaml
   │   │       └── prod.yaml
   │   └── orgs  # Organizations
   │       └── acme
   │           ├── _defaults.yaml
   │           └── plat  # 'plat' represents the "Platform" OU (a.k.a tenant)
   │               ├── _defaults.yaml
   │               ├── dev
   │               │   ├── _defaults.yaml
   │               │   ├── us-east-2.yaml
   │               │   └── us-west-2.yaml
   │               ├── staging
   │               │   ├── _defaults.yaml
   │               │   ├── us-east-2.yaml
   │               │   └── us-west-2.yaml
   │               └── prod
   │                   ├── _defaults.yaml
   │                   ├── us-east-2.yaml
   │                   └── us-west-2.yaml
   │   # Centralized library of reusable components
   └── components
       └── terraform  # Terraform components (a.k.a. Terraform "root" modules)
           └── vpc
```

Add the following minimal configuration to `atmos.yaml` [CLI config file](/cli/configuration) :

```yaml title="atmos.yaml"
components:
  terraform:
    base_path: "components/terraform"

stacks:
  base_path: "stacks"
  name_pattern: "{tenant}-{environment}-{stage}"
  included_paths:
    # Tell Atmos to search for the top-level stack manifests in the `orgs` folder and its sub-folders
    - "orgs/**/*"
  excluded_paths:
    # Tell Atmos that the `defaults` folder and all sub-folders don't contain top-level stack manifests
    - "defaults/**/*"

schemas:
  jsonschema:
    base_path: "stacks/schemas/jsonschema"
  opa:
    base_path: "stacks/schemas/opa"
  atmos:
    manifest: "stacks/schemas/atmos/atmos-manifest/1.0/atmos-manifest.json"
```

Add the following configuration for the base component `vpc/defaults` to the `stacks/catalog/vpc/defaults.yaml` manifest:

```yaml title="stacks/catalog/vpc/defaults.yaml"
components:
  terraform:
    vpc/defaults:
      settings:
        # All validation steps must succeed to allow the component to be provisioned
        validation:
          validate-vpc-component-with-jsonschema:
            schema_type: jsonschema
            schema_path: "vpc/validate-vpc-component.json"
            description: Validate 'vpc' component variables using JSON Schema
          check-vpc-component-config-with-opa-policy:
            schema_type: opa
            schema_path: "vpc/validate-vpc-component.rego"
            module_paths:
              - "catalog/constants"
            description: Check 'vpc' component configuration using OPA policy
      vars:
        enabled: true
        name: "common"
        max_subnet_count: 3
        map_public_ip_on_launch: true
        assign_generated_ipv6_cidr_block: false
        nat_gateway_enabled: true
        nat_instance_enabled: false
        vpc_flow_logs_enabled: true
        vpc_flow_logs_traffic_type: "ALL"
        vpc_flow_logs_log_destination_type: "s3"
        nat_eip_aws_shield_protection_enabled: false
        subnet_type_tag_key: "acme/subnet/type"
        ipv4_primary_cidr_block: 10.0.0.0/18
```

Configure the `vpc` Atmos component in the `stacks/orgs/acme/plat/prod/us-east-2.yaml` top-level stack. The `vpc` component inherits from
the `vpc/defaults` base component:

```yaml title="stacks/orgs/acme/plat/prod/us-east-2.yaml"
import:
  import:
    - orgs/acme/plat/prod/_defaults
    - mixins/region/us-east-2
    # Import the `vpc/defaults` component from the `catalog/vpc/defaults.yaml` manifest
    - catalog/vpc/defaults

components:
  terraform:
    # Atmos component `vpc`
    vpc:
      metadata:
        # Point to the Terraform component in `components/terraform/vpc`
        component: vpc
        # Inherit from the `vpc/defaults` Atmos base component
        # This is Single Inheritance: the Atmos component inherits from one base Atmos component
        inherits:
          - vpc/defaults
      # Define/override variables specific to this `vpc` component
      vars:
        name: my-vpc
        vpc_flow_logs_enabled: false
        ipv4_primary_cidr_block: 10.9.0.0/18
```

## Related Patterns

- [Abstract Component](/design-patterns/abstract-component)
- [Multiple Component Instances](/design-patterns/multiple-component-instances)
- [Component Catalog](/design-patterns/component-catalog)
- [Component Catalog with Mixins](/design-patterns/component-catalog-with-mixins)
- [Component Catalog Template](/design-patterns/component-catalog-template)
- [Inline Component Configuration](/design-patterns/inline-component-configuration)
- [Inline Component Customization](/design-patterns/inline-component-customization)

## References

- [Component-Oriented Programming](/core-concepts/stacks/inheritance)
- [Component Inheritance](/core-concepts/stacks/inheritance)

---

## Component Overrides

import File from '@site/src/components/File'
import PillBox from '@site/src/components/PillBox'
import Intro from '@site/src/components/Intro'

<PillBox>Atmos Design Pattern</PillBox>

<Intro>
The **Component Overrides** Design Pattern describes the mechanism of modifying and overriding the configuration and behavior of groups of Atmos components in the current scope. This is achieved by using the `overrides` section in Atmos stack manifests.
</Intro>

:::tip
Refer to [Component Overrides](/core-concepts/stacks/overrides) for more information on the `overrides` section
:::

## Use-cases

Use the **Component Overrides** pattern when:

- You need to modify or override the configuration and behavior of groups of Atmos components. It is handy to "override" settings when dealing with multiple levels of inheritance.

- Different people or teams need to manage groups of Atmos components

- You want to keep the configurations of the groups of Atmos components [DRY](https://en.wikipedia.org/wiki/Don%27t_repeat_yourself)

# Benefits

The **Component Overrides** pattern provides the following benefits:

- Allows to modify or override the configuration and behavior of groups of Atmos components without affecting other groups of Atmos components

- Makes the configurations of groups of Atmos components more straightforward to understand and [DRY](https://en.wikipedia.org/wiki/Don%27t_repeat_yourself)

## Example

:::note
The **Component Overrides** Design Pattern can be applied to the configuration structures described by
the [Partial Stack Configuration](/design-patterns/partial-stack-configuration)
and [Layered Stack Configuration](/design-patterns/layered-stack-configuration)
Atmos Design Patterns.

In this example, we'll use the structure described by the [Layered Stack Configuration](/design-patterns/layered-stack-configuration)
Design Pattern.
:::

In the following structure, we have many different Terraform components (Terraform root modules) in the `components/terraform` folder.

In the `stacks/catalog` folder, we define the defaults for each component using the [Component Catalog](/design-patterns/component-catalog) Atmos
Design Pattern.

In the `stacks/layers` folder, we define the following layers (groups of components), and import the related components into the layer manifests:

- `load-balancers.yaml`
- `data.yaml`
- `dns.yaml`
- `logs.yaml`
- `notifications.yaml`
- `firewalls.yaml`
- `networking.yaml`
- `eks.yaml`

We use the `terraform.overrides` section in each layer manifest to override the configurations of all the components in the layer (all Terraform
components in the layer will get the `Layer` and `Team` tags).

Finally, we import all the layer manifests into the top-level stacks.

```console
   │   # Centralized stacks configuration (stack manifests)
   ├── stacks
   │   ├── catalog  # component-specific defaults
   │   │   ├── alb
   │   │   │   └── defaults.yaml
   │   │   ├── aurora-postgres
   │   │   │   └── defaults.yaml
   │   │   ├── dns
   │   │   │   └── defaults.yaml
   │   │   ├── eks
   │   │   │   └── defaults.yaml
   │   │   ├── efs
   │   │   │   └── defaults.yaml
   │   │   ├── msk
   │   │   │   └── defaults.yaml
   │   │   ├── ses
   │   │   │   └── defaults.yaml
   │   │   ├── sns-topic
   │   │   │   └── defaults.yaml
   │   │   ├── network-firewall
   │   │   │   └── defaults.yaml
   │   │   ├── network-firewall-logs-bucket
   │   │   │   └── defaults.yaml
   │   │   ├── waf
   │   │   │   └── defaults.yaml
   │   │   ├── vpc
   │   │   │   └── defaults.yaml
   │   │   └── vpc-flow-logs-bucket
   │   │       └── defaults.yaml
   │   ├── layers  # grouping of components by category/function
   │   │   ├── load-balancers.yaml
   │   │   ├── data.yaml
   │   │   ├── dns.yaml
   │   │   ├── logs.yaml
   │   │   ├── notifications.yaml
   │   │   ├── firewalls.yaml
   │   │   ├── networking.yaml
   │   │   └── eks.yaml
   │   ├── mixins
   │   │   ├── tenant  # tenant-specific defaults
   │   │   │   └── plat.yaml
   │   │   ├── region  # region-specific defaults
   │   │   │   ├── us-east-2.yaml
   │   │   │   └── us-west-2.yaml
   │   │   └── stage  # stage-specific defaults
   │   │       ├── dev.yaml
   │   │       ├── staging.yaml
   │   │       └── prod.yaml
   │   └── orgs  # Organizations
   │       └── acme
   │           ├── _defaults.yaml
   │           └── plat  # 'plat' represents the "Platform" OU (a.k.a tenant)
   │               ├── _defaults.yaml
   │               ├── dev
   │               │   ├── _defaults.yaml
   │               │   ├── global-region.yaml
   │               │   ├── us-east-2.yaml
   │               │   └── us-west-2.yaml
   │               ├── staging
   │               │   ├── _defaults.yaml
   │               │   ├── global-region.yaml
   │               │   ├── us-east-2.yaml
   │               │   └── us-west-2.yaml
   │               └── prod
   │                   ├── _defaults.yaml
   │                   ├── global-region.yaml
   │                   ├── us-east-2.yaml
   │                   └── us-west-2.yaml
   │   # Centralized components configuration
   └── components
       └── terraform  # Terraform components (a.k.a Terraform "root" modules)
           ├── alb
           ├── aurora-postgres
           ├── dns
           ├── eks
           ├── efs
           ├── msk
           ├── ses
           ├── sns-topic
           ├── network-firewall
           ├── network-firewall-logs-bucket
           ├── waf
           ├── vpc
           └── vpc-flow-logs-bucket
```

Add the following minimal configuration to `atmos.yaml` [CLI config file](/cli/configuration) :

```yaml title="atmos.yaml"
components:
  terraform:
    base_path: "components/terraform"

stacks:
  base_path: "stacks"
  name_pattern: "{tenant}-{environment}-{stage}"
  included_paths:
    # Tell Atmos to search for the top-level stack manifests in the `orgs` folder and its sub-folders
    - "orgs/**/*"
  excluded_paths:
    # Tell Atmos that the `defaults` folder and all sub-folders don't contain top-level stack manifests
    - "defaults/**/*"

schemas:
  jsonschema:
    base_path: "stacks/schemas/jsonschema"
  opa:
    base_path: "stacks/schemas/opa"
  atmos:
    manifest: "stacks/schemas/atmos/atmos-manifest/1.0/atmos-manifest.json"
```

Add the following configuration to the `stacks/layers/load-balancers.yaml` layer manifest:

```yaml title="stacks/layers/load-balancers.yaml"
import:
  # Import the related component manifests into this layer manifest
  - catalog/alb/defaults
  # Import other Load Balancer components

# Override the configurations of all the components in this layer
# All Terraform components in this layer will get the 'Layer' and 'Team' tags
terraform:
  overrides:
    vars:
      tags:
        Layer: load-balancers
        Team: Load balancer managers
```

Add the following configuration to the `stacks/layers/data.yaml` layer manifest:

```yaml title="stacks/layers/data.yaml"
import:
  # Import the related component manifests into this layer manifest
  - catalog/aurora-postgres/defaults
  - catalog/msk/defaults
  - catalog/efs/defaults
  # Import other Data components

# Override the configurations of all the components in this layer
# All Terraform components in this layer will get the 'Layer' and 'Team' tags
terraform:
  overrides:
    vars:
      tags:
        Layer: data
        Team: Data managers
```

Add the following configuration to the `stacks/layers/dns.yaml` layer manifest:

```yaml title="stacks/layers/dns.yaml"
import:
  # Import the related component manifests into this layer manifest
  - catalog/dns/defaults
  # Import other DNS components

# Override the configurations of all the components in this layer
# All Terraform components in this layer will get the 'Layer' and 'Team' tags
terraform:
  overrides:
    vars:
      tags:
        Layer: dns
        Team: DNS managers
```

Add the following configuration to the `stacks/layers/logs.yaml` layer manifest:

```yaml title="stacks/layers/logs.yaml"
import:
  # Import the related component manifests into this layer manifest
  - catalog/network-firewall-logs-bucket/defaults
  - catalog/vpc-flow-logs-bucket/defaults
  # Import other Logs components

# Override the configurations of all the components in this layer
# All Terraform components in this layer will get the 'Layer' and 'Team' tags
terraform:
  overrides:
    vars:
      tags:
        Layer: logs
        Team: Log managers
```

Add the following configuration to the `stacks/layers/notifications.yaml` layer manifest:

```yaml title="stacks/layers/notifications.yaml"
import:
  # Import the related component manifests into this layer manifest
  - catalog/ses/defaults
  - catalog/sns-topic/defaults
  # Import other Notification components

# Override the configurations of all the components in this layer
# All Terraform components in this layer will get the 'Layer' and 'Team' tags
terraform:
  overrides:
    vars:
      tags:
        Layer: notifications
        Team: Notification managers
```

Add the following configuration to the `stacks/layers/firewalls.yaml` layer manifest:

```yaml title="stacks/layers/firewalls.yaml"
import:
  # Import the related component manifests into this layer manifest
  - catalog/network-firewall/defaults
  - catalog/waf/defaults
  # Import other Firewall components

# Override the configurations of all the components in this layer
# All Terraform components in this layer will get the 'Layer' and 'Team' tags
terraform:
  overrides:
    vars:
      tags:
        Layer: firewalls
        Team: Firewall managers
```

Add the following configuration to the `stacks/layers/networking.yaml` layer manifest:

```yaml title="stacks/layers/networking.yaml"
import:
  # Import the related component manifests into this layer manifest
  - catalog/vpc/defaults
  # Import other Networking components

# Override the configurations of all the components in this layer
# All Terraform components in this layer will get the 'Layer' and 'Team' tags
terraform:
  overrides:
    vars:
      tags:
        Layer: networking
        Team: Networking managers
```

Add the following configuration to the `stacks/layers/eks.yaml` layer manifest:

```yaml title="stacks/layers/eks.yaml"
import:
  # Import the related component manifests into this layer manifest
  - catalog/eks/defaults

# Override the configurations of all the components in this layer
# All Terraform components in this layer will get the 'Layer' and 'Team' tags
terraform:
  overrides:
    vars:
      tags:
        Layer: eks
        Team: EKS cluster managers
```

Import the required layers into the `stacks/orgs/acme/plat/dev/us-east-2.yaml` top-level stack manifest:

```yaml title="stacks/orgs/acme/plat/dev/us-east-2.yaml"
import:
  # The `orgs/acme/plat/dev/_defaults` and `mixins/region/us-east-2` manifests
  # define the top-level Atmos stack `plat-ue2-dev`
  - orgs/acme/plat/dev/_defaults
  - mixins/region/us-east-2
  # Import the layers (groups of components)
  - layers/load-balancers
  - layers/data
  - layers/dns
  - layers/logs
  - layers/notifications
  - layers/firewalls
  - layers/networking
  - layers/eks
```

Import the required layers into the `stacks/orgs/acme/plat/dev/us-west-2.yaml` top-level stack manifest:

```yaml title="stacks/orgs/acme/plat/dev/us-west-2.yaml"
import:
  # The `orgs/acme/plat/dev/_defaults` and `mixins/region/us-west-2` manifests
  # define the top-level Atmos stack `plat-uw2-dev`
  - orgs/acme/plat/dev/_defaults
  - mixins/region/us-west-2
  # Import the layers (groups of components)
  - layers/load-balancers
  - layers/data
  - layers/dns
  - layers/logs
  - layers/notifications
  - layers/firewalls
  - layers/networking
  - layers/eks
```

Import the required layers into the `stacks/orgs/acme/plat/prod/us-east-2.yaml` top-level stack manifest:

```yaml title="stacks/orgs/acme/plat/prod/us-east-2.yaml"
import:
  # The `orgs/acme/plat/prod/_defaults` and `mixins/region/us-east-2` manifests
  # define the top-level Atmos stack `plat-ue2-prod`
  - orgs/acme/plat/prod/_defaults
  - mixins/region/us-east-2
  # Import the layers (groups of components)
  - layers/load-balancers
  - layers/data
  - layers/dns
  - layers/logs
  - layers/notifications
  - layers/firewalls
  - layers/networking
  - layers/eks
```

Similarly, import the required layers into the other top-level stacks for the other organizations, OUs/tenants, accounts and regions.
Make sure to import only the layers that define the component that need to be provisioned in the stacks.

After the Atmos components are provisioned in the top-level stacks, all Terraform components will get the `Layer` and `Team` tags from the
corresponding layers.

## Related Patterns

- [Organizational Structure Configuration](/design-patterns/organizational-structure-configuration)
- [Partial Stack Configuration](/design-patterns/partial-stack-configuration)
- [Layered Stack Configuration](/design-patterns/layered-stack-configuration)
- [Component Catalog](/design-patterns/component-catalog)
- [Component Catalog with Mixins](/design-patterns/component-catalog-with-mixins)

## References

- [Catalogs](/core-concepts/stacks/catalogs)
- [Mixins](/core-concepts/stacks/inheritance/mixins)

---

## The _defaults.yaml Design Pattern

import Intro from '@site/src/components/Intro'
import PillBox from '@site/src/components/PillBox'

<PillBox>Atmos Design Pattern</PillBox>

<Intro>
The `_defaults.yaml` pattern is a **naming convention** (not an Atmos feature) for organizing hierarchical configuration defaults. Atmos has no special knowledge of these files—they are treated like any other YAML file that can be imported.
</Intro>

## What

The `_defaults.yaml` pattern is a naming convention used throughout Atmos configurations to organize default settings at different levels of your infrastructure hierarchy. This is purely a convention adopted by teams for better organization—Atmos itself has no built-in knowledge or special handling of files named `_defaults.yaml`.

## Why We Use This Convention

### 1. Lexicographic Sorting
The underscore prefix (`_`) ensures these files appear at the top of directory listings, making them immediately visible when browsing the filesystem. This saves time when navigating complex stack structures.

### 2. Visual Distinction
The naming pattern immediately signals to developers: "this file contains defaults, not a stack definition." This clear visual cue helps prevent confusion between actual stack configurations and their default settings.

### 3. Automatic Exclusion from Stack Discovery
By convention, we configure Atmos to exclude `**/_defaults.yaml` from stack discovery in `atmos.yaml`:

```yaml
stacks:
  excluded_paths:
    - "**/_defaults.yaml"
```

This prevents these files from being mistakenly processed as stacks when Atmos discovers stack configurations.

### 4. Explicit Import Control
Since they're excluded from discovery, these files must be explicitly imported, giving developers precise control over inheritance chains. This explicitness helps prevent unexpected configuration inheritance and makes the configuration flow easier to trace.

## How It Works

### Important: Not Automatic!

:::warning
Unlike some configuration systems, Atmos does **NOT** automatically import `_defaults.yaml` files. Each must be explicitly imported where needed. This is intentional—it provides explicit control over configuration inheritance.
:::

### Import Path Resolution

Atmos supports two types of import paths:

**Base-Relative Paths** (most common):
- Resolved relative to `stacks.base_path` configured in `atmos.yaml`
- Examples: `orgs/acme/_defaults`, `catalog/vpc/defaults`

**File-Relative Paths**:
- Start with `./` or `../`
- Resolved relative to the importing file's directory
- Examples: `./_defaults`, `../_defaults`

### Typical Usage Pattern

```console
stacks/
├── orgs/
│   └── acme/
│       ├── _defaults.yaml          # Organization defaults (NOT auto-imported)
│       ├── core/
│       │   ├── _defaults.yaml      # OU defaults - must import org defaults
│       │   └── prod/
│       │       ├── _defaults.yaml  # Stage defaults - must import OU defaults
│       │       └── us-east-2.yaml  # Stack - must import stage defaults
```

## The Convention in Practice

### Step 1: Create defaults at each level

```yaml title="stacks/orgs/acme/_defaults.yaml"
# Organization-wide defaults
vars:
  namespace: acme
  terraform_version: "1.5.0"

terraform:
  vars:
    tags:
      Organization: acme
      ManagedBy: Atmos
```

### Step 2: Import parent defaults in child defaults

```yaml title="stacks/orgs/acme/core/_defaults.yaml"
import:
  - orgs/acme/_defaults    # Must explicitly import parent defaults

# OU-specific defaults
vars:
  tenant: core

terraform:
  vars:
    tags:
      OrganizationalUnit: core
```

### Step 3: Import in actual stacks

```yaml title="stacks/orgs/acme/core/prod/us-east-2.yaml"
import:
  - orgs/acme/core/prod/_defaults  # Must explicitly import defaults
  - mixins/region/us-east-2

# Stack-specific configuration
terraform:
  vars:
    environment: prod
    region: us-east-2

components:
  terraform:
    vpc:
      vars:
        cidr_block: "10.0.0.0/16"
```

## Import Chain Example

Here's how a typical import chain works with the `_defaults.yaml` convention:

```mermaid
graph TD
    A[us-east-2.yaml] -->|imports| B[prod/_defaults.yaml]
    B -->|imports| C[core/_defaults.yaml]
    C -->|imports| D[acme/_defaults.yaml]

    style A fill:#e1f5fe
    style B fill:#fff3e0
    style C fill:#fff3e0
    style D fill:#fff3e0
```

Each level explicitly imports its parent's defaults, creating a clear inheritance chain.

## Why Not Just Call Them "defaults.yaml"?

Without the underscore prefix:
- Files would be sorted alphabetically mixed with other files
- Harder to distinguish defaults from other configurations at a glance
- Still need to be excluded from stack discovery (just with a different pattern)

The underscore is a small detail that provides significant organizational benefits.

## Alternative Naming Conventions

Teams can choose different conventions if preferred:
- `defaults.yaml` (without underscore)
- `.defaults.yaml` (hidden file on Unix systems)
- `base.yaml`
- `common.yaml`

Just remember to:
1. Update `excluded_paths` in `atmos.yaml` to match your pattern
2. Be consistent across your project
3. Document your choice for your team

## Common Misunderstandings

### ❌ Myth: Atmos automatically processes `_defaults.yaml` files
**Reality**: Atmos has no special knowledge of this naming pattern. These files must be explicitly imported like any other YAML file.

### ❌ Myth: The underscore has special meaning to Atmos
**Reality**: The underscore is purely for lexicographic sorting and visual distinction. Atmos treats `_defaults.yaml` the same as `defaults.yaml` or any other name.

### ❌ Myth: `_defaults.yaml` files are always inherited
**Reality**: Nothing is inherited unless explicitly imported. You have complete control over what gets imported and when.

### ❌ Myth: Files must be named `_defaults.yaml` for inheritance to work
**Reality**: You can import any YAML file. The `_defaults.yaml` name is just a helpful convention.

## Best Practices

### Do's
- **Be Explicit**: Always explicitly import parent defaults
- **Document Import Chains**: Comment your imports to show the inheritance hierarchy
- **Keep It Simple**: Don't create too many levels of defaults (typically 3-4 levels maximum)
- **Stay Consistent**: Use the same naming convention throughout your project
- **Order Imports Logically**: Import parent defaults before child defaults or mixins

### Don'ts
- **Don't Assume Auto-Import**: Never assume `_defaults.yaml` files are automatically imported
- **Don't Create Circular Imports**: Avoid import cycles which will cause errors
- **Don't Override Everything**: Override only what needs to change at each level
- **Don't Mix Conventions**: Pick one naming convention and stick with it

## Example: Multi-Tenant, Multi-Region Setup

Here's a real-world example showing how the convention organizes a complex infrastructure:

```yaml title="stacks/orgs/acme/_defaults.yaml"
# Organization defaults
vars:
  namespace: acme

terraform:
  backend:
    s3:
      bucket: "acme-terraform-state"
      dynamodb_table: "acme-terraform-state-lock"
```

```yaml title="stacks/orgs/acme/plat/_defaults.yaml"
import:
  - orgs/acme/_defaults

vars:
  tenant: platform

terraform:
  vars:
    tags:
      Tenant: platform
      CostCenter: engineering
```

```yaml title="stacks/orgs/acme/plat/dev/_defaults.yaml"
import:
  - orgs/acme/plat/_defaults
  - mixins/stage/dev    # Can also import mixins

vars:
  stage: dev

terraform:
  vars:
    instance_type: t3.small  # Smaller instances for dev
```

```yaml title="stacks/orgs/acme/plat/dev/us-west-2.yaml"
import:
  - orgs/acme/plat/dev/_defaults
  - mixins/region/us-west-2

# This is the actual stack configuration
components:
  terraform:
    vpc:
      vars:
        availability_zones: ["us-west-2a", "us-west-2b"]
```

## Troubleshooting

### My defaults aren't being applied

**Check these common issues:**
1. **Missing Import**: Ensure you've explicitly imported the `_defaults.yaml` file
2. **Wrong Path Style**: Verify you're using the correct path style (base-relative vs file-relative)
3. **Path Typo**: Check for typos in the import path
4. **File Location**: Confirm the `_defaults.yaml` file exists where expected

**Debug with:**
```bash
# Show the final configuration with all imports processed
atmos describe component <component> -s <stack>
```

### I see `_defaults.yaml` in my stack list

This means the exclusion pattern isn't working. Check:

```yaml title="atmos.yaml"
stacks:
  excluded_paths:
    - "**/_defaults.yaml"  # Ensure this pattern is present
```

### Import path not found

If you get an import error, verify:
1. The path is relative to `stacks.base_path` (unless it starts with `./` or `../`)
2. The file extension (`.yaml` is added automatically if omitted)
3. The file actually exists at that location

## Summary

The `_defaults.yaml` pattern is a simple but effective naming convention that helps organize Atmos configurations. Remember:

- It's a **convention**, not an Atmos feature
- Files must be **explicitly imported**
- The underscore provides **lexicographic sorting**
- It creates **visual distinction** in file listings
- You have **complete control** over inheritance

This convention has emerged from real-world usage as a best practice for organizing hierarchical configurations in a clear, maintainable way.

---

## Atmos Design Patterns

import DocCardList from '@theme/DocCardList'
import Intro from '@site/src/components/Intro'
import KeyPoints from '@site/src/components/KeyPoints'

<Intro>
Atmos Design Patterns are a collection of conventions, best practices, concepts, principles and insights derived from our experience building enterprise scale architectures. These are vital for effectively structuring and organizing infrastructures, components, and stacks. They empower implementers and cloud architects alike, by providing access to the collective wisdom of seasoned professionals.
</Intro>

These patterns serve as a guide for creating solutions that are not only reusable but also robust, flexible, and maintainable. Importantly, they foster convergence amongst engineers on a standard set of recognized patterns, effectively putting a name to the practices and strategies that have proven successful. This standardization aids in streamlining communication and understanding among professionals, enhancing collaboration and efficiency in the field.

<KeyPoints>
- How utilizing established patterns can simplify complex system architecture.
- The iterative and challenging process of achieving reusable, adaptable designs.
- Ways expert architects leverage past solutions to enhance efficiency and drive innovation.
</KeyPoints>

## Motivation

The importance of patterns in architecting complex systems has been long recognized across various fields.

A well-architected infrastructure is rich with patterns. Giving careful attention to these patterns during the development of a system can result in an architecture that is not only smaller and simpler but also more comprehensible compared to ignoring these patterns.

Designing for organizational complexity is challenging, and making all the components and configurations reusable is even harder. It involves identifying relevant components and configurations, transforming them into reusable artifacts at the appropriate granularity, establishing inheritance hierarchies, and defining crucial relationships among them. The design must be specific to the current problem while also being sufficiently general to accommodate future challenges and requirements. Additionally, minimizing or avoiding redesign is essential. Seasoned architects emphasize that creating a design that is both reusable and flexible is a daunting task, if not impossible, on the first attempt.

Nevertheless, experienced architects consistently produce effective designs. What do they know that the others don't? They possess insights that less-experienced architects may lack. One key aspect of their expertise is the avoidance of solving every problem from scratch. Instead, they leverage solutions that have proven successful in the past, applying them repeatedly. This accumulated experience is a crucial factor that distinguishes them as experts. Recurrent patterns are evident in well-designed systems, offering solutions to specific design challenges and enhancing the flexibility, elegance, and reusability of infrastructure configurations. These patterns enable architects to build upon successful designs by drawing on prior experience. An architect who is familiar with such patterns can immediately apply them to design problems without the need to rediscover them.

## What is a Design Pattern?

The well-known architect and design theorist [Christopher Alexander](https://en.wikipedia.org/wiki/Christopher_Alexander) said:

> <q>Each pattern describes a problem which occurs over and over again in our environment, and then describes the core of the solution
> to that problem, in such a way that you can use this solution a million times over, without ever doing it the same way twice. — **Christopher Alexander**</q>

Even though Christopher Alexander was describing the patterns in construction and buildings, what he said is true about DevOps design patterns.

:::tip Check Out Best Practices
Also, checkout our [best practices](/best-practices) for more insights on how to design and manage your infrastructure.
:::

## What are Atmos Design Patterns?

Atmos Design Patterns capture key concepts, principles, and insights vital for effectively structuring and organizing infrastructures,
components, and stacks. Inspired by [Cloud Posse's](https://cloudposse.com) extensive real-world experience in assisting Funded Startups and Enterprises, these patterns are meticulously developed to tackle organizational complexity. They focus on facilitating the creation of multi-account, enterprise-grade environments, specifically designed for the nuanced needs of complex organizations.

Atmos Design Patterns empower infrastructure designers and cloud architects by providing access to the collective wisdom of seasoned professionals. These patterns serve as a guide for creating solutions that are not only reusable but also robust, flexible, and maintainable. Importantly, they foster convergence on a standard set of recognized patterns, effectively putting a name to the practices and strategies that have proven successful. This standardization aids in streamlining communication and understanding among professionals, enhancing collaboration and efficiency in the field.

:::info Summary

- Derived from Cloud Posse’s experience, Atmos Design Patterns address complex organizational infrastructure needs.
- These patterns guide the creation of robust, flexible, and maintainable multi-account, enterprise-grade environments.
- Standardizing these patterns enhances professional communication, collaboration, and efficiency.

:::

## The Catalog of Design Patterns

The catalog of Atmos Design Patterns contains simple and elegant solutions to specific problems in infrastructure design and
configuration. These solutions have been developed and evolved over time. The Design Patterns encapsulate them in concise and readily applicable
forms.

Once you understand the Design Patterns and have experience with them, you won't ever think about infrastructure design in the same way.
You'll have knowledge and insights that can make your own designs more flexible, modular, reusable, and understandable.

:::note

We view this collection of Design Patterns as neither exhaustive nor fixed. Instead, it serves as a documentation of our present ideas, insights and knowledge. Your input, whether in the form of comments, critiques, references, or suggestions for Design Patterns we may have overlooked, is highly encouraged and appreciated.

:::

<DocCardList/>

---

## Inline Component Configuration

import File from '@site/src/components/File'
import PillBox from '@site/src/components/PillBox'
import Intro from '@site/src/components/Intro'

<PillBox>Atmos Design Pattern</PillBox>

<Intro>
The **Inline Component Configuration** pattern is used when the [components](/core-concepts/components) in a [stack](/core-concepts/stacks) are configured inline in the stack manifest without [importing](/core-concepts/stacks/imports) and reusing default/base configurations.
</Intro>

If you're starting with Atmos, inline configurations will be the easiest to understand until you need the more advanced features.

## Use-cases

Use the **Inline Component Configuration** pattern when:

- You have a very simple organizational structure, e.g. one OU, one or a few accounts, one region

- You have a component that is provisioned only in one stack (e.g. only in the `dev` account). In this case, the component is configured inline in the
  stack manifest and is not used in other stacks

- For testing or development purposes

## Benefits

The **Inline Component Configuration** pattern provides the following benefits:

- Very simple stack and component configurations

- Define all components in just one place (in one stack manifest) so it's easier to see what and where everything is provisioned

## Example

Suppose you need a simple setup with only `dev`, `staging` and `prod` stages (accounts). Here's how you might organize the stacks and
inline-component configuration for the `vpc` and `vpc-flow-logs-bucket` components.

```console
   │   # Centralized stacks configuration (stack manifests)
   ├── stacks
   │   ├── dev.yaml
   │   ├── staging.yaml
   │   └── prod.yaml
   │  
   │   # Centralized components configuration
   └── components
       └── terraform  # Terraform components (a.k.a Terraform "root" modules)
           ├── vpc
           ├── vpc-flow-logs-bucket
           ├── < other components >
```

Add the following minimal configuration to `atmos.yaml` [CLI config file](/cli/configuration) :

```yaml title="atmos.yaml"
components:
  terraform:
    base_path: "components/terraform"

stacks:
  base_path: "stacks"
  name_pattern: "{stage}"

schemas:
  jsonschema:
    base_path: "stacks/schemas/jsonschema"
  opa:
    base_path: "stacks/schemas/opa"
  atmos:
    manifest: "stacks/schemas/atmos/atmos-manifest/1.0/atmos-manifest.json"
```

Add the following component configurations to the `stacks/dev.yaml` stack manifest:

```yaml title="stacks/dev.yaml"
vars:
  stage: dev

components:
  terraform:
    vpc-flow-logs-bucket:
      metadata:
        # Point to the Terraform component
        component: vpc-flow-logs-bucket
      vars:
        enabled: true
        name: "vpc-flow-logs"
        traffic_type: "ALL"
        force_destroy: true
        lifecycle_rule_enabled: false

    vpc:
      metadata:
        # Point to the Terraform component
        component: vpc
      settings:
        # All validation steps must succeed to allow the component to be provisioned
        validation:
          validate-vpc-component-with-jsonschema:
            schema_type: jsonschema
            schema_path: "vpc/validate-vpc-component.json"
            description: Validate 'vpc' component variables using JSON Schema
          check-vpc-component-config-with-opa-policy:
            schema_type: opa
            schema_path: "vpc/validate-vpc-component.rego"
            # An array of filesystem paths (folders or individual files) to the additional modules for schema validation
            # Each path can be an absolute path or a path relative to `schemas.opa.base_path` defined in `atmos.yaml`
            # In this example, we have the additional Rego modules in `stacks/schemas/opa/catalog/constants`
            module_paths:
              - "catalog/constants"
            description: Check 'vpc' component configuration using OPA policy
      vars:
        enabled: true
        name: "common"
        max_subnet_count: 3
        map_public_ip_on_launch: true
        assign_generated_ipv6_cidr_block: false
        nat_gateway_enabled: true
        nat_instance_enabled: false
        vpc_flow_logs_enabled: true
        vpc_flow_logs_traffic_type: "ALL"
        vpc_flow_logs_log_destination_type: "s3"
        nat_eip_aws_shield_protection_enabled: false
        subnet_type_tag_key: "acme/subnet/type"
        ipv4_primary_cidr_block: 10.9.0.0/18
```

To provision the components, execute the following commands:

```shell
# `dev` stack
atmos terraform apply vpc-flow-logs-bucket -s dev
atmos terraform apply vpc -s dev
```

## Limitations

The **Inline Component Configuration** pattern has the following limitations and drawbacks:

- If you have more than one stack (e.g. `dev`, `staging`, `prod`), then the component definitions would be repeated in the stack manifests,
  which makes them not reusable and the entire stack configuration not [DRY](https://en.wikipedia.org/wiki/Don%27t_repeat_yourself)

- Should be used only for specific use-cases (e.g. you have just one stack, or you are designing and testing the components)

:::note

Use the [Inline Component Customization](/design-patterns/inline-component-customization) Design Pattern to address the limitations of the
**Inline Component Configuration** pattern.

:::

## Related Patterns

- [Inline Component Customization](/design-patterns/inline-component-customization)
- [Component Catalog](/design-patterns/component-catalog)
- [Component Catalog with Mixins](/design-patterns/component-catalog-with-mixins)
- [Component Catalog Template](/design-patterns/component-catalog-template)
- [Component Inheritance](/design-patterns/component-inheritance)
- [Partial Component Configuration](/design-patterns/partial-component-configuration)

---

## Inline Component Customization

import File from '@site/src/components/File'
import PillBox from '@site/src/components/PillBox'
import Intro from '@site/src/components/Intro'

<PillBox>Atmos Design Pattern</PillBox>

<Intro>
The **Inline Component Customization** pattern is used when the defaults for the [components](/core-concepts/components) in a [stack](/core-concepts/stacks) are configured in default manifests, the manifests are [imported](/core-concepts/stacks/imports) into the top-level stacks, and the components are customized inline in each top-level stack overriding the configuration for each environment (OU, account, region).
</Intro>

## Use-cases

Use the **Inline Component Customization** pattern when:

- You have components that are provisioned in multiple stacks (e.g. `dev`, `staging`, `prod` accounts) with different configurations for each stack

- You need to make the components' default/baseline configurations reusable across different stacks

- You want to keep the configurations [DRY](https://en.wikipedia.org/wiki/Don%27t_repeat_yourself)

## Benefits

The **Inline Component Customization** pattern provides the following benefits:

- The defaults for the components are defined in just one place making the entire
  configuration [DRY](https://en.wikipedia.org/wiki/Don%27t_repeat_yourself)

- The defaults for the components are reusable across many stacks

- Simple stack and component configurations

## Example

Suppose you need a simple setup with only `dev`, `staging` and `prod` stages (accounts). Here's how you might organize the stacks and
component configurations for the `vpc` and `vpc-flow-logs-bucket` components, and then customize the components in the stacks.

```console
   │   # Centralized stacks configuration (stack manifests)
   ├── stacks
   │   ├── defaults  # component-specific defaults
   │   │   ├── vpc-flow-logs-bucket.yaml
   │   │   └── vpc.yaml
   │   ├── dev.yaml
   │   ├── staging.yaml
   │   └── prod.yaml
   │  
   │   # Centralized components configuration
   └── components
       └── terraform  # Terraform components (a.k.a Terraform "root" modules)
           ├── vpc
           ├── vpc-flow-logs-bucket
           ├── < other components >
```

Add the following minimal configuration to `atmos.yaml` [CLI config file](/cli/configuration) :

```yaml title="atmos.yaml"
components:
  terraform:
    base_path: "components/terraform"

stacks:
  base_path: "stacks"
  name_pattern: "{stage}"
  excluded_paths:
    # Tell Atmos that the `defaults` folder and all sub-folders don't contain top-level stack manifests
    - "defaults/**/*"

schemas:
  jsonschema:
    base_path: "stacks/schemas/jsonschema"
  opa:
    base_path: "stacks/schemas/opa"
  atmos:
    manifest: "stacks/schemas/atmos/atmos-manifest/1.0/atmos-manifest.json"
```

Add the following default configuration to the `stacks/defaults/vpc-flow-logs-bucket.yaml` manifest:

```yaml title="stacks/defaults/vpc-flow-logs-bucket.yaml"
components:
  terraform:
    vpc-flow-logs-bucket:
      metadata:
        # Point to the Terraform component
        component: vpc-flow-logs-bucket
      vars:
        enabled: true
        name: "vpc-flow-logs"
        traffic_type: "ALL"
        force_destroy: true
        lifecycle_rule_enabled: false
```

Add the following default configuration to the `stacks/defaults/vpc.yaml` manifest:

```yaml title="stacks/defaults/vpc.yaml"
components:
  terraform:
    vpc:
      metadata:
        # Point to the Terraform component
        component: vpc
      settings:
        # All validation steps must succeed to allow the component to be provisioned
        validation:
          validate-vpc-component-with-jsonschema:
            schema_type: jsonschema
            schema_path: "vpc/validate-vpc-component.json"
            description: Validate 'vpc' component variables using JSON Schema
          check-vpc-component-config-with-opa-policy:
            schema_type: opa
            schema_path: "vpc/validate-vpc-component.rego"
            # An array of filesystem paths (folders or individual files) to the additional modules for schema validation
            # Each path can be an absolute path or a path relative to `schemas.opa.base_path` defined in `atmos.yaml`
            # In this example, we have the additional Rego modules in `stacks/schemas/opa/catalog/constants`
            module_paths:
              - "catalog/constants"
            description: Check 'vpc' component configuration using OPA policy
      vars:
        enabled: true
        name: "common"
        max_subnet_count: 3
        map_public_ip_on_launch: true
        assign_generated_ipv6_cidr_block: false
        nat_gateway_enabled: true
        nat_instance_enabled: false
        vpc_flow_logs_enabled: true
        vpc_flow_logs_traffic_type: "ALL"
        vpc_flow_logs_log_destination_type: "s3"
        nat_eip_aws_shield_protection_enabled: false
        subnet_type_tag_key: "acme/subnet/type"
        ipv4_primary_cidr_block: 10.9.0.0/18
```

Configure the `stacks/dev.yaml` top-level stack manifest:

```yaml title="stacks/dev.yaml"
vars:
  stage: dev

# Import the component default configurations
import:
  - defaults/vpc

components:
  terraform:
    # Customize the `vpc` component for the `dev` account
    # You can define variables or override the imported defaults
    vpc:
      vars:
        max_subnet_count: 2
        vpc_flow_logs_enabled: false
```

Configure the `stacks/staging.yaml` top-level stack manifest:

```yaml title="stacks/staging.yaml"
vars:
  stage: staging

# Import the component default configurations
import:
  - defaults/vpc-flow-logs-bucket
  - defaults/vpc

components:
  terraform:
    # Customize the `vpc` component for the `staging` account
    # You can define variables or override the imported defaults
    vpc:
      vars:
        map_public_ip_on_launch: false
        vpc_flow_logs_traffic_type: "REJECT"
```

Configure the `stacks/prod.yaml` top-level stack manifest:

```yaml title="stacks/prod.yaml"
vars:
  stage: prod

# Import the component default configurations
import:
  - defaults/vpc-flow-logs-bucket
  - defaults/vpc

components:
  terraform:
    # Customize the `vpc` component for the `prod` account
    # You can define variables or override the imported defaults
    vpc:
      vars:
        map_public_ip_on_launch: false
```

To provision the components, execute the following commands:

```shell
# `dev` stack
atmos terraform apply vpc -s dev

# `staging` stack
atmos terraform apply vpc-flow-logs-bucket -s staging
atmos terraform apply vpc -s staging

# `prod` stack
atmos terraform apply vpc-flow-logs-bucket -s prod
atmos terraform apply vpc -s prod
```

## Limitations

The **Inline Component Customization** pattern has the following limitations and drawbacks:

- The pattern is useful to customize components per account or region, but if you have more than one Organization, Organizational Unit (OU) or region,
  then the inline customizations would be repeated in the stack manifests, making the entire stack configuration
  not [DRY](https://en.wikipedia.org/wiki/Don%27t_repeat_yourself)

- Should be used only for specific use-cases, e.g. when you use just one region, Organization or Organizational Unit (OU)

:::note

To address the limitations of the **Inline Component Customization** Design Pattern, consider using the following patterns:

- [Organizational Structure Configuration](/design-patterns/organizational-structure-configuration)
- [Component Catalog](/design-patterns/component-catalog)
- [Component Catalog with Mixins](/design-patterns/component-catalog-with-mixins)

:::

## Related Patterns

- [Inline Component Configuration](/design-patterns/inline-component-configuration)
- [Organizational Structure Configuration](/design-patterns/organizational-structure-configuration)
- [Component Catalog](/design-patterns/component-catalog)
- [Component Catalog with Mixins](/design-patterns/component-catalog-with-mixins)
- [Component Catalog Template](/design-patterns/component-catalog-template)
- [Component Inheritance](/design-patterns/component-inheritance)
- [Partial Component Configuration](/design-patterns/partial-component-configuration)

---

## Layered Stack Configuration

import File from '@site/src/components/File'
import PillBox from '@site/src/components/PillBox'
import Intro from '@site/src/components/Intro'

<PillBox>Atmos Design Pattern</PillBox>

<Intro>
The **Layered Stack Configuration** Design Pattern describes the mechanism of grouping Atmos components by category or function,
adding the groups of components to layers, and importing the layers into top-level Atmos stacks.
</Intro>

Each layer imports or configures a set of related Atmos components. Each Atmos component belongs to just one layer. Each layer can be managed separately, possibly by different teams.

:::note

The **Layered Stack Configuration** Design Pattern works around the limitations of
the [Partial Stack Configuration](/design-patterns/partial-stack-configuration) pattern. Instead of splitting the top-level Atmos stacks into parts,
the **Layered Stack Configuration** pattern adds separate layers to group the related Atmos components by category, and then import the layer
manifests into the top-level Atmos stacks.

:::

## Use-cases

Use the **Layered Stack Configuration** pattern when:

- You have many Atmos components, and you need to group the components by category or function

- You want to split the components into layers. Each layer should be managed and modified independent of the other layers, possibly by different
  people or teams

- You want to keep the configuration easy to manage and [DRY](https://en.wikipedia.org/wiki/Don%27t_repeat_yourself)

## Benefits

The **Layered Stack Configuration** pattern provides the following benefits:

- Allows to group Atmos components by category or function

  people or teams. Furthermore, controls like
  GitHub's [`CODEOWNERS`](https://docs.github.com/en/repositories/managing-your-repositorys-settings-and-features/customizing-your-repository/about-code-owners)
  can be leveraged so that specific teams or individuals must review changes to these files.

- Allows importing only the required layers into the top-level stacks (only the groups of components that need to be provisioned in the stacks)

- Makes the configurations easier to understand

## Example

In the following structure, we have various Terraform components (Terraform root modules) in the `components/terraform` folder.

In the `stacks/catalog` folder, we define the defaults for each component using the [Component Catalog](/design-patterns/component-catalog) Atmos
Design Pattern.

In the `stacks/layers` folder, we define layers (groups of components), and import the related components into the layer manifests:

- `load-balancers.yaml`
- `data.yaml`
- `dns.yaml`
- `logs.yaml`
- `notifications.yaml`
- `firewalls.yaml`
- `networking.yaml`
- `eks.yaml`

```console
   │   # Centralized stacks configuration (stack manifests)
   ├── stacks
   │   ├── catalog  # component-specific defaults
   │   │   ├── alb
   │   │   │   └── defaults.yaml
   │   │   ├── aurora-postgres
   │   │   │   └── defaults.yaml
   │   │   ├── dns
   │   │   │   └── defaults.yaml
   │   │   ├── eks
   │   │   │   └── defaults.yaml
   │   │   ├── efs
   │   │   │   └── defaults.yaml
   │   │   ├── msk
   │   │   │   └── defaults.yaml
   │   │   ├── ses
   │   │   │   └── defaults.yaml
   │   │   ├── sns-topic
   │   │   │   └── defaults.yaml
   │   │   ├── network-firewall
   │   │   │   └── defaults.yaml
   │   │   ├── network-firewall-logs-bucket
   │   │   │   └── defaults.yaml
   │   │   ├── waf
   │   │   │   └── defaults.yaml
   │   │   ├── vpc
   │   │   │   └── defaults.yaml
   │   │   └── vpc-flow-logs-bucket
   │   │       └── defaults.yaml
   │   ├── layers  # grouping of components by category/function
   │   │   ├── load-balancers.yaml
   │   │   ├── data.yaml
   │   │   ├── dns.yaml
   │   │   ├── logs.yaml
   │   │   ├── notifications.yaml
   │   │   ├── firewalls.yaml
   │   │   ├── networking.yaml
   │   │   └── eks.yaml
   │   ├── mixins
   │   │   ├── tenant  # tenant-specific defaults
   │   │   │   └── plat.yaml
   │   │   ├── region  # region-specific defaults
   │   │   │   ├── us-east-2.yaml
   │   │   │   └── us-west-2.yaml
   │   │   └── stage  # stage-specific defaults
   │   │       ├── dev.yaml
   │   │       ├── staging.yaml
   │   │       └── prod.yaml
   │   └── orgs  # Organizations
   │       └── acme
   │           ├── _defaults.yaml
   │           └── plat  # 'plat' represents the "Platform" OU (a.k.a tenant)
   │               ├── _defaults.yaml
   │               ├── dev
   │               │   ├── _defaults.yaml
   │               │   ├── global-region.yaml
   │               │   ├── us-east-2.yaml
   │               │   └── us-west-2.yaml
   │               ├── staging
   │               │   ├── _defaults.yaml
   │               │   ├── global-region.yaml
   │               │   ├── us-east-2.yaml
   │               │   └── us-west-2.yaml
   │               └── prod
   │                   ├── _defaults.yaml
   │                   ├── global-region.yaml
   │                   ├── us-east-2.yaml
   │                   └── us-west-2.yaml
   │   # Centralized components configuration
   └── components
       └── terraform  # Terraform components (a.k.a Terraform "root" modules)
           ├── alb
           ├── aurora-postgres
           ├── dns
           ├── eks
           ├── efs
           ├── msk
           ├── ses
           ├── sns-topic
           ├── network-firewall
           ├── network-firewall-logs-bucket
           ├── waf
           ├── vpc
           └── vpc-flow-logs-bucket
```

Add the following minimal configuration to `atmos.yaml` [CLI config file](/cli/configuration) :

```yaml title="atmos.yaml"
components:
  terraform:
    base_path: "components/terraform"

stacks:
  base_path: "stacks"
  name_pattern: "{tenant}-{environment}-{stage}"
  included_paths:
    # Tell Atmos to search for the top-level stack manifests in the `orgs` folder and its sub-folders
    - "orgs/**/*"
  excluded_paths:
    # Tell Atmos that the `defaults` folder and all sub-folders don't contain top-level stack manifests
    - "defaults/**/*"

schemas:
  jsonschema:
    base_path: "stacks/schemas/jsonschema"
  opa:
    base_path: "stacks/schemas/opa"
  atmos:
    manifest: "stacks/schemas/atmos/atmos-manifest/1.0/atmos-manifest.json"
```

Add the following configuration to the `stacks/layers/load-balancers.yaml` layer manifest:

```yaml title="stacks/layers/load-balancers.yaml"
import:
  # Import the related component manifests into this layer manifest
  - catalog/alb/defaults
  # Import other Load Balancer components
```

Add the following configuration to the `stacks/layers/data.yaml` layer manifest:

```yaml title="stacks/layers/data.yaml"
import:
  # Import the related component manifests into this layer manifest
  - catalog/aurora-postgres/defaults
  - catalog/msk/defaults
  - catalog/efs/defaults
  # Import other Data components
```

Add the following configuration to the `stacks/layers/dns.yaml` layer manifest:

```yaml title="stacks/layers/dns.yaml"
import:
  # Import the related component manifests into this layer manifest
  - catalog/dns/defaults
  # Import other DNS components
```

Add the following configuration to the `stacks/layers/logs.yaml` layer manifest:

```yaml title="stacks/layers/logs.yaml"
import:
  # Import the related component manifests into this layer manifest
  - catalog/network-firewall-logs-bucket/defaults
  - catalog/vpc-flow-logs-bucket/defaults
  # Import other Logs components
```

Add the following configuration to the `stacks/layers/notifications.yaml` layer manifest:

```yaml title="stacks/layers/notifications.yaml"
import:
  # Import the related component manifests into this layer manifest
  - catalog/ses/defaults
  - catalog/sns-topic/defaults
  # Import other Notification components
```

Add the following configuration to the `stacks/layers/firewalls.yaml` layer manifest:

```yaml title="stacks/layers/firewalls.yaml"
import:
  # Import the related component manifests into this layer manifest
  - catalog/network-firewall/defaults
  - catalog/waf/defaults
  # Import other Firewall components
```

Add the following configuration to the `stacks/layers/networking.yaml` layer manifest:

```yaml title="stacks/layers/networking.yaml"
import:
  # Import the related component manifests into this layer manifest
  - catalog/vpc/defaults
  # Import other Networking components
```

Add the following configuration to the `stacks/layers/eks.yaml` layer manifest:

```yaml title="stacks/layers/eks.yaml"
import:
  # Import the related component manifests into this layer manifest
  - catalog/eks/defaults
```

Import the required layers into the `stacks/orgs/acme/plat/dev/us-east-2.yaml` top-level stack manifest:

```yaml title="stacks/orgs/acme/plat/dev/us-east-2.yaml"
import:
  # The `orgs/acme/plat/dev/_defaults` and `mixins/region/us-east-2` manifests
  # define the top-level Atmos stack `plat-ue2-dev`
  - orgs/acme/plat/dev/_defaults
  - mixins/region/us-east-2
  # Import the layers (groups of components)
  - layers/load-balancers
  - layers/data
  - layers/dns
  - layers/logs
  - layers/notifications
  - layers/firewalls
  - layers/networking
  - layers/eks
```

Import the required layers into the `stacks/orgs/acme/plat/dev/us-west-2.yaml` top-level stack manifest:

```yaml title="stacks/orgs/acme/plat/dev/us-west-2.yaml"
import:
  # The `orgs/acme/plat/dev/_defaults` and `mixins/region/us-west-2` manifests
  # define the top-level Atmos stack `plat-uw2-dev`
  - orgs/acme/plat/dev/_defaults
  - mixins/region/us-west-2
  # Import the layers (groups of components)
  - layers/load-balancers
  - layers/data
  - layers/dns
  - layers/logs
  - layers/notifications
  - layers/firewalls
  - layers/networking
  - layers/eks
```

Import the required layers into the `stacks/orgs/acme/plat/prod/us-east-2.yaml` top-level stack manifest:

```yaml title="stacks/orgs/acme/plat/prod/us-east-2.yaml"
import:
  # The `orgs/acme/plat/prod/_defaults` and `mixins/region/us-east-2` manifests
  # define the top-level Atmos stack `plat-ue2-prod`
  - orgs/acme/plat/prod/_defaults
  - mixins/region/us-east-2
  # Import the layers (groups of components)
  - layers/load-balancers
  - layers/data
  - layers/dns
  - layers/logs
  - layers/notifications
  - layers/firewalls
  - layers/networking
  - layers/eks
```

Similarly, import the required layers into the other top-level stacks for the other organizations, OUs/tenants, accounts and regions.
Make sure to import only the layers that define the component that need to be provisioned in the stacks.

## Related Patterns

- [Organizational Structure Configuration](/design-patterns/organizational-structure-configuration)
- [Partial Stack Configuration](/design-patterns/partial-stack-configuration)
- [Component Overrides](/design-patterns/component-overrides)

## References

- [Catalogs](/core-concepts/stacks/catalogs)
- [Mixins](/core-concepts/stacks/inheritance/mixins)

---

## Multiple Component Instances

import File from '@site/src/components/File'
import PillBox from '@site/src/components/PillBox'
import Intro from '@site/src/components/Intro'

<PillBox>Atmos Design Pattern</PillBox>

<Intro>
The **Multiple Component Instances** Design Pattern describes how to provision multiple instances of a single Terraform component in the same environment by configuring multiple Atmos component instances.
</Intro>

An Atmos component can have any name that can be different from the Terraform component name. More than one instance of the same Terraform component (with the same or different settings) can be provisioned into the same environment by defining multiple Atmos components that provide configuration for the Terraform component.

For example, two different Atmos components `vpc/1` and `vpc/2` can provide configuration for the same Terraform component `vpc`.

## Use-cases

Use the **Multiple Component Instances** pattern when:

- You need to provision multiple instances of a Terraform component in the same environment (organization, OU, account, region)

## Benefits

The **Multiple Component Instances** pattern provides the following benefits:

- Separation of the code (Terraform component) from the configuration (Atmos components)

- The same Terraform code is reused by multiple Atmos component instances with different configurations

- The defaults for the components are defined in just one place making the entire
  configuration [DRY](https://en.wikipedia.org/wiki/Don%27t_repeat_yourself)

## Example

The following example shows the Atmos stack and component configurations to provision two Atmos components (`vpc/1` and `vpc/2`) that use the
same Terraform component `vpc`. The `vpc/1` and `vpc/2` components are provisioned in the same stack. In the `catalog/vpc` folder, we have
the `defaults.yaml` manifest that configures the base abstract component `vpc/defaults` to be inherited by the derived VPC components `vpc/1`
and `vpc/2`.

```console
   │   # Centralized stacks configuration (stack manifests)
   ├── stacks
   │   ├── catalog  # component-specific defaults
   │   │   └── vpc
   │   │       └── defaults.yaml
   │   ├── mixins
   │   │   ├── tenant  # tenant-specific defaults
   │   │   │   └── plat.yaml
   │   │   ├── region  # region-specific defaults
   │   │   │   ├── us-east-2.yaml
   │   │   │   └── us-west-2.yaml
   │   │   └── stage  # stage-specific defaults
   │   │       ├── dev.yaml
   │   │       ├── staging.yaml
   │   │       └── prod.yaml
   │   └── orgs  # Organizations
   │       └── acme
   │           ├── _defaults.yaml
   │           └── plat  # 'plat' represents the "Platform" OU (a.k.a tenant)
   │               ├── _defaults.yaml
   │               ├── dev
   │               │   ├── _defaults.yaml
   │               │   ├── us-east-2.yaml
   │               │   └── us-west-2.yaml
   │               ├── staging
   │               │   ├── _defaults.yaml
   │               │   ├── us-east-2.yaml
   │               │   └── us-west-2.yaml
   │               └── prod
   │                   ├── _defaults.yaml
   │                   ├── us-east-2.yaml
   │                   └── us-west-2.yaml
   │   # Centralized components configuration
   └── components
       └── terraform  # Terraform components (a.k.a Terraform "root" modules)
           └── vpc
```

Add the following minimal configuration to `atmos.yaml` [CLI config file](/cli/configuration) :

```yaml title="atmos.yaml"
components:
  terraform:
    base_path: "components/terraform"

stacks:
  base_path: "stacks"
  name_pattern: "{tenant}-{environment}-{stage}"
  included_paths:
    # Tell Atmos to search for the top-level stack manifests in the `orgs` folder and its sub-folders
    - "orgs/**/*"
  excluded_paths:
    # Tell Atmos that the `defaults` folder and all sub-folders don't contain top-level stack manifests
    - "defaults/**/*"

schemas:
  jsonschema:
    base_path: "stacks/schemas/jsonschema"
  opa:
    base_path: "stacks/schemas/opa"
  atmos:
    manifest: "stacks/schemas/atmos/atmos-manifest/1.0/atmos-manifest.json"
```

Add the following default configuration to the `stacks/catalog/vpc/defaults.yaml` manifest:

```yaml title="stacks/catalog/vpc/defaults.yaml"
components:
  terraform:
    vpc/defaults:
      metadata:
        # Abstract components can't be provisioned and serve as base components (blueprints) for real components
        type: abstract
      settings:
        # All validation steps must succeed to allow the component to be provisioned
        validation:
          validate-vpc-component-with-jsonschema:
            schema_type: jsonschema
            schema_path: "vpc/validate-vpc-component.json"
            description: Validate 'vpc' component variables using JSON Schema
          check-vpc-component-config-with-opa-policy:
            schema_type: opa
            schema_path: "vpc/validate-vpc-component.rego"
            module_paths:
              - "catalog/constants"
            description: Check 'vpc' component configuration using OPA policy
      vars:
        enabled: true
        name: "common"
        max_subnet_count: 3
        map_public_ip_on_launch: true
        assign_generated_ipv6_cidr_block: false
        nat_gateway_enabled: true
        nat_instance_enabled: false
        vpc_flow_logs_enabled: true
        vpc_flow_logs_traffic_type: "ALL"
        vpc_flow_logs_log_destination_type: "s3"
        nat_eip_aws_shield_protection_enabled: false
        subnet_type_tag_key: "acme/subnet/type"
        ipv4_primary_cidr_block: 10.0.0.0/18
```

Configure multiple `vpc` component instances in the `stacks/orgs/acme/plat/prod/us-east-2.yaml` top-level stack:

```yaml title="stacks/orgs/acme/plat/prod/us-east-2.yaml"
import:
  import:
    - orgs/acme/plat/prod/_defaults
    - mixins/region/us-east-2
    # Import the defaults for all VPC components
    - catalog/vpc/defaults

components:
  terraform:
    # Atmos component `vpc/1`
    vpc/1:
      metadata:
        # Point to the Terraform component in `components/terraform/vpc`
        component: vpc
        # Inherit the defaults for all VPC components
        inherits:
          - vpc/defaults
      # Define/override variables specific to this `vpc/1` component
      vars:
        name: vpc-1
        ipv4_primary_cidr_block: 10.9.0.0/18

    # Atmos component `vpc/2`
    vpc/2:
      metadata:
        # Point to the Terraform component in `components/terraform/vpc`
        component: vpc
        # Inherit the defaults for all VPC components
        inherits:
          - vpc/defaults
      # Define/override variables specific to this `vpc/2` component
      vars:
        name: vpc-2
        ipv4_primary_cidr_block: 10.10.0.0/18
        map_public_ip_on_launch: false
```

To provision the components in the stack, execute the following commands:

```shell
atmos terraform apply vpc/1 -s plat-ue2-prod
atmos terraform apply vpc/2 -s plat-ue2-prod
```

The provisioned VPCs will have the following names:

```console
acme-plat-ue2-prod-vpc-1
acme-plat-ue2-prod-vpc-2
```

The names are generated from the context using the following template:

```console
{namespace}-{tenant}-{environment}-{stage}-{name}
```

## Related Patterns

- [Component Inheritance](/design-patterns/component-inheritance)
- [Abstract Component](/design-patterns/abstract-component)
- [Component Catalog](/design-patterns/component-catalog)
- [Component Catalog with Mixins](/design-patterns/component-catalog-with-mixins)
- [Component Catalog Template](/design-patterns/component-catalog-template)
- [Inline Component Configuration](/design-patterns/inline-component-configuration)
- [Inline Component Customization](/design-patterns/inline-component-customization)

---

## Organizational Structure Configuration

import File from '@site/src/components/File'
import PillBox from '@site/src/components/PillBox'
import Intro from '@site/src/components/Intro'

<PillBox>Atmos Design Pattern</PillBox>

<Intro>
The **Organizational Structure Configuration** pattern describes core concepts and best practices to structure and organize components
and stacks to design for organizational complexity and provision multi-account enterprise-grade environments.
</Intro>

The pattern is frequently used to model multi-region infrastructures for organizations with multiple organizational units/departments/tenants and multiple accounts.

## Use-cases

Use the **Organizational Structure Configuration** pattern when:

- You have one or more organizations with multiple organizational units/departments/tenants

- Each OU/department/tenant has multiple accounts

- You want to provision the infrastructure into many regions

## Benefits

The **Organizational Structure Configuration** pattern provides the following benefits:

- The defaults for the components, organizations, tenants/OUs, regions, account/stages are defined in just one place (and then imported) making the
  entire configuration extremely [DRY](https://en.wikipedia.org/wiki/Don%27t_repeat_yourself), reusable, and easy to understand and manage

- New Organizations can be easily added and configured without affecting the configurations for the existing organizations

- New tenants/OUs can be easily added to an organization without affecting the configurations for the existing tenants/OUs

- New stages/accounts can be easily added to a tenant/OU without affecting the configurations for the existing stages/accounts

- New regions can be added to the infrastructure, configured with Atmos, and components provisioned into the regions. New regions can be used for
  the general infrastructure, or for disaster recovery (DR), or for compliance and audit

- After adding new organizations, tenants, accounts or regions, the entire configuration will still remain DRY, reusable and easy to manage thanks to
  using the described folder structure, catalogs, mixins, and hierarchical [imports](/core-concepts/stacks/imports)

## The Role of _defaults.yaml Files

In the organizational structure pattern, `_defaults.yaml` files play a crucial role at each level of the hierarchy. These files:

- **Contain baseline configurations** for their respective level (organization, OU/tenant, account/stage)
- **Must be explicitly imported** - they are not automatically included (this is by design for explicit control)
- **Create inheritance chains** by importing parent-level defaults
- **Are excluded from stack discovery** via the `excluded_paths` configuration

:::info
The `_defaults.yaml` naming is a convention, not an Atmos feature. The underscore prefix ensures these files:
- Sort to the top of directory listings (lexicographic ordering)
- Are visually distinct from actual stack configurations
- Are excluded from stack discovery (when configured in `excluded_paths`)

See the [_defaults.yaml Design Pattern](/design-patterns/defaults-pattern) for a complete explanation of this convention.
:::

## Example

The following example shows the Atmos stack and component configurations to provision the `vpc` and `vpc-flow-logs-bucket` components into
a multi-org, multi-tenant, multi-account, multi-region environment. There are two organizations (`org1` and `org2`) with two
OUs/tenants (`core` and `plat`), multiple accounts in each OU/tenant, and two regions (`us-east-2` and `us-west-2`).

```console
   │   # Centralized stacks configuration (stack manifests)
   ├── stacks
   │   ├── catalog  # component-specific defaults
   │   │   ├── vpc-flow-logs-bucket
   │   │   │   └── defaults.yaml
   │   │   └── vpc
   │   │       └── defaults.yaml
   │   ├── mixins
   │   │   ├── tenant  # tenant-specific defaults
   │   │   │   ├── core.yaml
   │   │   │   └── plat.yaml
   │   │   ├── region  # region-specific defaults
   │   │   │   ├── global-region.yaml
   │   │   │   ├── us-east-2.yaml
   │   │   │   └── us-west-2.yaml
   │   │   └── stage  # stage-specific defaults
   │   │       ├── audit.yaml
   │   │       ├── automation.yaml
   │   │       ├── identity.yaml
   │   │       ├── root.yaml
   │   │       ├── dev.yaml
   │   │       ├── staging.yaml
   │   │       └── prod.yaml
   │   └── orgs  # Organizations
   │       ├── org1
   │       │   ├── _defaults.yaml
   │       │   ├── core  # 'core' represents the "Core" OU (a.k.a tenant)
   │       │   │   ├── _defaults.yaml
   │       │   │   ├── audit
   │       │   │   │   ├── _defaults.yaml
   │       │   │   │   ├── global-region.yaml
   │       │   │   │   ├── us-east-2.yaml
   │       │   │   │   └── us-west-2.yaml
   │       │   │   ├── automation
   │       │   │   │   ├── _defaults.yaml
   │       │   │   │   ├── global-region.yaml
   │       │   │   │   ├── us-east-2.yaml
   │       │   │   │   └── us-west-2.yaml
   │       │   │   ├── identity
   │       │   │   │   ├── _defaults.yaml
   │       │   │   │   ├── global-region.yaml
   │       │   │   │   ├── us-east-2.yaml
   │       │   │   │   └── us-west-2.yaml
   │       │   │   └── root
   │       │   │       ├── _defaults.yaml
   │       │   │       ├── global-region.yaml
   │       │   │       ├── us-east-2.yaml
   │       │   │       └── us-west-2.yaml
   │       │   └── plat  # 'plat' represents the "Platform" OU (a.k.a tenant)
   │       │       ├── _defaults.yaml
   │       │       ├── dev
   │       │       │   ├── _defaults.yaml
   │       │       │   ├── global-region.yaml
   │       │       │   ├── us-east-2.yaml
   │       │       │   └── us-west-2.yaml
   │       │       ├── staging
   │       │       │   ├── _defaults.yaml
   │       │       │   ├── global-region.yaml
   │       │       │   ├── us-east-2.yaml
   │       │       │   └── us-west-2.yaml
   │       │       └── prod
   │       │           ├── _defaults.yaml
   │       │           ├── global-region.yaml
   │       │           ├── us-east-2.yaml
   │       │           └── us-west-2.yaml
   │       └── org2
   │           ├── _defaults.yaml
   │           ├── core  # 'core' represents the "Core" OU (a.k.a tenant)
   │           │   ├── _defaults.yaml
   │           │   ├── audit
   │           │   ├── automation
   │           │   ├── identity
   │           │   └── root
   │           └── plat  # 'plat' represents the "Platform" OU (a.k.a tenant)
   │               ├── _defaults.yaml
   │               ├── dev
   │               ├── staging
   │               └── prod
   │  
   │   # Centralized components configuration
   └── components
       └── terraform  # Terraform components (a.k.a Terraform "root" modules)
           ├── vpc
           ├── vpc-flow-logs-bucket
           ├── < other components >
```

Add the following minimal configuration to `atmos.yaml` [CLI config file](/cli/configuration) :

```yaml title="atmos.yaml"
components:
  terraform:
    base_path: "components/terraform"

stacks:
  base_path: "stacks"
  included_paths:
    # Tell Atmos to search for the top-level stack manifests in the `orgs` folder and its sub-folders
    - "orgs/**/*"
  excluded_paths:
    # Tell Atmos that all `_defaults.yaml` files are not top-level stack manifests
    - "**/_defaults.yaml"
  # If you are using multiple organizations (namespaces), use the following `name_pattern`:
  name_pattern: "{namespace}-{tenant}-{environment}-{stage}"
  # If you are using a single organization (namespace), use the following `name_pattern`:
  # name_pattern: "{tenant}-{environment}-{stage}"

schemas:
  jsonschema:
    base_path: "stacks/schemas/jsonschema"
  opa:
    base_path: "stacks/schemas/opa"
  atmos:
    manifest: "stacks/schemas/atmos/atmos-manifest/1.0/atmos-manifest.json"
```

### Configure Component Catalogs

Add the following default configuration to the `stacks/catalog/vpc-flow-logs-bucket/defaults.yaml` manifest:

```yaml title="stacks/catalog/vpc-flow-logs-bucket/defaults.yaml"
components:
  terraform:
    vpc-flow-logs-bucket:
      metadata:
        # Point to the Terraform component
        component: vpc-flow-logs-bucket
      vars:
        enabled: true
        name: "vpc-flow-logs"
        traffic_type: "ALL"
        force_destroy: true
        lifecycle_rule_enabled: false
```

Add the following default configuration to the `stacks/catalog/vpc/defaults.yaml` manifest:

```yaml title="stacks/catalog/vpc/defaults.yaml"
components:
  terraform:
    vpc:
      metadata:
        # Point to the Terraform component
        component: vpc
      settings:
        # All validation steps must succeed to allow the component to be provisioned
        validation:
          validate-vpc-component-with-jsonschema:
            schema_type: jsonschema
            schema_path: "vpc/validate-vpc-component.json"
            description: Validate 'vpc' component variables using JSON Schema
          check-vpc-component-config-with-opa-policy:
            schema_type: opa
            schema_path: "vpc/validate-vpc-component.rego"
            module_paths:
              - "catalog/constants"
            description: Check 'vpc' component configuration using OPA policy
      vars:
        enabled: true
        name: "common"
        max_subnet_count: 3
        map_public_ip_on_launch: true
        assign_generated_ipv6_cidr_block: false
        nat_gateway_enabled: true
        nat_instance_enabled: false
        vpc_flow_logs_enabled: true
        vpc_flow_logs_traffic_type: "ALL"
        vpc_flow_logs_log_destination_type: "s3"
        nat_eip_aws_shield_protection_enabled: false
        subnet_type_tag_key: "acme/subnet/type"
        ipv4_primary_cidr_block: 10.9.0.0/18
```

### Configure OU/Tenant Manifests

In `stacks/mixins/tenant/core.yaml`, add the following config:

```yaml title="stacks/mixins/tenant/core.yaml"
vars:
  tenant: core

# Other defaults for the `core` tenant/OU
```

In `stacks/mixins/tenant/plat.yaml`, add the following config:

```yaml title="stacks/mixins/tenant/plat.yaml"
vars:
  tenant: plat

# Other defaults for the `plat` tenant/OU
```

### Configure Region Manifests

In `stacks/mixins/region/us-east-2.yaml`, add the following config:

```yaml title="stacks/mixins/region/us-east-2.yaml"
import:
  # All accounts (stages) in `us-east-2` region will have the `vpc-flow-logs-bucket` component
  - catalog/vpc-flow-logs-bucket/defaults
  # All accounts (stages) in `us-east-2` region will have the `vpc` component
  - catalog/vpc/defaults

vars:
  region: us-east-2
  environment: ue2

# Other defaults for the `us-east-2` region
```

In `stacks/mixins/region/us-west-2.yaml`, add the following config:

```yaml title="stacks/mixins/region/us-west-2.yaml"
import:
  # All accounts (stages) in `us-west-2` region will have the `vpc-flow-logs-bucket` component
  - catalog/vpc-flow-logs-bucket/defaults
  # All accounts (stages) in `us-east-2` region will have the `vpc` component
  - catalog/vpc/defaults

vars:
  region: us-west-2
  environment: uw2

# Other defaults for the `us-west-2` region
```

### Configure Stage/Account Manifests

In `stacks/mixins/stage/dev.yaml`, add the following config:

```yaml title="stacks/mixins/stage/dev.yaml"
vars:
  stage: dev

# Other defaults for the `dev` stage/account
```

In `stacks/mixins/stage/prod.yaml`, add the following config:

```yaml title="stacks/mixins/stage/prod.yaml"
vars:
  stage: prod

# Other defaults for the `prod` stage/account
```

In `stacks/mixins/stage/staging.yaml`, add the following config:

```yaml title="stacks/mixins/stage/staging.yaml"
vars:
  stage: staging

# Other defaults for the `staging` stage/account
```

### Configure Organization Defaults

:::note
The `_defaults.yaml` files in the organization, tenant, account and region folders is the recommended way to define the stack manifests with the
default configurations for organizations, OUs/tenants, accounts and regions. The `_defaults.yaml` files themselves are not top-level Atmos stacks,
they just contain the default values for the organizations, OUs/tenants, accounts and regions (to make the entire configuration reusable and DRY)
:::

In `stacks/orgs/org1/_defaults.yaml`, add the following config:

```yaml title="stacks/orgs/org1/_defaults.yaml"
vars:
  namespace: org1
```

The file defines the context variable `namespace` for the entire `org1` organization.

In `stacks/orgs/org2/_defaults.yaml`, add the following config:

```yaml title="stacks/orgs/org2/_defaults.yaml"
vars:
  namespace: org2
```

The file defines the context variable `namespace` for the entire `org2` organization.

### Configure Tenant/OU Defaults

In `stacks/orgs/org1/core/_defaults.yaml`, add the following config for the `org1` organization and `core` OU (tenant):

```yaml title="stacks/orgs/org1/core/_defaults.yaml"
import:
  - orgs/org1/_defaults
  - mixins/tenant/core
```

In `stacks/orgs/org1/plat/_defaults.yaml`, add the following config for the `org1` organization and `plat` OU (tenant):

```yaml title="stacks/orgs/org1/plat/_defaults.yaml"
import:
  - orgs/org1/_defaults
  - mixins/tenant/plat
```

In `stacks/orgs/org2/core/_defaults.yaml`, add the following config for the `org2` organization and `core` OU (tenant):

```yaml title="stacks/orgs/org2/core/_defaults.yaml"
import:
  - orgs/org2/_defaults
  - mixins/tenant/core
```

In `stacks/orgs/org2/plat/_defaults.yaml`, add the following config for the `org2` organization and `plat` OU (tenant):

```yaml title="stacks/orgs/org2/plat/_defaults.yaml"
import:
  - orgs/org2/_defaults
  - mixins/tenant/plat
```

### Configure Stage/Account Defaults

In `stacks/orgs/org1/plat/dev/_defaults.yaml`, add the following config for the `org1` organization, `plat` tenant, `dev` account:

```yaml title="stacks/orgs/org1/plat/dev/_defaults.yaml"
import:
  - orgs/org1/plat/_defaults
  - mixins/stage/dev
```

In `stacks/orgs/org2/plat/dev/_defaults.yaml`, add the following config for the `org2` organization, `plat` tenant, `dev` account:

```yaml title="stacks/orgs/org2/plat/dev/_defaults.yaml"
import:
  - orgs/org2/plat/_defaults
  - mixins/stage/dev
```

In `stacks/orgs/org1/plat/prod/_defaults.yaml`, add the following config for the `org1` organization, `plat` tenant, `prod` account:

```yaml title="stacks/orgs/org1/plat/prod/_defaults.yaml"
import:
  - orgs/org1/plat/_defaults
  - mixins/stage/prod
```

In `stacks/orgs/org2/plat/prod/_defaults.yaml`, add the following config for the `org2` organization, `plat` tenant, `prod` account:

```yaml title="stacks/orgs/org2/plat/prod/_defaults.yaml"
import:
  - orgs/org2/plat/_defaults
  - mixins/stage/prod
```

Similarly, configure the defaults for the other accounts in the `core` and `plat` tenants in the `org1` and `org2` organizations.

### Configure Top-Level Stack Manifests

After we've configured the catalog for the components, the mixins for the tenants, regions and stages, and the defaults for the organizations, OUs and
accounts, the final step is to configure the Atmos root (top-level) stacks and the Atmos components in the stacks.

In `stacks/orgs/org1/plat/dev/us-east-2.yaml`, add the following config:

```yaml title="stacks/orgs/org1/plat/dev/us-east-2.yaml"
import:
  - orgs/org1/plat/dev/_defaults
  - mixins/region/us-east-2
```

In `stacks/orgs/org1/plat/dev/us-west-2.yaml`, add the following config:

```yaml title="stacks/orgs/org1/plat/dev/us-west-2.yaml"
import:
  - orgs/org1/plat/dev/_defaults
  - mixins/region/us-west-2
```

In `stacks/orgs/org1/plat/prod/us-east-2.yaml`, add the following config:

```yaml title="stacks/orgs/org1/plat/prod/us-east-2.yaml"
import:
  - orgs/org1/plat/prod/_defaults
  - mixins/region/us-east-2
```

In `stacks/orgs/org1/plat/prod/us-west-2.yaml`, add the following config:

```yaml title="stacks/orgs/org1/plat/prod/us-west-2.yaml"
import:
  - orgs/org1/plat/prod/_defaults
  - mixins/region/us-west-2
```

In `stacks/orgs/org1/plat/staging/us-east-2.yaml`, add the following config:

```yaml title="stacks/orgs/org1/plat/staging/us-east-2.yaml"
import:
  - orgs/org1/plat/staging/_defaults
  - mixins/region/us-east-2
```

In `stacks/orgs/org1/plat/staging/us-west-2.yaml`, add the following config:

```yaml title="stacks/orgs/org1/plat/staging/us-west-2.yaml"
import:
  - orgs/org1/plat/staging/_defaults
  - mixins/region/us-west-2
```

Similarly, configure the top-level stack manifests for the `org2` organization.

### Provision the Atmos Components in the Stacks

To provision the components in the `org1` organization, execute the following commands:

```shell
# `dev` account, `us-east-2` region
atmos terraform apply vpc-flow-logs-bucket -s org1-plat-ue2-dev
atmos terraform apply vpc -s org1-plat-ue2-dev

# `dev` account, `us-west-2` region
atmos terraform apply vpc-flow-logs-bucket -s org1-plat-uw2-dev
atmos terraform apply vpc -s org1-plat-uw2-dev

# `staging` account, `us-east-2` region
atmos terraform apply vpc-flow-logs-bucket -s org1-plat-ue2-staging
atmos terraform apply vpc -s org1-plat-ue2-staging

# `staging` account, `us-west-2` region
atmos terraform apply vpc-flow-logs-bucket -s org1-plat-uw2-staging
atmos terraform apply vpc -s org1-plat-uw2-staging

# `prod` account, `us-east-2` region
atmos terraform apply vpc-flow-logs-bucket -s org1-plat-ue2-prod
atmos terraform apply vpc -s org1-plat-ue2-prod

# `prod` account, `us-west-2` region
atmos terraform apply vpc-flow-logs-bucket -s org1-plat-uw2-prod
atmos terraform apply vpc -s org1-plat-uw2-prod
```

To provision the components in the `org2` organization, execute the following commands:

```shell
# `dev` account, `us-east-2` region
atmos terraform apply vpc-flow-logs-bucket -s org2-plat-ue2-dev
atmos terraform apply vpc -s org2-plat-ue2-dev

# `dev` account, `us-west-2` region
atmos terraform apply vpc-flow-logs-bucket -s org2-plat-uw2-dev
atmos terraform apply vpc -s org2-plat-uw2-dev

# `staging` account, `us-east-2` region
atmos terraform apply vpc-flow-logs-bucket -s org2-plat-ue2-staging
atmos terraform apply vpc -s org2-plat-ue2-staging

# `staging` account, `us-west-2` region
atmos terraform apply vpc-flow-logs-bucket -s org2-plat-uw2-staging
atmos terraform apply vpc -s org2-plat-uw2-staging

# `prod` account, `us-east-2` region
atmos terraform apply vpc-flow-logs-bucket -s org2-plat-ue2-prod
atmos terraform apply vpc -s org2-plat-ue2-prod

# `prod` account, `us-west-2` region
atmos terraform apply vpc-flow-logs-bucket -s org2-plat-uw2-prod
atmos terraform apply vpc -s org2-plat-uw2-prod
```

## Limitations

The **Organizational Structure Configuration** pattern has the following limitations and drawbacks:

- The configuration described by the pattern can be complex for very simple infrastructures (e.g. just one organization, one organizational
  unit, a few accounts and regions)

:::note

Even if you are just starting with a very simple infrastructure (e.g. just one organization, one organizational unit, a few accounts, one or a few
regions), it's still recommended that you start with the configuration described by the **Organizational Structure Configuration** Design Pattern.

In the future, when you modify your infrastructure to provision multi-organization, multi-account, multi-region environments at scale, it will be
easy to extend the configuration without changing anything for the exiting environments.

:::

## Related Patterns

- [Inline Component Configuration](/design-patterns/inline-component-configuration)
- [Inline Component Customization](/design-patterns/inline-component-customization)
- [Component Catalog](/design-patterns/component-catalog)
- [Component Catalog with Mixins](/design-patterns/component-catalog-with-mixins)
- [Component Catalog Template](/design-patterns/component-catalog-template)
- [Component Inheritance](/design-patterns/component-inheritance)
- [Partial Component Configuration](/design-patterns/partial-component-configuration)
- [Partial Stack Configuration](/design-patterns/partial-stack-configuration)
- [Layered Stack Configuration](/design-patterns/layered-stack-configuration)

## References

- [Catalogs](/core-concepts/stacks/catalogs)
- [Mixins](/core-concepts/stacks/inheritance/mixins)

---

## Partial Component Configuration

import File from '@site/src/components/File'
import PillBox from '@site/src/components/PillBox'
import Intro from '@site/src/components/Intro'

<PillBox>Atmos Design Pattern</PillBox>

<Intro>
The **Partial Component Configuration** Design Pattern describes the mechanism of splitting an Atmos component's configuration across many Atmos manifests to manage, modify and apply them separately and independently in one top-level stack without affecting the others.
</Intro>

The mechanism is similar to [Partial Classes in C#](https://learn.microsoft.com/en-us/dotnet/csharp/programming-guide/classes-and-structs/partial-classes-and-methods).

This is not the same as Atmos [Component Inheritance](/core-concepts/stacks/inheritance) where more than one Atmos components
take part in the inheritance chain. The **Partial Component Configuration** pattern deals with just one Atmos component with its configuration split across a few configuration files.

:::note

Variations of the **Partial Component Configuration** Design Pattern were also implemented and described in the following patterns:

- [Component Catalog](/design-patterns/component-catalog)
- [Component Catalog with Mixins](/design-patterns/component-catalog-with-mixins)

:::

## Use-cases

Use the **Partial Component Configuration** pattern when:

- You have a component with a complex configuration. Some parts of the configuration must be managed and modified independently of the other parts
  of the component's configuration

- Different parts of the component's configuration can be applied to different stacks independently of the other stacks

- You want to keep the parts of the configuration reusable and [DRY](https://en.wikipedia.org/wiki/Don%27t_repeat_yourself)

## Benefits

The **Partial Component Configuration** pattern provides the following benefits:

- Allows managing components with complex configurations where some parts of the configurations must be managed and modified independently of the
  other parts

- Different parts of component' configurations can be applied to different stacks independently of the other stacks

- Allows keeping the parts of the configurations reusable across many stacks and [DRY](https://en.wikipedia.org/wiki/Don%27t_repeat_yourself)

## Example

Suppose that we have EKS clusters provisioned in many accounts and regions. The clusters can run different Kubernetes versions.
Each cluster will need to be upgraded to the next Kubernetes version independently without affecting the configurations for the other clusters in
the other accounts and regions.

The following example shows how to configure partial manifests for the `eks/cluster` Atmos component. The defaults for all clusters are defined in the
`stacks/catalog/eks/cluster/defaults.yaml` manifest, while the settings related to different Kubernetes versions are defined in the manifests in the
`stacks/catalog/eks/cluster/mixins` folder. Then, the defaults and one of the mixins are imported into a top-level stack to provide the final
configuration for the `eks/cluster` component in the stack.

```console
   │   # Centralized stacks configuration (stack manifests)
   ├── stacks
   │   └── catalog  # component-specific defaults
   │       └── eks
   │           └── cluster
   │               ├── defaults.yaml
   │               └── mixins
   │                   ├── k8s-1-27.yaml
   │                   ├── k8s-1-28.yaml
   │                   └── k8s-1-29.yaml
   │   # Centralized components configuration
   └── components
       └── terraform  # Terraform components (a.k.a Terraform "root" modules)
           └── eks
               └── cluster
```

Add the following minimal configuration to `atmos.yaml` [CLI config file](/cli/configuration) :

```yaml title="atmos.yaml"
components:
  terraform:
    base_path: "components/terraform"

stacks:
  base_path: "stacks"
  name_pattern: "{tenant}-{environment}-{stage}"
  included_paths:
    # Tell Atmos to search for the top-level stack manifests in the `orgs` folder and its sub-folders
    - "orgs/**/*"
  excluded_paths:
    # Tell Atmos that the `defaults` folder and all sub-folders don't contain top-level stack manifests
    - "defaults/**/*"

schemas:
  jsonschema:
    base_path: "stacks/schemas/jsonschema"
  opa:
    base_path: "stacks/schemas/opa"
  atmos:
    manifest: "stacks/schemas/atmos/atmos-manifest/1.0/atmos-manifest.json"
```

Add the following configuration to the `stacks/catalog/eks/clusters/defaults.yaml` manifest:

```yaml title="stacks/catalog/eks/clusters/defaults.yaml"
components:
  terraform:
    eks/cluster:
      metadata:
        # Point to the Terraform component in `components/terraform/eks/cluster`
        component: eks/cluster
      vars:
        name: eks
        availability_zones: [] # Use the VPC subnet AZs
        managed_node_groups_enabled: true
        node_groups:
          # will create 1 node group for each item in map
          main:
            # EKS AMI version to use, e.g. "1.16.13-20200821" (no "v").
            ami_release_version: null
            # Type of Amazon Machine Image (AMI) associated with the EKS Node Group
            ami_type: AL2_x86_64
            # Whether to enable Node Group to scale its AutoScaling Group
            cluster_autoscaler_enabled: false
            # Configure storage for the root block device for instances in the Auto Scaling Group
            block_device_map:
              "/dev/xvda":
                ebs:
                  encrypted: true
                  volume_size: 200 # GB
                  volume_type: "gp3"
            # Set of instance types associated with the EKS Node Group
            instance_types:
              - c6a.large
            # Desired number of worker nodes when initially provisioned
            desired_group_size: 2
            max_group_size: 3
            min_group_size: 2
```

Add the following configuration to the `stacks/catalog/eks/clusters/mixins/k8s-1-27.yaml` manifest:

```yaml title="stacks/catalog/eks/clusters/mixins/k8s-1-27.yaml"
components:
  terraform:
    eks/cluster:
      vars:
        cluster_kubernetes_version: "1.27"

        # https://docs.aws.amazon.com/eks/latest/userguide/eks-add-ons.html
        # https://docs.aws.amazon.com/eks/latest/userguide/managing-add-ons.html#creating-an-add-on
        addons:
          # https://docs.aws.amazon.com/eks/latest/userguide/cni-iam-role.html
          # https://docs.aws.amazon.com/eks/latest/userguide/managing-vpc-cni.html
          # https://docs.aws.amazon.com/eks/latest/userguide/cni-iam-role.html#cni-iam-role-create-role
          # https://aws.github.io/aws-eks-best-practices/networking/vpc-cni/#deploy-vpc-cni-managed-add-on
          vpc-cni:
            addon_version: "v1.12.6-eksbuild.2" # set `addon_version` to `null` to use the latest version
            # Set default resolve_conflicts to OVERWRITE because it is required on initial installation of
            # add-ons that have self-managed versions installed by default (e.g. vpc-cni, coredns), and
            # because any custom configuration that you would want to preserve should be managed by Terraform.
            resolve_conflicts_on_create: "OVERWRITE"
            resolve_conflicts_on_update: "OVERWRITE"
          # https://docs.aws.amazon.com/eks/latest/userguide/managing-kube-proxy.html
          kube-proxy:
            addon_version: "v1.27.1-eksbuild.1" # set `addon_version` to `null` to use the latest version
            resolve_conflicts_on_create: "OVERWRITE"
            resolve_conflicts_on_update: "OVERWRITE"
          # https://docs.aws.amazon.com/eks/latest/userguide/managing-coredns.html
          coredns:
            addon_version: "v1.10.1-eksbuild.1" # set `addon_version` to `null` to use the latest version
            resolve_conflicts_on_create: "OVERWRITE"
            resolve_conflicts_on_update: "OVERWRITE"
          # https://docs.aws.amazon.com/eks/latest/userguide/csi-iam-role.html
          # https://aws.amazon.com/blogs/containers/amazon-ebs-csi-driver-is-now-generally-available-in-amazon-eks-add-ons
          # https://docs.aws.amazon.com/eks/latest/userguide/managing-ebs-csi.html#csi-iam-role
          # https://github.com/kubernetes-sigs/aws-ebs-csi-driver
          aws-ebs-csi-driver:
            addon_version: "v1.23.0-eksbuild.1" # set `addon_version` to `null` to use the latest version
            resolve_conflicts_on_create: "OVERWRITE"
            resolve_conflicts_on_update: "OVERWRITE"
            # This disables the EBS driver snapshotter sidecar and reduces the amount of logging
            # https://github.com/aws/containers-roadmap/issues/1919
            configuration_values: '{"sidecars":{"snapshotter":{"forceEnable":false}}}'
```

Import the `stacks/catalog/eks/clusters/default.yaml` and `stacks/catalog/eks/clusters/mixins/k8s-1-27.yaml` manifests into a top-level stack,
for example `stacks/orgs/acme/plat/prod/us-east-2.yaml`:

```yaml title="stacks/orgs/acme/plat/prod/us-east-2.yaml"
import:
  - orgs/acme/plat/prod/_defaults
  - mixins/region/us-east-2

  # EKS cluster configuration
  - catalog/eks/clusters/defaults
  # Import the mixin for the required Kubernetes version to define the k8s version and addon versions for the EKS cluster in this stack.
  # This is an example of partial component configuration in Atmos where the config for the component is split across many Atmos stack manifests (stack config files).
  # It's similar to Partial Classes in C# (https://learn.microsoft.com/en-us/dotnet/csharp/programming-guide/classes-and-structs/partial-classes-and-methods).
  # This is not the same as 'Atmos Component Inheritance' (https://atmos.tools/core-concepts/stacks/inheritance)
  # where more than one Atmos component participates in the inheritance chain.
  - catalog/eks/clusters/mixins/k8s-1-27
```

Provision the component in the stack by executing the following command:

```shell
atmos terraform apply eks/cluster -s plat-ue2-prod
```

When the `eks/cluster` component is provisioned, Atmos imports the partial component configurations from the `catalog/eks/clusters/defaults.yaml` and
`catalog/eks/clusters/mixins/k8s-1-27.yaml` manifests, deep-merges the partial configs in the order they are defined in the imports, and generates the
final variables and settings for the component in the stack.

When you need to upgrade an EKS cluster in one account and region to the next Kubernetes version, just update the imported manifest in one top-level
stack and provision the EKS cluster without affecting the clusters in the other stacks. For example, to upgrade the cluster to the next Kubernetes
version `1.28`, update the imported mixin from `catalog/eks/clusters/mixins/k8s-1-27` to `catalog/eks/clusters/mixins/k8s-1-28`. All other EKS
clusters in the other accounts and regions will stay at the current Kubernetes version `1.27` until they are ready to be upgraded.

## Related Patterns

- [Component Catalog](/design-patterns/component-catalog)
- [Component Catalog with Mixins](/design-patterns/component-catalog-with-mixins)
- [Component Catalog Template](/design-patterns/component-catalog-template)
- [Component Inheritance](/design-patterns/component-inheritance)
- [Abstract Component](/design-patterns/abstract-component)
- [Inline Component Configuration](/design-patterns/inline-component-configuration)
- [Inline Component Customization](/design-patterns/inline-component-customization)
- [Organizational Structure Configuration](/design-patterns/organizational-structure-configuration)

## References

- [Catalogs](/core-concepts/stacks/catalogs)
- [Mixins](/core-concepts/stacks/inheritance/mixins)

---

## Partial Stack Configuration

import File from '@site/src/components/File'
import PillBox from '@site/src/components/PillBox'
import Intro from '@site/src/components/Intro'

<PillBox>Atmos Design Pattern</PillBox>

<Intro>
The **Partial Stack Configuration** Design Pattern describes the mechanism of splitting an Atmos top-level stack's configuration across many Atmos stack manifests to manage and modify them separately and independently.
</Intro>

Each partial top-level stack manifest imports or configures a set of related Atmos components. Each Atmos component belongs to just one of the partial top-level stack manifests. The pattern helps to group all components by category or function and to make each partial stack manifest smaller and easier to manage.

## Use-cases

Use the **Partial Stack Configuration** pattern when:

- You have top-level stacks with complex configurations. Some parts of the configurations must be managed and modified independent of the other
  parts, possibly by different people or teams

- You need to group the components in a top-level stack by category or function

- You want to keep the configuration easy to manage and [DRY](https://en.wikipedia.org/wiki/Don%27t_repeat_yourself)

## Benefits

The **Partial Stack Configuration** pattern provides the following benefits:

- Allows defining Atmos stacks with complex configurations by splitting the configurations into smaller manifests and by grouping the components by
  category or function

- Makes the configurations easier to understand

- Allows creating and modifying the partial stack manifests independently, possibly by different teams

## Example

In the following structure, we have many Terraform components (Terraform root modules) in the `components/terraform` folder.

In the `stacks/catalog` folder, we define the defaults for each component using the [Component Catalog](/design-patterns/component-catalog) Atmos
Design Pattern.

In the `stacks/orgs/acme/plat/dev/us-east-2` folder, we split the top-level stack manifest into the following parts by category:

- `load-balancers.yaml`
- `data.yaml`
- `dns.yaml`
- `logs.yaml`
- `notifications.yaml`
- `firewalls.yaml`
- `networking.yaml`
- `eks.yaml`

```console
   │   # Centralized stacks configuration (stack manifests)
   ├── stacks
   │   ├── catalog  # component-specific defaults
   │   │   ├── alb
   │   │   │   └── defaults.yaml
   │   │   ├── aurora-postgres
   │   │   │   └── defaults.yaml
   │   │   ├── dns
   │   │   │   └── defaults.yaml
   │   │   ├── eks
   │   │   │   └── defaults.yaml
   │   │   ├── efs
   │   │   │   └── defaults.yaml
   │   │   ├── msk
   │   │   │   └── defaults.yaml
   │   │   ├── ses
   │   │   │   └── defaults.yaml
   │   │   ├── sns-topic
   │   │   │   └── defaults.yaml
   │   │   ├── network-firewall
   │   │   │   └── defaults.yaml
   │   │   ├── network-firewall-logs-bucket
   │   │   │   └── defaults.yaml
   │   │   ├── waf
   │   │   │   └── defaults.yaml
   │   │   ├── vpc
   │   │   │   └── defaults.yaml
   │   │   └── vpc-flow-logs-bucket
   │   │       └── defaults.yaml
   │   ├── mixins
   │   │   ├── tenant  # tenant-specific defaults
   │   │   │   └── plat.yaml
   │   │   ├── region  # region-specific defaults
   │   │   │   ├── us-east-2.yaml
   │   │   │   └── us-west-2.yaml
   │   │   └── stage  # stage-specific defaults
   │   │       ├── dev.yaml
   │   │       ├── prod.yaml
   │   │       └── staging.yaml
   │   └── orgs  # Organizations
   │       └── acme
   │           ├── _defaults.yaml
   │           └── plat  # 'plat' represents the "Platform" OU (a.k.a tenant)
   │               ├── _defaults.yaml
   │               └── dev  # 'dev' account
   │                  ├── _defaults.yaml
   │                  ├── # Split the top-level stack 'plat-ue2-dev' by category of components
   │                  └── us-east-2
   │                      ├── load-balancers.yaml
   │                      ├── data.yaml
   │                      ├── dns.yaml
   │                      ├── logs.yaml
   │                      ├── notifications.yaml
   │                      ├── firewalls.yaml
   │                      ├── networking.yaml
   │                      └── eks.yaml
   │   # Centralized components configuration
   └── components
       └── terraform  # Terraform components (a.k.a Terraform "root" modules)
           ├── alb
           ├── aurora-postgres
           ├── dns
           ├── eks
           ├── efs
           ├── msk
           ├── ses
           ├── sns-topic
           ├── network-firewall
           ├── network-firewall-logs-bucket
           ├── waf
           ├── vpc
           └── vpc-flow-logs-bucket
```

Note that the partial stack manifests are parts of the same top-level Atmos stack `plat-ue2-dev` since they all import the same context variables
`namespace`, `tenant`, `environment` and `stage`. A top-level Atmos stack is defined by the context variables, not by the file names or locations
in the filesystem (file names can be anything, they are for people to better organize the entire configuration).

Add the following minimal configuration to `atmos.yaml` [CLI config file](/cli/configuration) :

```yaml title="atmos.yaml"
components:
  terraform:
    base_path: "components/terraform"

stacks:
  base_path: "stacks"
  name_pattern: "{tenant}-{environment}-{stage}"
  included_paths:
    # Tell Atmos to search for the top-level stack manifests in the `orgs` folder and its sub-folders
    - "orgs/**/*"
  excluded_paths:
    # Tell Atmos that the `defaults` folder and all sub-folders don't contain top-level stack manifests
    - "defaults/**/*"

schemas:
  jsonschema:
    base_path: "stacks/schemas/jsonschema"
  opa:
    base_path: "stacks/schemas/opa"
  atmos:
    manifest: "stacks/schemas/atmos/atmos-manifest/1.0/atmos-manifest.json"
```

Add the following configuration to the `stacks/orgs/acme/plat/dev/us-east-2/load-balancers.yaml` partial stack manifest:

```yaml title="stacks/orgs/acme/plat/dev/us-east-2/load-balancers.yaml"
import:
  # The `orgs/acme/plat/dev/_defaults` and `mixins/region/us-east-2` manifests
  # define the top-level Atmos stack `plat-ue2-dev`
  - orgs/acme/plat/dev/_defaults
  - mixins/region/us-east-2
  # Import the related component manifests into this partial stack manifest
  - catalog/alb/defaults
  # Import other Load Balancer components
```

Add the following configuration to the `stacks/orgs/acme/plat/dev/us-east-2/data.yaml` partial stack manifest:

```yaml title="stacks/orgs/acme/plat/dev/us-east-2/data.yaml"
import:
  # The `orgs/acme/plat/dev/_defaults` and `mixins/region/us-east-2` manifests
  # define the top-level Atmos stack `plat-ue2-dev`
  - orgs/acme/plat/dev/_defaults
  - mixins/region/us-east-2
  # Import the related component manifests into this partial stack manifest
  - catalog/aurora-postgres/defaults
  - catalog/msk/defaults
  - catalog/efs/defaults
  # Import other Data components
```

Add the following configuration to the `stacks/orgs/acme/plat/dev/us-east-2/dns.yaml` partial stack manifest:

```yaml title="stacks/orgs/acme/plat/dev/us-east-2/dns.yaml"
import:
  # The `orgs/acme/plat/dev/_defaults` and `mixins/region/us-east-2` manifests
  # define the top-level Atmos stack `plat-ue2-dev`
  - orgs/acme/plat/dev/_defaults
  - mixins/region/us-east-2
  # Import the related component manifests into this partial stack manifest
  - catalog/dns/defaults
  # Import other DNS components
```

Add the following configuration to the `stacks/orgs/acme/plat/dev/us-east-2/logs.yaml` partial stack manifest:

```yaml title="stacks/orgs/acme/plat/dev/us-east-2/logs.yaml"
import:
  # The `orgs/acme/plat/dev/_defaults` and `mixins/region/us-east-2` manifests
  # define the top-level Atmos stack `plat-ue2-dev`
  - orgs/acme/plat/dev/_defaults
  - mixins/region/us-east-2
  # Import the related component manifests into this partial stack manifest
  - catalog/network-firewall-logs-bucket/defaults
  - catalog/vpc-flow-logs-bucket/defaults
  # Import other Logs components
```

Add the following configuration to the `stacks/orgs/acme/plat/dev/us-east-2/notifications.yaml` partial stack manifest:

```yaml title="stacks/orgs/acme/plat/dev/us-east-2/notifications.yaml"
import:
  # The `orgs/acme/plat/dev/_defaults` and `mixins/region/us-east-2` manifests
  # define the top-level Atmos stack `plat-ue2-dev`
  - orgs/acme/plat/dev/_defaults
  - mixins/region/us-east-2
  # Import the related component manifests into this partial stack manifest
  - catalog/ses/defaults
  - catalog/sns-topic/defaults
  # Import other Notification components
```

Add the following configuration to the `stacks/orgs/acme/plat/dev/us-east-2/firewalls.yaml` partial stack manifest:

```yaml title="stacks/orgs/acme/plat/dev/us-east-2/firewalls.yaml"
import:
  # The `orgs/acme/plat/dev/_defaults` and `mixins/region/us-east-2` manifests
  # define the top-level Atmos stack `plat-ue2-dev`
  - orgs/acme/plat/dev/_defaults
  - mixins/region/us-east-2
  # Import the related component manifests into this partial stack manifest
  - catalog/network-firewall/defaults
  - catalog/waf/defaults
  # Import other Firewall components
```

Add the following configuration to the `stacks/orgs/acme/plat/dev/us-east-2/networking.yaml` partial stack manifest:

```yaml title="stacks/orgs/acme/plat/dev/us-east-2/networking.yaml"
import:
  # The `orgs/acme/plat/dev/_defaults` and `mixins/region/us-east-2` manifests
  # define the top-level Atmos stack `plat-ue2-dev`
  - orgs/acme/plat/dev/_defaults
  - mixins/region/us-east-2
  # Import the related component manifests into this partial stack manifest
  - catalog/vpc/defaults
  # Import other Networking components
```

Add the following configuration to the `stacks/orgs/acme/plat/dev/us-east-2/eks.yaml` partial stack manifest:

```yaml title="stacks/orgs/acme/plat/dev/us-east-2/eks.yaml"
import:
  # The `orgs/acme/plat/dev/_defaults` and `mixins/region/us-east-2` manifests
  # define the top-level Atmos stack `plat-ue2-dev`
  - orgs/acme/plat/dev/_defaults
  - mixins/region/us-east-2
  # Import the related component manifests into this partial stack manifest
  - catalog/eks/defaults
```

## Limitations

The **Partial Stack Configuration** pattern has the following limitations and drawbacks:

- The structure described by the pattern can become big and complex in a production-ready enterprise-grade infrastructure

- In the example above, we showed how to split just one top-level stack manifest (one organization, one OU/tenant, one account, one region) into
  smaller parts and import the related components. This is useful and not complicated with one or a few top-level stacks, but the configuration
  can become too complex when we need to do the same for all organizations, OUs/tenants, accounts and regions. We'll have to repeat the same
  filesystem layout many times and import the same components into many partial stack manifests

:::note

To address the limitations of the **Partial Stack Configuration** Design Pattern, consider
the [Layered Stack Configuration](/design-patterns/layered-stack-configuration) Design Pattern

:::

## Related Patterns

- [Organizational Structure Configuration](/design-patterns/organizational-structure-configuration)
- [Layered Stack Configuration](/design-patterns/layered-stack-configuration)
- [Component Overrides](/design-patterns/component-overrides)
- [Component Catalog](/design-patterns/component-catalog)
- [Component Catalog with Mixins](/design-patterns/component-catalog-with-mixins)

## References

- [Catalogs](/core-concepts/stacks/catalogs)
- [Mixins](/core-concepts/stacks/inheritance/mixins)

---

## Summary

import Intro from '@site/src/components/Intro'

<Intro>
Architecting and provisioning enterprise-grade infrastructure is challenging, and making all the components and
configurations [reusable](https://en.wikipedia.org/wiki/Reusability) and [DRY](https://en.wikipedia.org/wiki/Don%27t_repeat_yourself) is even more difficult. These Atmos Design Patterns can help with structuring and organizing your design to provision multi-account enterprise-grade environments for complex organizations.
</Intro>

We've already identified and documented many Atmos Design Patterns.
You may be wondering, "Where do I begin and what steps should I follow?"

Here are some recommendations.

### Quick Start Repository Introduction

This [Quick Start](https://github.com/cloudposse/atmos/tree/main/examples/quick-start-advanced) repository presents an example of an infrastructure managed
by Atmos. You can clone it and configure to your own needs. The repository should be a good start to get yourself familiar with Atmos and the
Design Patterns. The [Quick Start Guide](/quick-start) describes the steps required to configure and start using the repository.

The [Atmos Quick Start](https://github.com/cloudposse/atmos/tree/main/examples/quick-start-advanced) repository, that is described in
the [Quick Start](/quick-start) guide, uses the following Atmos Design Patterns:

- [Organizational Structure Configuration](/design-patterns/organizational-structure-configuration)
- [Component Catalog](/design-patterns/component-catalog)
- [Partial Component Configuration](/design-patterns/partial-component-configuration)
- [Inline Component Customization](/design-patterns/inline-component-customization)

### Development and Exploration with Atmos

If you are just developing or kicking the tires on Atmos, you can use any of the Design Patterns described in this guide, and find out which ones
are best suited to your requirements. The easiest ones to get started with will be:

  - [Inline Component Configuration](/design-patterns/inline-component-configuration)
  - [Inline Component Customization](/design-patterns/inline-component-customization)

### Advanced Infrastructure Architecture

If you are architecting the infrastructure to provision multi-account multi-region environments at scale, we suggest you start with the
  following Design Patterns:

  - [Organizational Structure Configuration](/design-patterns/organizational-structure-configuration)
  - [Component Catalog](/design-patterns/component-catalog)
  - [Inline Component Customization](/design-patterns/inline-component-customization)
  - [Component Inheritance](/design-patterns/component-inheritance)
  - [Abstract Component](/design-patterns/abstract-component)
  - [Multiple Component Instances](/design-patterns/multiple-component-instances)
  - [Partial Component Configuration](/design-patterns/partial-component-configuration)
  - [Layered Stack Configuration](/design-patterns/layered-stack-configuration)

---

## Atmos Functions

import DocCardList from '@theme/DocCardList'
import Intro from '@site/src/components/Intro'

<Intro>
Atmos provides two types of functions for dynamic configuration: **YAML Functions** (recommended) and **Template Functions**. Understanding the difference is crucial for writing reliable, maintainable configurations.
</Intro>

## Critical Differences

### YAML Functions (Recommended) ✅

YAML functions are **native YAML features** that work with the YAML parser. They are processed **after** the YAML is parsed into a structured document.

**Advantages:**
- **Type-safe**: Work with actual YAML data types (strings, numbers, booleans, objects, arrays)
- **Error-resistant**: Cannot break YAML syntax since they operate on parsed data
- **Predictable**: Results are always valid YAML structures
- **Debuggable**: Errors point to specific YAML nodes
- **Native**: Part of the YAML specification via custom tags

**Example:**
```yaml
# Safe and predictable - the function receives and returns proper YAML types
vpc_id: !terraform.output vpc.vpc_id
environment: !env AWS_ENVIRONMENT
config: !include common-config.yaml
```

### Template Functions (Use with Caution) ⚠️

Template functions use Go's text templating to **manipulate the raw text** before YAML parsing. They operate on the document as a string, not as structured data.

**Disadvantages:**
- **Error-prone**: Can easily generate invalid YAML syntax
- **Type-unsafe**: Work with text, not data types
- **Fragile**: Whitespace and indentation issues are common
- **Hard to debug**: Errors appear as YAML parsing failures
- **Side effects**: Can accidentally modify unintended parts of the document

**Example of potential issues:**
```yaml
# DANGEROUS - template functions can break YAML structure
# If the template returns multi-line text or special characters, YAML parsing fails
value: {{ atmos.Component "vpc" .stack }}  # May break if output contains colons or newlines
```

## Best Practices

:::warning
**Always prefer YAML functions over template functions when possible.** Template functions should only be used when absolutely necessary for complex logic that cannot be achieved with YAML functions.
:::

### When to Use YAML Functions
- Reading Terraform outputs: Use `!terraform.output` or `!terraform.state`
- Including external files: Use `!include`
- Environment variables: Use `!env`
- External data stores: Use `!store`
- Command execution: Use `!exec`

### When Template Functions Might Be Necessary
- Complex conditional logic that requires if/else statements
- Loops and iteration over data structures
- String manipulation that requires Go template functions
- Dynamic key generation in YAML structures

### Safety Guidelines for Template Functions

If you must use template functions:

1. **Test thoroughly** - Template errors often only appear with specific input values
2. **Keep templates simple** - Complex templates are hard to debug
3. **Validate output** - Ensure generated YAML is always valid
4. **Use proper escaping** - Handle special characters in template outputs
5. **Document extensively** - Explain why templates are needed and what they do
6. **Consider alternatives** - Can you restructure to use YAML functions instead?

## Function Categories

<DocCardList/>

## Migration Path

If you have existing template functions, consider migrating to YAML functions:

| Instead of (Template) | Use (YAML) |
|----------------------|------------|
| `{{ exec "command" }}` | `!exec command` |
| `{{ env "VAR" }}` | `!env VAR` |
| `{{ toJson .values }}` | Native YAML structures |
| Reading outputs via templates | `!terraform.output` or `!terraform.state` |

## Performance Considerations

- **YAML functions** are generally faster as they work with parsed data structures
- **Template functions** require an additional pre-processing step before YAML parsing
- **`!terraform.state`** is the fastest way to read Terraform outputs (10-100x faster than `!terraform.output`)

---

## atmos.Component

import File from '@site/src/components/File'
import Intro from '@site/src/components/Intro'
import Terminal from '@site/src/components/Terminal'

<Intro>
The `atmos.Component` template function allows reading any Atmos section or any attribute from a section for an
Atmos component in a stack, and use it in `Go` templates in Atmos component configurations.
</Intro>

## Usage

```yaml
  {{ (atmos.Component "<component>" "<stack>").<section>.<attribute> }}
```

## Arguments

<dl>
  <dt>`component`</dt>
  <dd>Atmos component name</dd>

  <dt>`stack`</dt>
  <dd>Atmos stack name</dd>

  <dt>`section`</dt>
  <dd>Atmos section name. Any section returned by the CLI command
  atmos describe component can be used.
  A special `outputs` section is also supported to get the outputs (remote state) of Terraform/OpenTofu components.</dd>

  <dt>`outputs`</dt>
  <dd>Using the `outputs` section in the `atmos.Component` command is an convenient way to read the outputs (remote state)
  of a component in a stack directly in Atmos stack manifests. It is an alternative to using the `remote-state`
  module and configuring Terraform/OpenTofu components to use the `remote-state` module as described in
  Component Remote State</dd>

  <dt>`attribute`</dt>
  <dd>
  Attribute name (field) from the `section`. `attribute` is optional, you can use the `section` itself
  if it's a simple type (e.g. `string`). Any number of attributes can be chained using the dot (`.`) notation.
  For example, if the first two attributes are maps, you can chain them and get a field from the last map:
  ```yaml
  {{ (atmos.Component "<component>" "<stack>").<section>.<attribute1>.<attribute2>.<field1> }}
  ```
  </dd>
</dl>

## Specifying Atmos `stack`

There are multiple ways you can specify the Atmos stack parameter in the `atmos.Component` function.

The `stack` argument is the second argument of the `atmos.Component` function, and it can be specified in a few different ways:

### Hardcoded Stack Name

Hardcoded stack name. Use it if you want to get an output from a component from a different (well-known and static) stack. For example, you have a `tgw` component in a stack `plat-ue2-dev` that requires the `vpc_id` output from the `vpc` component from the stack `plat-ue2-prod`:

```yaml title="plat-ue2-dev"
  components:
    terraform:
      tgw:
        vars:
          vpc_id: '{{ (atmos.Component "vpc" "plat-ue2-prod").outputs.vpc_id }}'
```

### Reference the Current Stack Name

Use the `.stack` (or `.atmos_stack`) template identifier to specify the same stack as the current component is in (for which the `atmos.Component` function is executed):

```yaml
  {{ (atmos.Component "<component>" .stack).<section>.<attribute> }}
  {{ (atmos.Component "<component>" .atmos_stack).<section>.<attribute> }}
```

For example, you have a `tgw` component that requires the `vpc_id` output from the `vpc` component in the same stack:

```yaml
  components:
    terraform:
      tgw:
        vars:
          vpc_id: '{{ (atmos.Component "vpc" .stack).outputs.vpc_id }}'
```

### Use a Format Function

Use the `printf` template function to construct stack names using static strings and dynamic identifiers. This is convenient when you want to override some identifiers in the stack name:

```yaml
  {{ (atmos.Component "<component>" (printf "%s-%s-%s" .vars.tenant .vars.environment .vars.stage)).<section>.<attribute> }}

  {{ (atmos.Component "<component>" (printf "plat-%s-prod" .vars.environment)).<section>.<attribute> }}

  {{ (atmos.Component "<component>" (printf "%s-%s-%s" .settings.context.tenant .settings.context.region .settings.context.account)).<section>.<attribute> }}
```

For example, you have a `tgw` component deployed in the stack `plat-ue2-dev`. The `tgw` component requires the
`vpc_id` output from the `vpc` component from the same environment (`ue2`) and same stage (`dev`), but from a different
tenant `net` (instead of `plat`):

```yaml title="plat-ue2-dev"
  components:
    terraform:
      tgw:
        vars:
          vpc_id: '{{ (atmos.Component "vpc" (printf "net-%s-%s" .vars.environment .vars.stage)).outputs.vpc_id }}'
```

:::tip Important
    By using the `printf "%s-%s-%s"` function, you are constructing stack names using the stack context variables/identifiers.

    For more information on Atmos stack names and how to define them, refer to `stacks.name_pattern` and `stacks.name_template`
    sections in [`atmos.yaml` CLI config file](/cli/configuration/)
:::

## Handling Outputs Containing Maps or Lists

You can use the `atmos.Component` function to read complex outputs (remote state) from Terraform/OpenTofu components, and use those in your stack manifests.
To effectively use `atmos.Component` outputs in stack configurations, it’s important to understand the relationship between YAML and JSON.
YAML is a superset of JSON, meaning you can embed JSON directly inside YAML, but the reverse is not true.

This means:
  1. The output of a template function must be valid YAML (including the relative whitespace based on its current position) or JSON to be parsed correctly.
  2. Simple types like strings are straightforward — they can be returned as-is.
  3. Complex types like lists or maps must be serialized into JSON to ensure compatibility.

:::note
There are a few ways to handle the complex output types (maps and lists) from the `atmos.Component` function:

  - Using the `!template` Atmos YAML function and the `toRawJson` or `toJson` template functions (this is the recommended way)
  - Adjusting the template delimiters in `atmos.yaml` and using the `toRawJson` or `toJson` template functions
:::

### Using the `!template` Atmos YAML Function and `toRawJson` or `toJson` template functions

You can use the [`!template`](/functions/yaml/template) Atmos YAML function to handle the outputs containing maps or lists.

The __`!template`__ YAML function  correctly handles the JSON returned from the
[`toRawJson` or `toJson`](https://masterminds.github.io/sprig/defaults.html) functions and converts the result into the list or map YAML types.
The final result of type list or map can be sent directly to the corresponding Terraform/OpenTofu component without modifying the types of the input variables.

<File title="stack.yaml">
```yaml
components:
  terraform:
    my_lambda_component:
      vars:
        vpc_config:
          security_group_ids: ['{{ (atmos.Component "security-group/lambda" .stack).outputs.id }}']

          # Output of type list from the `atmos.Component` template function
          subnet_ids: !template '{{ toJson (atmos.Component "vpc" .stack).outputs.private_subnet_ids }}'

          # Output of type map from the `atmos.Component` template function
          config_map: !template '{{ toJson (atmos.Component "config" .stack).outputs.config_map }}'
```
</File>

### Adjusting the template delimiters in `atmos.yaml` and using the `toRawJson` or `toJson` template functions

First, we need to adjust the delimiters in `atmos.yaml`, to avoid issues with YAML parsing:

```yaml title="atmos.yaml"
templates:
  settings:
    # Note the deliberate use of single quotes around the braces to avoid YAML parsing issues
    delimiters: ["'{{", "}}'"]
```

<details>
<summary>Detailed Explanation</summary>

In YAML, the default `{{` and `}}` delimiters used for templating can conflict with YAML’s syntax rules, leading to parsing errors. For example, YAML interprets unquoted braces as JSON maps, which can cause YAML parsing to fail.

1. YAML Parsing Phase First, the file is parsed as YAML to extract its structure, such as imports. At this stage, the file must be valid YAML for Atmos to proceed. If the template expressions inside `{{ ... }}` are not properly handled, they can break the YAML syntax, leading to parsing errors.

2. Go Template Processing Phase: After successfully parsing the YAML structure, Atmos treats the file as a Go template and processes the content. Here’s what happens:
   - Everything between the delimiters (`{{ ... }}`) is replaced by the result of the template function or expression.
   - The output of this replacement must also be valid YAML so the file can remain usable for the next steps (e.g., Terraform processing).
3. How Adjusted Delimiters Work: By configuring delimiters as `["'{{", "}}'"]` in `atmos.yaml`, you ensure that:
   - During the YAML Parsing Phase, the single quotes (`'`) around the template expressions make them valid YAML strings, allowing the file to pass YAML validation.
   - During the Go Template Processing Phase, Atmos processes the content within '`{{` and `}}`' and replaces it with the evaluated result.

</details>

Then we can use the [`toRawJson` or `toJson`](https://masterminds.github.io/sprig/defaults.html) functions to serialize complex types into JSON:

```yaml
  components:
  terraform:
    my_lambda_component:
      vars:
        vpc_config:
          security_group_ids: ['{{ (atmos.Component "security-group/lambda" .stack).outputs.id }}']
          subnet_ids: '{{ toRawJson ((atmos.Component "vpc" .stack).outputs.private_subnet_ids) }}'
```

The results of the `toRawJson` function are serialized into JSON, which can be safely embedded in the YAML stack configuration.
The single quotes around the braces in the `delimiters` setting ensure that the JSON is not parsed as YAML, but then processed as a template function which removes the single quotes and replaces it with the JSON string.

## Examples

The following configurations show different ways of using the `atmos.Component` template function to read values from
different Atmos sections directly in Atmos stack manifests, including the outputs of other
(already provisioned) components.

<File>
```yaml
# Global `settings` section
# It will be added and deep-merged to the `settings` section of all components
settings:
  test: true

components:
  terraform:
    test:
      metadata:
        # Point to the Terraform/OpenTofu component
        component: "test"
      vars:
        name: "test"

    test1:
      metadata:
        # Point to the Terraform/OpenTofu component
        component: "test1"
      vars:
        name: "test1"

    test2:
      metadata:
        # Point to the Terraform/OpenTofu component
        component: "test2"
      vars:
        name: "test2"
        # Use the `atmos.Component` function to get the outputs of the Atmos component `test1`
        # The `test1` component must be already provisioned and its outputs stored in the Terraform/OpenTofu state
        # Atmos will execute `terraform output` on the `test1` component in the same stack to read its outputs
        test1_id: '{{ (atmos.Component "test1" .stack).outputs.test1_id }}'
        tags:
          # Get the `settings.test` field from the `test` component in the same stack
          test: '{{ (atmos.Component "test" .stack).settings.test }}'
          # Get the `metadata.component` field from the `test` component in the same stack
          test_terraform_component: '{{ (atmos.Component "test" .stack).metadata.component }}'
          # Get the `vars.name` field from the `test1` component in the same stack
          test1_name: '{{ (atmos.Component "test1" .stack).vars.name }}'
```
</File>

## Caching the result of `atmos.Component` function

Atmos caches (in memory) the results of `atmos.Component` template function execution.
If you call the function for the same component in a stack more than once, the first call will produce the result
and cache it, and all the consecutive calls will just use the cached data. This is useful when you use the
`atmos.Component` function for the same component in a stack in multiple places in Atmos stack manifests.
It will speed up the function execution and stack processing.

For example:

<File>
```yaml
components:
  terraform:
    test2:
      vars:
        tags:
          test: '{{ (atmos.Component "test" .stack).outputs.id }}'
          test2: '{{ (atmos.Component "test" .stack).outputs.id }}'
          test3: '{{ (atmos.Component "test" .stack).outputs.id }}'
```
</File>

In the example, the `test2` Atmos component uses the outputs (remote state) of the `test` Atmos component from the same stack.
The template function `{{ atmos.Component "test" .stack }}` is executed three times (once for each tag).

After the first execution, Atmos caches the result in memory (all the component sections, including the `outputs`),
and reuses it in the next two calls to the function. The caching makes the stack processing about three times faster in this
particular example. In a production environment where many components are used, the speedup can be even more significant.

## Using `atmos.Component` with `static` remote state backend

Atmos supports [brownfield configuration by using the remote state of type `static`](/core-concepts/components/terraform/brownfield/#hacking-remote-state-with-static-backends).

For example:

<File title="stack.yaml">
```yaml
components:
  terraform:
    # Component `static-backend` is configured with the remote state backend of type `static`
    static-backend:
      remote_state_backend_type: static
      remote_state_backend:
        static:
          region: "us-west-2"
          cluster_name: "production-cluster"
          vpc_cidr: "10.0.0.0/16"
          database:
            type: "postgresql"
            version: "12.7"
            storage_gb: 100
          allowed_ips:
            - "192.168.1.0/24"
            - "10.1.0.0/16"
          tags:
            Environment: "production"
            Owner: "infra-team"

    eks-cluster:
      vars:
        region: '{{ (atmos.Component "static-backend" .stack).outputs.region }}'
        cluster_name: '{{ (atmos.Component "static-backend" .stack).outputs.cluster_name }}'
        vpc_cidr: '{{ (atmos.Component "static-backend" .stack).outputs.vpc_cidr }}'
        db_type: '{{ (atmos.Component "static-backend" .stack).outputs.database.type }}'
        db_storage: '{{ (atmos.Component "static-backend" .stack).outputs.database.storage_gb }}'
        # Use the `!template` YAML function to correctly handle the outputs of types map and list
        allowed_ips: !template '{{ (atmos.Component "static-backend" .stack).outputs.allowed_ips }}'
        tags: !template '{{ (atmos.Component "static-backend" .stack).outputs.tags }}'
```
</File>

When the functions are executed, Atmos detects that the `static-backend` component has the `static` remote state configured,
and instead of executing `terraform output`, it just returns the static values from the `remote_state_backend.static` section.

Executing the command `atmos describe component eks-cluster -s <stack>` produces the following result:

<Terminal title="atmos describe component eks-cluster -s <stack>">
```yaml
vars:
  region: us-west-2
  cluster_name: production-cluster
  vpc_cidr: 10.0.0.0/16
  db_type: postgresql
  db_storage: 100
  allowed_ips:
    - 192.168.1.0/24
    - 10.1.0.0/16
  tags:
    Environment: production
    Owner: infra-team
```
</Terminal>

---

## atmos.GomplateDatasource

import File from '@site/src/components/File'
import Intro from '@site/src/components/Intro'

<Intro>
**Stop hardcoding values in your infrastructure configurations.** The `atmos.GomplateDatasource` function lets you pull live data from anywhere - APIs, cloud services, databases, files - directly into your Atmos stacks. Instead of manually updating IP addresses, account IDs, or configuration values, your infrastructure can fetch them automatically at runtime. Plus, with built-in caching, it's lightning fast even when you reference the same data hundreds of times.
</Intro>

## What are Datasources?

Think of datasources as **live connections to your data**, wherever it lives. Instead of copying and pasting values into your configurations (and keeping them updated), datasources let Atmos fetch the latest data automatically when it runs.

**The Problem:** Traditional infrastructure configurations require you to:
- Hardcode values that change (IP addresses, account IDs, API keys)
- Manually update configurations when external systems change
- Duplicate data across multiple files
- Risk using outdated or incorrect values

**The Solution:** Datasources let you:
- **Pull live data** from any API, database, or cloud service
- **Stay synchronized** with external systems automatically
- **Centralize configuration** in its proper home (not in your IaC)
- **Access secrets securely** from vaults without exposing them
- **Query cloud metadata** like account IDs, regions, or resource tags dynamically

### Real-World Example

**Without Datasources (Manual, Error-Prone):**
```yaml
components:
  terraform:
    vpc:
      vars:
        account_id: "123456789012"  # Hardcoded - wrong in other environments!
        allowed_ip: "203.0.113.45"  # Bob's IP - outdated when he moves
        db_password: "prod-pass-123" # SECURITY RISK! Password in git
```

**With Datasources (Dynamic, Secure):**
```yaml
components:
  terraform:
    vpc:
      vars:
        account_id: '{{ (atmos.GomplateDatasource "aws").account_id }}'
        allowed_ip: '{{ (atmos.GomplateDatasource "myip").ip }}'
        db_password: '{{ (atmos.GomplateDatasource "vault").database.password }}'
```

### Common Use Cases

- **Service Discovery**: Query current IP addresses, endpoints, or service locations
- **Dynamic Configuration**: Fetch configuration values from external systems
- **Secret Management**: Retrieve secrets from HashiCorp Vault, AWS Secrets Manager, etc.
- **Metadata Queries**: Get AWS account info, region data, or resource tags
- **External Data Import**: Load data from CSV files, JSON APIs, or databases

## Usage

```yaml
  {{ (atmos.GomplateDatasource "<alias>").<attribute> }}
```

## Arguments

<dl>
  <dt>`alias`</dt>
  <dd>The datasource alias defined in your Atmos configuration</dd>

  <dt>`attribute`</dt>
  <dd>Attribute name (field) to extract from the datasource response</dd>
</dl>

## Supported Datasource Types

Gomplate supports numerous datasource types:
- **HTTP/HTTPS**: REST APIs and web services
- **File**: Local files (JSON, YAML, TOML, CSV, etc.)
- **AWS**: Systems Manager Parameter Store, Secrets Manager, S3
- **Cloud**: Azure Key Vault, Google Cloud Storage
- **Vault**: HashiCorp Vault secrets
- **Environment**: Environment variables
- **Git**: Git repository data
- **Consul**: HashiCorp Consul key-value store

## How it Works

### Automatic Caching

The `atmos.GomplateDatasource` function automatically caches results in memory during stack processing:

1. **First call**: Fetches data from the external source and stores in cache
2. **Subsequent calls**: Returns cached data without additional external calls
3. **Cache scope**: Per Atmos execution (cache is cleared between runs)

This caching mechanism provides several benefits:
- **Performance**: Dramatically faster stack processing with multiple datasource references
- **Reliability**: Reduces failures from rate limiting or network issues
- **Cost savings**: Fewer API calls to metered services
- **Consistency**: All references get the same data within a single execution

### Comparison with Direct Gomplate Datasources

| Feature | `datasource` (Gomplate) | `atmos.GomplateDatasource` |
|---------|------------------------|---------------------------|
| External calls | Every invocation | Once per alias |
| Performance | Slower with multiple uses | Fast (cached) |
| Rate limiting risk | High | Low |
| Network failures | Each call can fail | Single point of failure |
| Use case | Single reference | Multiple references |

## Examples

### Basic API Call

<File>
```yaml
settings:
  templates:
    settings:
      gomplate:
        timeout: 5
        datasources:
          # Define a datasource to get current IP
          ip:
            url: "https://api.ipify.org?format=json"
            headers:
              accept:
                - "application/json"

components:
  terraform:
    vpc:
      vars:
        # This will be cached after first call
        public_ip: '{{ (atmos.GomplateDatasource "ip").ip }}'

    firewall:
      vars:
        # Reuses cached result - no additional API call
        allowed_ip: '{{ (atmos.GomplateDatasource "ip").ip }}'
```
</File>

### AWS Systems Manager Parameter

<File>
```yaml
settings:
  templates:
    settings:
      gomplate:
        datasources:
          # Fetch from AWS SSM Parameter Store
          database:
            url: "aws+smp:///myapp/database/config?region=us-east-1"

components:
  terraform:
    app:
      vars:
        # All these use cached result after first fetch
        db_host: '{{ (atmos.GomplateDatasource "database").host }}'
        db_port: '{{ (atmos.GomplateDatasource "database").port }}'
        db_name: '{{ (atmos.GomplateDatasource "database").name }}'
```
</File>

### Loading External Configuration

<File>
```yaml
settings:
  templates:
    settings:
      gomplate:
        datasources:
          # Load configuration from external YAML file
          config:
            url: "file:///configs/app-config.yaml"

components:
  terraform:
    application:
      vars:
        # Access nested configuration values
        feature_flags: '{{ (atmos.GomplateDatasource "config").features }}'
        api_limits: '{{ (atmos.GomplateDatasource "config").limits.api }}'
```
</File>

## Performance Example

Consider this configuration that references the same datasource multiple times:

<File>
```yaml
settings:
  templates:
    settings:
      gomplate:
        timeout: 5
        datasources:
          metadata:
            url: "https://api.example.com/metadata"

components:
  terraform:
    component1:
      vars:
        region: '{{ (datasource "metadata").region }}'           # API call #1
        account: '{{ (atmos.GomplateDatasource "metadata").account }}'  # API call #2 (cached)

    component2:
      vars:
        region: '{{ (atmos.GomplateDatasource "metadata").region }}'    # Cached
        env: '{{ (atmos.GomplateDatasource "metadata").environment }}'  # Cached

    component3:
      vars:
        cluster: '{{ (atmos.GomplateDatasource "metadata").cluster }}'  # Cached
```
</File>

**Without caching**: 5 API calls (potential for timeouts, rate limiting)
**With `atmos.GomplateDatasource`**: 1 API call (4 cache hits)

## Best Practices

1. **Always use `atmos.GomplateDatasource` for repeated references** to the same external data
2. **Configure appropriate timeouts** in the `gomplate.timeout` setting
3. **Handle errors gracefully** - external sources can fail
4. **Use caching strategically** - cache expensive or rate-limited APIs
5. **Document your datasources** - explain what external data is being fetched and why

## See Also

- [Gomplate Datasources Documentation](https://docs.gomplate.ca/datasources/)
- [Atmos Template Settings](/cli/configuration#templates)
- [Template Functions](/functions/template)

---

## atmos.Store

import File from '@site/src/components/File'
import Intro from '@site/src/components/Intro'
import Terminal from '@site/src/components/Terminal'

<Intro>
The `atmos.Store` template function allows reading the values from a remote [store](/core-concepts/projects/configuration/stores)
(e.g. SSM Parameter Store, Artifactory, Redis, etc.) into Atmos stack manifests.
</Intro>

## Usage

The `atmos.Store` template function accepts four parameters:

```yaml
# Read a simple value (string, number, boolean) from the store
var1: '{{ atmos.Store "<store_name>" "<stack>" "<component>" "<key>" }}'

# Read a complex object (e.g. map) from the store and get an individual value (attribute) from the result
var2: '{{ (atmos.Store "<store_name>" "<stack>" "<component>" "<key>").<attribute> }}'
```

## Arguments

<dl>
    <dt>`store_name`</dt>
    <dd>The name of the store to read from (as defined in the `atmos.yaml` file)</dd>

    <dt>`stack`</dt>
    <dd>Atmos stack name</dd>

    <dt>`component`</dt>
    <dd>Atmos component name</dd>

    <dt>`key`</dt>
    <dd>The key to read from the store</dd>
</dl>

## Specifying Atmos `stack`

There are multiple ways you can specify the Atmos stack parameter in the `atmos.Store` function.

The `stack` argument is the second argument of the `atmos.Store` function, and it can be specified in a few different ways:

### Hardcoded Stack Name

Use it if you want to get a value from the store for a component from a different (well-known and static) stack.
For example, you have a `tgw` component in a stack `plat-ue2-dev` that requires the `vpc_id` key from the `vpc` component from the stack `plat-ue2-prod`:

```yaml title="plat-ue2-dev"
  components:
    terraform:
      tgw:
        vars:
          vpc_id: '{{ atmos.Store "prod/ssm" "plat-ue2-prod" "vpc" "vpc_id" }}'
```

### Reference the Current Stack Name

Use the `.stack` (or `.atmos_stack`) template identifier to specify the same stack as the current component is in
(for which the `atmos.Store` function is executed):

```yaml
{{ atmos.Store "<store_name>" .stack "<component>" "<key>" }}
{{ atmos.Store "<store_name>" .atmos_stack "<component>" "<key>" }}
```

For example, you have a `tgw` component that requires the `vpc_id` key from the store for the `vpc` component in the same stack:

```yaml
  components:
    terraform:
      tgw:
        vars:
          vpc_id: '{{ atmos.Store "prod/ssm" .stack "vpc" "vpc_id" }}'
```

### Use a Format Function

Use the `printf` template function to construct stack names using static strings and dynamic identifiers.
This is convenient when you want to override some identifiers in the stack name:

```yaml
{{ atmos.Store "<store_name>" (printf "%s-%s-%s" .vars.tenant .vars.environment .vars.stage) "<component>" "<key>" }}

{{ atmos.Store "<store_name>" (printf "plat-%s-prod" .vars.environment) "<component>" "<key>" }}

{{ atmos.Store "<store_name>" (printf "%s-%s-%s" .settings.context.tenant .settings.context.region .settings.context.account) "<component>" "<key>" }}
```

For example, you have a `tgw` component deployed in the stack `plat-ue2-dev`. The `tgw` component requires the
`vpc_id` key from the store for the `vpc` component from the same environment (`ue2`) and same stage (`dev`), but from a different
tenant `net` (instead of `plat`):

```yaml title="plat-ue2-dev"
  components:
    terraform:
      tgw:
        vars:
          vpc_id: '{{ atmos.Store "prod/ssm" (printf "net-%s-%s" .vars.environment .vars.stage) "vpc" "vpc_id" }}'
```

:::tip Important
By using the `printf "%s-%s-%s"` function, you are constructing stack names using the stack context variables/identifiers.

For more information on Atmos stack names and how to define them, refer to `stacks.name_pattern` and `stacks.name_template`
sections in [`atmos.yaml` CLI config file](/cli/configuration/)
:::

## Examples

The following configuration shows different ways of using the `atmos.Store` template function to read values from
a Redis store:

### Configure Redis store in `atmos.yaml`

<File title="atmos.yaml">
```yaml
components:
  terraform:
    base_path: "components/terraform"

stacks:
  base_path: "stacks"
  included_paths:
    - "deploy/**/*"
  excluded_paths:
    - "**/_defaults.yaml"
  name_template: "{{ .vars.stage }}"

logs:
  file: "/dev/stderr"
  level: Info

# `Go` templates in Atmos manifests
templates:
  settings:
    enabled: true
    evaluations: 1
    sprig:
      enabled: true
    gomplate:
      enabled: true

stores:
  redis:
    type: redis
    # The ATMOS_REDIS_URL environment variable will be used if
    # no URL is specified in the options
```
</File>

### Configure Atmos stacks and components

<File>
```yaml
vars:
  stage: nonprod

components:
  terraform:
    component-1:
      vars:
        # Use the static (hardcoded) stack name `prod`
        cidr: '{{ atmos.Store "redis" "prod" "vpc" "cidr" }}'
        # Using the template identifier `.stack` allows specifying the current stack name `nonprod` w/o hardcoding it
        instance_count: '{{ (atmos.Store "redis" .stack "config" "config_map").instance_count }}'
        # Use the Atmos section `.vars.stage` for the stack name
        subnets_count: '{{ (atmos.Store "redis" .vars.stage "config" "config_map").vpc_config.subnets_count }}'
        # The `!template` YAML function converts the JSON-encoded string into a map
        defaults: !template '{{ (atmos.Store "redis" .stack "config" "config_map").defaults | toJSON }}'
        lambda_environment:
          # Example of using the `atmos.Store` template function in a multi-line string
          ENGINE_CONFIG_JSON: |
            {
              "cidr": {{ atmos.Store "redis" "prod" "vpc" "cidr" | quote }},
              "defaults": {{ (atmos.Store "redis" .stack "config" "config_map").defaults | toJSON }},
              "subnets_count": {{ (atmos.Store "redis" .stack "config" "config_map").vpc_config.subnets_count }}
            }
```
</File>

Execute the `atmos describe component component-1 -s nonprod` command.
It will read the values from the store and assign to the component variables:

<Terminal title="atmos describe component component-1 -s nonprod">
```yaml
vars:
  cidr: 172.16.0.0/16
  defaults:
    account_id: 987654321
    team: networking
  instance_count: 2
  lambda_environment:
    ENGINE_CONFIG_JSON: |
      {
        "cidr": "172.16.0.0/16",
        "defaults": {"account_id":987654321,"team":"networking"},
        "subnets_count": 3
      }
  subnets_count: 3
  stage: nonprod
```
</Terminal>

## Using `atmos.Store` function in YAML multi-line strings

You can use the `atmos.Store` template function in [YAML multi-line strings](https://yaml-multiline.info/)
at any position inside a string. For example, to encode the results from the `atmos.Store` template function as JSON, we could do the following:

<File>
```yaml
components:
  terraform:
    component-1:
      vars:
        lambda_environment:
          ENGINE_CONFIG_JSON: |
            {
                "cidr": {{ atmos.Store "redis" "prod" "vpc" "cidr" | quote }},
                "defaults": {{ (atmos.Store "redis" .stack "config" "config_map").defaults | toJSON }},
                "subnets_count": {{ (atmos.Store "redis" .stack "config" "config_map").vpc_config.subnets_count }}
            }
```
</File>

This is one advantage of using the `atmos.Store` template function over the
[`!store` YAML function](/functions/yaml/store).

:::tip
The [`!store` YAML function](/functions/yaml/store) is generally preferred for its simpler, more readable syntax and for avoiding the complexity of `Go` templates. However, it is less flexible than the `{{ atmos.Store }}` template function. Template functions can reduce readability and are more prone to misuse, potentially resulting in malformed YAML.
:::

## Caching the result of `atmos.Store` function

Atmos caches (in memory) the results of `atmos.Store` template functions when executing any Atmos command that processes stacks
(e.g. `atmos describe component` or `atmos terraform apply`).

If you call the function for the same store, stack, component and key more than once, the first call will produce the result
and cache it, and all the consecutive calls will just use the cached data. This is useful when you use the
`atmos.Store` function for the same store, stack, component and key in multiple places in Atmos stack manifests.
It will speed up the function execution and stack processing.

For example:

<File>
```yaml
components:
  terraform:
    test2:
      vars:
        tags:
          test: '{{ atmos.Store "prod/ssm" .stack "vpc" "vpc_id" }}'
          test2: '{{ atmos.Store "prod/ssm" .stack "vpc" "vpc_id" }}'
          test3: '{{ atmos.Store "prod/ssm" .stack "vpc" "vpc_id" }}'
```
</File>

In the example, the `atmos.Store` template function is executed three times (once for each tag) for the same store, stack, component and key.

After the first execution, Atmos caches the result in memory,
and reuses it in the next two calls to the function. The caching makes the stack processing faster.

## Considerations

- Using `atmos.Store` with secrets can expose sensitive data to standard output (stdout) in any commands that describe stacks or components.
- When using `atmos.Store` with [`atmos describe affected`](/cli/commands/describe/affected), Atmos requires access to all referenced stores.
If you operate with limited permissions (e.g., scoped to `dev`) and reference production stacks, the command will fail.
- Be mindful of disaster recovery (DR) implications when using it across regions.
- Consider cold-start scenarios: if the dependent component has not yet been provisioned, the value in the store may not
yet be available and the `atmos.Store` function call will fail unless you provide a default value
using the `Go` template `or` or `if-else` expressions, the [Sprig `default` function](https://masterminds.github.io/sprig/defaults.html),
or the [Gomplate `default` function](https://docs.gomplate.ca/functions/conv/#convdefault).

---

## Atmos Template Functions

import DocCardList from '@theme/DocCardList'
import Intro from '@site/src/components/Intro'
import PillBox from '@site/src/components/PillBox'

<PillBox>Use with Caution</PillBox>

<Intro>
    Template functions provide powerful text manipulation capabilities using Go's template language, but they operate on raw text **before** YAML parsing. This makes them error-prone and should only be used when YAML functions cannot achieve the required functionality.
</Intro>

:::warning Important
**Template functions manipulate the document as raw text before YAML parsing.** This means:
- They can easily break YAML syntax with incorrect indentation or special characters
- Errors appear as cryptic YAML parsing failures, not clear function errors
- Debugging is difficult because you're debugging generated text, not data structures
- Small changes can have unintended side effects on document structure

**Always prefer [YAML Functions](/functions/yaml) when possible.** They are safer, more predictable, and easier to debug.
:::

## When to Use Template Functions

Template functions should **only** be used for scenarios that YAML functions cannot handle:

1. **Complex conditional logic** - When you need if/else branching
2. **Loops and iteration** - When generating repeated structures dynamically
3. **Advanced string manipulation** - Complex transformations not available in YAML
4. **Dynamic key generation** - When YAML keys themselves need to be computed

## Common Pitfalls and How to Avoid Them

### 1. Indentation Issues
```yaml
# WRONG - Template output may break indentation
vars:
  config: {{ atmos.Component "vpc" .stack }}  # Multi-line output breaks YAML

# BETTER - Use YAML function
vars:
  config: !terraform.output vpc
```

### 2. Special Characters
```yaml
# WRONG - Colons and quotes in output break YAML
description: {{ .vars.description }}  # Fails if description contains ": " or quotes

# BETTER - Use proper quoting or YAML functions
description: "{{ .vars.description }}"  # Quote the entire expression
```

### 3. Type Confusion
```yaml
# WRONG - Template always returns strings
count: {{ .vars.instance_count }}  # Returns "3" not 3

# BETTER - Use YAML for proper types
count: 3  # Or use a YAML function that preserves types
```

## Safety Guidelines

If you must use template functions:

1. **Always quote template expressions** in YAML values to prevent syntax breaks
2. **Test with various inputs** including special characters, multi-line text, and empty values
3. **Use template pipelines** for escaping: `{{ .value | quote }}`, `{{ .value | toJson }}`
4. **Keep templates simple** - Complex templates are maintenance nightmares
5. **Document why** you're using templates instead of YAML functions
6. **Validate generated YAML** using a YAML linter in your CI/CD pipeline

## Supported Functions

Atmos supports an exhaustive list of functions that can be used in [`Go` templates in Atmos stack manifests](/core-concepts/stacks/templates).

You can use the following functions and data sources:

 - [Go `text/template` functions](https://pkg.go.dev/text/template#hdr-Functions)
 - [Sprig Functions](https://masterminds.github.io/sprig/)
 - [Gomplate Functions](https://docs.gomplate.ca/functions/)
 - [Gomplate Datasources](https://docs.gomplate.ca/datasources/)
 - [Atmos Template Functions](/functions/template)

## Native Functions

Atmos also provides template functions that are native to Atmos. Use these with the same caution as other template functions.

<DocCardList/>

## Migration Strategy

Consider migrating template functions to YAML functions:

| Template Function Use Case | YAML Function Alternative |
|---------------------------|---------------------------|
| `{{ exec "terraform output" }}` | `!terraform.output` or `!terraform.state` |
| `{{ env "VAR_NAME" }}` | `!env VAR_NAME` |
| Including external data | `!include file.yaml` |
| Reading from stores | `!store secret_name` |
| Simple value lookups | Direct YAML references |

## Best Practices

1. **Isolate template logic** - Keep templates in separate sections when possible
2. **Use YAML functions for data** - Reserve templates only for control flow
3. **Validate early and often** - Catch template errors before production
4. **Provide fallbacks** - Use template conditionals to handle missing data gracefully
5. **Consider restructuring** - Sometimes changing your YAML structure eliminates the need for templates

---

## !env

import Intro from '@site/src/components/Intro'

<Intro>
    The `!env` Atmos YAML function is used to retrieve environment variables
    and assign them to the sections in Atmos stack manifests.
</Intro>

## Usage

The `!env` function can be called with either one or two parameters:

```yaml
  # Get the value of an environment variable.
  # If the environment variable is not present in the environment, `null` will be assigned
  !env <env-var-name>

  # Get the value of an environment variable.
  # If the environment variable is not present in the environment, the `default-value` will be assigned
  !env <env-var-name> <default-value>
```

## Arguments

<dl>
    <dt>`env-var-name`</dt>
    <dd>
        Environment variable name
    </dd>

    <dt>`default-value`</dt>
    <dd>(Optional) Default value to use if the environment variable is not present in the environment</dd>
</dl>

If the function is called with one argument (the name of the environment variable), and the environment variable is
not present, `null` will be assigned to the corresponding section in the Atmos manifest.

If the function is called with two arguments (the name of the environment variable and the default value), and the
environment variable is not present, the default value will be assigned to the corresponding section in the Atmos manifest.

## Examples

```yaml
vars:
  # `api_key` will be set to `null` if the environment variable `API_KEY` is not present in the environment
  api_key: !env API_KEY
  # `app_name` will be set to the default value `my-app` if the environment variable `APP_NAME` is not present in the environment
  app_name: !env APP_NAME my-app

settings:
  # `provisioned_by_user` will be set to `null` if the environment variable `ATMOS_USER` is not present in the environment
  provisioned_by_user: !env ATMOS_USER
```

## Handling default values with spaces

If you need to provide default values with spaces, enclose them in double quotes and use single quotes around the whole expression.

For example:

```yaml
  # `app_name` will be set to the default value `my app` if the environment variable `APP_NAME` is not present in the environment
  app_name: !env 'APP_NAME "my app"'

  # `app_description` will be set to the default value `my app description` if the environment variable `APP_DESCRIPTION` is not present in the environment
  app_description: !env 'APP_DESCRIPTION "my app description"'
```

:::tip
You can use [Atmos Stack Manifest Templating](/core-concepts/stacks/templates) in the environment variable names and default values when calling the `!env` YAML function.
Atmos processes the templates first, and then executes the `!env` function, allowing you to provide the parameters to
the function dynamically.
:::

---

## !exec

import Intro from '@site/src/components/Intro'

<Intro>
    The `!exec` Atmos YAML function is used to execute shell scripts and assign
    the results to the sections in Atmos stack manifests.
</Intro>

## Usage

```yaml
var1: !exec echo 42

var2: !exec get-data.sh

var3: !exec atmos terraform output <component> -s <stack> --skip-init -- -json test_label_id

var4: !exec atmos terraform output <component> -s {{ .stack }} --skip-init -- -json test_map

var5: !exec atmos terraform output <component> -s {{ .stack }} --skip-init -- -json test_list

var6: |
  !exec
    foo=0
    for i in 1 2 3; do
      foo+=$i
    done
    echo $foo
```

You can execute inline shell scripts, scripts defined in external files, or even Atmos or Terraform commands
(e.g. `!exec atmos terraform output`).

:::warning
`!exec atmos terraform output` is just as examples to show that you can get the outputs of an Atmos component using
the `!exec` YAML function, but it's not recommended to use.
Instead, use the [`!terraform.output`](/functions/yaml/terraform.output)
YAML function. It produces the same results, correctly handles complex types (lists and maps),
has a much simpler syntax, and automatically caches the results for the same component and stack (so if you are calling
it many times on the same component in the same stack, it will execute `terraform output` only once and the stack
processing will be much faster)
:::

The result of a script execution can be a simple type (string, number, or boolean), in which case Atmos
assigns it without modification.

If the result is a complex type (list, map or object), the script must return it as a JSON-encoded string.
After receiving the JSON-encoded string, Atmos automatically decodes it into the corresponding YAML complex type.

:::info
Atmos uses the [`interp`](https://pkg.go.dev/mvdan.cc/sh/v3@v3.10.0/interp) `Go` package to execute the shell scripts
in the `!exec` YAML function.

Package `interp` implements an interpreter that executes shell programs.
It aims to support POSIX and to behave like Bash, but it does not support all of its features.
:::

:::tip
You can use [Atmos Stack Manifest Templating](/core-concepts/stacks/templates) in the `!exec` YAML function expressions.
Atmos processes the templates first, and then executes the `!exec` function, allowing you to provide the parameters to
the function dynamically.
:::

---

## !include

import Intro from '@site/src/components/Intro'
import File from '@site/src/components/File'
import Note from "@site/src/components/Note"

<Intro>
    The `!include` function lets you load files — either local or remote — and insert their contents or specific values
    directly into sections of your stack manifests.
</Intro>

The YAML standard provides [anchors and aliases](https://yaml.org/spec/1.2.2/#3222-anchors-and-aliases), that allow you
to reuse and reference pieces of your YAML file, making it more efficient and reducing duplication.

Atmos supports YAML anchors and aliases, but the biggest limitation is that they are only available within the file in
which they are defined. You cannot reuse anchors across different files.

The `!include` Atmos YAML function overcomes this limitation by allowing you to include the content or specific values
from different local and remote sources. The `!include` function also provides the following features:

- Supports local files with absolute and relative paths.

- Supports the remote protocols provided by the [`go-getter`](https://github.com/hashicorp/go-getter) library.

- Allows you to use [YQ](https://mikefarah.gitbook.io/yq) expressions to query and filter the content of the files to retrieve individual values.

- Determines the format based on file extensions. It supports files in JSON (`.json`), YAML (`.yaml`, `.yml`),
and [HCL](https://github.com/hashicorp/hcl) (`.hcl`, `.tf`, `.tfvars`) formats, and automatically converts them into correct
YAML structures (simple and complex types like maps and lists are supported).
All other file extensions (including `.txt` or files without extensions) are returned as raw strings,
allowing you to include text and [Markdown](https://www.markdownguide.org/) files as strings in Atmos manifests.

Atmos resolves the `!include` functions during the initial loading of YAML files from the local filesystem or remote sources,
injecting the contents of the referenced files directly into the current location.
Files are parsed based on their extension: JSON (`.json`), YAML (`.yaml`, `.yml`), HCL (`.hcl`, `.tf`, `.tfvars`)
are converted into the appropriate type (`boolean`, `map`, `list`, etc.), while other extensions return raw strings.

## Supported File Formats

With `!include` it's possible to import multiple different file formats into Atmos configurations. File parsing is determined by the file extension:

- **JSON** - Files with `.json` extension are parsed as JSON
- **YAML** - Files with `.yaml` or `.yml` extensions are parsed as YAML
- **HCL** - Files with `.hcl`, `.tf`, or `.tfvars` extensions are parsed as HCL
- **Text** - All other extensions (including `.txt` or no extension) are loaded as plain text

<Note>
    File type detection is based on the file extension, not the content. For example, a `.txt` file containing JSON
    will be returned as a raw string. To force any file to be included as raw text regardless of extension,
    use the [`!include.raw`](./include.raw) function.

    **Important for URLs:** URLs that lack a file extension (like `https://api.github.com/meta`) will be treated as
    plain text. If you need to parse such URLs as JSON or YAML, ensure the URL ends with the appropriate extension
    (e.g., `.json`) or save the content to a local file with the correct extension first.
</Note>

### URL Query Strings and Fragments

When including files from URLs with query strings or fragments, the extension detection ignores these URL components:

```yaml
# Extension is detected as .json (query string ignored)
vars: !include https://api.example.com/config.json?version=2&format=raw

# Extension is detected as .yaml (fragment ignored)
settings: !include https://example.com/settings.yaml#section1

# Extension is detected from the path (query/fragment ignored)
config: !include https://example.com/data.json?v=1#field
```

## Supported Sources

### Local Sources

The `!include` function supports the following local file sources:
  - Absolute paths
    ```yaml
    vars: !include /Users/me/Documents/vars.yaml
    ```

  - Paths relative to the current Atmos manifest (where the `!include` function is executed)
    ```yaml
    vars: !include ../config/vars.yaml
    ```

  - Paths relative to the [`base_path`](/cli/configuration/#base-path) defined in `atmos.yaml` CLI config file
    ```yaml
    vars: !include stacks/catalog/vpc/vars.yaml
    ```

### Remote Sources

To download remote files, Atmos uses [`go-getter`](https://github.com/hashicorp/go-getter)
(used by [Terraform](https://www.terraform.io/) for downloading modules) and supports the following protocols to download a single file:

- **http/https** - the file must be publicly accessible (not inside a private repository)
  ```yaml
  vars: !include https://raw.githubusercontent.com/org/repo/main/path/to/vars.yaml
  ```

- **s3** (Amazon S3) - requires the correct AWS permissions and credentials configured
  ```yaml
  vars: !include s3::https://my-bucket.s3.amazonaws.com/path/to/vars.yaml
  ```

- **gcs** (Google Cloud Storage) - requires valid Google Cloud credentials
  ```yaml
  vars: !include gcs::gs://my-bucket/path/to/vars.yaml
  ```

- **scp/sftp** (SSH-based File Transfer) - requires SSH access to the remote server
   ```yaml
   vars: !include scp://user@remote-server:/path/to/vars.yaml
   settings: !include sftp://user@remote-server:/path/to/settings.yaml
   ```

- **oci** (Open Container Initiative)
  ```yaml
  vars: !include oci://ghcr.io/my-org/my-image:path/to/vars.yaml
  ```
  - The file must be exposed as part of the OCI image and exist as a layer in the image (not hidden inside layers)
  - The registry must support OCI artifact downloads (e.g., AWS ECR, Docker Hub, GHCR, GCR)

## Key Benefits of `!include`

The `!include` directive enables modular, reusable configuration patterns across all Atmos YAML files.
This has several important implications:

- **Modularization & Reuse**
  `!include` supports a [DRY](https://en.wikipedia.org/wiki/Don%27t_repeat_yourself) approach by allowing shared configuration fragments to be stored in local or remote
   files and reused across multiple stacks or components. You can use it in place of YAML anchors, which don't work across files.

- **Preprocessing for Inheritance**
  Includes are resolved *before* Atmos processes stacks and components, enabling powerful [inheritance](/core-concepts/stacks/inheritance/) and deep-merging
  behaviors using fully expanded configuration data.

- **Adopting Existing Terraform/OpenTofu Root Modules as Atmos Components**
  If you're already managing your root modules using `.tfvars` files — for example, separate files for `dev`, `staging`,
  and `prod` — you can reference them directly in Atmos using `!include`. This makes it easy to adopt Atmos without
  having to translate all variables into Atmos stacks.

### Example: Referencing Different TFVAR Files Per Environment

For example, instead of rewriting existing `.tfvars` varfiles into inline YAML, Atmos lets you bring them in as-is into your [Stacks](/core-concepts/stacks/).
You can continue managing your root modules as you always have, while gaining Atmos features like stack inheritance,
environment promotion, and deep-merging.

Let's say you already have environment-specific `.tfvars` files like:

- `config/dev.tfvars`
- `config/staging.tfvars`
- `config/prod.tfvars`

You can keep using these files in Atmos by referencing them in your stack configurations:

<File title="stacks/org/dev/app.yaml">
```yaml
components:
  terraform:
    app:
      vars: !include config/dev.tfvars
```
</File>

<File title="stacks/org/staging/app.yaml">
```yaml
components:
  terraform:
    app:
      vars: !include config/staging.tfvars
```
</File>

<File title="stacks/org/prod/app.yaml">
```yaml
components:
  terraform:
    app:
      vars: !include config/prod.tfvars
```
</File>

This pattern allows you to plug Atmos into your existing Terraform/OpenTofu root modules with minimal changes — no need to
duplicate or reformat your varfiles. You also unlock additional capabilities like [listing all your stacks](/cli/commands/list/stacks)
and [components](/cli/commands/list/components), leveraging layered configurations, [stack inheritance with imports](/core-concepts/stacks/imports),
and consistent promotion of settings across environments.

## Usage

The `!include` function can be called with either one or two parameters:

```yaml
  # Download the file and inject the content directly into the current location in the YAML
  !include <file-path>

  # Download the file, filter the content using the YQ expression,
  # and inject the result directly into the current location in the YAML
  !include <file-path> <yq-expression>
```

## Arguments

<dl>
    <dt>`file-path`</dt>
    <dd>
        Path to a local or remote file
    </dd>

    <dt>`yq-expression`</dt>
    <dd>(Optional) [YQ](https://mikefarah.gitbook.io/yq) expression to retrieve individual values from the file</dd>
</dl>

## Using YQ Expressions to retrieve individual values from files

To retrieve individual values from complex types such as maps and lists, or do any kind of filtering or querying,
you can utilize [YQ](https://mikefarah.gitbook.io/yq) expressions.

For example:

- Retrieve the first item from a list

```yaml
subnet_id1: !include <file-path> .private_subnet_ids[0]
```

- Read a key from a map

```yaml
username: !include <file-path> .config_map.username
```

For more details, review the following docs:

- [YQ Guide](https://mikefarah.gitbook.io/yq)
- [YQ Recipes](https://mikefarah.gitbook.io/yq/recipes)

## Handling file paths and YQ expressions with spaces

If you have spaces in the file names or YQ expressions, enclose the file paths and YQ expressions in double quotes
and use single quotes around the whole expression.

For example, on Windows:

```yaml
  vars:
    values: !include '"~/My Documents/dev/values.yaml"'
    config: !include '"~/My Documents/dev/config.json" "<yq-expression-with-spaces>"'
```

On macOS and Linux:

```yaml
  vars:
    values: !include './values.yaml "<yq-expression-with-spaces>"'
    description: !include '"component description.md"'
```

## Examples

<File title="stack.yaml">
```yaml
components:
  terraform:
    my-component:
      vars:
        # Include a local file with the path relative to the current Atmos manifest
        values: !include ./values.yaml
        # Include a local file with the path relative to the current Atmos manifest and query the `vars.ipv4_primary_cidr_block` value from the file using YQ
        ipv4_primary_cidr_block: !include ./vpc_config.yaml .vars.ipv4_primary_cidr_block
        # Include a local file relative to the `base_path` setting in `atmos.yaml`
        vpc_defaults: !include stacks/catalog/vpc/defaults.yaml
        # Include a local file in HCL format
        hcl_values: !include ./values.hcl
        # Include a local varfile in HCL format
        tfvars_values: !include ../components/terraform/vpc/vpc.tfvars
        # Include a local Markdown file
        description: !include ./description.md
        # Include a local text file
        text: !include a.txt
        # Include a local text file with spaces in the file name
        text2: !include '"my config.txt"'
        # Include a local text file on Windows with spaces in the file name, and get the `config.tests` value from the file
        tests: !include '"~/My Documents/dev/tests.yaml" .config.tests'
        # Download and include a remote YAML file using HTTPS protocol, and query the `vars` section from the file
        region_values: !include https://raw.githubusercontent.com/cloudposse/atmos/refs/heads/main/examples/quick-start-advanced/stacks/mixins/region/us-east-2.yaml .vars
        # Download and include a remote JSON file from a URL with .json extension
        # Note: For URLs without extensions, save locally first or use a URL with .json extension
        github_config: !include ./github-meta.json .api
```
</File>

<File title="stack.yaml">
```yaml
# The `config` folder is relative to the `base_path` setting in `atmos.yaml`
import: !include config/import.yaml

# Download the remote file using `go-getter` and assign the `components.terraform.component-1.settings` section
# from the file to the `settings` section in the current stack
settings: !include https://raw.githubusercontent.com/cloudposse/atmos/main/tests/fixtures/scenarios/stack-templates-2/stacks/deploy/nonprod.yaml .components.terraform.component-1.settings

components:
  terraform:
    component-1:
      vars:
        # The `config` folder is relative to the `base_path` setting in `atmos.yaml`
        string_var: !include config/vars.json .string_var
        boolean_var: !include config/vars.yaml .boolean_var
        list_var: !include config/vars.tfvars .list_var
        map_var: !include config/vars.tfvars .map_var

    component-2:
      vars: !include config/vars.tfvars

    component-3:
      vars: !include config/vars.json

    component-4:
      vars: !include config/vars.yaml
```
</File>

---

## !include.raw

import Intro from '@site/src/components/Intro'
import File from '@site/src/components/File'
import Note from "@site/src/components/Note"

<Intro>
    The `!include.raw` function forces any file to be included as a raw string,
    bypassing automatic parsing based on file extension.
</Intro>

While the regular [`!include`](./include) function parses files based on their extension (JSON, YAML, HCL),
`!include.raw` always returns the file content as a plain string. This is useful when you need
the actual file content as text rather than parsed data structures.

## Usage

```yaml
vars:
  # Always injects content from file or URL as string
  json_string: !include.raw config.json
  yaml_string: !include.raw settings.yaml
  hcl_string: !include.raw terraform.tfvars
```

## Common Use Cases

### Template Files

Include template files that need to be processed later:

```yaml
components:
  terraform:
    my-service:
      vars:
        # Kubernetes manifest template as string
        manifest_template: !include.raw k8s-deployment-template.yaml

        # JSON config template for interpolation
        config_template: !include.raw config-template.json

        # Terraform HCL template
        tf_template: !include.raw module-template.tf
```

### Documentation and Scripts

Include documentation or scripts as strings:

```yaml
metadata:
  # Include markdown documentation
  readme: !include.raw README.md
  changelog: !include.raw CHANGELOG.md

  # Include shell scripts
  install_script: !include.raw ./scripts/install.sh
  deploy_script: !include.raw ./scripts/deploy.sh
```

### Preserving Formatting

When you need to preserve the exact formatting of a file:

```yaml
vars:
  # Preserve JSON formatting for display or logging
  example_config: !include.raw example-config.json

  # Keep YAML with comments intact
  annotated_yaml: !include.raw annotated-config.yaml

  # Include HCL with original formatting
  terraform_example: !include.raw example.tf
```

### Configuration as Strings

When you need configuration files as strings for further processing:

```yaml
components:
  terraform:
    app:
      vars:
        # Pass JSON config as string to Terraform
        json_config_string: !include.raw app-config.json

        # YAML config for base64 encoding
        yaml_config_string: !include.raw settings.yaml
```

## Remote Files

The `!include.raw` function works with all remote sources supported by `!include`:

```yaml
vars:
  # GitHub raw content
  remote_template: !include.raw https://raw.githubusercontent.com/org/repo/main/template.yaml

  # S3 bucket files
  s3_script: !include.raw s3://my-bucket/scripts/deploy.sh

  # Google Cloud Storage
  gcs_config: !include.raw gcs://my-bucket/configs/app.json

  # With query parameters (extension detection ignores query strings)
  versioned_template: !include.raw https://api.example.com/template.yaml?version=2
```

## Comparison with !include

| Function | File: `config.json` | Content | Result |
|----------|-------------------|---------|---------|
| `!include` | `config.json` | `{"key": "value"}` | Parsed JSON object: `{key: value}` |
| `!include.raw` | `config.json` | `{"key": "value"}` | Raw string: `'{"key": "value"}'` |

### Example

Given a file `config.json`:
```json
{
  "database": {
    "host": "localhost",
    "port": 5432
  }
}
```

Using in a stack manifest:
```yaml
components:
  terraform:
    app:
      vars:
        # Parsed as JSON object
        config_parsed: !include config.json
        # Result: {database: {host: localhost, port: 5432}}

        # Raw string
        config_string: !include.raw config.json
        # Result: '{"database":{"host":"localhost","port":5432}}'
```

## YQ Expressions Not Supported

<Note>
The `!include.raw` function does not support YQ expressions since it always returns raw strings.
Use the regular `!include` function when you need to query structured data with YQ.
</Note>

```yaml
vars:
  # ❌ YQ expressions are not supported with !include.raw
  # This will fail - !include.raw does not process YQ expressions
  invalid: !include.raw config.json .database.host

  # ✅ Use regular !include for YQ expressions
  valid: !include config.json .database.host
```

## File Extension Handling

Unlike `!include` which uses file extensions to determine parsing behavior,
`!include.raw` completely ignores file extensions:

```yaml
vars:
  # All return raw strings regardless of extension
  json_raw: !include.raw data.json          # Raw string
  yaml_raw: !include.raw config.yaml        # Raw string
  hcl_raw: !include.raw terraform.tfvars    # Raw string
  text_raw: !include.raw readme.txt         # Raw string
  no_ext_raw: !include.raw LICENSE          # Raw string
```

## Supported Sources

The `!include.raw` function supports the same sources as `!include`:

### Local Files
- Absolute paths: `!include.raw /path/to/file.json`
- Relative paths: `!include.raw ../configs/settings.yaml`
- Base path relative: `!include.raw stacks/catalog/template.tf`

### Remote Files
- HTTP/HTTPS: `!include.raw https://example.com/config.json`
- S3: `!include.raw s3::https://bucket.s3.amazonaws.com/file.yaml`
- GCS: `!include.raw gcs::gs://bucket/path/file.hcl`
- Git: `!include.raw git::https://github.com/org/repo.git//file.txt?ref=v1.0.0`

## See Also

- [`!include`](./include) - Standard include function with automatic parsing based on file extension
- [YAML Functions](/functions/yaml) - Overview of all YAML functions in Atmos

---

## Atmos YAML Functions

import File from '@site/src/components/File'
import PillBox from '@site/src/components/PillBox'
import Intro from '@site/src/components/Intro'
import DocCardList from '@theme/DocCardList'

<PillBox>Recommended</PillBox>

<Intro>
    **YAML Functions are the recommended way to add dynamic behavior to your Atmos configurations.** They are native YAML features that work with the YAML parser, making them type-safe, predictable, and error-resistant. Unlike template functions which manipulate raw text, YAML functions operate on structured data after parsing.
</Intro>

## Why YAML Functions Are Safer

YAML functions are based on [YAML Explicit typing](https://yaml.org/spec/1.2.2/)
and user-defined [Explicit Tags](https://yaml.org/spec/1.2.2/#tags) (local data types).
Explicit tags are denoted by the exclamation point (__"!"__) symbol.

**Key advantages over template functions:**
- **Cannot break YAML syntax** - Functions execute after YAML parsing, not before
- **Type-safe** - Work with actual YAML types, not raw strings
- **No indentation issues** - The YAML structure is already established
- **Clear error messages** - Errors point to specific YAML nodes, not parsing failures
- **Predictable behavior** - Always return valid YAML data structures

Atmos detects the tags in the stack manifests and executes the corresponding functions.

:::info
YAML supports three types of data: core, defined, and user-defined.

- **Core types**: Universally supported, including floats, integers, strings, lists, and maps.
- **Defined types**: Advanced types like binary data, specified in the YAML standard but not always supported.
- **User-defined types**: Custom extensions for classes, structures, and functions. Atmos leverages user-defined types to implement its custom functions and extend YAML's capabilities.
:::

## Use-cases

 - The [__`!terraform.output`__](/functions/yaml/terraform.output) YAML function allows you to
   [access component outputs (remote state) directly within Atmos stack manifests](/core-concepts/share-data/remote-state).
   Note that this requires initializing each component (`terraform/tofu init`), which initializes all Terraform/OpenTofu
   modules and downloads all Terraform/OpenTofu providers, which may significantly impact performance. Consider using `!store` or `!terraform.state` functions instead, which don't have this performance penalty.

 - The [__`!terraform.state`__](/functions/yaml/terraform.state) YAML function reads outputs **directly from the configured Terraform or OpenTofu backend**, without initializing providers or running Terraform — it's **very fast** and currently supports [S3 and local backends](/core-concepts/components/terraform/backends) for accessing [remote state](/core-concepts/share-data/remote-state).

 - The [__`!store`__](/functions/yaml/store) YAML function allows reading the values from a
   remote [store](/core-concepts/projects/configuration/stores) (e.g. SSM Parameter Store, Artifactory, etc.)
   into Atmos stack manifests

 - The [__`!store.get`__](/functions/yaml/store.get) YAML function allows retrieving arbitrary keys
   directly from a [store](/core-concepts/projects/configuration/stores)
   without following the Atmos stack/component/key naming convention.
   This is useful for accessing values stored by external systems
   or for retrieving global configuration that doesn't belong to a specific component

 - The [__`!include`__](/functions/yaml/include) YAML function allows downloading local or remote files from different sources,
   and assigning the file contents or individual values to the sections in Atmos stack manifests

 - The [__`!template`__](/functions/yaml/template) YAML function is designed to [evaluate and inject outputs containing maps or lists](/functions/template/atmos.Component#handling-outputs-containing-maps-or-lists)
   into the YAML document, whether generated by the [__`atmos.Component`__](/functions/template/atmos.Component) template function or any Go template.

 - The [__`!exec`__](/functions/yaml/exec) YAML function is used to execute shell scripts and assign
   the results to the sections in Atmos stack manifests

 - The [__`!env`__](/functions/yaml/env) YAML function is used to retrieve environment variables
   and assign them to the sections in Atmos stack manifests

 - The [__`!repo-root`__](/functions/yaml/repo-root) YAML function is used to retrieve the
   root directory of the Atmos repository

:::tip
You can combine [Atmos Stack Manifest Templating](/core-concepts/stacks/templates) with Atmos YAML functions within the same stack configuration.
Atmos processes templates first, followed by YAML functions, enabling you to dynamically provide parameters to the YAML functions.
:::

## Atmos sections supporting YAML functions

You can use the YAML functions in all Atmos stack manifest sections:

  - `vars`
  - `settings`
  - `env`
  - `metadata`
  - `command`
  - `component`
  - `providers`
  - `overrides`
  - `backend`
  - `backend_type`
  - `remote_state_backend`
  - `remote_state_backend_type`

## Examples

<File title="stack.yaml">
```yaml
components:
  terraform:
    component2:
      settings:
        s1: !exec echo 's1'
      env:
        ENV_VAR_1: !template '{{ (atmos.Component "component3" .stack).settings.env.ENV_VAR_1 }}'
      vars:
        # Handle the output of type list from the `atmos.Component` template function
        test_1: !template '{{ toJson (atmos.Component "component1" "plat-ue2-dev").outputs.test_list }}'

        # Handle the output of type map from the `atmos.Component` template function
        test_2: !template '{{ toJson (atmos.Component "component1" .stack).outputs.test_map }}'

        # Execute the shell script and assign the result to the `test_3` variable
        test_3: !exec echo 42

        # Execute the shell script to get the `test_label_id` output from the `component1` component in the stack `plat-ue2-dev`
        test_4: !exec atmos terraform output component1 -s plat-ue2-dev --skip-init -- -json test_label_id

        # Execute the shell script to get the `test_map` output from the `component1` component in the current stack
        test_5: !exec atmos terraform output component1 -s {{ .stack }} --skip-init -- -json test_map

        # Execute the shell script to get the `test_list` output from the `component1` component in the current stack
        test_6: !exec atmos terraform output component1 -s {{ .stack }} --skip-init -- -json test_list

        # Get the `test_label_id` output of type string from the `component1` component in the stack `plat-ue2-dev`
        test_7: !terraform.output component1 plat-ue2-dev test_label_id

        # Get the `test_label_id` output of type string from the `component1` component in the current stack
        test_8: !terraform.output component1 {{ .stack }} test_label_id

        # Get the `test_list` output of type list from the `component1` component in the current stack
        test_9: !terraform.state component1 {{ .stack }} test_list

        # Get the `test_map` output of type map from the `component1` component in the current stack
        test_10: !terraform.state component1 {{ .stack }} test_map

        # Retrieve the value of an environment variable
        api_key: !env API_KEY

        # Include a local file
        config: !include ./dev-config.yaml

        # Download a remote JSON file with .json extension and query data using YQ
        # Note: URLs without extensions are treated as text, so use URLs with appropriate extensions
        github_data: !include ./github-meta.json .api
```
</File>

## Native Atmos YAML Functions

Atmos natively supports the following YAML functions:

<DocCardList/>

---

## !repo-root

import Intro from '@site/src/components/Intro'

<Intro>
    The `!repo-root` Atmos YAML function is used to retrieve the root directory of the Atmos repository.
</Intro>

## Usage

The `!repo-root` function can be called with default value:

```yaml
  # Get the root directory of the Atmos repository.
  # If the Git root is not found, it will return a default value if specified; otherwise, it returns an error.
  `!repo-root <default-value>`
```

## Examples

```yaml
vars:
  # `base_path` will be set to the root directory of the Atmos repository if present otherwise it will return an error
  base_path: !repo-root
```

```yaml
vars:
  # `base_path` will be set to the default value `default-value` if the Atmos repository root is not present
  base_path: !repo-root <default-value>
```

---

## !store.get

import Intro from '@site/src/components/Intro'
import File from '@site/src/components/File'

<Intro>
    The `!store.get` YAML function allows retrieving arbitrary keys directly from a [store](/core-concepts/projects/configuration/stores)
    without following the Atmos stack/component/key naming convention. This is useful for accessing values stored by external systems
    or for retrieving global configuration that doesn't belong to a specific component.
</Intro>

### Usage

The `!store.get` function is called with two parameters, and optionally a default value and
a [YQ](https://mikefarah.gitbook.io/yq) query expression:

```yaml
  !store.get <store_name> <key> | default <default-value> | query <yq-expression>
```

Usage examples:

<File title="stack.yaml">
```yaml
  # Get an arbitrary key from the store
  !store.get <store_name> <key>

  # Get a key and provide a default value
  !store.get <store_name> <key> | default <default-value>

  # Get a key and extract a value using a YQ expression
  !store.get <store_name> <key> | query <yq-expression>

  # Combine default value and query expression
  !store.get <store_name> <key> | default "{}" | query .field
```
</File>

### Arguments

<dl>
    <dt>`store_name`</dt>
    <dd>The name of the store to read from (as defined in the `atmos.yaml` file)</dd>

    <dt>`key`</dt>
    <dd>The exact key name or path to retrieve from the store. This is the literal key as it exists in the store, not constructed from stack/component/key patterns</dd>

    <dt>`default-value`</dt>
    <dd>(optional) The default value to return if the key is not found in the store</dd>

    <dt>`yq-expression`</dt>
    <dd>(optional) [YQ](https://mikefarah.gitbook.io/yq) expression to retrieve individual values from the result</dd>
</dl>

:::tip
You can use [Atmos Stack Manifest Templating](/core-concepts/stacks/templates) in the `!store.get` YAML function expressions.
Atmos processes the templates first, and then executes the `!store.get` function, allowing you to provide the key name
dynamically.
:::

### Key Differences from [`!store`](/functions/yaml/store)

| Feature           | `!store`                                        | `!store.get`                                    |
|-------------------|-------------------------------------------------|-------------------------------------------------|
| **Key Format**    | Constructs key from stack/component/key pattern | Uses exact key as provided                      |
| **Use Case**      | Retrieve Atmos-managed component outputs        | Retrieve arbitrary values from external systems |
| **Key Pattern**   | `prefix/stack/component/key`                    | Any format the store supports                   |
| **Typical Usage** | Cross-component dependencies                    | Global configs, external secrets                |

:::tip
If you need to retrieve values that follow the Atmos stack/component/key pattern, use the
[`!store`](/functions/yaml/store) function instead, as it provides better integration with Atmos component dependencies.
:::

### Using YQ Expressions

You can use [YQ](https://mikefarah.gitbook.io/yq) expressions to extract specific values
from complex data structures:

<File title="stack.yaml">
```yaml
# Extract a field from a JSON object
database_host: !store.get redis app-config | query .database.host

# Get the first item from an array
primary_zone: !store.get azure-keyvault availability-zones | query .[0]

# Extract nested values with a default
api_key: !store.get ssm /external/config | default "{}" | query .api.key
```
</File>

### Examples

#### Redis Store

<File title="stack.yaml">
```yaml
vars:
  # Retrieve a global configuration object
  global_config: !store.get redis global-config

  # Get a specific field from the configuration
  api_version: !store.get redis global-config | query .version

  # Use templating to construct the key dynamically
  regional_config: !store.get redis "config-{{ .vars.region }}"
```
</File>

#### AWS SSM Parameter Store

<File title="stack.yaml">
```yaml
vars:
  # Retrieve a parameter by its full path
  database_password: !store.get ssm /myapp/prod/db/password

  # Get a parameter with a default value
  feature_flag: !store.get ssm /features/new-feature | default "disabled"
```
</File>

#### Azure Key Vault

<File title="stack.yaml">
```yaml
vars:
  # Retrieve a secret by its exact name
  api_secret: !store.get azure-keyvault external-api-key

  # Get a certificate with error handling
  ssl_cert: !store.get azure-keyvault ssl-certificate | default ""
```
</File>

#### Google Secret Manager

<File title="stack.yaml">
```yaml
vars:
  # Retrieve a secret by its resource ID
  service_account: !store.get gsm projects/my-project/secrets/service-account/versions/latest

  # Parse JSON secret and extract a field
  client_id: !store.get gsm oauth-config | query .client_id
```
</File>

### Considerations

- **Exact Key Matching**: The key you provide must match exactly how it is stored in the backend, including any required
prefix, path, or normalization specific to that store
- **External Systems**: This function is ideal for interoperability with values written by external systems that don't
follow Atmos naming conventions
- **Performance**: Direct key access is typically faster than pattern-based retrieval since no key construction is needed
- **Security**: Like `!store`, using `!store.get` with secrets will expose them to `stdout` in describe commands
- **Access Control**: Ensure your store credentials have permission to access the keys you're trying to retrieve

---

## !store

import File from '@site/src/components/File'
import Intro from '@site/src/components/Intro'
import Terminal from '@site/src/components/Terminal'

<Intro>
The `!store` YAML function allows reading the values from a remote [store](/core-concepts/projects/configuration/stores)
(e.g. SSM Parameter Store, Artifactory, Redis, etc.) for an Atmos component in a stack into Atmos stack manifests.
</Intro>

## Usage

The `!store` function can be called with either two, three, or four parameters, and optionally a default value and
a [YQ](https://mikefarah.gitbook.io/yq) query expression:

```yaml
  !store <store_name> <stack> <component> <key> | default <default-value> | query <yq-expression>
```

Usage examples:

```yaml
  # Get the `key` from the store of a `component` in the current stack
  !store <store_name> <component> <key>

  # Get the `key` from the store of a `component` in the current stack,
  # and retrieve an individual value from the result using the YQ expression
  !store <store_name> <component> <key> | query <yq-expression>

  # Get the `key` from the store of a `component` in a different stack
  !store <store_name> <stack> <component> <key>

  # Get the `key` from the store of a `component` in a different stack,
  # and retrieve an individual value from the result using the YQ expression
  !store <store_name> <stack> <component> <key> | query <yq-expression>

  # Get the `key` from the store of a `component` in a different stack, with a default value
  !store <store_name> <stack> <component> <key> | default <default-value>
```

## Arguments

<dl>
  <dt>`store_name`</dt>
  <dd>The name of the store to read from (as defined in the `atmos.yaml` file)</dd>

  <dt>`stack`</dt>
  <dd>(optional) Atmos stack name</dd>

  <dt>`component`</dt>
  <dd>Atmos component name</dd>

  <dt>`key`</dt>
  <dd>The key to read from the store</dd>

  <dt>`default-value`</dt>
  <dd>(optional) The default value to return if the key is not found in the store</dd>

  <dt>`yq-expression`</dt>
  <dd>(optional) [YQ](https://mikefarah.gitbook.io/yq) expression to retrieve individual values from the result</dd>
</dl>

:::tip
You can use [Atmos Stack Manifest Templating](/core-concepts/stacks/templates) in the `!store` YAML function expressions.
Atmos processes the templates first, and then executes the `!store` function, allowing you to provide the parameters to
the function dynamically.
:::

## Using YQ Expressions to retrieve individual values

To retrieve individual values from complex types such as maps and lists, or do any kind of filtering or querying,
you can utilize [YQ](https://mikefarah.gitbook.io/yq) expressions.

For example:

- Retrieve the first item from a list

```yaml
subnet_id1: !store <store_name> <stack> <component> <key> | query .private_subnet_ids[0]
```

- Read a key from a map

```yaml
username: !store <store_name> <stack> <component> <key> | query .config_map.username
```

For more details, review the following docs:

- [YQ Guide](https://mikefarah.gitbook.io/yq)
- [YQ Recipes](https://mikefarah.gitbook.io/yq/recipes)

## Examples

<File title="stack.yaml">
```yaml
components:
  terraform:
    my_lambda_component:
      vars:
        vpc_config:
          security_group_id: !store prod/ssm security-group/lambda id
          security_group_id2: !store prod/ssm {{ .stack }} security-group/lambda2 id
          security_group_id3: !store prod/ssm {{ .atmos_stack }} security-group/lambda3 id
        kms_key_arn: !store prod/ssm kms config | query .arn
```
</File>

## Specifying Atmos `stack`

If you call the `!store` function with three parameters, you need to specify the stack as the second argument.

There are multiple ways you can specify the Atmos stack parameter in the `!terraform.output` function.

### Hardcoded Stack Name

Use it if you want to get a value from the store for a component from a different (well-known and static) stack.
For example, you have a `tgw` component in a stack `plat-ue2-dev` that requires the `vpc_id` key from the `vpc` component from the stack `plat-ue2-prod`:

```yaml title="plat-ue2-dev"
  components:
    terraform:
      tgw:
        vars:
          vpc_id: !store prod/ssm plat-ue2-prod vpc vpc_id
```

### Reference the Current Stack Name

Use the `.stack` (or `.atmos_stack`) template identifier to specify the same stack as the current component is in
(for which the `!store` function is executed):

```yaml
  !store <store_name> {{ .stack }} <component> <key>
  !store <store_name> {{ .atmos_stack }} <component> <key>
```

For example, you have a `tgw` component that requires the `vpc_id` key from the store for the `vpc` component in the same stack:

```yaml
  components:
    terraform:
      tgw:
        vars:
          vpc_id: !store prod/ssm {{ .stack }} vpc vpc_id
```

:::note Using the `.stack` or `.atmos_stack` template identifiers to specify the stack is the same as calling the
`!store` function with two parameters without specifying the current stack, but without using `Go` templates. If you
need to get a value from the store from a component in the current stack, using the `!store` function with two
parameters is preferred because it has a simpler syntax and executes faster.
:::

### Use a Format Function

Use the `printf` template function to construct stack names using static strings and dynamic identifiers.
This is convenient when you want to override some identifiers in the stack name:

```yaml
  !store <store_name> {{ printf "%s-%s-%s" .vars.tenant .vars.environment .vars.stage }} <component> <key>

  !store <store_name> {{ printf "plat-%s-prod" .vars.environment }} <component> <key>

  !store <store_name> {{ printf "%s-%s-%s" .settings.context.tenant .settings.context.region .settings.context.account }} <component> <key>
```

<dl>
  <dt><code>&lt;component&gt;</code></dt>
  <dd>Placeholder for an actual component name (e.g. `vpc`)</dd>
  <dt><code>&lt;key&gt;</code></dt>
  <dd>Placeholder for an actual key (e.g. `subnet_ids`)</dd>
</dl>

For example, you have a `tgw` component deployed in the stack `plat-ue2-dev`. The `tgw` component requires the
`vpc_id` key from the store for the `vpc` component from the same environment (`ue2`) and same stage (`dev`), but from a different
tenant `net` (instead of `plat`):

```yaml title="plat-ue2-dev"
  components:
    terraform:
      tgw:
        vars:
          vpc_id: !store prod/ssm {{ printf "net-%s-%s" .vars.environment .vars.stage }} vpc vpc_id
```

:::tip Important
    By using the `printf "%s-%s-%s"` function, you are constructing stack names using the stack context variables/identifiers.

    For more information on Atmos stack names and how to define them, refer to `stacks.name_pattern` and `stacks.name_template`
    sections in [`atmos.yaml` CLI config file](/cli/configuration/)
:::

## Considerations

- Using `!store` with secrets can expose sensitive data to standard output (stdout) in any commands that describe stacks or components.
- When using `!store` with [`atmos describe affected`](/cli/commands/describe/affected), Atmos requires access to all referenced stores.
   If you operate with limited permissions (e.g., scoped to `dev`) and reference production stacks, the command will fail.
- Be mindful of disaster recovery (DR) implications when using it across regions.
- Consider cold-start scenarios: if the dependent component has not yet been provisioned, the value in the store may not
  yet be available and the `!store` function call will fail unless you provide a default value with `| default`.

---

## !template

import Intro from '@site/src/components/Intro'

<Intro>
    The `!template` Atmos YAML function is used to [handle the outputs containing maps or
    lists](/functions/template/atmos.Component#handling-outputs-containing-maps-or-lists)
    returned from the [`atmos.Component`](/functions/template/atmos.Component) template function.
</Intro>

## Usage

```yaml
# Process the output of type list from the `atmos.Component` template function in the provided stack
var1: !template '{{ toJson (atmos.Component "<component>" "<stack>").outputs.test_list }}'

# Process the output of type map from the `atmos.Component` template function in the current stack
var2: !template '{{ toJson (atmos.Component "component1" .stack).outputs.test_map }}'
```

## Why `!template` is needed?

You can use the [`atmos.Component`](/functions/template/atmos.Component) template function to
read outputs (remote state) from Terraform/OpenTofu components, and use those in your stack manifests.

When the output of the `atmos.Component` function is a simple type (string or number), it's correctly handled in YAML,
and is sent to the Terraform/OpenTofu component as a simple type.

For example, this function:

```yaml
var1: '{{ (atmos.Component "<component>" "<stack>").outputs.test_string }}'
```

produces the following result:

```yaml
var1: test
```

When the outputs are complex types (list or map):

```yaml
var1: '{{ toJson (atmos.Component "<component>" "<stack>").outputs.test_list }}'
var2: '{{ toJson (atmos.Component "component1" "<stack>").outputs.test_map }}'
```

we'll get the following results:

```yaml
var1: '["item_1","item_2","item_3"]'
var2: '{"a":1,"b":2,"c":3}'
```

Because the template expressions are quoted, the results are JSON-encoded strings, not objects.

The results can be sent to the `var1` and `var2` Terraform variables, but the variables need to be of type `string`, and
you'll have to decode the strings into Terraform list and map using the
[`jsondecode`](https://developer.hashicorp.com/terraform/language/functions/jsondecode) function in your Terraform code.
In many cases, this is not an acceptable solution because the Terraform variables `var1` and `var2` are already of type list and map,
and you can't (or don't want to) change the Terraform code to convert them into strings.

We can try to un-quote the template expressions:

```yaml
var1: {{ toJson (atmos.Component "<component>" "<stack>").outputs.test_list }}
var2: {{ toJson (atmos.Component "component1" "<stack>").outputs.test_map }}
```

but it does not work because it's not a valid YAML.
In YAML, curly braces `{ }` are used to denote a JSON-like inline mapping, which corresponds to a map or dictionary in YAML,
and the double curly braces are not valid in YAML.

We can try to use [YAML multiline strings](https://yaml-multiline.info/) with the block style indicator, and un-quote the templates:

```yaml
var1: >-
  {{ toJson (atmos.Component "<component>" "<stack>").outputs.test_list }}

var2: >-
  {{ toJson (atmos.Component "component1" "<stack>").outputs.test_map }}
```

but it still generates the same result (JSON-encoded strings, not JSON objects):

```yaml
var1: |
  ["item_1","item_2","item_3"]

var2: |
  {"a":1,"b":2,"c":3}
```

__The `!template` Atmos YAML function to the rescue!__

The `!template` YAML function receives the result
from the [`atmos.Component`](/functions/template/atmos.Component)
and [`toJson`](https://masterminds.github.io/sprig/defaults.html) functions, and converts it into the complex
types (list or map) by decoding the JSON strings.

The following `!template` function calls:

```yaml
var1: !template '{{ toJson (atmos.Component "<component>" "<stack>").outputs.test_list }}'
var2: !template '{{ toJson (atmos.Component "component1" .stack).outputs.test_map }}'
```

generates the following YAML:

```yaml
var1:
  - item_1
  - item_2
  - item_3

var2:
  a: 1
  b: 2
  c: 3
```

The results are correct list and map YAML types, and can be sent to the Terraform component without modifying the types
of its input variables.

:::tip

When reading Atmos components outputs (remote state) in Atmos stack manifests, instead of using the three functions
`atmos.Component`, `toJson` and `!template`, use the [`!terraform.output`](/functions/yaml/terraform.output)
YAML function. It produces the same results, correctly handles the complex types (lists and maps), and has a much simpler syntax.

:::

## Advanced Examples

The `!template` Atmos YAML function can be used to make your stack configuration DRY and reusable.

For example, suppose we need to restrict the Security Group ingresses on all components provisioned in the infrastructure
(e.g. EKS cluster, RDS Aurora cluster, MemoryDB cluster, Istio Ingress Gateway) to a specific list of IP CIDR blocks.

We can define the list of allowed CIDR blocks in the global `settings` section (used by all components in all stacks)
in the `allowed_ingress_cidrs` variable:

```yaml
settings:
  allowed_ingress_cidrs:
    - "10.20.0.0/20"  # VPN 1
    - "10.30.0.0/20"  # VPN 2
```

We can then use the `!template` function with the following template in all components that need their Security Group
to be restricted:

```yaml
# EKS cluster
# Allow ingress only from the allowed CIDR blocks
allowed_cidr_blocks: !template '{{ toJson .settings.allowed_ingress_cidrs }}'
```

```yaml
# RDS cluster
# Allow ingress only from the allowed CIDR blocks
cidr_blocks: !template '{{ toJson .settings.allowed_ingress_cidrs }}'
```

```yaml
# Istio Ingress Gateway
# Allow ingress only from the allowed CIDR blocks
security_group_ingress_cidrs: !template '{{ toJson .settings.allowed_ingress_cidrs }}'
```

The `!template` function and the `'{{ toJson .settings.allowed_ingress_cidrs }}'` expression allows you to
use the global `allowed_ingress_cidrs` variable and the same template even if the components have different
variable names for the allowed CIDR blocks (which would be difficult to implement using
[Atmos inheritance](/core-concepts/stacks/inheritance) or other [Atmos design patterns](/design-patterns)).

:::tip
To append additional CIDRs to the template itself, use the `list` and [Sprig](https://masterminds.github.io/sprig/lists.html)
`concat` functions:

```yaml
allowed_cidr_blocks: !template '{{ toJson (concat .settings.allowed_ingress_cidrs (list "172.20.0.0/16")) }}'
```
:::

---

## !terraform.output

import File from '@site/src/components/File'
import Intro from '@site/src/components/Intro'
import Terminal from '@site/src/components/Terminal'

<Intro>
The `!terraform.output` YAML function allows reading the outputs ([remote state](/core-concepts/share-data/remote-state))
of components directly in Atmos stack manifests by internally executing a
[`terraform output`](https://developer.hashicorp.com/terraform/cli/commands/output) or
[`tofu output`](https://opentofu.org/docs/cli/commands/output/) command.
</Intro>

## Usage

The `!terraform.output` function can be called with either two or three parameters:

```yaml
  # Get the `output` of the `component` in the current stack
  !terraform.output <component> <output>

  # Get the `output` of the `component` in the provided `stack`
  !terraform.output <component> <stack> <output>

  # Get the output of the `component` by evaluating the YQ expression
  !terraform.output <component> <yq-expression>

 # Get the output of the `component` in the provided `stack` by evaluating the YQ expression
  !terraform.output <component> <stack> <yq-expression>
```

## Arguments

<dl>
  <dt>`component`</dt>
  <dd>Atmos component name</dd>

  <dt>`stack`</dt>
  <dd>(Optional) Atmos stack name</dd>

  <dt>`output` or `yq-expression`</dt>
  <dd>Terraform output or [YQ](https://mikefarah.gitbook.io/yq) expression to evaluate the output</dd>
</dl>

:::tip
You can use [Atmos Stack Manifest Templating](/core-concepts/stacks/templates) in the `!terraform.output` YAML function expressions.
Atmos processes the templates first, and then executes the `!terraform.output` function, allowing you to provide the parameters to
the function dynamically.
:::

## `!terraform.output` Function Execution Flow

When processing the `!terraform.output` YAML function for a component in a stack, Atmos executes the following steps:

- **Stack and Component Context Resolution**
Atmos resolves the full context for the specified component within the given stack, including all inherited and merged
configuration layers (globals, environment and component-level config).

- **Terraform/OpenTofu Variables**
Based on the resolved context, Atmos generates a `varfile` with all the variables for the component in the stack.

- **Terraform/OpenTofu Backend**
Atmos generates a backend configuration file for the component in the stack based on the resolved context.

- **Terraform/OpenTofu Provider Overrides**
Atmos generates a provider override file for the component in the stack (if provider overrides are configured in the stack manifests).

- **Terraform/OpenTofu Workspace**
Based on the resolved context, Atmos selects (or creates new) Terraform/OpenTofu workspace by executing
`terraform/tofu workspace` commands.

- **Terraform/OpenTofu Init**
Atmos executes a `terraform/tofu init` command to initialize the component and download all Terraform/OpenTofu modules and
providers (if they are not present in the cache).

- **Terraform/OpenTofu Output**
Atmos executes a `terraform/tofu output` command to read the outputs of the component in the stack.

- **Output Parsing and Interpolation**
The relevant output variable is extracted from the state file (using a [YQ](https://mikefarah.gitbook.io/yq/) parser).
Atmos parses and interpolates the value into the final configuration structure, replacing the `!terraform.output`
directive in the YAML stack manifest with the final value.

:::tip
Since Atmos executes the `terraform/tofu init` and `terraform/tofu output` commands when processing the `!terraform.output`
YAML functions, it can significantly impact performance because it requires initializing the components,
which initializes all Terraform/OpenTofu modules and downloads all Terraform/OpenTofu providers (if they are not present in the cache).

To improve performance, consider using the [__`!store`__](/functions/yaml/store)
or [__`!terraform.state`__](/functions/yaml/terraform.state)
YAML function.

To understand the performance implications of the `!terraform.output` and `!terraform.state` functions,
compare the [!terraform.output Execution Flow](/functions/yaml/terraform.output#terraformoutput-function-execution-flow) with the
[!terraform.state Execution Flow](/functions/yaml/terraform.state#terraformstate-function-execution-flow).
:::

## Using YQ Expressions to retrieve items from complex output types

To retrieve items from complex output types such as maps and lists, or do any kind of filtering or querying,
you can utilize [YQ](https://mikefarah.gitbook.io/yq) expressions.

For example:

- Retrieve the first item from a list

```yaml
subnet_id1: !terraform.output vpc .private_subnet_ids[0]
```

- Read a key from a map

```yaml
username: !terraform.output config .config_map.username
```

For more details, review the following docs:

- [YQ Guide](https://mikefarah.gitbook.io/yq)
- [YQ Recipes](https://mikefarah.gitbook.io/yq/recipes)

## Using YQ Expressions to provide a default value

If the component for which you are reading the output has not been provisioned yet, the `!terraform.output` function
will return the string `<no value>` unless you specify a [default value](https://mikefarah.gitbook.io/yq/operators/alternative-default-value)
in the YQ expression, in which case the function will return the default value.

This will allow you to mock outputs when executing `atmos terraform plan` where there are dependencies between components,
and the dependent components are not provisioned yet.

:::note
To provide a default value, you use the `//` YQ operator.
The whole YQ expression contains spaces, and to make it a single parameter, you need to double-quote it.

YQ requires the strings in the default values to be double-quoted as well.
This means that you have to escape the double-quotes in the default values by using two double-quotes.
:::

For example:

- Specify a string default value.
  Read the `username` output from the `config` component in the current stack.
  If the `config` component has not been provisioned yet, return the default value `default-user`

```yaml
username: !terraform.output config ".username // ""default-user"""
```

- Specify a list default value.
  Read the `private_subnet_ids` output from the `vpc` component in the current stack.
  If the `vpc` component has not been provisioned yet, return the default value `["mock-subnet1", "mock-subnet2"]`

```yaml
subnet_ids: !terraform.output vpc ".private_subnet_ids // [""mock-subnet1"", ""mock-subnet2""]"
```

- Specify a map default value.
  Read the `config_map` output from the `config` component in the current stack.
  If the `config` component has not been provisioned yet, return the default value `{"api_endpoint": "localhost:3000", "user": "test"}`

```yaml
config_map: !terraform.output 'config ".config_map // {""api_endpoint"": ""localhost:3000"", ""user"": ""test""}"'
```

For more details, review the following docs:

- [YQ Alternative (Default value)](https://mikefarah.gitbook.io/yq/operators/alternative-default-value)

## Using YQ Expressions to modify values returned from the remote state

Since the `output` parameter of the `!terraform.output` function is a [YQ](https://mikefarah.gitbook.io/yq) expression,
you can use [YQ pipes](https://mikefarah.gitbook.io/yq/operators/pipe) and
[YQ operators](https://mikefarah.gitbook.io/yq/operators) (including [YQ string concatenation functions](https://mikefarah.gitbook.io/yq/operators/add#string-concatenation)
and [YQ arithmetic functions](https://mikefarah.gitbook.io/yq/operators/add))
to modify the values returned from the remote state.

For example, suppose you have an `aurora-postgres` Atmos component which has the output `master_hostname`.

To read the output without modification, you can use the following expressions:

```yaml
postgres_url: !terraform.output aurora-postgres master_hostname
```

```yaml
postgres_url: !terraform.output aurora-postgres .master_hostname
```

To prepend and append strings to the output, you can use YQ pipes and the `add` function (`+` operator):

```yaml
postgres_url: !terraform.output aurora-postgres ".master_hostname | ""jdbc:postgresql://"" + . + "":5432/events"""
```

:::note
The double quotes are required around the whole YQ expression, and around the strings inside the YQ expression,
hence the double-double quotes `""` to escape it.
:::

After the `!terraform.output` function is executed, the `postgres_url` variable will have the final value similar to:

```yaml
postgres_url: "jdbc:postgresql://aurora-postgres-cluster-writer.prod.plat.mydomain.net:5432/events"
```

For more details, review the following docs:

- [YQ pipe](https://mikefarah.gitbook.io/yq/operators/pipe)
- [YQ operators](https://mikefarah.gitbook.io/yq/operators)
- [YQ string concatenation](https://mikefarah.gitbook.io/yq/operators/add#string-concatenation)
- [YQ `add` function](https://mikefarah.gitbook.io/yq/operators/add)

## Examples

<File title="stack.yaml">
```yaml
components:
  terraform:
    my_lambda_component:
      vars:
        vpc_config:
          # Output of type string
          security_group_id: !terraform.output security-group/lambda id
          security_group_id2: !terraform.output security-group/lambda2 {{ .stack }} id
          security_group_id3: !terraform.output security-group/lambda3 {{ .atmos_stack }} id
          # Output of type list
          subnet_ids: !terraform.output vpc private_subnet_ids
          # Use a YQ expression to get an item from the list
          subnet_id1: !terraform.output vpc .private_subnet_ids[0]
          # Output of type map
          config_map: !terraform.output config {{ .stack }} config_map
          # Use a YQ expression to get a value from the map
          username: !terraform.output config .config_map.username
```
</File>

## Specifying Atmos `stack`

If you call the `!terraform.output` function with three parameters, you need to specify the stack as the second argument.

There are multiple ways you can specify the Atmos stack parameter in the `!terraform.output` function.

### Hardcoded Stack Name

Use it if you want to get an output from a component from a different (well-known and static) stack.
For example, you have a `tgw` component in a stack `plat-ue2-dev` that requires the `vpc_id` output from the `vpc` component from the stack `plat-ue2-prod`:

```yaml title="plat-ue2-dev"
  components:
    terraform:
      tgw:
        vars:
          vpc_id: !terraform.output vpc plat-ue2-prod vpc_id
```

### Reference the Current Stack Name

Use the `.stack` (or `.atmos_stack`) template identifier to specify the same stack as the current component is in
(for which the `!terraform.output` function is executed):

```yaml
  !terraform.output <component> {{ .stack }} <output>
  !terraform.output <component> {{ .atmos_stack }} <output>
```

For example, you have a `tgw` component that requires the `vpc_id` output from the `vpc` component in the same stack:

```yaml
  components:
    terraform:
      tgw:
        vars:
          vpc_id: !terraform.output vpc {{ .stack }} vpc_id
```

:::note
Using the `.stack` or `.atmos_stack` template identifiers to specify the stack is the same as calling the `!terraform.output`
function with two parameters without specifying the current stack, but without using `Go` templates.
If you need to get an output of a component in the current stack, using the `!terraform.output` function with two parameters
is preferred because it has a simpler syntax and executes faster.
:::

### Use a Format Function

Use the `printf` template function to construct stack names using static strings and dynamic identifiers.
This is convenient when you want to override some identifiers in the stack name:

```yaml
  !terraform.output <component> {{ printf "%s-%s-%s" .vars.tenant .vars.environment .vars.stage }} <output>

  !terraform.output <component> {{ printf "plat-%s-prod" .vars.environment }} <output>

  !terraform.output <component> {{ printf "%s-%s-%s" .settings.context.tenant .settings.context.region .settings.context.account }} <output>
```

<dl>
  <dt>`<component>`</dt>
  <dd>Placeholder for an actual component name (e.g. `vpc`)</dd>
  <dt>`<output>`</dt>
  <dd>Placeholder for an actual Terraform output (e.g. `subnet_ids`)</dd>
</dl>

For example, you have a `tgw` component deployed in the stack `plat-ue2-dev`. The `tgw` component requires the
`vpc_id` output from the `vpc` component from the same environment (`ue2`) and same stage (`dev`), but from a different
tenant `net` (instead of `plat`):

```yaml title="plat-ue2-dev"
  components:
    terraform:
      tgw:
        vars:
          vpc_id: !terraform.output vpc {{ printf "net-%s-%s" .vars.environment .vars.stage }} vpc_id
```

:::tip Important
    By using the `printf "%s-%s-%s"` function, you are constructing stack names using the stack context variables/identifiers.

    For more information on Atmos stack names and how to define them, refer to `stacks.name_pattern` and `stacks.name_template`
    sections in [`atmos.yaml` CLI config file](/cli/configuration/)
:::

## Caching the result of `!terraform.output` function

Atmos caches (in memory) the results of `!terraform.output` function.

The cache is per Atmos CLI command execution, e.g., each new execution of a command like `atmos terraform plan`,
`atmos terraform apply` or `atmos describe component` will create and use a new memory cache, which involves re-invoking `terraform outputs` after reinitialization.

If you define the function in stack manifests for the same component in a stack more than once, the first call will
produce the result and cache it, and all the consecutive calls will just use the cached data. This is useful when you use the
`!terraform.output` function for the same component in a stack in multiple places in Atmos stack manifests.
It will speed up the function execution and stack processing.

For example:

<File>
```yaml
components:
  terraform:
    test2:
      vars:
        tags:
          test: !terraform.output test id
          test2: !terraform.output test id
          test3: !terraform.output test {{ .stack }} id
```
</File>

In the example, the `test2` Atmos component uses the outputs (remote state) of the `test` Atmos component from the same stack.
The YAML function `!terraform.output` is executed three times (once for each tag).

After the first execution, Atmos caches the result in memory,
and reuses it in the next two calls to the function. The caching makes the stack processing much faster.
In a production environment where many components are used, the speedup can be significant.

## Using `!terraform.output` with `static` remote state backend

Atmos supports [brownfield configuration by using the remote state of type `static`](/core-concepts/components/terraform/brownfield/#hacking-remote-state-with-static-backends).

For example:

<File title="stack.yaml">
```yaml
components:
  terraform:
    # Component `static-backend` is configured with the remote state backend of type `static`
    static-backend:
      remote_state_backend_type: static
      remote_state_backend:
        static:
          region: "us-west-2"
          cluster_name: "production-cluster"
          vpc_cidr: "10.0.0.0/16"
          database:
            type: "postgresql"
            version: "12.7"
            storage_gb: 100
          allowed_ips:
            - "192.168.1.0/24"
            - "10.1.0.0/16"
          tags:
            Environment: "production"
            Owner: "infra-team"

    eks-cluster:
      vars:
        region: !terraform.output static-backend region
        cluster_name: !terraform.output static-backend cluster_name
        vpc_cidr: !terraform.output static-backend vpc_cidr
        db_type: !terraform.output static-backend database.type
        db_storage: !terraform.output static-backend database.storage_gb
        allowed_ips: !terraform.output static-backend allowed_ips
        tags: !terraform.output static-backend tags
```
</File>

When the functions are executed, Atmos detects that the `static-backend` component has the `static` remote state configured,
and instead of executing `terraform output`, it just returns the static values from the `remote_state_backend.static` section.

Executing the command `atmos describe component eks-cluster -s <stack>` produces the following result:

<Terminal title="atmos describe component eks-cluster -s <stack>">
```yaml
vars:
  region: us-west-2
  cluster_name: production-cluster
  vpc_cidr: 10.0.0.0/16
  db_type: postgresql
  db_storage: 100
  allowed_ips:
    - 192.168.1.0/24
    - 10.1.0.0/16
  tags:
    Environment: production
    Owner: infra-team
```
</Terminal>

## Considerations

 - Using `!terraform.output` with secrets can expose sensitive data to standard output (stdout) in any commands that describe stacks or components.

 - When using `!terraform.output` with [`atmos describe affected`](/cli/commands/describe/affected), Atmos requires access to all referenced remote states.
   If you operate with limited permissions (e.g., scoped to `dev`) and reference production stacks, the command will fail.

 - Overusing the function within stacks to reference multiple components can significantly impact performance since Atmos internally executes a
  [`terraform output`](https://developer.hashicorp.com/terraform/cli/commands/output) or
  [`tofu output`](https://opentofu.org/docs/cli/commands/output/) command, which requires initializing the component,
  which initializes all Terraform/OpenTofu modules and downloads all Terraform/OpenTofu providers.

 - Be mindful of disaster recovery (DR) implications when using it across regions.

 - Consider cold-start scenarios: if the dependent component has not yet been provisioned, `terraform output` will fail.

---

## !terraform.state

import File from '@site/src/components/File'
import Intro from '@site/src/components/Intro'
import Terminal from '@site/src/components/Terminal'

<Intro>
The `!terraform.state` YAML function is the **fastest** way to read Terraform/OpenTofu outputs ([remote state](/core-concepts/share-data/remote-state))
in Atmos stack manifests. It retrieves outputs directly from the configured [backends](/core-concepts/components/terraform/backends)
without the overhead of initializing Terraform, downloading providers, or generating configuration files - making it significantly faster than `!terraform.output`.
</Intro>

:::warning
Currently, the `!terraform.state` YAML function supports the following backend types:
- `local` ([Terraform](https://developer.hashicorp.com/terraform/language/settings/backends/local) and [OpenTofu](https://opentofu.org/docs/language/settings/backends/local))
- `s3` ([Terraform](https://developer.hashicorp.com/terraform/language/settings/backends/s3) and [OpenTofu](https://opentofu.org/docs/language/settings/backends/s3))

As support for new backend types is added (e.g. `azurerm`, `gcs`), this document will be updated accordingly.

Meanwhile, if you are using backends other than `local` and `s3`, consider the [__`!store`__](/functions/yaml/store)
or [__`!terraform.output`__](/functions/yaml/terraform.output) YAML function to read remote state
and [share data between components](/core-concepts/share-data).
:::

## Usage

The `!terraform.state` function can be called with either two or three parameters:

```yaml
  # Get the `output` of the `component` in the current stack
  !terraform.state <component> <output>

  # Get the `output` of the `component` in the provided `stack`
  !terraform.state <component> <stack> <output>

  # Get the output of the `component` by evaluating the YQ expression
  !terraform.state <component> <yq-expression>

 # Get the output of the `component` in the provided `stack` by evaluating the YQ expression
  !terraform.state <component> <stack> <yq-expression>
```

## Arguments

<dl>
  <dt>`component`</dt>
  <dd>Atmos component name</dd>

  <dt>`stack`</dt>
  <dd>(Optional) Atmos stack name</dd>

  <dt>`output` or `yq-expression`</dt>
  <dd>Terraform output or [YQ](https://mikefarah.gitbook.io/yq) expression to evaluate the output</dd>
</dl>

:::tip
You can use [Atmos Stack Manifest Templating](/core-concepts/stacks/templates) in the `!terraform.state` YAML function expressions.
Atmos processes the templates first, and then executes the `!terraform.state` function, allowing you to provide the parameters to
the function dynamically.
:::

## `!terraform.state` Function Execution Flow

When processing the `!terraform.state` YAML function for a component in a stack, Atmos executes the following steps:

- **Stack and Component Context Resolution**
Atmos resolves the full context for the specified component within the given stack, including all inherited and merged
configuration layers (globals, environment and component-level config).

- **Terraform State Backend Lookup and Read**
Based on the resolved context, Atmos identifies the corresponding backend for the component in the stack
and reads the state file directly from the backend. If access to the backend requires a role assumption
(e.g. `assume_role.role_arn` for the `s3` backend), Atmos assumes the role before accessing the backend state file.

- **Output Parsing and Interpolation**
The relevant output variable is extracted from the state file (using a [YQ](https://mikefarah.gitbook.io/yq/) parser).
Atmos parses and interpolates the value into the final configuration structure, replacing the `!terraform.state`
directive in the YAML stack manifest with the final value.

:::tip Performance Advantage
**`!terraform.state` is dramatically faster than `!terraform.output`** - often 10-100x faster depending on your infrastructure size.

While `!terraform.output` must:
- Initialize Terraform/OpenTofu
- Download and initialize all providers
- Generate varfiles and backend configs
- Execute terraform commands

`!terraform.state` bypasses all of this overhead by reading directly from the state backend.

Use `!terraform.state` when you need the **fastest possible** access to Terraform outputs. The functions accept the same parameters and produce identical results.

Compare the [!terraform.output Execution Flow](/functions/yaml/terraform.output#terraformoutput-function-execution-flow) with the
[!terraform.state Execution Flow](/functions/yaml/terraform.state#terraformstate-function-execution-flow) to see the dramatic difference.
:::

## Using YQ Expressions to retrieve items from complex output types

To retrieve items from complex output types such as maps and lists, or do any kind of filtering or querying,
you can utilize [YQ](https://mikefarah.gitbook.io/yq) expressions.

For example:

- Retrieve the first item from a list

```yaml
subnet_id1: !terraform.state vpc .private_subnet_ids[0]
```

- Read a key from a map

```yaml
username: !terraform.state config .config_map.username
```

For more details, review the following docs:

- [YQ Guide](https://mikefarah.gitbook.io/yq)
- [YQ Recipes](https://mikefarah.gitbook.io/yq/recipes)

## Using YQ Expressions to provide a default value

If the component for which you are reading the output has not been provisioned yet, the `!terraform.state` function
will return `null`, unless you specify a [default value](https://mikefarah.gitbook.io/yq/operators/alternative-default-value)
in the YQ expression, in which case the function will return the default value.

This will allow you to mock outputs when executing `atmos terraform plan` where there are dependencies between components,
and the dependent components are not provisioned yet.

:::note
To provide a default value, you use the `//` YQ operator.
The whole YQ expression contains spaces, and to make it a single parameter, you need to double-quote it.

YQ requires the strings in the default values to be double-quoted as well.
This means that you have to escape the double-quotes in the default values by using two double-quotes.
:::

For example:

- Specify a string default value.
  Read the `username` output from the `config` component in the current stack.
  If the `config` component has not been provisioned yet, return the default value `default-user`

```yaml
username: !terraform.state config ".username // ""default-user"""
```

- Specify a list default value.
  Read the `private_subnet_ids` output from the `vpc` component in the current stack.
  If the `vpc` component has not been provisioned yet, return the default value `["mock-subnet1", "mock-subnet2"]`

```yaml
subnet_ids: !terraform.state vpc ".private_subnet_ids // [""mock-subnet1"", ""mock-subnet2""]"
```

- Specify a map default value.
  Read the `config_map` output from the `config` component in the current stack.
  If the `config` component has not been provisioned yet, return the default value `{"api_endpoint": "localhost:3000", "user": "test"}`

```yaml
config_map: !terraform.state 'config ".config_map // {""api_endpoint"": ""localhost:3000"", ""user"": ""test""}"'
```

For more details, review the following docs:

- [YQ Alternative (Default value)](https://mikefarah.gitbook.io/yq/operators/alternative-default-value)

## Using YQ Expressions to modify values returned from the remote state

Since the `output` parameter of the `!terraform.state` function is a [YQ](https://mikefarah.gitbook.io/yq) expression,
you can use [YQ pipes](https://mikefarah.gitbook.io/yq/operators/pipe) and
[YQ operators](https://mikefarah.gitbook.io/yq/operators) (including [YQ string concatenation functions](https://mikefarah.gitbook.io/yq/operators/add#string-concatenation)
and [YQ arithmetic functions](https://mikefarah.gitbook.io/yq/operators/add))
to modify the values returned from the remote state.

For example, suppose you have an `aurora-postgres` Atmos component which has the output `master_hostname`.

To read the output without modification, you can use the following expressions:

```yaml
postgres_url: !terraform.state aurora-postgres master_hostname
```

```yaml
postgres_url: !terraform.state aurora-postgres .master_hostname
```

To prepend and append strings to the output, you can use YQ pipes and the `add` function (`+` operator):

```yaml
postgres_url: !terraform.state aurora-postgres ".master_hostname | ""jdbc:postgresql://"" + . + "":5432/events"""
```

:::note
The double quotes are required around the whole YQ expression, and around the strings inside the YQ expression,
hence the double-double quotes `""` to escape it.
:::

After the `!terraform.state` function is executed, the `postgres_url` variable will have the final value similar to:

```yaml
postgres_url: "jdbc:postgresql://aurora-postgres-cluster-writer.prod.plat.mydomain.net:5432/events"
```

For more details, review the following docs:

- [YQ pipe](https://mikefarah.gitbook.io/yq/operators/pipe)
- [YQ operators](https://mikefarah.gitbook.io/yq/operators)
- [YQ string concatenation](https://mikefarah.gitbook.io/yq/operators/add#string-concatenation)
- [YQ `add` function](https://mikefarah.gitbook.io/yq/operators/add)

## Examples

<File title="stack.yaml">
```yaml
components:
  terraform:
    my_lambda_component:
      vars:
        vpc_config:
          # Output of type string
          security_group_id: !terraform.state security-group/lambda id
          security_group_id2: !terraform.state security-group/lambda2 {{ .stack }} id
          security_group_id3: !terraform.state security-group/lambda3 {{ .atmos_stack }} id
          # Output of type list
          subnet_ids: !terraform.state vpc private_subnet_ids
          # Use a YQ expression to get an item from the list
          subnet_id1: !terraform.state vpc .private_subnet_ids[0]
          # Output of type map
          config_map: !terraform.state config {{ .stack }} config_map
          # Use a YQ expression to get a value from the map
          username: !terraform.state config .config_map.username
```
</File>

## Specifying Atmos `stack`

If you call the `!terraform.state` function with three parameters, you need to specify the stack as the second argument.

There are multiple ways you can specify the Atmos stack parameter in the `!terraform.state` function.

### Hardcoded Stack Name

Use it if you want to get an output from a component from a different (well-known and static) stack.
For example, you have a `tgw` component in a stack `plat-ue2-dev` that requires the `vpc_id` output from the `vpc` component from the stack `plat-ue2-prod`:

```yaml title="plat-ue2-dev"
  components:
    terraform:
      tgw:
        vars:
          vpc_id: !terraform.state vpc plat-ue2-prod vpc_id
```

### Reference the Current Stack Name

Use the `.stack` (or `.atmos_stack`) template identifier to specify the same stack as the current component is in
(for which the `!terraform.state` function is executed):

```yaml
  !terraform.state <component> {{ .stack }} <output>
  !terraform.state <component> {{ .atmos_stack }} <output>
```

For example, you have a `tgw` component that requires the `vpc_id` output from the `vpc` component in the same stack:

```yaml
  components:
    terraform:
      tgw:
        vars:
          vpc_id: !terraform.state vpc {{ .stack }} vpc_id
```

:::note
Using the `.stack` or `.atmos_stack` template identifiers to specify the stack is the same as calling the `!terraform.state`
function with two parameters without specifying the current stack, but without using `Go` templates.
If you need to get an output of a component in the current stack, using the `!terraform.state` function with two parameters
is preferred because it has a simpler syntax and executes faster.
:::

### Use a Format Function

Use the `printf` template function to construct stack names using static strings and dynamic identifiers.
This is convenient when you want to override some identifiers in the stack name:

```yaml
  !terraform.state <component> {{ printf "%s-%s-%s" .vars.tenant .vars.environment .vars.stage }} <output>

  !terraform.state <component> {{ printf "plat-%s-prod" .vars.environment }} <output>

  !terraform.state <component> {{ printf "%s-%s-%s" .settings.context.tenant .settings.context.region .settings.context.account }} <output>
```

<dl>
  <dt>`<component>`</dt>
  <dd>Placeholder for an actual component name (e.g. `vpc`)</dd>
  <dt>`<output>`</dt>
  <dd>Placeholder for an actual Terraform output (e.g. `subnet_ids`)</dd>
</dl>

For example, you have a `tgw` component deployed in the stack `plat-ue2-dev`. The `tgw` component requires the
`vpc_id` output from the `vpc` component from the same environment (`ue2`) and same stage (`dev`), but from a different
tenant `net` (instead of `plat`):

```yaml title="plat-ue2-dev"
  components:
    terraform:
      tgw:
        vars:
          vpc_id: !terraform.state vpc {{ printf "net-%s-%s" .vars.environment .vars.stage }} vpc_id
```

:::tip Important
    By using the `printf "%s-%s-%s"` function, you are constructing stack names using the stack context variables/identifiers.

    For more information on Atmos stack names and how to define them, refer to `stacks.name_pattern` and `stacks.name_template`
    sections in [`atmos.yaml` CLI config file](/cli/configuration/)
:::

## Caching the result of `!terraform.state` function

Atmos caches (in memory) the results of `!terraform.state` function.

The cache is per Atmos CLI command execution, e.g., each new execution of a command like `atmos terraform plan`,
`atmos terraform apply` or `atmos describe component` will create and use a new memory cache, which involves re-reading the remote state after reinitialisation.

If you define the function in stack manifests for the same component in a stack more than once, the first call will
produce the result and cache it, and all the consecutive calls will just use the cached data. This is useful when you use the
`!terraform.state` function for the same component in a stack in multiple places in Atmos stack manifests.
It will speed up the function execution and stack processing.

For example:

<File>
```yaml
components:
  terraform:
    test2:
      vars:
        tags:
          test: !terraform.state test id
          test2: !terraform.state test id
          test3: !terraform.state test {{ .stack }} id
```
</File>

In the example, the `test2` Atmos component uses the outputs (remote state) of the `test` Atmos component from the same stack.
The YAML function `!terraform.state` is executed three times (once for each tag).

After the first execution, Atmos caches the result in memory,
and reuses it in the next two calls to the function. The caching makes the stack processing much faster.
In a production environment where many components are used, the speedup can be significant.

## Using `!terraform.state` with `static` remote state backend

Atmos supports [brownfield configuration by using the remote state of type `static`](/core-concepts/components/terraform/brownfield/#hacking-remote-state-with-static-backends).

For example:

<File title="stack.yaml">
```yaml
components:
  terraform:
    # Component `static-backend` is configured with the remote state backend of type `static`
    static-backend:
      remote_state_backend_type: static
      remote_state_backend:
        static:
          region: "us-west-2"
          cluster_name: "production-cluster"
          vpc_cidr: "10.0.0.0/16"
          database:
            type: "postgresql"
            version: "12.7"
            storage_gb: 100
          allowed_ips:
            - "192.168.1.0/24"
            - "10.1.0.0/16"
          tags:
            Environment: "production"
            Owner: "infra-team"

    eks-cluster:
      vars:
        region: !terraform.state static-backend region
        cluster_name: !terraform.state static-backend cluster_name
        vpc_cidr: !terraform.state static-backend vpc_cidr
        db_type: !terraform.state static-backend database.type
        db_storage: !terraform.state static-backend database.storage_gb
        allowed_ips: !terraform.state static-backend allowed_ips
        tags: !terraform.state static-backend tags
```
</File>

When the functions are executed, Atmos detects that the `static-backend` component has the `static` remote state configured,
and instead of executing `terraform output`, it just returns the static values from the `remote_state_backend.static` section.

Executing the command `atmos describe component eks-cluster -s <stack>` produces the following result:

<Terminal title="atmos describe component eks-cluster -s <stack>">
```yaml
vars:
  region: us-west-2
  cluster_name: production-cluster
  vpc_cidr: 10.0.0.0/16
  db_type: postgresql
  db_storage: 100
  allowed_ips:
    - 192.168.1.0/24
    - 10.1.0.0/16
  tags:
    Environment: production
    Owner: infra-team
```
</Terminal>

## Considerations

 - Using `!terraform.state` with secrets can expose sensitive data to standard output (stdout) in any commands that describe stacks or components.

 - When using `!terraform.state` with [`atmos describe affected`](/cli/commands/describe/affected), Atmos requires access to all referenced remote states.
  If you operate with limited permissions (e.g., scoped to `dev`) and reference production stacks, the command will fail.

 - Overusing the function within stacks to reference multiple components can impact performance.

 - Be mindful of disaster recovery (DR) implications when using it across regions.

 - Consider cold-start scenarios: if the dependent component has not yet been provisioned, `terraform output` will fail.

---

## Abstract Component(Glossary)

An *Abstract Component* is a component baseline required to be inherited by a *Real Component* in order to be instantiated.

---

## Catalog

A *Catalog* is a collection of reusable configurations, such as Stack configurations.

---

## Component Instance

A *Component Instance* is a configuration of a real component in a parent stack that can be instantiated, such as deployed.

---

## Component

*Components* are reusable building blocks of some tool, such as terraform "root" modules.

---

## Concrete Component

Also known as a *Real Component*.

---

## Environment

An *Environment* is a location where resources are deployed, such as `us-east-1`.

---

## Programming Framework

Think of a programming framework like a toolkit or a set of building blocks for creating software. Just like how a carpenter uses specific tools and materials to build a house, programmers use frameworks to build applications. A framework provides ready-made solutions and guidelines for common tasks, saving time and effort. It helps ensure that the software is built in a consistent and efficient way, without having to start from scratch every time.

---

## Imports

*Imports* are a mechanism to include one configuration in another for the purpose of reducing duplication.

---

## Glossary

Here's a list of the terms, concepts and conventions used throughout the Atmos project.

:::info
Atmos borrows from many of the concepts
of [Object-Oriented Programming](https://en.wikipedia.org/wiki/Object-oriented_programming) and applies them
to configuration, enabling you to model configuration in a way that makes sense for your organization.
:::

Click on a term for a more detailed description.

---

## Inheritance

*Inheritance* is a mechanism to derive a configuration from a hierarchy of other configurations that share a set of attributes.

---

## Integration

An *Integration* is a mechanism of working with other tools and APIs.

---

## Kubernetes Namespace

Frequently it will be represented by a variable named `kubernetes_namespace`; it should not be confused with what Atmos calls namespace.

---

## Library

A *Library* is a collection of "Components" that can be treated like reusable building blocks.

---

## Mixins

*Mixins* are a partial configuration that is imported for use by other stacks without having to be the parent stack.

---

## Multiple Inheritance

*Multiple Inheritance* is a mechanism to inherit configurations from multiple sources.

---

## Namespace

A *Namespace* is a prefix for all resources in a Stack.

---

## Parent Stack

The *Parent Stack* configuration defines all components for an environment.

---

## Real Component

A *Real Component* is a component that is instantiated in a Parent Stack.

---

## Spacelift Stack

A Spacelift Stack is a Terraform root module. Atmos on the other hand treats a stack as a collection of Terraform root modules.

---

## Stack Manifest

Stack manifests are YAML files in which the configuration for all Atmos stacks and components are defined.

---

## Stack

An Atmos Stack serves as a configuration blueprint that describes a cohesive collection of components, automating the deployment, management,
and teardown of Terraform resources for uniform setups across environments. A stack can be detailed in one or more stack manifests, outlining
all necessary components to ensure comprehensive and consistent infrastructure orchestration.

---

## Stage

Atmos defines "Stage" as a classification for different lifecycles in infrastructure management, such as Development, Production, Staging, QA, etc. It is recommended to allocate at least one account per stage to maintain clear boundaries and management practices. It's worth noting that the term "Environment" is frequently used in many organizations to describe what Atmos refers to as "Stage."

---

## Tenant

A *Tenant* is a logical grouping of resources. In AWS, we use the Tenant to represent the Organizational Unit (OU).

---

## Terraform "Child Module"

Any Terraform module that is invoked by another module, distinguishing itself by not maintaining a separate Terraform state.
These modules are integral components of larger infrastructure setups, designed to modularize and encapsulate specific functionality
within the overarching Terraform configuration.

---

## Terralith

A monolithic Terraform "root module" is also known as a Terralith. It's characterized by an expansive, all-encompassing Terraform configuration that attempts to manage every aspect of the infrastructure within a single module.

```mermaid
---
title: Anatomy of a Terralith
---
graph TD
    subgraph TerraformRootModule[Terraform Root Module]
        subgraph Network[Network VPC]
            subgraph Cluster[Kubernetes Cluster]
                App1(Application 1)
                App2(Application 2)
            end
            LB[(Load Balancer Module)]
            DB[(Database Module)]
            Cache[(Cache Module)]
            ObjectStorage[(Object Storage Module)]
        end
        LB --> App1
        LB --> App2
        App1 --> DB
        App1 --> Cache
        App2 --> ObjectStorage
    end
```

In Atmos, this is considered an anti-pattern; instead, this should be broken down into [components](/core-concepts/components) and configured with [stacks](/core-concepts/stacks).

To learn more, see [Stage 2](/introduction/why-atmos/stage-2) of the typical Terraform maturity path.

---

## Vendoring(Glossary)

*Vendoring* is a mechanism of making a copy of 3rd-party components within your repository.

---

## Atlantis Integration

import Terminal from '@site/src/components/Terminal'
import Intro from '@site/src/components/Intro'

<Intro>
Atmos natively supports [Atlantis](https://runatlantis.io) for Terraform Pull Request Automation.
</Intro>

## How it Works

With Atmos, all your configurations are neatly defined in YAML. This makes transformations of that data very easy.

Atmos supports three commands that, when combined, make it easy to use Atlantis:

1. Generate the [`atlantis.yaml`](https://www.runatlantis.io/docs/repo-level-atlantis-yaml.html) repo-level
   configuration: [`atmos atlantis generate repo-config`](/cli/commands/atlantis/generate-repo-config)

2. Generate the backend configuration for all
   components: [`atmos terraform generate backends --format=backend-config|hcl`](/cli/commands/terraform/generate-backends)

3. Generate the full deep-merged configurations of all stacks for each
   component: [`atmos terraform generate varfiles`](/cli/commands/terraform/generate-varfiles)

## Configuration

Atlantis Integration can be configured in two different ways (or a combination of them):

- In the `integrations.atlantis` section in `atmos.yaml`
- In the `settings.atlantis` sections in the stack config files

### Configure Atlantis Integration in `integrations.atlantis` section in `atmos.yaml`

To configure Atmos to generate the Atlantis repo configurations, update the `integrations.atlantis` section in `atmos.yaml`.

Here's an example to get you started. As with *everything* in Atmos, it supports deep-merging at all levels. Anything under
the `integrations.atlantis` section in `atmos.yaml` can be overridden in the stack config sections `settings.atlantis` at any level of the inheritance
chain.

```yaml title=atmos.yaml
# atmos.yaml CLI config

# Integrations
integrations:

  # Atlantis integration
  # https://www.runatlantis.io/docs/repo-level-atlantis-yaml.html
  atlantis:
    # Path and name of the Atlantis config file `atlantis.yaml`
    # Supports absolute and relative paths
    # All the intermediate folders will be created automatically (e.g. `path: /config/atlantis/atlantis.yaml`)
    # Can be overridden on the command line by using `--output-path` command-line argument in `atmos atlantis generate repo-config` command
    # If not specified (set to an empty string/omitted here, and set to an empty string on the command line), the content of the file will be dumped to `stdout`
    # On Linux/macOS, you can also use `--output-path=/dev/stdout` to dump the content to `stdout` without setting it to an empty string in `atlantis.path`
    path: "atlantis.yaml"

    # Config templates
    # Select a template by using the `--config-template <config_template>` command-line argument in `atmos atlantis generate repo-config` command
    config_templates:
      config-1:
        version: 3
        automerge: true
        delete_source_branch_on_merge: true
        parallel_plan: true
        parallel_apply: true
        allowed_regexp_prefixes:
          - dev/
          - staging/
          - prod/

    # Project templates
    # Select a template by using the `--project-template <project_template>` command-line argument in `atmos atlantis generate repo-config` command
    project_templates:
      project-1:
        # generate a project entry for each component in every stack
        name: "{tenant}-{environment}-{stage}-{component}"
        workspace: "{workspace}"
        dir: "{component-path}"
        terraform_version: v1.8
        delete_source_branch_on_merge: true
        autoplan:
          enabled: true
          when_modified:
            - "**/*.tf"
            - "varfiles/$PROJECT_NAME.tfvars"
          apply_requirements:
            - "approved"

    # Workflow templates
    # https://www.runatlantis.io/docs/custom-workflows.html#custom-init-plan-apply-commands
    # https://www.runatlantis.io/docs/custom-workflows.html#custom-run-command
    workflow_templates:
      workflow-1:
        plan:
          steps:
            - run: terraform init -input=false
            # When using workspaces, you need to select the workspace using the $WORKSPACE environment variable
            - run: terraform workspace select $WORKSPACE
            # You must output the plan using `-out $PLANFILE` because Atlantis expects plans to be in a specific location
            - run: terraform plan -input=false -refresh -out $PLANFILE -var-file varfiles/$PROJECT_NAME.tfvars
        apply:
          steps:
            - run: terraform apply $PLANFILE
```

Using the config and project templates, Atmos generates a separate atlantis project for each Atmos component in every stack.

For example, by running this command:

```shell
atmos atlantis generate repo-config --config-template config-1 --project-template project-1
```

the following Atlantis repo-config would be generated:

```yaml title=atlantis.yaml
version: 3
automerge: true
delete_source_branch_on_merge: true
parallel_plan: true
parallel_apply: true
allowed_regexp_prefixes:
  - dev/
  - staging/
  - prod/
projects:
  - name: tenant1-ue2-staging-test-test-component-override-3
    workspace: test-component-override-3-workspace
    workflow: workflow-1
    dir: tests/fixtures/scenarios/complete/components/terraform/test/test-component
    terraform_version: v1.8
    delete_source_branch_on_merge: true
    autoplan:
      enabled: true
      when_modified:
        - '**/*.tf'
        - varfiles/$PROJECT_NAME.tfvars
      apply_requirements:
        - approved
  - name: tenant1-ue2-staging-infra-vpc
    workspace: tenant1-ue2-staging
    workflow: workflow-1
    dir: tests/fixtures/scenarios/complete/components/terraform/infra/vpc
    terraform_version: v1.8
    delete_source_branch_on_merge: true
    autoplan:
      enabled: true
      when_modified:
        - '**/*.tf'
        - varfiles/$PROJECT_NAME.tfvars
      apply_requirements:
        - approved
workflows:
  workflow-1:
    apply:
      steps:
        - run: terraform apply $PLANFILE
    plan:
      steps:
        - run: terraform init -input=false
        - run: terraform workspace select $WORKSPACE
        - run: terraform plan -input=false -refresh -out $PLANFILE -var-file varfiles/$PROJECT_NAME.tfvars
```

__NOTE:__ If Atlantis Integration is configured only in the `integrations.atlantis` section in `atmos.yaml`, the command-line
flags `--config-template` and `--project-template` are required to specify a config template and a project template from the collection of
templates defined in the `integrations.atlantis.config_templates` and `integrations.atlantis.project_templates` sections in `atmos.yaml`. You can
change this behavior by using the `settings.atlantis` sections in stack config files.

### Configure Atlantis Integration in `settings.atlantis` sections in stack configs

The `integrations.atlantis.config_templates`, `integrations.atlantis.config_templates` and `integrations.atlantis.config_templates` sections
in `atmos.yaml` can be overridden in the `settings.atlantis` sections in stack config files. In fact, you don't have to define the sections
in `atmos.yaml` at all and instead use only the `settings.atlantis` sections in stack configs to configure work with the Atlantis Integration.

Configuring the Atlantis Integration in the `settings.atlantis` sections in the stack configs has the following advantages:

- The `settings` section is a first class section in Atmos (similar to `vars`). It participates in deep-merging and in the inheritance chain. It can
  be defined and overridden at any level (organization/namespace, OU/tenant, region/environment, account/stage, base component, component). You can
  define the base settings at the org, tenant or account level, and then override some settings at the component level, making the whole configuration
  DRY

- When executing the `atmos atlantis generate repo-config` command, you don't need to pass the `--config-template` and `--project-template` flags to
  specify which config and project templates to use. Instead, Atmos will get this information from the `settings.atlantis` section

- When executing the `atmos describe component <component> -s <stack>` command, you will see the configured Atlantis Integration in the outputs. For
  example:

<Terminal title="atmos describe component test/test-component-override -s tenant1-ue2-dev">
  ```yaml
    atmos_component: test/test-component-override
    atmos_stack: tenant1-ue2-dev
    component: test/test-component
    settings:
      atlantis:
        config_template:
          allowed_regexp_prefixes:
          - dev/
          automerge: false
          delete_source_branch_on_merge: false
          parallel_apply: false
          parallel_plan: true
          version: 3
        config_template_name: config-1
        project_template:
          apply_requirements:
          - approved
          autoplan:
            enabled: true
            when_modified:
            - '**/*.tf'
            - varfiles/$PROJECT_NAME.tfvars.json
          delete_source_branch_on_merge: false
          dir: '{component-path}'
          name: '{tenant}-{environment}-{stage}-{component}'
          terraform_version: v1.8
          workflow: workflow-1
          workspace: '{workspace}'
        project_template_name: project-1
        workflow_templates:
          workflow-1:
            apply:
              steps:
              - run: terraform apply $PLANFILE
            plan:
              steps:
              - run: terraform init
              - run: terraform workspace select $WORKSPACE || terraform workspace new $WORKSPACE
              - run: terraform plan -out $PLANFILE -var-file varfiles/$PROJECT_NAME.tfvars.json
    vars:
      enabled: true
      environment: ue2
      namespace: cp
      region: us-east-2
      stage: dev
      tenant: tenant1
    workspace: test-component-override-workspace-override
  ```
</Terminal>

- If you configure the Atlantis Integration in the `settings.atlantis` sections in the stack configs, then the
  command [`atmos describe affected`](/cli/commands/describe/affected) will be able to use it and output the
  affected Atlantis projects in the `atlantis_project` field. For example:

<Terminal title="atmos describe affected">
  ```json
  [
     {
        "component": "infra/vpc",
        "component_type": "terraform",
        "component_path": "components/terraform/infra/vpc",
        "stack": "tenant1-ue2-dev",
        "atlantis_project": "tenant1-ue2-dev-infra-vpc",
        "affected": "component"
     },
     {
        "component": "infra/vpc",
        "component_type": "terraform",
        "component_path": "components/terraform/infra/vpc",
        "stack": "tenant1-ue2-prod",
        "atlantis_project": "tenant1-ue2-prod-infra-vpc",
        "affected": "component"
     }
  ]
  ```
</Terminal>

#### Configure `settings.atlantis.workflow_templates` section in stack configs

If you are using the [Atlantis Repo Level workflows](https://www.runatlantis.io/docs/repo-level-atlantis-yaml.html), you can configure the workflows
in the `settings.atlantis.workflow_templates` section.

If the `settings.atlantis.workflow_templates` section is configured in stack configs, it's copied to the generated `atlantis.yaml` file verbatim.
For example, add the `workflow_templates` section at the org level in the config file `stacks/orgs/cp/_defaults.yaml`:

```yaml title="stacks/orgs/cp/_defaults.yaml"
settings:
  atlantis:
    workflow_templates:
      workflow-1:
        apply:
          steps:
            - run: terraform apply $PLANFILE
        plan:
          steps:
            - run: terraform init
            - run: terraform workspace select $WORKSPACE || terraform workspace new $WORKSPACE
            - run: terraform plan -out $PLANFILE -var-file varfiles/$PROJECT_NAME.tfvars.json
```

then execute the `atmos atlantis generate repo-config` command:

```yaml title="atlantis.yaml"
version: 3
workflows:
  workflow-1:
    apply:
      steps:
        - run: terraform apply $PLANFILE
    plan:
      steps:
        - run: terraform init
        - run: terraform workspace select $WORKSPACE || terraform workspace new $WORKSPACE
        - run: terraform plan -out $PLANFILE -var-file varfiles/$PROJECT_NAME.tfvars
```

:::note

The `settings.atlantis.workflow_templates` section in stack configs has higher priority then the `integration.atlantis.workflow_templates`
section in `atmos.yaml`. If both are defined, Atmos will select the workflows from the `settings.atlantis.workflow_templates` section and copy them
into the generated `atlantis.yaml` file. On the other hand, if the `settings.atlantis.workflow_templates` section is not defined in stack configs,
Atmos will use the workflows from the `integration.atlantis.workflow_templates` section from `atmos.yaml`.

:::

#### Define config template and project template in `settings.atlantis` section in stack configs

The Atlantis config template and project template can be defined in the `settings.atlantis` section in two different ways:

- Define `config_template_name` and `project_template_name` in the `settings.atlantis` section. These attributes tell Atmos to select a config
  template and a project template from the `integration.atlantis` section in `atmos.yaml`. For example:

  ```yaml title="atmos.yaml"
  integrations:
    atlantis:
      path: "atlantis.yaml"

      # Config templates
      config_templates:
        config-1:
          version: 3
          automerge: true
          delete_source_branch_on_merge: true
          parallel_plan: true
          parallel_apply: true
          allowed_regexp_prefixes:
            - dev/
            - staging/
            - prod/

      # Project templates
      project_templates:
        project-1:
          # generate a project entry for each component in every stack
          name: "{tenant}-{environment}-{stage}-{component}"
          workspace: "{workspace}"
          dir: "{component-path}"
          terraform_version: v1.8
          delete_source_branch_on_merge: true
          autoplan:
            enabled: true
            when_modified:
              - "**/*.tf"
              - "varfiles/$PROJECT_NAME.tfvars.json"
          apply_requirements:
            - "approved"
  ```

  ```yaml title="stacks/orgs/cp/_defaults.yaml"
  settings:
    atlantis:
      # Select a config template defined in `atmos.yaml` in
      # the `integrations.atlantis.config_templates` section
      config_template_name: "config-1"

      # Select a project template defined in `atmos.yaml` in
      # the `integrations.atlantis.project_templates` section
      project_template_name: "project-1"
  ```

  In this case, the `config_template_name` and `project_template_name` attributes are used instead of specifying the `--config-template`
  and `--project-template` flags on the command line when executing the command `atmos atlantis generate repo-config`. And the attributes can be
  defined at any level in the stack configs and they participate in deep-merging and inheritance (meaning they can be overridden per tenant,
  environment, stage and component).

- Define `config_template` and `project_template` in the `settings.atlantis` section. These attributes tell Atmos to use the templates instead of
  searching for them in the `integration.atlantis` section in `atmos.yaml`. For example:

  ```yaml title="stacks/orgs/cp/tenant1/dev/us-east-2.yaml"
  settings:
    atlantis:

      # For this `tenant1-ue2-dev` stack, override the org-wide config template
      # specified in `stacks/orgs/cp/_defaults.yaml`
      # in the `settings.atlantis.config_template_name` section
      config_template:
        version: 3
        automerge: false
        delete_source_branch_on_merge: false
        parallel_plan: true
        parallel_apply: false
        allowed_regexp_prefixes:
          - dev/

      # For this `tenant1-ue2-dev` stack, override the org-wide project template
      # specified in `stacks/orgs/cp/_defaults.yaml`
      # in the `settings.atlantis.project_template_name` section
      project_template:
        # generate a project entry for each component in every stack
        name: "{tenant}-{environment}-{stage}-{component}"
        workspace: "{workspace}"
        workflow: "workflow-1"
        dir: "{component-path}"
        terraform_version: v1.8
        delete_source_branch_on_merge: false
        autoplan:
          enabled: true
          when_modified:
            - "**/*.tf"
            - "varfiles/$PROJECT_NAME.tfvars.json"
        apply_requirements:
          - "approved"
  ```

:::note summary

- Atlantis integration can be configured in the `integrations.atlantis` section in `atmos.yaml`. If this is the only place where it's configured, then
  you need to pass the `--config-template` and `--project-template` flags to the `atmos atlantis generate repo-config` command

- Atlantis integration can also be configured in the `settings.atlantis` section in the stack configs. The `config_template_name`
  and `project_template_name` attributes can be used to select the config and project templates from the `integrations.atlantis` section
  in `atmos.yaml` instead of specifying the `--config-template` and `--project-template` flags on the command line

- The `config_template` and `project_template` sections in `settings.atlantis` can be used to define the config and project template for the
  particular stack or component. If defined, the sections will override all the configurations in the `integrations.atlantis` section in `atmos.yaml`,
  and will override the `config_template_name` and `project_template_name` attributes in `settings.atlantis`. These sections have the highest
  priority.

:::

## Atlantis Workflows

Atlantis workflows can be defined in two different ways:

- In the [Server Side Config](https://www.runatlantis.io/docs/server-side-repo-config.html) using the `workflows` section and `workflow` attribute

  ```yaml title=server.yaml
  repos:
    - id: /.*/
      branch: /.*/

      # 'workflow' sets the workflow for all repos that match.
      # This workflow must be defined in the workflows section.
      workflow: custom

      # allowed_overrides specifies which keys can be overridden by this repo in
      # its atlantis.yaml file.
      allowed_overrides: [apply_requirements, workflow, delete_source_branch_on_merge, repo_locking]

      # allowed_workflows specifies which workflows the repos that match
      # are allowed to select.
      allowed_workflows: [custom]

      # allow_custom_workflows defines whether this repo can define its own
      # workflows. If false (default), the repo can only use server-side defined
      # workflows.
      allow_custom_workflows: true

  # workflows lists server-side custom workflows
  workflows:
    custom:
      plan:
        steps:
          - init
          - plan
      apply:
        steps:
          - run: echo applying
          - apply
  ```

- In the [Repo Level atlantis.yaml Config](https://www.runatlantis.io/docs/repo-level-atlantis-yaml.html) using the `workflows` section and
  the `workflow` attribute in each Atlantis project in `atlantis.yaml`

  ```yaml title=atlantis.yaml
  version: 3
  projects:
    - name: my-project-name
      branch: /main/
      dir: .
      workspace: default
      workflow: myworkflow
  workflows:
    myworkflow:
      plan:
        steps:
          - init
          - plan
      apply:
        steps:
          - run: echo applying
          - apply
  ```

If you use the [Server Side Config](https://www.runatlantis.io/docs/server-side-repo-config.html) to define the Atlantis workflows, you don't need to
define workflows in the [CLI Config Atlantis Integration](/cli/configuration/#integrations) section in `atmos.yaml` or in
the `settings.atlantis.workflow_templates` section in the stack configurations. When you defined the workflows in the server config `workflows`
section, you can reference a workflow to be used for each generated Atlantis project in the project templates.

On the other hand, if you use [Repo Level workflows](https://www.runatlantis.io/docs/repo-level-atlantis-yaml.html),
you need to provide at least one workflow template in the `integrations.atlantis.workflow_templates` section in
the [Atlantis Integration](/cli/configuration/#integrations) in `atmos.yaml`, or in the `settings.atlantis.workflow_templates` section in the stack
configurations.

For example, after executing the following command:

```console
atmos atlantis generate repo-config --config-template config-1 --project-template project-1
```

the generated `atlantis.yaml` file would look like this:

```yaml title=atlantis.yaml
version: 3
projects:
  - name: tenant1-ue2-dev-infra-vpc
    workspace: tenant1-ue2-dev
    workflow: workflow-1

workflows:
  workflow-1:
    apply:
      steps:
        - run: terraform apply $PLANFILE
    plan:
      steps:
        - run: terraform init -input=false
        - run: terraform workspace select $WORKSPACE || terraform workspace new $WORKSPACE
        - run: terraform plan -input=false -refresh -out $PLANFILE -var-file varfiles/$PROJECT_NAME.tfvars.json
```

## Dynamic Repo Config Generation

If you want to generate the `atlantis.yaml` file before Atlantis can parse it, you can use
the [Dynamic Repo Config Generation](https://www.runatlantis.io/docs/pre-workflow-hooks.html#dynamic-repo-config-generation) feature of Atlantis. You
can add a `run` command to `pre_workflow_hooks`. The repo config will be generated right before Atlantis can parse it.

```yaml
repos:
  - id: /.*/
    pre_workflow_hooks:
      - run: "./repo-config-generator.sh"
        description: "Generating configs"
```

See also [Pre Workflow Hooks](https://www.runatlantis.io/docs/pre-workflow-hooks.html)
and [Post Workflow Hooks](https://www.runatlantis.io/docs/post-workflow-hooks.html) for more information.

To help with dynamic repo config generation, the `atmos atlantis generate repo-config` command accepts the `--affected-only` flag.
If set to `true`, Atmos will generate Atlantis projects only for the Atmos components changed between two Git commits.

```yaml
repos:
  - id: /.*/
    pre_workflow_hooks:
      - run: "atmos atlantis generate repo-config --affected-only=true"
        description: "Generating configs"
```

If the `--affected-only=true` flag is passed, Atmos uses two different Git commits to produce a list of affected Atmos components and stacks and then
generate the `atlantis.yaml` file for the affected Atlantis projects only.

For the first commit, the command assumes that the current repo root is a Git checkout. An error will be thrown if the current repo is not a Git
repository (the `.git` folder does not exist or is configured incorrectly).

The second commit can be specified on the command line by using
the `--ref` ([Git References](https://git-scm.com/book/en/v2/Git-Internals-Git-References)) or `--sha` (commit SHA) flags.

Either `--ref` or `--sha` should be used. If both flags are provided at the same time, the command will first clone the remote branch pointed to by
the `--ref` flag and then checkout the Git commit pointed to by the `--sha` flag (`--sha` flag overrides `--ref` flag).

__NOTE:__ If the flags are not provided, the `ref` will be set automatically to the reference to the default branch (e.g. `main`) and the commit SHA
will point to the `HEAD` of the branch.

If you specify the `--repo-path` flag with the path to the already cloned repository, the command will not clone the target
repository, but instead will use the already cloned one to compare the current branch with. In this case, the `--ref`, `--sha`, `--ssh-key`
and `--ssh-key-password` flags are not used, and an error will be thrown if the `--repo-path` flag and any of the `--ref`, `--sha`, `--ssh-key`
or `--ssh-key-password` flags are provided at the same time.

The command works by:

- Cloning the target branch (`--ref`) or checking out the commit (`--sha`) of the remote target branch, or using the already cloned target repository
  specified by the `--repo-path` flag

- Deep merging all stack configurations for both the current working branch and the remote target branch

- Looking for changes in the component directories

- Comparing each section of the stack configuration looking for differences

- Generating the `atlantis.yaml` file with the `projects` sections consisting of a list of the affected Atlantis projects

Since Atmos first checks the component folders for changes, if it finds any affected files, it will mark all related components and stacks as
affected. Atmos will then skip evaluating those stacks for differences since we already know that they are affected.

Refer to [`atmos atlantis generate repo-config`](/cli/commands/atlantis/generate-repo-config) for the description of the command and all flags.

## Working with Private Repositories

If the flag `--affected-only=true` is passed on the command line (e.g. `atmos atlantis generate repo-config --affected-only=true`), the command
will clone and checkout the remote target repo (which can be the default `refs/heads<default_branch>` reference, or specified by the command-line
flags `--ref`, `--sha` or `--repo-path`). If the remote target repo is private, special attention needs to be given to how to work with private
repositories.

There are a few ways to work with private repositories with which the current local branch is compared to detect the changed files and affected Atmos
stacks and components:

- Using the `--ssh-key` flag to specify the filesystem path to a PEM-encoded private key to clone private repos using SSH, and
  the `--ssh-key-password` flag to provide the encryption password for the PEM-encoded private key if the key contains a password-encrypted PEM block

- Execute the `atmos atlantis generate repo-config --affected-only=true --repo-path <path_to_cloned_target_repo>` command in
  a [GitHub Action](https://docs.github.com/en/actions). For this to work, clone the remote target repository using
  the [checkout](https://github.com/actions/checkout) GitHub action. Then use the `--repo-path` flag to specify the path to the already cloned
  target repository with which to compare the current branch

- It should just also work with whatever SSH config/context has been already set up, for example, when
  using [SSH agents](https://www.ssh.com/academy/ssh/agent). In this case, you don't need to use the `--ssh-key`, `--ssh-key-password`
  and `--repo-path` flags to clone private repositories

## Using with GitHub Actions

If the `atmos atlantis generate repo-config --affected-only=true` command is executed in a [GitHub Action](https://docs.github.com/en/actions), and
you don't want to store or generate a long-lived SSH private key on the server, you can do the following (__NOTE:__ This is only required if the
action is attempting to clone a private repo which is not itself):

- Create a GitHub
  [Personal Access Token (PAT)](https://docs.github.com/en/authentication/keeping-your-account-and-data-secure/creating-a-personal-access-token)
  with scope permissions to clone private repos

- Add the created PAT as a repository or GitHub organization [secret](https://docs.github.com/en/actions/security-guides/encrypted-secrets)

- In your GitHub Action, clone the remote repository using the [checkout](https://github.com/actions/checkout) GitHub Action

- Execute `atmos atlantis generate repo-config --affected-only=true --repo-path <path_to_cloned_target_repo>` command with the `--repo-path` flag set
  to the cloned repository path using the [`GITHUB_WORKSPACE`](https://docs.github.com/en/actions/learn-github-actions/variables) `ENV` variable (
  which
  points to the default working directory on the GitHub runner for steps, and the default location of the repository when using
  the [checkout](https://github.com/actions/checkout) action). For example:

    ```shell
    atmos atlantis generate repo-config --affected-only=true --repo-path $GITHUB_WORKSPACE
    ```

## Example GitHub Action

Here's an example GitHub Action to use Atlantis with Atmos.

The action executes the `atmos generate varfiles/backends` commands to generate Terraform varfiles and backend config files for all Atmos stacks,
then executes the `atmos atlantis generate repo-config` command to generate the Atlantis repo config file (`atlantis.yaml`) for all Atlantis projects,
then commits all the generated files and calls Atlantis via a webhook.

You can adopt and modify it to your own needs.

```yaml
name: atmos

on:
  workflow_dispatch:

  issue_comment:
    types:
      - created

  pull_request:
    types:
      - opened
      - edited
      - synchronize
      - closed
    branches: [ main ]

env:
  ATMOS_VERSION: 1.193.0
  ATMOS_CLI_CONFIG_PATH: ./

jobs:
  generate-atlantis-yaml:
    name: Generate varfiles, backend config and atlantis.yaml
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@v3
        if: github.event.pull_request.state == 'open' || ${{ github.event.issue.pull_request }}
        with:
          ref: ${{ github.event.pull_request.head.ref }}
          fetch-depth: 2

      # Install Atmos and generate tfvars and backend config files
      - name: Generate TF var files and backend configs
        if: github.event.pull_request.state == 'open' || ${{ github.event.issue.pull_request }}
        shell: bash
        run: |
          wget -q https://github.com/cloudposse/atmos/releases/download/v${ATMOS_VERSION}/atmos_${ATMOS_VERSION}_linux_amd64 && \
          mv atmos_${ATMOS_VERSION}_linux_amd64 /usr/local/bin/atmos && \
          chmod +x /usr/local/bin/atmos
          atmos terraform generate varfiles --file-template={component-path}/varfiles/{namespace}-{environment}-{component}.tfvars.json
          atmos terraform generate backends --format=backend-config --file-template={component-path}/backends/{namespace}-{environment}-{component}.backend

      # Commit changes (if any) to the PR branch
      - name: Commit changes to the PR branch
        if: github.event.pull_request.state == 'open' || ${{ github.event.issue.pull_request }}
        shell: bash
        run: |
          untracked=$(git ls-files --others --exclude-standard)
          changes_detected=$(git diff --name-only)
          if [ -n "$untracked" ] || [ -n "$changes_detected" ]; then
            git config --global user.name github-actions
            git config --global user.email github-actions@github.com
            git add -A *
            git commit -m "Committing generated autogenerated var files"
            git push
          fi

      # Generate atlantis.yaml with atmos
      - name: Generate Dynamic atlantis.yaml file
        if: github.event.pull_request.state == 'open' || ${{ github.event.issue.pull_request }}
        shell: bash
        run: |
          atmos atlantis generate repo-config --config-template config-1 --project-template project-1

      # Commit changes (if any) to the PR branch
      - name: Commit changes to the PR branch
        if: github.event.pull_request.state == 'open' || ${{ github.event.issue.pull_request }}
        shell: bash
        run: |
          yaml_changes=$(git diff --name-only)
          untracked=$(git ls-files --others --exclude-standard atlantis.yaml)
          if [ -n "$yaml_changes" ] || [ -n "$untracked" ]; then
            git config --global user.name github-actions
            git config --global user.email github-actions@github.com
            git add -A *
            git commit -m "Committing generated atlantis.yaml"
            git push
          fi

  call-atlantis:
    if: ${{ always() }}
    needs: generate-atlantis-yaml
    name: Sending data to Atlantis
    runs-on: ubuntu-latest
    steps:
      - name: Invoke deployment hook
        uses: distributhor/workflow-webhook@v2
        env:
          webhook_type: 'json-extended'
          webhook_url: ${{ secrets.WEBHOOK_URL }}
          webhook_secret: ${{ secrets.WEBHOOK_SECRET }}
          verbose: false
```

## Next Steps

Generating the Atlantis `repo-config` is only part of what's needed to use Atmos with Atlantis. The rest will depend on your organization's
preferences for generating the Terraform `.tfvars` files and backends.

You can use pre-commit hooks and/or GitHub Actions (or similar) to generate the `.tfvars` files and state backend configurations, which are derived
from the Atmos stack configurations.

The following commands will generate those files.

- [`atmos terraform generate backends --format=backend-config|hcl`](/cli/commands/terraform/generate-backends)
- [`atmos terraform generate varfiles`](/cli/commands/terraform/generate-varfiles)

You can commit the resulting files back to VCS (e.g. `git add -A`) and push upstream. That way Atlantis will trigger on the "affected
files" and propose a plan.

Or you can use the [Dynamic Repo Config Generation](#dynamic-repo-config-generation) in the Atlantis pre-workflow hooks and
the `atmos atlantis generate repo-config` command with the `--affected-only=true` flag to dynamically generate the `atlantis.yaml` file with the
affected (changed) Atlantis projects to avoid the need of committing those files to VCS.

## References

For more information, refer to:

- [Configuring Atlantis](https://www.runatlantis.io/docs/configuring-atlantis.html)
- [Server Side Config](https://www.runatlantis.io/docs/server-side-repo-config.html)
- [Repo Level atlantis.yaml Config](https://www.runatlantis.io/docs/repo-level-atlantis-yaml.html)
- [Server Configuration](https://www.runatlantis.io/docs/server-configuration.html)
- [Atlantis Custom Workflows](https://www.runatlantis.io/docs/custom-workflows.html)
- [Pre Workflow Hooks](https://www.runatlantis.io/docs/pre-workflow-hooks.html)
- [Post Workflow Hooks](https://www.runatlantis.io/docs/post-workflow-hooks.html)
- [Dynamic Repo Config Generation](https://www.runatlantis.io/docs/pre-workflow-hooks.html#dynamic-repo-config-generation)

---

## Affected Stacks

import Intro from '@site/src/components/Intro'
import RemoteFile from '@site/src/components/RemoteFile'

<Intro>
The [Atmos Affected Stacks GitHub Action](https://github.com/cloudposse/github-action-atmos-affected-stacks) makes it easy identify the affected [atmos stacks](/core-concepts/stacks/) for a pull request. Use it to build a matrix so you can run other actions based on what was affected.
</Intro>

This GitHub Action installs Atmos, then runs [`atmos describe affected`](/cli/commands/describe/affected), and outputs a comprehensive list of affected stacks, both as raw output and as a matrix to be used in subsequent GitHub action jobs.

Discover more details, including the full list of `inputs` and `outputs`, in the [GitHub Action repository](https://github.com/cloudposse/github-action-atmos-affected-stacks) on GitHub.

## How it works

The [`describe affected`](/cli/commands/describe/affected) command works by comparing two different Git commits to generate a list of affected Atmos components and stacks. It assumes that the current repo root is a Git checkout and accepts a parameter to specify the second commit.

Overall Process:
1.  Clone the target branch (`--ref`), check out the commit, or use the pre-cloned target repository
2.  Deep merge all stack configurations for the current working and remote target branches.
3.  Identify changes in the component directories.
4.  Compare each section of the stack configuration to detect differences.
5.  Output a matrix containing a list of affected components and stacks

Atmos checks component folders for changes first, marking all related components and stacks as affected when changes are detected. It then skips evaluating those stacks for differences, streamlining the process.

## Usage Example

<RemoteFile source="https://raw.githubusercontent.com/cloudposse/docs/refs/heads/master/examples/legacy/snippets/.github/workflows/atmos-terraform-plan.yaml" />

## Requirements

This action has the requirements as [Github Actions](/integrations/github-actions/). Use the same config described there.

---

## Atmos Terraform Apply

import Intro from '@site/src/components/Intro'
import RemoteFile from '@site/src/components/RemoteFile'

<Intro>
Simplify provisioning Terraform entirely from within GitHub Action workflows. This action makes it very easy to apply that changes from a `terraform plan` from directly within the GitHub UI. Use this action in your workflows to apply changes to your infrastructure.
</Intro>

Given any component and stack in an Atmos supported infrastructure environment, [`github-action-atmos-terraform-apply`](https://github.com/cloudposse/github-action-atmos-terraform-apply) will retrieve an existing Terraform [planfile](https://developer.hashicorp.com/terraform/tutorials/automation/automate-terraform) from a given S3 bucket using metadata stored inside a DynamoDB table, run `atmos terraform apply` with that planfile, and format the Terraform Apply result as part of a [GitHub Workflow Job Summary](https://github.blog/2022-05-09-supercharging-github-actions-with-job-summaries/).

This action is intended to be used together with [Atmos Terraform Plan](/integrations/github-actions/atmos-terraform-plan), as well as integrated into drift detection with [Atmos Terraform Detection and Remediation](/integrations/github-actions/atmos-terraform-drift-detection) GitHub Actions.

## Features

This GitHub Action incorporates superior GitOps support for Terraform by utilizing the capabilities of Atmos, enabling efficient management of large enterprise-scale environments.

* **Implements Native GitOps** with Atmos and Terraform tightly integrated with GitHub UI
* **No hardcoded credentials.** Use GitHub OIDC to assume roles.
* **Compatible with GitHub Cloud & Self-hosted Runners** for maximum flexibility.
* **Beautiful Job Summaries** don't clutter up pull requests with noisy GitHub comments
* **100% Open Source with Permissive APACHE2 License** means there are no expensive subscriptions or long-term commitments.

## Screenshots

In the following screenshot, we see a successful "apply" Job Summary report. The report utilizes badges to clearly indicate success or failure. Unnecessary details are neatly hidden behind a collapsible `<details/>` block, providing a streamlined view. Additionally, a direct link is provided to view the job run, eliminating the need for developers to search for information about any potential issues.

![Example Image](/img/github-actions/apply.png)

## Usage Example

In this example, the action is triggered when certain events occur, such as a manual workflow dispatch or the opening, synchronization, or reopening of a pull request, specifically on the main branch. It specifies specific permissions related to assuming roles in AWS. Within the "apply" job, the "component" and "stack" are hardcoded (`foobar` and `plat-ue2-sandbox`). In practice, these are usually derived from another action.

:::tip Passing Affected Stacks

We recommend combining this action with the [`affected-stacks`](/integrations/github-actions/affected-stacks) GitHub Action inside a matrix to plan all affected stacks in parallel.

:::

<RemoteFile source="https://raw.githubusercontent.com/cloudposse/docs/refs/heads/master/examples/legacy/snippets/.github/workflows/atmos-terraform-apply.yaml"/>

:::info Why Do We have Two Workflows?

GitHub Actions support 256 matrix jobs in a single workflow at most! When planning all stacks in an Atmos environment, we frequently plan more than 256 component in the stacks at a time. In order to work around this limitation by GitHub, we can add an additional layer of abstraction using reusable workflows.

[256 Matrix Limitation](/integrations/github-actions/atmos-terraform-drift-detection#256-matrix-limitation)

:::

<RemoteFile source="https://raw.githubusercontent.com/cloudposse/docs/refs/heads/master/examples/legacy/snippets/.github/workflows/atmos-terraform-apply-matrix.yaml"/>

## Requirements

This action has the requirements as [Github Actions](/integrations/github-actions/). Use the same S3 Bucket, DynamoDB table, IAM Roles and config described there.

---

## Atmos Terraform Drift Detection

import Intro from '@site/src/components/Intro'
import RemoteFile from '@site/src/components/RemoteFile'

<Intro>
The  "Atmos Terraform Drift Detection" and "Atmos Terraform Drift Remediation" GitHub Actions provide a scalable pattern for detecting and remediating Terraform drift from within GitHub by utilizing a combination of scheduled GitHub Workflows and GitHub Issues.
</Intro>

The "Atmos Terraform Drift Detection" will determine drifted Terraform state by running [Atmos Terraform Plan](/integrations/github-actions/atmos-terraform-plan) and creating GitHub Issues for any drifted component and stack. Furthermore, "Atmos Terraform Drift Remediation" will run [Atmos Terraform Apply](/integrations/github-actions/atmos-terraform-apply) for any open Issue if called and close the given Issue. With these two actions, we can fully support drift detection for Terraform directly within the GitHub UI.

These actions are intended to be used together with [Atmos Terraform Plan](/integrations/github-actions/atmos-terraform-plan) and [Atmos Terraform Apply](/integrations/github-actions/atmos-terraform-apply).

## Features

This GitHub Action incorporates superior GitOps support for Terraform by utilizing the capabilities of Atmos, enabling efficient management of large enterprise-scale environments.

* **Implements Native GitOps** with Atmos and Terraform.
* **No hardcoded credentials.** Use GitHub OIDC to assume roles.
* **Compatible with GitHub Cloud & Self-hosted Runners** for maximum flexibility.
* **Beautiful Job Summaries** don't clutter up pull requests with noisy GitHub comments.
* **Automated Drift Detection** Regularly check and track all resources for drift.
* **Free Tier GitHub** Use GitHub Issues to track drifted resources.
* **100% Open Source with Permissive APACHE2 License** means you have no expensive subscriptions or long-term commitments.

## Usage Example

```mermaid
---
title: Atmos Terraform Drift Detection
---
stateDiagram-v2
  direction LR
  [*] --> detection : Scheduled Workflow (cron)
  detection --> remediate : Labeled Issue triggers workflow

  state "Atmos Terraform Drift Detection" as detection {
    [*] --> gather
    gather --> plan
    plan --> issue : if changes
    issue --> [*]

    state "Gather every component and stack" as gather
    state "Atmos Terraform Plan" as plan
    state "Create GitHub Issue" as issue
  }

  state "Atmos Terraform Drift Remediation" as remediate {
    [*] --> fetch
    fetch --> apply
    apply --> close
    close --> [*]

    state "Retrieve Terraform Plan" as fetch
    state "Atmos Terraform Apply" as apply
    state "Close GitHub Issue" as close
  }
```

Drift Detection with Atmos requires two separate workflows.

### Atmos Terraform Drift Detection

First, we trigger the "Atmos Terraform Drift Detection" workflow on a schedule. This workflow will gather every single component and stack in the repository. Then using that list of components and stacks, run `atmos terraform plan <component> --stack <stack>` for the given component and stack. If there are any changes, the workflow will create a GitHub Issue.

For example in this screenshot, the workflow has gathered two components. Only one has drift, and therefore one new Issue has been created.

![Example Drift Summary](/img/github-actions/drift-summary.png)

Now we can see the new Issue, including a Terraform Plan summary and metadata for applying.

![Example Issue](/img/github-actions/drift-issue.png)

:::tip Limiting the number of Issues

Without a limit, the number of Issues for drifted components can quickly get out-of-hand. In order to create a more manageable developer experience, set a limit for the maximum number of Issues created by the "Atmos Terraform Drift Detection" action with `max-opened-issues`.

The default value is `10`.

See [cloudposse/github-action-atmos-terraform-drift-detection](https://github.com/cloudposse/github-action-atmos-terraform-drift-detection/#inputs) for details.

:::

We can quickly see a complete list of all drift components in the "Issues" tab in the GitHub UI.

![Example Issue List](/img/github-actions/drift-issue-list.png)

#### Usage Example

<RemoteFile source="https://raw.githubusercontent.com/cloudposse/docs/refs/heads/master/examples/legacy/snippets/.github/workflows/atmos-terraform-drift-detection.yaml" />

#### 256 Matrix Limitation

:::warning

GitHub Actions support 256 matrix jobs in a single workflow at most, [ref](https://docs.github.com/en/actions/using-jobs/using-a-matrix-for-your-jobs#using-a-matrix-strategy).

:::

When planning all stacks in an Atmos environment, we frequently plan more than 256 component in the stacks at a time. In order to work around this limitation by GitHub, we can add an additional layer of abstraction using reusable workflows.

For example, the "Atmos Terraform Plan" workflow can call "Atmos Terraform Plan Matrix" workflow which then calls the "Atmos Terraform Plan" Composite Action.

```mermaid
---
title: GitHub Job Matrices
---
stateDiagram-v2
  direction LR
  [*] --> top
  top --> reusable1
  top --> reusable2
  top --> reusable3
  reusable1 --> component1
  reusable1 --> component2
  reusable1 --> component3
  reusable2 --> component4
  reusable2 --> component5
  reusable2 --> component6
  reusable3 --> component7
  reusable3 --> component8
  reusable3 --> component9

  state "Atmos Terraform Plan (Top Level Workflow)" as top
  state "Atmos Terraform Plan Matrix (Reusable Workflow) - Group A" as reusable1
  state "Atmos Terraform Plan Matrix (Reusable Workflow) - Group B" as reusable2
  state "Atmos Terraform Plan Matrix (Reusable Workflow) - Group C" as reusable3
  state "Atmos Terraform Plan (Composite Action) - Component 1" as component1
  state "Atmos Terraform Plan (Composite Action) - Component 2" as component2
  state "Atmos Terraform Plan (Composite Action) - Component 3" as component3
  state "Atmos Terraform Plan (Composite Action) - Component 4" as component4
  state "Atmos Terraform Plan (Composite Action) - Component 5" as component5
  state "Atmos Terraform Plan (Composite Action) - Component 6" as component6
  state "Atmos Terraform Plan (Composite Action) - Component 7" as component7
  state "Atmos Terraform Plan (Composite Action) - Component 8" as component8
  state "Atmos Terraform Plan (Composite Action) - Component 9" as component9
```

### Atmos Terraform Drift Remediation

Once we have an open Issue for a drifted component, we can trigger another workflow to remediate the drifted Terraform resources. When an Issue is labeled with `apply`, the "Atmos Terraform Drift Remediation" workflow will take the component and stack in the given Issue and run `atmos terraform apply <component> --stack <stack>` using the latest Terraform Planfile. If the apply is successful, the workflow will close the given Issue as resolved.

#### Usage Example

<RemoteFile source="https://raw.githubusercontent.com/cloudposse/docs/refs/heads/master/examples/legacy/snippets/.github/workflows/atmos-terraform-drift-remediation.yaml" />

## Requirements

This action has the requirements as [Github Actions](/integrations/github-actions/). Use the same S3 Bucket, DynamoDB table, IAM Roles and config described there.

---

## Atmos Terraform Drift Remediation

import Intro from '@site/src/components/Intro'
import RemoteFile from '@site/src/components/RemoteFile'

<Intro>
The  "Atmos Terraform Drift Remediation" GitHub Action provides a way for easily remediating Terraform drift and works with GitHub Issues using IssueOps.
</Intro>

This action is used for drift remediation together with it's [companion action for drift detection](/integrations/github-actions/atmos-terraform-drift-detection).

## Usage

### Config

The action expects the atmos configuration file `atmos.yaml` to be present in the repository.
The config should have the following structure:

```yaml
integrations:
  github:
    gitops:
      terraform-version: 1.5.2
      infracost-enabled: false
      artifact-storage:
        region: us-east-2
        bucket: cptest-core-ue2-auto-gitops
        table: cptest-core-ue2-auto-gitops-plan-storage
        role: arn:aws:iam::xxxxxxxxxxxx:role/cptest-core-ue2-auto-gitops-gha
      role:
        plan: arn:aws:iam::yyyyyyyyyyyy:role/cptest-core-gbl-identity-gitops
        apply: arn:aws:iam::yyyyyyyyyyyy:role/cptest-core-gbl-identity-gitops
      matrix:
        sort-by: .stack_slug
        group-by: .stack_slug | split("-") | [.[0], .[2]] | join("-")
```

> [!IMPORTANT]
> **Please note!** This GitHub Action only works with `atmos >= 1.63.0`. If you are using `atmos < 1.63.0` please use `v1` version of this action.

### Workflow example

In this example drift will be remediated when user sets label `apply` to an issue.

<RemoteFile source="https://raw.githubusercontent.com/cloudposse/docs/refs/heads/master/examples/legacy/snippets/.github/workflows/atmos-terraform-drift-remediation.yaml" />

## Requirements

This action has the requirements as [Github Actions](/integrations/github-actions/). Use the same S3 Bucket, DynamoDB table, IAM Roles and config described there.

## Inputs

<dl>
    <dt>`action`, _required_, default: `remediate`</dt>
    <dd>
        Drift remediation action. One of ['remediate', 'discard']
    </dd>

  <dt>`atmos-config-path`, _required_</dt>
  <dd>The path to the `atmos.yaml` file</dd>

  <dt>`atmos-version`, _optional_, default: `>= 1.63.0`</dt>
  <dd>The version of `atmos` to install</dd>

  <dt>`debug`, _optional_, default: `false`</dt>
  <dd>Enable action debug mode</dd>

  <dt>`issue-number`, _required_</dt>
  <dd>Issue Number</dd>

  <dt>`token`, _optional_</dt>
    <dd>
    Used to pull node distributions for Atmos from Cloud Posse's GitHub repository. Since there's a default, this is typically not supplied by the user. When running this action on github.com, the default value is sufficient. When running on GHES, you can pass a personal access token for github.com if you are experiencing rate limiting.
    Default:
    ```
    ${{ github.server\_url == 'https://github.com' && github.token \|\| '' }}
    ```
  </dd>
</dl>

---

## Atmos Terraform Plan

import Intro from '@site/src/components/Intro'
import RemoteFile from '@site/src/components/RemoteFile'

<Intro>
Simplify provisioning Terraform entirely from within GitHub Action workflows. This action makes it very easy to understand exactly what will happen from from the changes in a Pull Request (or any commit) by running a `terraform plan` and surfacing the output as a neatly formatted Job Summary viewable directly within the GitHub UI.
</Intro>

The Cloud Posse GitHub Action for "Atmos Terraform Plan" simplifies provisioning Terraform from within GitHub using workflows. Understand precisely what to expect from running a `terraform plan` from directly within the GitHub UI for any Pull Request.

Given any component and stack in an Atmos supported infrastructure environment, [`github-action-atmos-terraform-plan`](https://github.com/cloudposse/github-action-atmos-terraform-plan) will run `atmos terraform plan`, generate a Terraform [planfile](https://developer.hashicorp.com/terraform/tutorials/automation/automate-terraform), store this planfile in an S3 Bucket with metadata in DynamodDB, and finally format the Terraform Plan result as part of a [GitHub Workflow Job Summary](https://github.blog/2022-05-09-supercharging-github-actions-with-job-summaries/).

This action is intended to be used together with [Atmos Terraform Apply](/integrations/github-actions/atmos-terraform-apply) and [Atmos Affected Stacks](/integrations/github-actions/affected-stacks), as well as integrated into drift detection with [Atmos Terraform Detection and Remediation](/integrations/github-actions/atmos-terraform-drift-detection).

## Features

This GitHub Action incorporates superior GitOps support for Terraform by utilizing the capabilities of Atmos, enabling efficient management of large enterprise-scale environments.

* **Implements Native GitOps** with Atmos and Terraform
* **No hardcoded credentials.** Use GitHub OIDC to assume roles.
* **Compatible with GitHub Cloud & Self-hosted Runners** for maximum flexibility.
* **Beautiful Job Summaries** don't clutter up pull requests with noisy GitHub comments
* **100% Open Source with Permissive APACHE2 License** means you have no expensive subscriptions or long-term commitments.

## Screenshots

The following screenshot showcases a successful "plan" Job Summary report. The report effectively utilizes badges to clearly indicate success or failure. Furthermore, it specifically highlights any potentially destructive operations, mitigating the risk of unintentional destructive actions. A `diff`-style format is employed to illustrate the creation, recreation, destruction, or modification of resources. Unnecessary details are neatly hidden behind a collapsible `<details/>` block, ensuring a streamlined view. Additionally, developers are provided with a direct link to access the job run, eliminating the need for manual searching to gather information about any potential issues.

![Example Image](/img/github-actions/create.png)

By expanding the "Terraform Plan Summary" block (clicking on the `<details/>` block), the full details of the plan are visible.

![Example Create Extended](/img/github-actions/create-extended.png)

Furthermore, when a resource is marked for deletion, the Plan Summary will include a warning admonition.

![Example Destroy](/img/github-actions/destroy.png)

## Usage

In this example, the action is triggered when certain events occur, such as a manual workflow dispatch or the opening, synchronization, or reopening of a pull request, specifically on the main branch. It specifies specific permissions related to assuming roles in AWS. Within the "plan" job, the "component" and "stack" are hardcoded (`foobar` and `plat-ue2-sandbox`). In practice, these are usually derived from another action.

:::tip Passing Affected Stacks

We recommend combining this action with the [`affected-stacks`](/integrations/github-actions/affected-stacks) GitHub Action inside a matrix to plan all affected stacks in parallel.

:::

<RemoteFile source="https://raw.githubusercontent.com/cloudposse/docs/refs/heads/master/examples/legacy/snippets/.github/workflows/atmos-terraform-plan.yaml"/>

:::info Why Do We have Two Workflows?

GitHub Actions support 256 matrix jobs in a single workflow at most! When planning all stacks in an Atmos environment, we frequently plan more than 256 component in the stacks at a time. In order to work around this limitation by GitHub, we can add an additional layer of abstraction using reusable workflows.

[256 Matrix Limitation](/integrations/github-actions/atmos-terraform-drift-detection#256-matrix-limitation)

:::

<RemoteFile source="https://raw.githubusercontent.com/cloudposse/docs/refs/heads/master/examples/legacy/snippets/.github/workflows/atmos-terraform-plan-matrix.yaml"/>

with the following configuration as an example:

```yaml
  # .github/config/atmos-gitops.yaml
  atmos-config-path: ./rootfs/usr/local/etc/atmos/
  atmos-version: 1.65.0
  aws-region: us-east-2
  enable-infracost: false
  group-by: .stack_slug | split("-") | [.[0], .[2]] | join("-")
  sort-by: .stack_slug
  terraform-apply-role: arn:aws:iam::yyyyyyyyyyyy:role/cptest-core-gbl-identity-gitops
  terraform-plan-role: arn:aws:iam::yyyyyyyyyyyy:role/cptest-core-gbl-identity-gitops
  terraform-state-bucket: cptest-core-ue2-auto-gitops
  terraform-state-role: arn:aws:iam::xxxxxxxxxxxx:role/cptest-core-ue2-auto-gitops-gha
  terraform-state-table: cptest-core-ue2-auto-gitops
  terraform-version: 1.65.0
```

## Requirements

This GitHub Action depends on a few resources:
* **S3 bucket** for storing planfiles
* **DynamoDB table** for retrieving metadata about planfiles
* **2x IAM roles** for "planning" and accessing the "state" bucket

### S3 Bucket

This action can use any S3 Bucket to keep track of your planfiles. Just ensure the bucket is properly locked down since planfiles may contain secrets.

For example, [vendor in](/core-concepts/vendor/component-manifest) the [`s3-component`](https://docs.cloudposse.com/components/library/aws/s3-bucket/), then using an [Atmos stack configuration](/core-concepts/stacks/), define a bucket using the [`s3-bucket` component](https://github.com/cloudposse/terraform-aws-components/tree/main/modules/s3-bucket) with this catalog configuration:

```yaml
import:
  - catalog/s3-bucket/defaults

components:
  terraform:
    # S3 Bucket for storing Terraform Plans
    gitops/s3-bucket:
      metadata:
        component: s3-bucket
        inherits:
          - s3-bucket/defaults
      vars:
        name: gitops-plan-storage
        allow_encrypted_uploads_only: false
```

Assign this S3 Bucket ARN to the `terraform-plan-bucket` input.

### DynamoDB Table

Similarly, a simple DynamoDB table can be provisioned using our [`dynamodb` component](https://docs.cloudposse.com/components/library/aws/dynamodb/). Set the **Hash Key** and create a **Global Secondary Index** as follows:

```yaml
import:
  - catalog/dynamodb/defaults

components:
  terraform:
    # DynamoDB table used to store metadata for Terraform Plans
    gitops/dynamodb:
      metadata:
        component: dynamodb
        inherits:
          - dynamodb/defaults
      vars:
        name: gitops-plan-storage
        # This key (case-sensitive) is required for the cloudposse/github-action-terraform-plan-storage action
        hash_key: id
        range_key: ""
        # Only these 2 attributes are required for creating the GSI,
        # but there will be several other attributes on the table itself
        dynamodb_attributes:
          - name: 'createdAt'
            type: 'S'
          - name: 'pr'
            type: 'N'
        # This GSI is used to Query the latest plan file for a given PR.
        global_secondary_index_map:
          - name: pr-createdAt-index
            hash_key: pr
            range_key: createdAt
            projection_type: ALL
            non_key_attributes: []
            read_capacity: null
            write_capacity: null
        # Auto delete old entries
        ttl_enabled: true
        ttl_attribute: ttl
```

Pass the ARN of this table as the input to the `terraform-plan-table` of the [`cloudposse/github-action-atmos-terraform-plan`](https://github.com/cloudposse/github-action-atmos-terraform-plan) GitHub Action.

### IAM Access Roles

First create an access role for storing and retrieving planfiles from the S3 Bucket and DynamoDB table. We deploy this role using the [`gitops` component](https://docs.cloudposse.com/components/library/aws/gitops/). Assign this role ARN to the `terraform-state-role` input.

Next, create a role for GitHub workflows to use to plan and apply Terraform. We typically create an "AWS Team" with our [`aws-teams` component](https://docs.cloudposse.com/components/library/aws/aws-teams/), and then allow this team to assume `terraform` in the delegated accounts with our [`aws-team-roles` component](https://docs.cloudposse.com/components/library/aws/aws-team-roles/). Assign this role ARN to the `terraform-plan-role` input

---

## Component Updater

import File from '@site/src/components/File'
import RemoteFile from '@site/src/components/RemoteFile'
import Intro from '@site/src/components/Intro'
import Admonition from '@theme/Admonition';

import Step from '@site/src/components/Step'
import StepNumber from '@site/src/components/StepNumber'

<Intro>
Keep your infrastructure repositories current with the latest versions of [vendored components](/core-concepts/vendor) using the [Atmos Component Updater GitHub Action](https://github.com/cloudposse/github-action-atmos-component-updater). This powerful action simplifies and accelerates the management of component updates across repositories by using pull requests, ensuring that updates are processed quickly, accurately, and without hassle.
</Intro>

Use Cloud Posse's GitHub Action for updating [Atmos components](/core-concepts/components/) (e.g. [like the ones provided by Cloud Posse](https://github.com/cloudposse/terraform-aws-components/)) to streamline your infrastructure management.

With its customizable features, you can design an automated workflow tailored to your needs, making infrastructure repository maintenance more efficient and less error-prone.

## Key Features:

- **Selective Component Processing:** Configure the action to `exclude` or `include` specific components using wildcards, ensuring that only relevant updates are processed.
- **PR Management:** Limit the number of PRs opened at a time, making it easier to manage large-scale updates without overwhelming the system. Automatically close old component-update PRs, so they don't pile up.
- **Material Changes Focus:** Automatically open pull requests only for components with significant changes, skipping minor updates to `component.yaml` files to reduce unnecessary PRs and maintain a streamlined system.
- **Informative PRs:** Link PRs to release notes for new components, providing easy access to relevant information, and use consistent naming for easy tracking.
- **Scheduled Updates:** Run the action on a cron schedule tailored to your organization's needs, ensuring regular and efficient updates.

Discover more details and a comprehensive list of `inputs` and `outputs` in the [GitHub Action repository](https://github.com/cloudposse/github-action-atmos-component-updater) on GitHub.

## Usage Examples

### Workflow example

<RemoteFile source="https://raw.githubusercontent.com/cloudposse/docs/refs/heads/master/examples/legacy/snippets/.github/workflows/atmos-components-updater.yml" />

### Requirements

This action will automatically open pull requests for updated components in your repository.
To use it, we recommend installing a GitHub App to allow GitHub Actions to create pull requests within your GitHub Organization.

<Step>
  #### <StepNumber/> Create and install a GitHub App

  Follow the [Using a GitHub App](#using-a-github-app) steps below.
</Step>
<Step>
  #### <StepNumber/> Grant GitHub Actions workflows read and write permission

  If you're using GitHub Enterprise, first update this setting under "enterprises". The setting for the Organization will not be available until it's allowed for the Enterprise.
  `https://github.com/enterprises/YOUR_ORG/settings/actions`

  In either case, then enable the setting for your "organization":
  `https://github.com/organizations/YOUR_ORG/settings/actions`

  1. Find the section called "Workflow permissions"
  1. Select "Read and write permissions"
</Step>
<Step>
  #### <StepNumber/> Allow GitHub Actions to create and approve pull requests

  Again, if you're using GitHub Enterprise first update this setting under "enterprises":
  `https://github.com/enterprises/YOUR_ORG/settings/actions`

  In either case, then enable the setting for your "organization":
  `https://github.com/organizations/YOUR_ORG/settings/actions`

  1. Check "Allow GitHub Actions to create and approve pull requests"

  :::info Repository-Level Action Settings

  Enabling these action settings at an Organization level will enable the same settings for all repositories in your Organization. Confirm the same settings have been enabled for your infrastructure repository and optionally disable these settings for other repositories in your Organization at your own discretion.

  :::
</Step>

### Using a GitHub App

You may notice that we pass a generated token from a GitHub App to `github-access-token` instead of using the native `GITHUB_TOKEN`. We do this because Pull Requests will only trigger other GitHub Action Workflows if the Pull Request is created by a GitHub App or PAT. For reference, see [Triggering a workflow from a workflow](https://docs.github.com/en/actions/using-workflows/triggering-a-workflow#triggering-a-workflow-from-a-workflow).

1. Create a new GitHub App
2. Name this new app whatever you prefer. For example, `Atmos Component Updater [<YOUR NAMESPACE>]`.
  <Admonition type="tip" title="GitHub Requires Unique App Names">
  GitHub requires all GitHub Apps to have globally unique names. The name you select for this GitHub App can be whatever you'd prefer, so long as it's not already in use. For example, an `acme` organization might name their GitHub App `Atmos Component Updater [ACME]`.
  </Admonition>

3. List a Homepage URL of your choosing. This is required by GitHub, but you can use any URL. For example use our documentation page: `https://atmos.tools/integrations/github-actions/component-updater/`
4. (Optional) Add an icon for your new app (See below)

<details>
<summary>Feel free to download and use our Atmos icon with your GitHub App!</summary>

![App Icon](/img/github-actions/github-app.png)

</details>

5. Assign only the following Repository permissions:

```diff
+ Contents: Read and write
+ Pull Requests: Read and write
+ Metadata: Read-only
```

6. Generate a new private key [following the GitHub documentation](https://docs.github.com/en/apps/creating-github-apps/authenticating-with-a-github-app/managing-private-keys-for-github-apps#generating-private-keys).

:::tip Private Key Token Value

The private key token should start and end with `-----BEGIN RSA PRIVATE KEY-----` and `-----END RSA PRIVATE KEY-----` respectively, and include everything in between.
When you click "Generate Private Key", a `.pem` file is automatically downloaded. This file's content is the token value.

:::

7. Finally, save both the App ID and the new private key as secrets for GitHub Actions with `ATMOS_APP_ID` and `ATMOS_PRIVATE_KEY` respectively. Note, if using GitHub Enterprise, we recommend using "GitHub Environments" to scope the Secrets to protected branches. If that's not available, use repository-scoped GitHub Secrets instead.

:::tip App ID

The App ID is shown under GitHub App settings under `General > About > App ID`. The App ID is _not_ the same as the Installation ID that is given in the URL when you install the App.

`https://github.com/settings/apps/<YOUR APP NAME>`

:::

### Using GitHub Environments

We recommend creating a new GitHub environment for Atmos (requires GitHub Enterprise). With environments, the Atmos Component Updater workflow will be required to follow any branch protection rules before running or accessing the environment's secrets. Plus, GitHub natively organizes these Deployments separately in the GitHub UI.
To configure your environment, perform the following:
1. Open "Settings" for your repository
1. Navigate to "Environments"
1. Select "New environment"
1. Name the new environment, "`atmos`"
1. In the drop-down next to "Deployment branches and tags", select "Protected branches only"
1. In "Environment secrets", create the two required secrets for App ID (`ATMOS_APP_ID`) and App Private Key (`ATMOS_PRIVATE_KEY`) from [Using a GitHub App](#using-a-github-app) step.

Now the Atmos Component Updater workflow will create a new Deployment in the `atmos` environment for each workflow run, easily accessible from the GitHub UI.

![Example Environment](/img/github-actions/github-deployment-environment.png)

### Using a Custom Atmos CLI Config Path (`atmos.yaml`)

If your [`atmos.yaml` file](https://atmos.tools/cli/configuration) is not located in the root of the infrastructure repository, you can specify the path to it using [`ATMOS_CLI_CONFIG_PATH` env variable](https://atmos.tools/cli/configuration/#environment-variables).

<File title=".github/workflows/atmos-component-updater.yaml">
```yaml
  # ...
  - name: Update Atmos Components
    uses: cloudposse/github-action-atmos-component-updater@v1
    env:
      # Directory containing the `atmos.yaml` file
      ATMOS_CLI_CONFIG_PATH: ${{ github.workspace }}/rootfs/usr/local/etc/atmos/
    with:
      github-access-token: ${{ secrets.GITHUB_TOKEN }}
      max-number-of-prs: 5
```
</File>

### Customize Pull Request labels, title and body

<File title=".github/workflows/atmos-component-updater.yaml">
```yaml
  # ...
  - name: Update Atmos Components
    uses: cloudposse/github-action-atmos-component-updater@v1
    with:
      github-access-token: ${{ secrets.GITHUB_TOKEN }}
      max-number-of-prs: 5
      pr-title: 'Update Atmos Component \`{{ component_name }}\` to {{ new_version }}'
      pr-body: |
        ## what
        Component \`{{ component_name }}\` was updated [{{ old_version }}]({{ old_version_link }}) → [{{ old_version }}]({{ old_version_link }}).

        ## references
        - [{{ source_name }}]({{ source_link }})
      pr-labels: |
        component-update
        automated
        atmos
```
</File>

:::important Escape Backticks
The backtick symbols must be escaped in the GitHub Action parameters. This is because GitHub evaluates whatever is in the backticks and it will render as an empty string.
:::

#### For `title` template these placeholders are available:
- `component_name`
- `source_name`
- `old_version`
- `new_version`

#### For `body` template these placeholders are available:
- `component_name`
- `source_name`
- `source_link`
- `old_version`
- `new_version`
- `old_version_link`
- `new_version_link`
- `old_component_release_link`
- `new_component_release_link`

---

## GitHub Actions

import File from '@site/src/components/File'
import DocCardList from '@theme/DocCardList'
import Intro from '@site/src/components/Intro'
import PrimaryCTA from '@site/src/components/PrimaryCTA'

<Intro>
GitHub Actions are a powerful way to automate your workflows with Atmos. Use these actions to plan, apply, and manage your Terraform infrastructure with Atmos.
</Intro>

This collection of GitHub Actions is designed to work specifically with Atmos in an opinionated manner, enabling you to implement a modern change management system entirely within the native GitHub UI. These Actions use the standard [`atmos.yaml` configuration](/cli/configuration/#integrations) and [some backing services](#requirements) designed to properly manage Terraform plan files, including their invalidation.

These GitHub Actions strive to be cloud-agnostic; however, most of our instructions focus on AWS, where we predominantly use them. None of these actions require hardcoded credentials, and all work using GitHub OIDC and GitHub Apps managed by your organization. These Actions do not require any subscriptions and are based entirely on open source.

## GitHub Actions for Atmos

<DocCardList/>

## Requirements

GitHub Actions that utilize "plan file" storage depends on a few resources:
* **S3 bucket** for storing planfiles
* **DynamoDB table** for retrieving metadata about planfiles
* **2x IAM roles** for "planning" and accessing the "state" bucket
* `atmos.yaml` config with GitOps settings

### S3 Bucket

This action can use any S3 Bucket to keep track of your planfiles. Just ensure the bucket is properly locked down since planfiles may contain secrets.

For example, [vendor in](/core-concepts/vendor) the [`s3-component`](https://docs.cloudposse.com/components/library/aws/s3-bucket/), then using an [Atmos stack configuration](/core-concepts/stacks/), define a bucket using the [`s3-bucket` component](https://github.com/cloudposse/terraform-aws-components/tree/main/modules/s3-bucket) with this catalog configuration:

<File title="stacks/catalog/s3-bucket/gitops.yaml">
```yaml
import:
  - catalog/s3-bucket/defaults

components:
  terraform:
    # S3 Bucket for storing Terraform Plans
    gitops/s3-bucket:
      metadata:
        component: s3-bucket
        inherits:
          - s3-bucket/defaults
      vars:
        name: gitops-plan-storage
        allow_encrypted_uploads_only: false
```
</File>

Assign this S3 Bucket ARN to the `terraform-plan-bucket` input.

### DynamoDB Table

Similarly, a simple DynamoDB table can be provisioned using our [`dynamodb` component](https://docs.cloudposse.com/components/library/aws/dynamodb/). Set the **Hash Key** and create a **Global Secondary Index** as follows:

<File title="stacks/catalog/dynamodb/gitops.yaml">
```yaml
import:
  - catalog/dynamodb/defaults

components:
  terraform:
    # DynamoDB table used to store metadata for Terraform Plans
    gitops/dynamodb:
      metadata:
        component: dynamodb
        inherits:
          - dynamodb/defaults
      vars:
        name: gitops-plan-storage
        # This key (case-sensitive) is required for the cloudposse/github-action-terraform-plan-storage action
        hash_key: id
        range_key: ""
        # Only these 2 attributes are required for creating the GSI,
        # but there will be several other attributes on the table itself
        dynamodb_attributes:
          - name: 'createdAt'
            type: 'S'
          - name: 'pr'
            type: 'N'
        # This GSI is used to Query the latest plan file for a given PR.
        global_secondary_index_map:
          - name: pr-createdAt-index
            hash_key: pr
            range_key: createdAt
            projection_type: ALL
            non_key_attributes: []
            read_capacity: null
            write_capacity: null
        # Auto delete old entries
        ttl_enabled: true
        ttl_attribute: ttl
```
</File>

Pass the ARN of this table as the input to the `terraform-plan-table` of the [`cloudposse/github-action-atmos-terraform-plan`](https://github.com/cloudposse/github-action-atmos-terraform-plan) GitHub Action.

### IAM Access Roles

First create an access role for storing and retrieving planfiles from the S3 Bucket and DynamoDB table. We deploy this role using the [`gitops` component](https://docs.cloudposse.com/components/library/aws/gitops/). Assign this role ARN to the `terraform-state-role` input.

Next, create a role for GitHub workflows to use to plan and apply Terraform. We typically create an "AWS Team" with our [`aws-teams` component](https://docs.cloudposse.com/components/library/aws/aws-teams/), and then allow this team to assume `terraform` in the delegated accounts with our [`aws-team-roles` component](https://docs.cloudposse.com/components/library/aws/aws-team-roles/). Assign this role ARN to the `terraform-plan-role` input

### Atmos Configuration

The actions that works with atmos `>= 1.63.0` expects the Atmos configuration file `atmos.yaml` to be present in the repository.
The config should have the following structure:

<File title="rootfs/usr/local/etc/atmos/atmos.yaml">
```yaml
integrations:
  github:
    gitops:
      terraform-version: 1.5.2
      infracost-enabled: false
      artifact-storage:
        region: us-east-2
        bucket: cptest-core-ue2-auto-gitops
        table: cptest-core-ue2-auto-gitops-plan-storage
        role: arn:aws:iam::xxxxxxxxxxxx:role/cptest-core-ue2-auto-gitops-gha
      role:
        plan: arn:aws:iam::yyyyyyyyyyyy:role/cptest-core-gbl-identity-gitops
        apply: arn:aws:iam::yyyyyyyyyyyy:role/cptest-core-gbl-identity-gitops
      matrix:
        sort-by: .stack_slug
        group-by: .stack_slug | split("-") | [.[0], .[2]] | join("-")
```
</File>

For actions that use atmos `< 1.63.0` the settings passed as github action inputs.
Please follow documentation for each action to see the required inputs.

## Compatibility Matrix

:::important Important!

Our GitHub Actions depend on specific versions of Atmos.

:::

### Artifacts Upgrade

With version `v2` of
[cloudposse/github-action-atmos-terraform-drift-detection](https://github.com/cloudposse/github-action-atmos-terraform-drift-detection)
and version `v3` of
[cloudposse/github-action-atmos-terraform-plan](https://github.com/cloudposse/github-action-atmos-terraform-plan), the
artifact storage configuration was updated to use the same structure. Both will need to be updated to these versions or
later to pass artifacts across the actions.

Please see the release notes:

1. https://github.com/cloudposse/github-action-atmos-terraform-plan/releases/tag/v3.0.0
1. https://github.com/cloudposse/github-action-atmos-terraform-drift-detection/releases/tag/v2.0.0

### Atmos `< 1.63.0`

If you are using Atmos `< 1.63.0`, please refer to the following table:

| Github action                                                                                                                      | Atmos `< 1.63.0`                                                                              | Atmos `>= 1.63.0`                                                                                        |
| ---------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------- |
| [`github-action-atmos-affected-stacks`](https://github.com/cloudposse/github-action-atmos-affected-stacks)                         | [`v2`](https://github.com/cloudposse/github-action-atmos-affected-stacks/tree/v2)             | [`v1`](https://github.com/cloudposse/github-action-atmos-affected-stacks/tree/v1) or greater             |
| [`github-action-atmos-terraform-plan`](https://github.com/cloudposse/github-action-atmos-terraform-plan)                           | [`v1`](https://github.com/cloudposse/github-action-atmos-terraform-plan/tree/v1)              | [`v2`](https://github.com/cloudposse/github-action-atmos-terraform-plan/tree/v2) or greater              |
| [`github-action-atmos-terraform-apply`](https://github.com/cloudposse/github-action-atmos-terraform-apply)                         | [`v1`](https://github.com/cloudposse/github-action-atmos-terraform-apply/tree/v1)             | [`v2`](https://github.com/cloudposse/github-action-atmos-terraform-apply/tree/v2) or greater             |
| [`github-action-atmos-terraform-drift-remediation`](https://github.com/cloudposse/github-action-atmos-terraform-drift-remediation) | [`v1`](https://github.com/cloudposse/github-action-atmos-terraform-drift-remediation/tree/v1) | [`v2`](https://github.com/cloudposse/github-action-atmos-terraform-drift-remediation/tree/v2) or greater |
| [`github-action-atmos-terraform-drift-detection`](https://github.com/cloudposse/github-action-atmos-terraform-drift-detection)     | [`v0`](https://github.com/cloudposse/github-action-atmos-terraform-drift-detection/tree/v0)   | [`v1`](https://github.com/cloudposse/github-action-atmos-terraform-drift-detection/tree/v1) or greater   |

---

## Setup Atmos

import File from '@site/src/components/File'
import Intro from '@site/src/components/Intro'

<Intro>
The Cloud Posse GitHub Action to “Setup Atmos” makes it easy to run atmos in your GitHub Action workflows by installing Atmos.
</Intro>

Easily integrate Atmos into your GitHub Action workflows using [`github-action-setup-atmos`](https://github.com/cloudposse/github-action-setup-atmos). To simplify your workflows, we offer a [GitHub Action](https://github.com/cloudposse/github-action-setup-atmos) that streamlines the process of [installing Atmos](/install).

We provide a [GitHub Action](https://github.com/cloudposse/github-action-setup-atmos) to make that easier for CI/CD applications.

## Usage Example

<File title=".github/workflows/example.yaml">
```yaml
on:
  workflow_dispatch:
  pull_request:

jobs:
  build:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3

      - name: Setup Atmos
        uses: cloudposse/github-action-setup-atmos
        with:
          # Version can be pinned but defaults to latest if not specified
          atmos-version: 1.88.0
```
</File>

---

## Infrastructure CI/CD (GitOps)

import DocCardList from '@theme/DocCardList'

<DocCardList/>

---

## Spacelift Integration

import Intro from '@site/src/components/Intro'

<Intro>
Atmos natively supports [Spacelift](https://spacelift.io). This is accomplished using this [terraform module](https://github.com/cloudposse/terraform-spacelift-cloud-infrastructure-automation) that reads the YAML Stack configurations and provisions the Spacelift resources.
</Intro>

Cloud Posse provides two terraform components for Spacelift support:

- [Terraform Component](/core-concepts/components/) for provisioning a
  [Spacelift Worker Pool](https://github.com/cloudposse/terraform-aws-components/tree/main/modules/spacelift/worker-pool)

- [Terraform Component](/core-concepts/components/) for
  provisioning [Spacelift Stacks](https://github.com/cloudposse/terraform-aws-components/tree/main/modules/spacelift/admin-stack)

## Stack Configuration

Atmos components support the following `spacelift` specific settings:

```yaml
components:
  terraform:
    example:
      settings:
        spacelift:
          # enable the stack in Spacelift
          workspace_enabled: true

          administrative: true

          # auto deploy this stack
          autodeploy: true

          # commands to run before init
          before_init: []

          # Specify which component directory to use
          component_root: components/terraform/example

          description: Example component

          # whether to auto destroy resources if the stack is deleted
          stack_destructor_enabled: false

          worker_pool_name: null

          # Do not add normal set of child policies to admin stacks
          policies_enabled: []

          # set explicitly below
          administrative_trigger_policy_enabled: false

          # policies to enable
          policies_by_id_enabled:
            - trigger-administrative-policy
```

## OpenTofu Support

Spacelift is compatible with [OpenTofu](https://opentofu.org) and configurable on a global and per stack or component basis.

To make OpenTofu the default, add the following to your top-level stack manifest:

```yaml
settings:
  spacelift:
    # Use OpenTofu
    terraform_workflow_tool: OPEN_TOFU
```

Similarly, to override this behavior, or to only configure it on specific components, add the following to the component
configuration:

```yaml
components:
  terraform:
    my-component:
      settings:
        spacelift:
          # Use OpenTofu
          terraform_workflow_tool: OPEN_TOFU
```

For more details on [Atmos support for OpenTofu](/core-concepts/projects/configuration/opentofu) see our integration page.

## Spacelift Stack Dependencies

Atmos supports [Spacelift Stack Dependencies](https://docs.spacelift.io/concepts/stack/stack-dependencies) in component configurations.

You can define component dependencies by using the `settings.depends_on` section. The section used to define all the Atmos components (in
the same or different stacks) that the current component depends on.

The `settings.depends_on` section is a map of objects. The map keys are just the descriptions of dependencies and can be strings or numbers.
Provide meaningful descriptions or numbering so that people can understand what the dependencies are about.

Each object in the `settings.depends_on` section has the following schema:

- `component` (required) - an Atmos component that the current component depends on
- `namespace` (optional) - the `namespace` where the Atmos component is provisioned
- `tenant` (optional) - the `tenant` where the Atmos component is provisioned
- `environment` (optional) - the `environment` where the Atmos component is provisioned
- `stage` (optional) - the `stage` where the Atmos component is provisioned

The `component` attribute is required. The rest are the context variables and are used to define Atmos stacks other than the current stack.
For example, you can specify:

- `namespace` if the `component` is from a different Organization
- `tenant` if the `component` is from a different Organizational Unit
- `environment` if the `component` is from a different region
- `stage` if the `component` is from a different account
- `tenant`, `environment` and `stage` if the component is from a different Atmos stack (e.g. `tenant1-ue2-dev`)

In the following example, we specify that the `top-level-component1` component depends on the following:

- The `test/test-component-override` component in the same Atmos stack
- The `test/test-component` component in Atmos stacks in the `dev` stage
- The `my-component` component from the `tenant1-ue2-staging` Atmos stack

```yaml
components:
  terraform:
    top-level-component1:
      settings:
        depends_on:
          1:
            # If the `context` (namespace, tenant, environment, stage) is not provided,
            # the `component` is from the same Atmos stack as this component
            component: "test/test-component-override"
          2:
            # This component (in any stage) depends on `test/test-component`
            # from the `dev` stage (in any `environment` and any `tenant`)
            component: "test/test-component"
            stage: "dev"
          3:
            # This component depends on `my-component`
            # from the `tenant1-ue2-staging` Atmos stack
            component: "my-component"
            tenant: "tenant1"
            environment: "ue2"
            stage: "staging"
      vars:
        enabled: true
```

:::tip

Refer to [`atmos describe dependents` CLI command](/cli/commands/describe/dependents) for more information.

:::

---

## Atmos FAQ

import ReactPlayer from 'react-player'

### Why is the tool called Atmos?

*Once upon a time*, in the vast expanse of cloud computing, there was a need for a tool that could orchestrate and manage the complex layers of infrastructure with ease and precision. This tool would need to rise above the rest, be completely automated, providing oversight and control much like a protective layer enveloping the Earth. Thus, the idea of "atmos" was born, drawing inspiration from the visionary science fiction of the 1986 "Aliens" movie, where [atmospheric terraformers](https://avp.fandom.com/wiki/Atmosphere_Processing_Plant) transform alien worlds into hospitable realms.

<ReactPlayer className={"react-player"} controls url='https://www.youtube.com/watch?v=y9eM8SrvNdY&t=5s'/>But there's more to the story. As the [Cloud Posse](https://cloudposse.com) delved deeper into crafting this tool, they discovered a wonderful
coincidence. "Atmos" could stand for **"Automated Terraform Management & Orchestration Software"** perfectly encapsulating its purpose
and capabilities. This serendipitous acronym was a delightful surprise, further cementing the name's destiny.

### Are there any reference architectures for Atmos?

Yes and no. Cloud Posse sells [reference architectures for AWS](https://cloudposse.com/services/) based on Atmos and Terraform, that are ideal for Funded Startups and Enterprises.

We plan to release some free reference architectures soon but cannot commit to a specific date.

Until then, please review our [Quick Start](/quick-start/), which takes you through deploying your first Terraform components using Atmos.

### Is Atmos similar to Terragrunt?

Yes, Atmos is similar to Terragrunt in that it offers a robust command-line tool for operating with Terraform at scale. Both tools are designed to enhance Terraform's capabilities, allowing for more efficient management of complex infrastructure configurations. They support similar functionality, such as DRY (Don't Repeat Yourself) code practices, module management, and workflow orchestration, but they diverge significantly in their approach and philosophy.

### How is Atmos unlike Terragrunt?

#### Atmos uses YAML over HCL

Atmos leverages YAML to introduce an opinionated Domain-Specific Language (DSL) for defining what constitutes a Stack,
optimizing it for the management and orchestration of Terraform configurations. Designed to be tool-agnostic, Atmos offers
a user-friendly CLI experience that supports a broad spectrum of workflows and infrastructure patterns, extending beyond
just Terraform. Preferring YAML over HCL, Atmos advocates for using YAML for configuration due to its tool
compatibility—any tool can read or write YAML, unlike HCL. With its reliance on YAML for configuration, Atmos seamlessly integrates with
other tools that manipulate YAML, such as [`yq`](https://github.com/mikefarah/yq) and templating engines like [`gomplate`](https://github.com/hairyhenderson/gomplate). This compatibility enhances its utility and flexibility in various environments.

#### Atmos uses vendoring instead of just-in-time downloading of root modules

Terragrunt facilitates the use of remote "root" modules, a feature Atmos also supports but implements more explicitly
through "vendoring." This approach involves directly incorporating external modules into component folders ahead of time,
rather than retrieving them on-the-fly. By committing these as artifacts, Atmos aligns more with the principles of immutable
infrastructure, ensuring stability and reliability.

#### Atmos works with vanilla Terraform

Atmos is designed to be fully compatible with vanilla Terraform, ensuring that your Terraform HCL (HashiCorp Configuration Language) code remains operational with Terraform's native functionality, even if you choose not to use Atmos. This compatibility underscores Atmos's flexibility and its ability to integrate seamlessly into existing workflows. In contrast, Terragrunt employs a code generation strategy, utilizing a proprietary extension of HCL to introduce constructs and methods not directly interpretable by native Terraform. This approach involves dynamically generating code and assembling files and code fragments at runtime, distinguishing Terragrunt's methodology from the straightforward compatibility offered by Atmos.

#### Atmos does not promote code generation for root modules

Unlike Terragrunt, Atmos eschews code generation for composing DRY (Don't Repeat Yourself) Terraform root modules, because that makes
them more challenging to test. Instead, it encourages the use of modular architecture to maintain DRY principles, advocating for sufficient parameterization of components (root modules) to keep them versatile and highly reusable. The complexity is managed through configuration,
with Atmos focusing on generating `.tfvars` files before running Terraform. This approach allows seamless integration with vanilla Terraform
and offers an easy exit strategy by [generating and committing these files](/cli/commands/terraform/generate-varfiles).

#### Atmos provides prescriptive guidance on operating Terraform at scale

Furthermore, Atmos delivers prescriptive guidance on [design patterns](/design-patterns/) and [component architecture](/core-concepts/stacks/inheritance) for Terraform root modules, establishing itself as an opinionated framework designed to enhance Terraform's capabilities. It offers [advanced features](/core-concepts/stacks/inheritance) tailored for both straightforward and intricate deployment scenarios. Essentially acting as a comprehensive framework, Atmos is exceptionally well-suited for complex, regulated enterprise environments. Its robust support for [Stack manifests](/core-concepts/stacks) underscores its versatility as a tool, enabling precise and effortless infrastructure management and orchestration across various scales and complexities.

### Can Atmos be used together with Terragrunt?

Yes, technically, Atmos and Terragrunt can be used in conjunction. However, it's important to acknowledge that this combination introduces a significant overlap in functionality and some philosophical differences. Developers generally prefer to minimize the number of layers in their tooling to avoid complexity, and integrating these two tools could steepen the learning curve for teams.

The key motivation for integrating Terragrunt within Atmos would be to offer a seamless CLI (Command Line Interface) experience, facilitating a gradual transition to Atmos's methodologies. This strategy allows teams to utilize Terragrunt for existing infrastructure management ("for historical reasons") while adopting Atmos for new projects. Essentially, it's a strategic approach to support migration to Atmos, enabling the tool to invoke Terragrunt where necessary, as teams progressively shift towards fully embracing Atmos for infrastructure orchestration.

There are a few ways to accomplish it, depending on your needs or preferences.

1. Set the default the `command` to call in the Atmos CLI Configuration for Terraform components.
2. Override the `command` in the `settings` section for each component definition in the stack configuration (this respects inheritances, so the [`mixin`](/core-concepts/stacks/inheritance/mixins) pattern can be used.)

### What are some of the alternatives to Atmos?

Check out the [list of alternatives](/reference/alternatives) that have influenced its design.

### Is Atmos a replacement for Terraform?

No, Atmos is an orchestration tool that supports terraform, along with other tools. Plus, it even supports
custom commands, so any tool can be powered by atmos.

### Does Atmos work with Terraform Cloud and Terraform Enterprise?

Probably, but we haven't tested it.

Here's why it should work with any CI/CD system.

- Atmos works with *vanilla* Terraform HCL code (we do not introduce any proprietary HCL code formats)
- Atmos can [generate `.tfvar` files](/cli/commands/terraform/generate-varfiles) from the deep-merged configurations for any given stack, or all stacks at once.
- Commit those `.tfvar`, then run Terraform as normal, passing the appropriate `.tfvar` file to the `terraform` command with the `-vars` argument.

Our recommended way of practicing GitOps with Terraform is with our turn-key [GitHub Actions](/integrations/github-actions/).

### Does Atmos work with Atlantis?

Yes, it does. See our [integration page](/integrations/atlantis).

### Does atmos support OpenTofu (OpenTF)

Yes, it does.

### Does Atmos only work with AWS?

No, **Atmos is entirely cloud-agnostic**. There are funded startups and enterprises alike, using Atmos on Amazon Web Services (AWS),
Google Compute Cloud (GCP) and Microsoft Azure (Azure), among others.

### Does Atmos require any subscriptions?

No, Atmos operates purely as a Command Line Interface (CLI) tool. It does not require any subscriptions, services, or daemons to run,
ensuring straightforward and accessible functionality without the need for ongoing subscriptions.

### Is Atmos a commercial product?

Atmos is free and open source under the permissive APACHE2 license.

:::tip Words of Wisdom
We believe the most successful open-source tools are not themselves the end products sold in the market; instead, they serve as enablers,
empowering others to build and innovate successful products.
:::

[Cloud Posse](https://cloudposse.com/), a DevOps Accelerator for Funded Startups and Enterprises, funds Atmos development.

### How do we make money?

[Cloud Posse](https://cloudposse.com/) sells [reference architectures for AWS](https://cloudposse.com/services/) that are based on Atmos and Terraform.
Our typical customers are **Funded Startups** and **Enterprises** that want to leverage AWS together with platforms
like GitHub, GitHub Actions, Datadog, OpsGenie, Okta, etc.

---

## Atmos Features

import Intro from '@site/src/components/Intro'

<Intro>
Atmos streamlines Terraform orchestration, environment, and configuration management, offering developers and DevOps a framework and a set of powerful tools to tackle deployment challenges.
</Intro>

Designed to be cloud agnostic, it enables you to operate consistently across various cloud platforms. These features boost efficiency, clarity, and control across various environments, making it an indispensable asset for managing complex infrastructures with confidence.

### Core Features
- **Terminal UI** Polished interface for easier interaction with Terraform, workflows, and commands.
- **Native Terraform Support:** Orchestration, backend generation, varfile generation, ensuring compatibility with vanilla Terraform.
- **Stacks:** Powerful abstraction layer defined in YAML for orchestrating and deploying components.
- **Components:** A generic abstraction for deployable units, such as Terraform "root" modules.
- **Vendoring:** Pulls dependencies from remote sources, supporting immutable infrastructure practices.
- **Custom Commands:** Extends Atmos's functionality, allowing integration of any command with stack configurations.
- **Workflow Orchestration:** Comprehensive support for managing the lifecycle of cloud infrastructure from initiation to maintenance.

### Configuration and Management
- **Service Catalogs and Component Libraries:** Collections of reusable components for efficient infrastructure management.
- **Schema Validation:** Ensures configurations are correct against a JSON schema.
- **Deep-merged Imports & Inheritance:** Streamlines configuration reuse and simplifies managing complex setups.
- **Mixins:** Reusable snippets of configuration to avoid repetition across stack configurations.
- **OPA Policy Controls:** Integrates Open Policy Agent for configuration-level policy enforcement, suitable for varied stack policies.
- **Automatic Component Updates** Using the [Atmos Component Updater GitHub Action](/integrations/github-actions/component-updater),
   Pull Requests are automatically generated to update components to their latest versions, ensuring the infrastructure remains continually up-to-date.

### Integrations
- **GitHub Actions Support:** Facilitates GitOps workflows with Terraform Plan, Apply, and Drift Detection.
- **Helmfile Support:** Provides declarative specifications for deploying helm charts, supporting Kubernetes management.
- **Works with Atlantis, and Spacelift:** Ensures compatibility with popular Terraform Automation & Collaboration Software (TACOS).
- **Terraform Provider** Atmos has a Terraform provider that can be used to manage Atmos configurations and stacks natively from HCL.

---

## Getting Started with Atmos

import ReactPlayer from 'react-player'
import Link from '@docusaurus/Link'
import Intro from '@site/src/components/Intro'

Atmos is a cloud architecture framework for native Terraform.

<Intro>
Use Atmos to break your architecture into reusable [Components](/core-concepts/components) that you implement using [Terraform "root modules"](/core-concepts/components/terraform). Then, tie everything together using [Stack](/core-concepts/stacks) configurations defined in YAML.
</Intro>

If you're familiar with mainstream programming frameworks, then you'll feel right at home with Atmos. It's a cloud-agnostic framework for managing your cloud infrastructure using native Terraform. You'll benefit from a [well-organized project layout](/core-concepts/projects), [best practices](/best-practices/), [design patterns](/design-patterns/), and a [supportive community](/community) ready to help.

If you're [new to Terraform or struggling to manage your infrastructure at scale](/introduction/why-atmos), then [Atmos is you](/introduction/why-atmos/nirvana). It's been [proven to work for countless venture-backed startups and enterprises](https://cloudposse.com), and we're confident it will work for you too.

#### Are you tired of doing Terraform the old way? Try the new way.

<Link
    to="/quick-start/mindset"
    className="button button--lg button--primary">
    Try Quick Start
</Link>

<Link
    to="/core-concepts"
    className="button button--lg button--outline button--primary ml20">
    Learn Atmos
</Link>

## Screenshots

To get an idea of what it looks like using `atmos` on the command line, just [try our quickstart](/quick-start/) and run the [`atmos`](/cli) command to start
an interactive UI in the terminal. Use the arrow keys to select stacks and components to deploy.

![Atmos Screenshot](../../../docs/demo.gif)

In Atmos, you can easily search and navigate your configuration from the built-in UI.

![`atmos` CLI command example 2](/img/cli/atmos/atmos-cli-command-1.png)

## Introduction

With Atmos, you gain simplified control over your cloud resources, enabling you to manage unlimited environments, services,
and configurations using simple, clear YAML configuration files. Best of all, it works without writing a single line of HCL or code generation.
This approach not only ensures that your infrastructure's deployment and scaling are as straightforward as executing a single command,
 but also cleanly separates your configuration from your Terraform code keeping it portable, thereby streamlining the entire process.

Designed to operate seamlessly across diverse environments—be it production, staging, development, or testing — Atmos integrates
perfectly into continuous integration (CI/CD) workflows with its [GitHub Actions](/integrations/github-actions/) or support for other [TACOS](/integrations/).
It offers a comprehensive set of commands for managing the lifecycle of your cloud infrastructure, from creation to maintenance and beyond.

:::tip Did you know?
By leveraging Atmos in conjunction with Cloud Posse's [*expertise in AWS*](https://cloudposse.com),
[*terraform blueprints*](https://cloudposse.com/services/), and our [*knowledgeable community*](https://slack.cloudposse.com), teams can achieve
operational mastery and innovation faster, transforming their infrastructure management practices into a competitive advantage.
:::

## How does Atmos work?

At a high-level, Atmos is responsible for managing the configuration that gets passed to [Components](/terms/component) when they are instantiated.

To make this less abstract, we're going to focus on Terraform and how configuration gets passed to it.
The secret to scaling architectures with Terraform, is to decompose it into smaller root modules. We want to keep these root modules
as generic and reusable as possible by parameterizing them. In doing so, we shift the complexity from the HCL code to managing
the configuration itself. In terraform, there is the concept of variable files (aka "var files"),
which are JSON-esque configuration files and how external configuration is passed to `terraform plan` using
the [`-var-file` argument](https://developer.hashicorp.com/terraform/language/values/variables#variable-definitions-tfvars-files).
All we need to do is tap into this power, and pass our own configuration that comes from Atmos.

With Atmos, we organize our configuration into [Stacks](/core-concepts/stacks). Stacks are YAML manifests that compose all the
[Components](/core-concepts/components) (e.g. Terraform root modules) we need to bring up our environment. For example, in its
simplest form, we may have `production`, `dev`, and `staging` stacks.  We can then deploy any of these stacks to provision
everything using Terraform. All that Atmos does is deep-merge the variables from the stack configurations and produce a single "var file"
that we pass to the `terraform plan`. There's very little magic, which is how to ensure future compatibility with Terraform and
interoperability with the entire Terraform ecosystem.

:::info Summary
- Terraform configuration is difficult to manage at scale without tooling
- Terraform by itself is not enough to be successful
- Atmos makes managing those configurations easier
:::

## Major Benefits

Atmos offers crucial benefits for **funded startups** and **enterprises** aiming to efficiently scale their infrastructure and teams.
In dynamic, **growth-focused environments**, its integration into DevOps practices ensures **agility, compliance, and cost-effectiveness,**
providing key competitive advantages for managing complex systems with precision.

1. **Remove Guesswork and Confusion:** Make Terraform operations easier to understand so that teams can manage their own Terraform projects.
2. **Establish Company-Wide Standards:** Simplify the management and enforcement of infrastructure standards, with documented processes for repeatable and efficient collaboration.
3. **Ease of Configuration Management & Environments:** Simplify managing complex configurations, ensure uniform deployments across environments, and replace homegrown solutions.
4. **Policy Compliance:** Help maintain compliance with internal and external regulations.
5. **Cost Savings:** Leverage your existing GitHub and GitHub Enterprise setups for cost-effective Terraform automation and collaboration, avoiding expensive tools.

## Core Features

Atmos streamlines Terraform orchestration, environment, and configuration management, offering developers and DevOps a set of
powerful tools to tackle deployment challenges. Designed to be cloud agnostic, it enables you to operate consistently across
various cloud platforms. These features boost efficiency, clarity, and control across various environments, making it an
indispensable asset for managing complex infrastructures with confidence.

- **Terminal UI** Polished interface for easier interaction with Terraform, workflows, and commands.
- **Native Terraform Support:** Orchestration, backend generation, varfile generation, ensuring compatibility with vanilla Terraform.
- **Stacks:** Powerful abstraction layer defined in YAML for orchestrating and deploying components.
- **Components:** A generic abstraction for deployable units, such as Terraform "root" modules.
- **Vendoring:** Pulls dependencies from remote sources, supporting immutable infrastructure practices.
- **Custom Commands:** Extends Atmos's functionality, allowing integration of any command with stack configurations.
- **Workflow Orchestration:** Comprehensive support for managing the lifecycle of cloud infrastructure from initiation to maintenance.

See [all features of Atmos](/features).

## Atmos is Open Source (APACHE2)

[![Last Commit](https://img.shields.io/github/last-commit/cloudposse/atmos/main?style=social)](https://github.com/cloudposse/atmos/commits/main/)
[![Atmos Repo](https://img.shields.io/github/stars/cloudposse/atmos)](https://github.com/cloudposse/atmos)

Best of all [Atmos is truly open-source](https://github.com/cloudposse/atmos) (APACHE2) with an [active community](https://slack.cloudposse.com)
supporting it. There are no strings attached, and it's under active development with hundreds of GitHub stars.

There's also an ecosystem of tools that it works with, and [Terraform modules](https://github.com/cloudposse/?q=&type=all&language=hcl&sort=)
downloaded hundreds of millions of times.

:::tip Become a part of our Movement
We encourage you to become a part of our movement and make a significant impact on the industry as a whole.

Join us on [Slack](https://slack.cloudposse.com/) or tune-in on our [Weekly "DevOps" Office Hours](https://cloudposse.com/office-hours/)
:::



## What Atmos is Not...

It might help to also understand what Atmos is not attempting to replace:

- **Atmos is not an alternative for certain tools.** Under the hood, it still executes `terraform`, `kubectl`, `docker`, `helm`, etc.
  Atmos composes tools together via [Stack](/core-concepts/stacks/) configurations and orchestrates their execution via [workflows](/core-concepts/workflows/), [custom commands](/core-concepts/custom-commands/), or [other integrations](/integrations/).
- **Atmos is not an alternative to CI/CD systems.**  It complements these systems by being an operational tool that can be invoked
  from within CI/CD pipelines, as detailed in our [integrations](/integrations/).
- **Atmos is not a competitor to Terraform Cloud or Terraform Enterprise.** It’s a command-line tool that makes working with other
  CLI’s like `terraform`, `helm` and other infrastructure management tools better. It streamlines infrastructure configuration management
  and integrates nicely with existing CI/CD pipelines. Atmos is not a commercial product and has no proprietary lock-ins.
- **Atmos is not tied to any specific cloud provider.** It is cloud-agnostic, and designed to work across various cloud platforms
  (e.g. AWS, GCP, Azure, etc.), enhancing interoperability rather than tying users to a single cloud environment.
- **Atmos is not a configuration management database ([CMDB](https://en.wikipedia.org/wiki/Configuration_management_database)).** Although it can manage and deploy the desired state of configurations,
  it doesn't serve as a repository for tracking the actual state of IT assets and configurations in the _traditional_ sense of a CMDB.
- **Atmos is not a replacement for development environments or IDEs.** It focuses on the orchestration and deployment aspects of infrastructure, without aiming to replace the tools developers use to write and test code.

## Live Demo of Atmos

Here's a live demo we did on [our weekly "Office Hours"](https://cloudposse.com/office-hours/).

<ReactPlayer className={"react-player"} controls url='https://www.youtube.com/watch?v=0dFEixCK0Wk&t=1415s'/>
## How do we make money?

[Cloud Posse](https://cloudposse.com/) sells [reference architectures for AWS](https://cloudposse.com/services/) that are based on Atmos and Terraform.
Our typical customers are Funded Startups and Enterprises that want to leverage AWS together with platforms
like GitHub, GitHub Actions, Datadog, OpsGenie, Okta, etc.

## Next Steps

* [Review the Core Concepts](/core-concepts). Learn about our conventions.
* [Check out our Quick Start](/quick-start). Get started in as little as 30 minutes.
* [Check out our slides](/reference/slides). Present a compelling case for using Atmos at your company.

---

## Atmos Use-Cases

import Intro from '@site/src/components/Intro'

<Intro>
Atmos has proven time and again that it works great for these key use cases. It’s versatile like a Swiss Army knife and reliable like a well-oiled machine, making it a solid choice for cloud infrastructure and DevOps.
</Intro>

- **Managing Large Multi-Account Cloud Environments.**  Suitable for organizations using multiple cloud accounts to separate different projects or stages of development.
- **Cross-Platform Cloud Architectures.**  Ideal for businesses that need to manage the configuration of services across AWS, GCP, Azure, etc., to build a cohesive system.
- **Multi-Tenant Systems for SaaS.**  Perfect for SaaS companies looking to host multiple customers within a unified infrastructure. Simply define a baseline tenant configuration once, and then seamlessly onboard new tenants by reusing this baseline through pure configuration, bypassing the need for further code development.
- **Efficient Multi-Region Deployments.**  Atmos facilitates streamlined multi-region deployments by enabling businesses to define baseline configurations with [stacks](/core-concepts/stacks/) and extend them across regions with DRY principles through [imports](/core-concepts/stacks/imports) and [inheritance](/core-concepts/stacks/inheritance).
- **Compliant Infrastructure for Regulated Industries.**  Atmos empowers DevOps and SecOps teams to create vetted configurations that comply with SOC2, HIPAA, HITRUST, PCI, and other regulatory standards. These configurations can then be efficiently shared and reused across the organization via [service catalogs](/core-concepts/stacks/catalogs), [component libraries](/core-concepts/components/library), [vendoring](/core-concepts/vendor), and [OPA policies](/core-concepts/validate/opa), simplifying the process of achieving and maintaining rigorous compliance.
- **Empowering Teams with Self-Service Infrastructure.**  Allows teams to manage their infrastructure needs independently, using
  predefined templates and policies.
- **Streamlining Deployment with Service Catalogs, Landing Zones, and Blueprints:** Provides businesses the ability to craft their own ready-to-use templates and guidelines for setting up cloud environments quickly and consistently, tailored to the unique needs of the organization.

Don't see your use-case listed? Ask us in the [`#atmos`](/community/slack) Slack channel, or [join us for "Office Hours"](/community/office-hours) every week.

---

## Nirvana 💫

import Link from '@docusaurus/Link'
import Intro from '@site/src/components/Intro'
import ActionCard from '@site/src/components/ActionCard'
import PrimaryCTA from '@site/src/components/PrimaryCTA'
import SecondaryCTA from '@site/src/components/SecondaryCTA'

<Intro>
## The Atmos Difference

Atmos lets you model the configuration of your cloud architecture in a way that makes logical sense.
You can then develop components using any toolchain you want, including Terraform, to implement
the components of your infrastructure.
</Intro>

Most teams don’t have a Terraform problem; they struggle with modeling their architecture in a reusable manner. Terraform provides everything needed to provision infrastructure, but an effective strategy to model cloud architecture is missing. Atmos fills this gap with a robust framework, offering design patterns, best practices, and advanced tooling to optimize your cloud architecture.

**A well-designed framework does not limit you; it enables you. It doesn’t hold you back; it empowers you.**

## The Benefits of Using Atmos

Atmos provides a lightweight framework around Terraform, making it easier to manage and deploy your infrastructure
across cloud platforms. It leverages vanilla Terraform for all its strengths, ensuring you can harness the full power
of Terraform. Additionally, with Atmos, you no longer need to rely on code generation or templating HCL.

You'll have a clear separation of concerns, consistent conventions, and organized project structures.

Configuration is organized into stacks defined by YAML and separate from components (terraform root modules).
This ensures root modules remain highly reusable across teams and promotes testing.

It was designed to be used with CI/CD, so you can integrate with [GitHub Actions](https://atmos.tools/integrations/github-actions), [Spacelift](https://atmos.tools/integrations/spacelift), or even [Atlantis](https://atmos.tools/integrations/atlantis).

And it was [inspired by giants](/reference/alternatives). We took the best of what we learned from all these other tools stuck into Atmos. So if you like the concepts of imports, inheritance, layering, and composition, you'll love Atmos.

Oh yes, and it's entirely free and truly Open Source (licensed APACHE2) like [everything else Cloud Posse builds](https://github.com/cloudposse)! 🔥

<ActionCard>
    Now that we're all on the same page, start your first Atmos project in 30 minutes!

    Try out our Simple Quick Start or delve deeper into the syntax and concepts used by Atmos.

        <PrimaryCTA to="/quick-start/simple">Try our Simple Tutorial</PrimaryCTA>
        <SecondaryCTA to="/core-concepts">Learn Atmos Concepts</SecondaryCTA>

</ActionCard>

---

## Stage 0: ClickOps

import Link from '@docusaurus/Link'
import Intro from '@site/src/components/Intro'

<Intro>
ClickOps is the "art" of provisioning or setting up your infrastructure using the _click_ of the mouse,
aimed at the web console of your chosen cloud provider.
</Intro>

Think of ClickOps as the sketching phase of designing your cloud infrastructure — it’s a way to quickly
and visually design how your setup should look without investing the time to codify it as formal blueprints.
Initially, ClickOps is often the fastest way to get up and running, allowing you to understand your infrastructure
through hands-on experimentation.

However, just as sketches aren’t used to construct buildings, you need blueprints for a successful, repeatable implementation.
These blueprints ensure that any team member can reproduce the setup accurately. Although, sometimes ClickOps is unavoidable
because the underlying APIs aren’t available, or a Terraform resource isn’t available by a provider; most of the time,
infrastructure can and should be codified.

:::warning New Problems
1. No consistency or reproducibility
2. Repetitive error-prone work
3. No history of changes
:::

The biggest problem with ClickOps is reproducibility. No self-respecting engineer enjoys repetitive work and visually comparing
two configurations or spot-checking two environments side-by-side in the web console. These issues disappear when you adopt infrastructure as code (IaC). IaC provides the blueprints for consistent, automated, and scalable infrastructure management,
eliminating the pitfalls of manual, repetitive ClickOps processes.

## Realization

So developers [begin their journey with Terraform](/introduction/why-atmos/stage-1), a tool that allows them to define their infrastructure as code,

<Link
    to="/introduction/why-atmos/nirvana"
    className="button button--lg button--primary">
    Try Atmos!
</Link>

---

## Stage 1: Introduction to Terraform

import Link from '@docusaurus/Link'
import Intro from '@site/src/components/Intro'

<Intro>
At this initial stage, developers begin their Terraform journey with the basics, focusing on getting a grasp of Terraform's
core concepts with a straightforward and simple implementation.
</Intro>

1. Developers roll up their sleeves to get a simple terraform example up and running. They create resources directly in the
   configuration without the use of modules. This phase is characterized by hard-coded settings and a hands-on approach to learning
   Terraform's syntax and capabilities.
2. Local state files are used since this is just an exploration. This approach simplifies the learning process by avoiding the complexities of remote state management.
3. Version control systems are not yet in use. Developers store their Terraform configurations directly on their local workstations
   allowing them to focus on learning Terraform's mechanics without the added complexity of collaboration tools or best practices.

:::warning New Problems
1. How do we handle secrets?
2. Where do we store the state file?
3. How do we maintain something with so many hardcoded settings?
:::

## Realization

With this quick win, developers realize Terraform's power, so they are eager to [Terraform everything in sight](/introduction/why-atmos/stage-2).

<Link
    to="/introduction/why-atmos/nirvana"
    className="button button--lg button--primary">
    Try Atmos!
</Link>

---

## Stage 10: Terraform Bankruptcy 💥

import Link from '@docusaurus/Link'
import KeyPoints from '@site/src/components/KeyPoints'
import Intro from '@site/src/components/Intro'
import Definition from '@site/src/components/Definition'

<Intro>
At this point, developers might be ready to throw in the towel on Terraform.
They question the hype and wonder if it's all worth it.

They might even start looking for alternatives like [CDK](https://aws.amazon.com/cdk/),
[CDK TF](https://developer.hashicorp.com/terraform/cdktf), [Pulumi](https://www.pulumi.com/),
or platforms like [Crossplane](https://www.crossplane.io/).
</Intro>

As the complexity and scale of Terraform projects reach a tipping point, developers face a stark realization.
This wasn't easy. Questions arise about the inherent difficulties of managing sprawling infrastructure code bases
using Terraform, leading to a moment of reckoning. Some might question if Terraform is still the right tool for them.

:::danger

- Scripts all over the place that call Terraform
- No clear separation of concerns
- Inconsistent conventions, or a total lack of conventions
- Disorganized project structures
- Interdependencies that complicate deployment processes
- Unsafe CI/CD practices and hardcoded credentials
- Incomplete or out-of-date documentation
- **No one understands how it all works anymore ⚠️**

:::

## But the problem isn't Terraform.

**The problem is how they are using Terraform.**

Changing the tool, will not change the outcome. Even if they decide to switch to a different tool,
they will face the same challenges, just with a different set of limitations.

Terraform is not a framework. It's a tool. One that can be used by a framework. Just as Docker is not Kubernetes.

What developers _really_ need is a coherent strategy with some rigor for approaching cloud architecture that translates to
how they manage the configuration and design all the components that comprise their infrastructure.

It's not a surprise that teams fail with Terraform. Most developers never get the opportunity to write their own framework from scratch;
Instead, they use one that is well-established and proven by countless enterprises and supported by a community.
So why should we expect DevOps teams to be equipped and prepared to design an extensible framework when they’ve never done it before?

## Realization

After a little bit of Googling, searching Reddit and asking around, they then [stumble upon Atmos](/introduction/why-atmos/nirvana). A framework that will make their lives easier.

<Definition term="framework" />

<Link
    to="/introduction/why-atmos/nirvana"
    className="button button--lg button--primary">
    Try Atmos!
</Link>

---

## Stage 2: The Monolithic Root Module

import Link from '@docusaurus/Link'
import Intro from '@site/src/components/Intro'

<Intro>
As developers grow more comfortable with Terraform, they often transition to a more ambitious approach of automating everything. This results in creating a monolithic "root module," also known as a [Terralith](/terms/terralith).
</Intro>

This stage is characterized by an expansive,
all-encompassing Terraform configuration that attempts to manage every aspect of the infrastructure within a single module.

1. Developers begin by composing a single root module that continuously expands.
2. Define all environments as code (dev, staging, production) within the single Terraform root module.
3. Extensive use of feature flags for everything (e.g. `prod_cluster_size`, `staging_cluster_size`, etc.)

:::warning New Problems
1. Massive blast radius for every change
2. Brittle and prolonged plan/apply cycles
3. Large root modules become more complicated
4. Far from DRY with significant code duplication
5. Testing every parameter combination is impossible
6. No practical way to separate responsibilities
:::

After a while, it becomes scary to apply the changes because, literally, anything can go wrong. Deployments are often interrupted by transient errors,
expiring credentials, and API rate limits that require frequent and manual retries. To get around these problems or as a workaround
for cycles, developers perform targeted applies (e.g. `terraform apply -target`), which is an anti-pattern.
Unfortunately, it will take developers a few more stages of toil [to realize the inefficiencies of this approach (stage 5)](/introduction/why-atmos/stage-5).

## Realization

When developers first started using Terraform, they didn’t appreciate the purpose behind modules.
Everything seemed easy enough; just using plain resources worked. However, as the codebase grew, the need for structure and organization became clear. Now, it all makes sense, and developers commit to writing [everything new as a module](/introduction/why-atmos/stage-3).

<Link
    to="/introduction/why-atmos/nirvana"
    className="button button--lg button--primary">
    Try Atmos!
</Link>

---

## Stage 3: Move Towards Modularization

import Link from '@docusaurus/Link'
import Intro from '@site/src/components/Intro'

<Intro>
As developers dive deeper into Terraform's capabilities, they begin to recognize the inefficiencies of their initial approaches.
The move towards modularization represents a significant leap in managing infrastructure as code more effectively.
</Intro>

1. Developers realize that a lot of code is getting copied and pasted, so they advance to writing modules to avoid duplication.
2. Modules act like functions in Terraform: reusable and parameterized. They can be called multiple times and accept parameters.
3. It works well for now, and developers succeed in bringing up all the environments. It certainly beats [ClickOps](/introduction/why-atmos/stage-0).

:::warning New Problems
1. These modules are "reusable" but usually just within the project itself and not written to be reused across the organization.
2. Many assumptions are made on specific requirements of the problem at hand, and generalizing them is not of paramount concern.
3. Lots of similar modules appear in the organization, some more maintained than others.
4. No automated tests (E.g. `terratest`).
5. Everyone is probably an Administrator.
:::

## Realization

As developers researched how to accomplish various tasks with Terraform while writing their modules, they came across one
of [Cloud Posse's hundreds of terraform modules](https://github.com/cloudposse). Then it dawned on them that they could
[avoid reinventing the wheel by using these modules](/introduction/why-atmos/stage-4).

<Link
    to="/introduction/why-atmos/nirvana"
    className="button button--lg button--primary">
    Try Atmos!
</Link>

---

## Stage 4: Adoption of Open Source Modules

import Link from '@docusaurus/Link'
import Intro from '@site/src/components/Intro'

<Intro>
Developers begin their Terraform journey by often crafting their own modules. But as they research how to accomplish various things with Terraform, they usually stumble across one of [Cloud Posse's hundreds of terraform modules](https://github.com/cloudposse). Then it dawns on them that they could avoid reinventing the wheel by using these modules instead.
</Intro>

1. Developers, initially crafting bespoke modules, discover hundreds of freely available open-source Terraform modules.
2. With the recognition of high-quality, well-maintained open-source modules, developers start to replace their custom solutions (like VPCs and clusters) with those from the community, streamlining their infrastructure code.
3. The switch to open-source modules often leads to pull requests that dramatically reduce the complexity and lines of code,
   sometimes cutting out hundreds or even thousands of lines, to the team's astonishment and delight.

:::warning New Problems
1. Now, you need to stay up-to-date with the latest changes in the modules.
2. The scope of the monolithic "root module" is at a tipping point.
3. Code is a mishmash of different styles, naming conventions, and approaches.
:::

## Realization

Developers recognize the pitfalls of having so much code one root module, especially after moving to using open source modules.

They realize the time has come to do a refactor, [decomposing the monolithic root module into smaller, more manageable pieces](/introduction/why-atmos/stage-5).

<Link
    to="/introduction/why-atmos/nirvana"
    className="button button--lg button--primary">
    Try Atmos!
</Link>

---

## Stage 5: Refactor Root Modules

import Link from '@docusaurus/Link'
import Intro from '@site/src/components/Intro'

<Intro>
Developers recognize the pitfalls of having all environments in one root module. After having refactored their code heavily into modules, Developers realized it was a bad idea to define dev, staging and prod, all in the same root module. Developers realize terraform gets very slow and brittle when a root module is too large.
</Intro>

1. Initiate the split of the monolithic root module into smaller, more manageable units while incorporating additional feature flags
   and expanding configuration management through `.tfvars` files.
2. Move towards a structure where multiple root modules are used, each one precisely aligned with specific environments
   to enhance maintainability and performance.
3. Recognize the efficiency gains and increased adaptability that came from segregating large root modules, leading to quicker
   Terraform operations and easier adjustments per environment.

:::warning New Problems
1. Increased feature flags and configuration management using `.tfvars`.
2. Performance becomes a concern with overly large root modules.
3. Very poor reusability across the organization due to the "snowflake" nature of the root modules.
:::

## Realization

After refactoring to smaller "root modules", now there are some dependencies between them, including shared configurations
that were once easily passed between resources, but are now variable inputs.

Developers hate repeating themselves, so attention shifts to [DRY Configuration](/introduction/why-atmos/stage-6).

<Link
    to="/introduction/why-atmos/nirvana"
    className="button button--lg button--primary">
    Try Atmos!
</Link>

---

## Stage 6: Optimize for DRY Configuration

import Link from '@docusaurus/Link'
import Intro from '@site/src/components/Intro'

<Intro>
As developers navigate the complexities of managing multiple environments, root modules, and accounts or organizations,
the focus shifts from merely defining infrastructure as code to the overarching challenge of maintaining these expansive
configurations efficiently.
</Intro>

1. With numerous environments, root modules, and accounts or organizations, the challenge shifts from defining infrastructure as code to maintaining the extensive configuration of parameters that get passed to Terraform.
2. In an effort to manage repetitive configurations, developers resort to using symlinks or other methods to link common files across projects, seeking to reduce redundancy.
3. Code Generation is adopted to overcome *perceived* limitations of Terraform (when, in fact it's often a flaw in the architecture of the project).

:::warning New Problems
1. For every new application developed, the automatic response is to create bespoke root modules for specific needs, despite
   reusing child modules, raising the question of why a new root module is necessary for each application in the first place.
2. As the number of root modules grows, the Terraform state gets divided by component. Managing these inter-component dependencies
   falls outside Terraform's capabilities and needs to be solved in another way.
3. The adoption of code generation tools to address Terraform's *perceived* limitations (e.g., [inability to](https://github.com/hashicorp/terraform/issues/19932#issuecomment-1817043906) iterate over providers](https://github.com/hashicorp/terraform/issues/19932#issuecomment-1817043906)) that often can mask underlying architectural issues as well as make automated testing complicated.
:::

## Realization

Now with all these new configuration files and additional parameters to run Terraform, developers invest
more time in [scripting and automation](/introduction/why-atmos/stage-7) to manage these configurations.

<Link
    to="/introduction/why-atmos/nirvana"
    className="button button--lg button--primary">
    Try Atmos!
</Link>

---

## Stage 7: Terraform Scripting

import Link from '@docusaurus/Link'
import Intro from '@site/src/components/Intro'

<Intro>
As Terraform projects grow in complexity and scale, developers seek ways to automate and streamline their workflows beyond what native Terraform commands offer. This leads to exploring scripting as a means to orchestrate Terraform operations.
</Intro>

1. The first path usually involves adopting a simple `bash` script or `Makefile` to orchestrate Terraform. This works well, up to a point (we know this because it is how we started using Terraform at Cloud Posse).
2. These scripts evolved to solve specific problems that made Terraform cumbersome as a tool.
3. Terraform scripts still run on local machines, reflecting the initial stages of development focus.

:::warning New Problems
1. What happens in practice is every team/company using Terraform ends up building different homegrown scripts,
   often then combining those with `Makefiles`, into a hodgepodge of configurations with different levels of validation.
   The scripts grow in complexity and have to survive generations of developers that come and go.
2. New patterns emerge, for example, hacks that involve Jinja templates, string concatenation, symlinks, and worse...
   `sed` & `awk` replacements!!!
3. Terraform is run mostly from local workstations/laptops, with no thought for how it will run in CI/CD.
:::

## Realization

With these scripts, they are ready to [introduce the rest of the team or company to Terraform](/introduction/why-atmos/stage-8).

<details>
<summary>The Atmos Difference</summary>

### No More Homegrown Scripts

When you use Atmos, you eliminate the need for rampant scripting that is constantly breaking, never tested, poorly parameterized and seldom validated. Instead of relying on scripts that vary between companies and teams, Atmos provides a [purpose-built tool designed for Terraform]( /core-concepts/components/terraform). It introduces a framework that ensures all your Terraform code and configurations are consistently organized and validated, reducing sprawl and inconsistency across the enterprise.

### Standardized Framework & Best Practices

The issue with using scripts is the lack of a framework and standardization, leading to chaotic and inconsistent practices.
Unlike many DevOps teams that reinvent the wheel, Atmos provides a validated, thoroughly documented framework, ensuring all
your developers are on the same page.

### Terraform Runs Natively with CI/CD

Atmos comes with [native support for multiple CI/CD platforms](/integrations), including [GitHub Actions](/integrations/github-actions),
[Spacelift](/integrations/spacelift), and [Atlantis](/integrations/atlantis).

This means you no longer need to run your workflows on your laptop, enabling anyone to contribute to infrastructure
just like they do with application development.

### Stop Generating Terraform Code from Templates

The main reason teams resort to using Jinja templates is due to perceived limitations with Terraform itself.
However, these limitations stem from the architecture, design, and use of Terraform. With Atmos, you no longer need to generate your Terraform code. Instead, you can use native Terraform code the way it was meant to be used, building more reusable and easier-to-test Terraform root modules.

<Link
    to="/introduction/why-atmos/nirvana"
    className="button button--lg button--primary">
    Try Atmos!
</Link>
</details>

---

## Stage 8: Team Challenges

import Link from '@docusaurus/Link'
import Intro from '@site/src/components/Intro'

<Intro>
As Terraform use grows within the team, it becomes clear that what facilitated the initial success, is insufficient for the next level of growth and teamwork. This stage is a turning point, emphasizing the need for evolved workflows, enhanced collaboration tools, and a
more structured approach to managing scalable infrastructure projects.
</Intro>

1. Change velocity increases dramatically.
2. Codebase increases in size by 10x (but with lots of duplication).
3. New SLA/Commitment to keep it all running all the time.

:::warning New Problems
1. Configuration starts drifting more often as the team neglects to apply changes consistently, or "ClickOps" persists in parts of the organization.
2. Developers are stepping on each other's toes. What worked when there were only a few developers no longer scales.
3. Poor controls and lack of consistency of Terraform code. Tons of duplication.
:::

## Realization

But it turns out, it wasn't that easy for everyone to run these scripts. Lot's tooling needs to be installed.
Some developers are on Windows, some on Mac, and others on Linux. The scripts weren't that not portable,
so the team is struggling to keep up with the changes. And the documentation, isn't that great either.

To solve these problems, developers realize it's time to bite the bullet and [implement a CI/CD process for their Terraform code](/introduction/why-atmos/stage-9).

<Link
    to="/introduction/why-atmos/nirvana"
    className="button button--lg button--primary">
    Try Atmos!
</Link>

---

## Stage 9: DIY CI/CD with Terraform

import Link from '@docusaurus/Link'
import Intro from '@site/src/components/Intro'

<Intro>
With the greater adoption of Terraform and DevOps principles, Developers are now using Terraform daily. They decide to use the same patterns for deploying applications with Terraform.
</Intro>

Only Terraform is exceptionally different from deploying containerized apps. There are no rollbacks. It's more akin to performing database migrations without transactions (YOLO!). It's a scary business. Controls are needed.

1. Developers stick their scripts in a CI/CD pipeline with hardcoded credentials set in the environment variables.
2. Pipeline posts comments for each commit on every PR containing the raw output of the `terraform plan`, to validate what *should* happen during `terraform apply`.
3. On merge to main, `terraform apply` is run *without* a planfile. 🤞

:::warning New Problems
- _Still using Jenkins? 🧌🔥_
- CI/CD system promoted to *God Mode*. 🤞 Static administrative cloud credentials are exposed as environment variables, ripe for exfiltration
- No visibility into the impact of the changes across multiple environments at the same time
<details>
<summary>And then there's...</summary>
- No recourse for when a `terraform apply` fails
- Inadequate security mechanisms create a critical risk of catastrophic business loss
- Lack of plan file storage means incorrect implementation of plan/apply workflows
- Missing project-level locks, so PRs can easily clobber each other, rolling back state.
- The entire organization is spammed by GitHub comments every time someone pushes a commit, and a plan is run on the PR
- Automated drift detection is still missing, so you have no idea if what is in `main` is actually deployed
</details>
:::

## Realization

So, it turns out that implementing CI/CD was not as straightforward as developers thought.
It was not as easy as deploying a container that could be easily rolled back. It's more like performing
database migrations without transactions. That's a scary business, and controls are needed.

Developers have [hit a wall](/introduction/why-atmos/stage-10) and realize they [need a better way to manage their Terraform code](/introduction/why-atmos/nirvana).

<Link
    to="/introduction/why-atmos/nirvana"
    className="button button--lg button--primary">
    Try Atmos!
</Link>

---

## Why Does Atmos Exist?

import DocCardList from '@theme/DocCardList'
import Intro from '@site/src/components/Intro'

# Why Does Atmos Exist?
<Intro>
Atmos fills the gap for Terraform users by offering a proven framework complete with conventions, methodologies, design patterns, and best practices, ensuring teams succeed with Terraform from the start.
</Intro>

Every team progresses through a similar path when adopting Terraform, maturing over time.

Each advancement brings new challenges, adding complexities that are ultimately justified by the benefits.
Often, the true value of these evolutions only becomes clear after we encounter and address these intricate challenges first-hand.

**Most companies today wouldn't dream of starting a new product without using a mainstream framework like React or NextJS.**

Despite [Terraform’s HCL language being a decade old](https://en.wikipedia.org/wiki/Terraform_(software)), many companies still begin their Terraform journey with a blank slate, oblivious to the existing tooling and frameworks like Atmos. This is reminiscent of the early days of the web, with languages like NodeJS, PHP, and Ruby, before the rise of modern frameworks like Rails and React revolutionized application design. However, it doesn’t have to be this way. It's no wonder some teams endure over 10 stages of evolution before reaching Terraform maturity.

From [Cloud Posse's experience working with countless venture-backed startups all the up to Fortune 100 companies](https://cloudposse.com), here's the typical journey we see them take when using Terraform without a framework to guide them.

:::tip Shortcuts
[You can shortcut this entire process by using Atmos from the start.](/introduction/why-atmos/nirvana) 😎
:::

# What stage are you at?

Let's explore the challenges and constraints we faced using Terraform without a framework.

<DocCardList/>

---

## Add Custom Commands

import Terminal from '@site/src/components/Terminal'
import File from '@site/src/components/File'

Atmos can be easily extended to support any number of custom CLI commands.

:::tip
Refer to [Atmos Custom Commands](/core-concepts/custom-commands) for more information about Atmos Custom Commands
:::

Custom commands are defined in the `commands` section in `atmos.yaml` CLI configuration file.

In this Quick Start guide, we'll define two custom commands to list the Atmos stacks in the infrastructure and the components in the stacks.

<File title="atmos.yaml">
```yaml
# Custom CLI commands
commands:
  - name: list
    description: Execute 'atmos list' commands
    # subcommands
    commands:
      - name: stacks
        description: |
          List all Atmos stacks.
        steps:
          - >
            atmos describe stacks --process-templates=false --sections none | grep -e "^\S" | sed s/://g
      - name: components
        description: |
          List all Atmos components in all stacks or in a single stack.

          Example usage:
            atmos list components
            atmos list components -s plat-ue2-dev
            atmos list components --stack plat-uw2-prod
            atmos list components -s plat-ue2-dev --type abstract
            atmos list components -s plat-ue2-dev -t enabled
            atmos list components -s plat-ue2-dev -t disabled
        flags:
          - name: stack
            shorthand: s
            description: Name of the stack
            required: false
          - name: type
            shorthand: t
            description: Component types - abstract, enabled, or disabled
            required: false
        steps:
          - >
            {{ if .Flags.stack }}
              {{ if eq .Flags.type "enabled" }}
                atmos describe stacks --stack {{ .Flags.stack }} --format json | jq '.[].components.terraform | to_entries[] | select(.value.vars.enabled == true)' | jq -r .key
              {{ else if eq .Flags.type "disabled" }}
                atmos describe stacks --stack {{ .Flags.stack }} --format json | jq '.[].components.terraform | to_entries[] | select(.value.vars.enabled == false)' | jq -r .key
              {{ else if eq .Flags.type "abstract" }}
                atmos describe stacks --stack {{ .Flags.stack }} --format json | jq '.[].components.terraform | to_entries[] | select(.value.metadata.type == "abstract")' | jq -r .key
              {{ else }}
                atmos describe stacks --stack {{ .Flags.stack }} --format json --sections none | jq ".[].components.terraform" | jq -s add | jq -r "keys[]"
              {{ end }}
            {{ else }}
              {{ if eq .Flags.type "enabled" }}
                atmos describe stacks --format json | jq '.[].components.terraform | to_entries[] | select(.value.vars.enabled == true)' | jq -r '[.key]' | jq -s 'add' | jq 'unique | sort' | jq -r "values[]"
              {{ else if eq .Flags.type "disabled" }}
                atmos describe stacks --format json | jq '.[].components.terraform | to_entries[] | select(.value.vars.enabled == false)' | jq -r '[.key]' | jq -s 'add' | jq 'unique | sort' | jq -r "values[]"
              {{ else if eq .Flags.type "abstract" }}
                atmos describe stacks --format json | jq '.[].components.terraform | to_entries[] | select(.value.metadata.type == "abstract")' | jq -r '[.key]' | jq -s 'add' | jq 'unique | sort' | jq -r "values[]"
              {{ else }}
                atmos describe stacks --format json --sections none | jq ".[].components.terraform" | jq -s add | jq -r "keys[]"
              {{ end }}
            {{ end }}
```
</File>

Run the following Atmos command to list all stacks in the infrastructure:

<Terminal title="atmos list stacks">
```console
plat-gbl-dev
plat-gbl-prod
plat-gbl-staging
plat-ue2-dev
plat-ue2-prod
plat-ue2-staging
plat-uw2-dev
plat-uw2-prod
plat-uw2-staging
```
</Terminal>

Run the following Atmos command to list all components in the stack `plat-ue2-dev`:

<Terminal title="atmos list components -s plat-ue2-dev">
```console
vpc
vpc-flow-logs-bucket
```
</Terminal>

---

## Advanced Tutorial

import PillBox from '@site/src/components/PillBox'

<PillBox>Advanced</PillBox>

# Advanced Tutorial

Take 30 minutes to learn the most important Atmos concepts. We recommend starting with the [Simple Quick Start](/quick-start/simple) tutorial before diving into this advanced tutorial. This tutorial will take you through the process of configuring and provisioning infrastructure on AWS using Atmos. It requires administrative access to an AWS organization.

:::tip

This Quick Start guide describes the steps to configure and provision the infrastructure
from this [Quick Start](https://github.com/cloudposse/atmos/tree/main/examples/quick-start-advanced) repository.

You can clone it and configure to your own needs. The repository should be a good start to get yourself familiar with Atmos.

:::

In this advanced tutorial, we’ll delve into concepts like [inheritance](/core-concepts/stacks/inheritance)
and [state management](/core-concepts/components/terraform/state-backend).
Additionally, we’ll cover how to read the remote state from other components using native Terraform.
This example focuses on AWS, and while Atmos isn’t AWS-specific, this tutorial will be.

## Requirements

We’ll assume you have administrative access to an AWS organization, as this tutorial will also provision
AWS accounts and the IAM architecture. If you don’t have these prerequisites, our [Simple Quick Start](/quick-start/simple) tutorial
might be a more practical starting point.

## Overview

The 10 steps to configure and provision the infrastructure are as follows:

1. [Install Atmos](/install)
2. [Configure Project Repository](/quick-start/advanced/configure-repository)
3. [Configure Atmos CLI](/quick-start/advanced/configure-cli)
4. [Vendor Component Dependencies](/quick-start/advanced/vendor-components)
5. [Create Atmos Stacks](/quick-start/advanced/create-atmos-stacks)
6. [Configure Validation](/quick-start/advanced/configure-validation)
7. [Automate Common Workflows](/quick-start/advanced/create-workflows)
8. [Add Custom Commands](/quick-start/advanced/add-custom-commands)
9. [Configure Terraform Backend](/quick-start/advanced/configure-terraform-backend)
10. [Deploy Everything](/quick-start/advanced/provision)

We'll then conclude with some [final notes](/quick-start/advanced/final-notes) and [next steps](/quick-start/advanced/next-steps)

import DocCardList from '@theme/DocCardList'

<DocCardList/>

---

## Configure Atmos CLI(Advanced)

import File from '@site/src/components/File'

In the previous step, we've decided on the following:

- Use a monorepo to configure and provision two Terraform components into three AWS accounts and two AWS regions
- The filesystem layout for the infrastructure monorepo
- To be able to use [Component Remote State](/core-concepts/share-data/remote-state), we put the `atmos.yaml` CLI config file
  into `/usr/local/etc/atmos/atmos.yaml` folder and set the ENV var `ATMOS_BASE_PATH` to point to the absolute path of the root of the repo

Next step is to configure `atmos.yaml`.

`atmos.yaml` configuration file is used to control the behavior of the `atmos` CLI. The file supports many features that are configured in different
sections of the `atmos.yaml` file. For the description of all the sections, refer to [CLI Configuration](/cli/configuration).

For the purpose of this Quick Start, below is the minimum configuration required for Atmos to work with Terraform and to
configure [Atmos components](/core-concepts/components) and [Atmos stacks](/core-concepts/stacks). Copy the YAML config below into your `atmos.yaml`
file.

<File title="atmos.yaml">
```yaml
# CLI config is loaded from the following locations (from lowest to highest priority):
# system dir ('/usr/local/etc/atmos' on Linux, '%LOCALAPPDATA%/atmos' on Windows)
# home dir (~/.atmos)
# current directory
# ENV vars
# Command-line arguments
#
# It supports POSIX-style Globs for file names/paths (double-star '**' is supported)
# https://en.wikipedia.org/wiki/Glob_(programming)

# Base path for components, stacks and workflows configurations.
# Can also be set using 'ATMOS_BASE_PATH' ENV var, or '--base-path' command-line argument.
# Supports both absolute and relative paths.
# If not provided or is an empty string, 'components.terraform.base_path', 'components.helmfile.base_path', 'stacks.base_path'
# and 'workflows.base_path' are independent settings (supporting both absolute and relative paths).
# If 'base_path' is provided, 'components.terraform.base_path', 'components.helmfile.base_path', 'stacks.base_path'
# and 'workflows.base_path' are considered paths relative to 'base_path'.
base_path: ""

components:
  terraform:
    # Can also be set using 'ATMOS_COMPONENTS_TERRAFORM_BASE_PATH' ENV var, or '--terraform-dir' command-line argument
    # Supports both absolute and relative paths
    base_path: "components/terraform"
    # Can also be set using 'ATMOS_COMPONENTS_TERRAFORM_APPLY_AUTO_APPROVE' ENV var
    apply_auto_approve: false
    # Can also be set using 'ATMOS_COMPONENTS_TERRAFORM_DEPLOY_RUN_INIT' ENV var, or '--deploy-run-init' command-line argument
    deploy_run_init: true
    # Can also be set using 'ATMOS_COMPONENTS_TERRAFORM_INIT_RUN_RECONFIGURE' ENV var, or '--init-run-reconfigure' command-line argument
    init_run_reconfigure: true
    # Can also be set using 'ATMOS_COMPONENTS_TERRAFORM_AUTO_GENERATE_BACKEND_FILE' ENV var, or '--auto-generate-backend-file' command-line argument
    auto_generate_backend_file: true
    init:
      # Can also be set using 'ATMOS_COMPONENTS_TERRAFORM_INIT_PASS_VARS' ENV var, or '--init-pass-vars' command-line argument
      pass_vars: false
    plan:
      # Can also be set using 'ATMOS_COMPONENTS_TERRAFORM_PLAN_SKIP_PLANFILE' ENV var, or '--skip-planfile' command-line argument
      skip_planfile: false

stacks:
  # Can also be set using 'ATMOS_STACKS_BASE_PATH' ENV var, or '--config-dir' and '--stacks-dir' command-line arguments
  # Supports both absolute and relative paths
  base_path: "stacks"
  # Can also be set using 'ATMOS_STACKS_INCLUDED_PATHS' ENV var (comma-separated values string)
  included_paths:
    - "orgs/**/*"
  # Can also be set using 'ATMOS_STACKS_EXCLUDED_PATHS' ENV var (comma-separated values string)
  excluded_paths:
    - "**/_defaults.yaml"
  # Can also be set using 'ATMOS_STACKS_NAME_PATTERN' ENV var
  name_pattern: "{tenant}-{environment}-{stage}"

workflows:
  # Can also be set using 'ATMOS_WORKFLOWS_BASE_PATH' ENV var, or '--workflows-dir' command-line arguments
  # Supports both absolute and relative paths
  base_path: "stacks/workflows"

logs:
  # Can also be set using 'ATMOS_LOGS_FILE' ENV var, or '--logs-file' command-line argument
  # File or standard file descriptor to write logs to
  # Logs can be written to any file or any standard file descriptor, including `/dev/stdout`, `/dev/stderr` and `/dev/null`
  file: "/dev/stderr"
  # Supported log levels: Trace, Debug, Info, Warning, Off
  # Can also be set using 'ATMOS_LOGS_LEVEL' ENV var, or '--logs-level' command-line argument
  level: Info

# Custom CLI commands
commands: []

# Integrations
integrations: {}

# Validation schemas (for validating atmos stacks and components)
schemas:
  # https://json-schema.org
  jsonschema:
    # Can also be set using 'ATMOS_SCHEMAS_JSONSCHEMA_BASE_PATH' ENV var, or '--schemas-jsonschema-dir' command-line arguments
    # Supports both absolute and relative paths
    base_path: "stacks/schemas/jsonschema"
  # https://www.openpolicyagent.org
  opa:
    # Can also be set using 'ATMOS_SCHEMAS_OPA_BASE_PATH' ENV var, or '--schemas-opa-dir' command-line arguments
    # Supports both absolute and relative paths
    base_path: "stacks/schemas/opa"
  # JSON Schema to validate Atmos manifests
  # https://atmos.tools/cli/schemas/
  # https://atmos.tools/cli/commands/validate/stacks/
  # https://atmos.tools/quick-start/advanced/configure-validation/
  # https://atmos.tools/schemas/atmos/atmos-manifest/1.0/atmos-manifest.json
  # https://json-schema.org/draft/2020-12/release-notes
  atmos:
    # Can also be set using 'ATMOS_SCHEMAS_ATMOS_MANIFEST' ENV var, or '--schemas-atmos-manifest' command-line arguments
    # Supports both absolute and relative paths (relative to the `base_path` setting in `atmos.yaml`)
    manifest: "stacks/schemas/atmos/atmos-manifest/1.0/atmos-manifest.json"
```
</File>

The `atmos.yaml` configuration file defines the following sections.

__NOTE:__ below is the description of the sections relevant to this Quick Start guide. For the description of all the sections, refer
to [CLI Configuration](/cli/configuration).

<dl>
  <dt>`base_path`</dt>
  <dd>The base path for components, stacks and workflows configurations. We set it to `./` so it will use the current working directory. Alternatively, we can override this behavior by setting the ENV var `ATMOS_BASE_PATH` to point to another directory location.</dd>

  <dt>`components.terraform.base_path`</dt>
  <dd>The base path to the Terraform components (Terraform root modules). As described in [Configure Repository](/quick-start/advanced/configure-repository), we've decided to put the Terraform components into the `components/terraform` directory, and this setting tells Atmos where to find them. Atmos will join the base path (set in the `ATMOS_BASE_PATH` ENN var) with `components.terraform.base_path` to calculate the final path to the Terraform components.</dd>

  <dt>`components.terraform.apply_auto_approve`</dt>
  <dd>If set to `true`, Atmos automatically adds the `-auto-approve` option to instruct Terraform to apply the plan without asking for confirmation when executing `terraform apply` command.</dd>

  <dt>`components.terraform.deploy_run_init`</dt>
  <dd>If set to `true`, Atmos runs `terraform init` before executing [`atmos terraform deploy`](/cli/commands/terraform/deploy) command.</dd>

  <dt>`components.terraform.init_run_reconfigure`</dt>
  <dd>If set to `true`, Atmos automatically adds the `-reconfigure` option to update the backend configuration when executing `terraform init` command.</dd>

  <dt>`components.terraform.auto_generate_backend_file`</dt>
  <dd>If set to `true`, Atmos automatically generates the Terraform backend file from the component configuration when executing `terraform plan` and `terraform apply` commands.</dd>

  <dt>`stacks.base_path`</dt>
  <dd>The base path to the Atmos stacks. As described in [Configure Repository](/quick-start/advanced/configure-repository), we've decided to put the stack configurations into the `stacks` directory, and this setting tells Atmos where to find them. Atmos will join the base path (set in the `ATMOS_BASE_PATH` ENN var) with `stacks.base_path` to calculate the final path to the stacks.</dd>

  <dt>`stacks.included_paths`</dt>
  <dd>List of file paths to the top-level stacks in the `stacks` directory to include in search when Atmos searches for the stack where the component is defined when executing `atmos` commands.</dd>

  <dt>`stacks.excluded_paths`</dt>
  <dd>List of file paths to the top-level stacks in the `stacks` directory to exclude from search when Atmos searches for the stack where the component is defined when executing `atmos` commands.</dd>

  <dt>`stacks.name_pattern`</dt>
  <dd>Atmos stack name pattern. When executing `atmos` commands, Atmos does not use the configuration file names and their filesystem locations to search for the stack where the component is defined. Instead, Atmos uses the context variables (`namespace`, `tenant`, `environment`, `stage`) to search for the stack. The stack config file names can be anything, and they can be in any folder in any sub-folder in the `stacks` directory. For example, when executing the `atmos terraform apply vpc -s tenant1-ue2-dev` command, the stack `tenant1-ue2-dev` is specified by the `-s` flag. By looking at `name_pattern: "{tenant}-{environment}-{stage}"` and processing the tokens, Atmos knows that the first part of the stack name is `tenant`, the second part is `environment`, and the third part is `stage`. Then Atmos searches for the stack configuration file (in the `stacks` directory) where `tenant: tenant1`, `environment: ue2` and `stage: dev` are defined (inline or via imports).</dd>

  <dt>`workflows.base_path`</dt>
  <dd>The base path to Atmos [Workflows](/core-concepts/workflows) files.</dd>

  <dt>`logs.verbose`</dt>
  <dd>Set to `true` to increase log verbosity. When set to `true`, Atmos prints to the console all the steps it takes to find and process the `atmos.yaml` CLI config file, and all the steps it takes to find the stack and find and process the component in the stack.</dd>

  <dt>`commands`</dt>
  <dd>Configuration for [Atmos Custom Commands](/core-concepts/custom-commands).</dd>

  <dt>`schemas`</dt>
  <dd>See [JSON Schema](https://json-schema.org/) and [OPA Policy](https://www.openpolicyagent.org/) configurations for: [Atmos Schema Validation](/cli/schemas), [Atmos Custom Validation](/core-concepts/validate).</dd>
</dl>

---

## Configure Project Repository

Atmos supports both the [monorepo and polyrepo architectures](https://en.wikipedia.org/wiki/Monorepo) when managing the configurations for components
and stacks.

:::info

A "monorepo" is a version-controlled repository that stores all the code, configurations and scripts for the entire infrastructure composed of individual components with independent lifecycles. Monorepos usually improve collaboration, CI/CD build speed, and overall productivity. A monorepo should not be confused with a [monolith](https://en.wikipedia.org/wiki/Monolithic_application), which is a single, often large, codebase for an application.

Polyrepo architectures consists of several version-controlled repositories for code, configurations and scripts for different parts of the
infrastructure. For example, depending on various requirements (including security, lifecycle management, access control, audit, etc.), separate repositories can be used to manage infrastructure per account (e.g. `dev`, `staging`, `prod`), per service, or per team.

:::

In this Quick Start guide, we will be using a monorepo to provision the following resources into multiple AWS accounts (`dev`, `staging`, `prod`)
and regions (`us-east-2` and `us-west-2`):

- [vpc-flow-logs-bucket](https://github.com/cloudposse/atmos/tree/main/examples/quick-start-advanced/components/terraform/vpc-flow-logs-bucket)
- [vpc](https://github.com/cloudposse/atmos/tree/main/examples/quick-start-advanced/components/terraform/vpc)

## Common Directories and Files

Atmos requires a few common directories and files, which need to be configured in the infrastructure repo:

- `components` directory (required) - contains centralized component configurations
- `stacks` directory (required) - contains centralized stack configurations
- `atmos.yaml` (required) - CLI config file
- `vendor.yaml` (optional) - Atmos vendor config file
- `Makefile` (optional)
- `Dockerfile` (optional)
- `rootfs` directory (optional) - root filesystem for the Docker image (if `Dockerfile` is used)

Atmos separates code from configuration (separation of concerns). The code is in the `components` directories and the configurations for different environments are in the `stacks` directory. This allows the code (Terraform and Helmfile components) to be environment-agnostic, meaning the components don't know and don't care how and where they will be provisioned. They can be provisioned into many accounts and regions - the configurations for different environments are defined in the `stacks` directory.

:::note
While it's recommended to use the directory names as shown above, the `stacks` and `components` directory names and filesystem locations are
configurable in the `atmos.yaml` CLI config file. Refer to [Configure CLI](/quick-start/advanced/configure-cli) for more details.
:::

The following example provides the simplest filesystem layout that Atmos can work with:

```console
   │   # Centralized stacks configuration
   ├── stacks/
   │   ├── <stack_1>.yaml
   │   ├── <stack_2>.yaml
   │   └── <stack_3>.yaml
   │  
   │   # Centralized components configuration. Components are broken down by tool
   ├── components/
   │   ├── terraform/   # Terraform components (Terraform root modules)
   │   │   ├── <terraform_component_1>/
   │   │   ├── <terraform_component_2>/
   │   │   └── <terraform_component_3>/
   │   └── helmfile/  # Helmfile components are organized by Helm chart
   │       ├── <helmfile_component_1>/
   │       ├── <helmfile_component_2>/
   │       └── <helmfile_component_3>/
   │
   │   # Atmos CLI configuration
   ├── atmos.yaml
   │   # Atmos vendoring configuration
   └── vendor.yaml
```

## `atmos.yaml` CLI Config File Location

While placing `atmos.yaml` at the root of the repository will work for the `atmos` CLI, it will not work
for [Component Remote State](/core-concepts/share-data/remote-state) because it uses
the [terraform-provider-utils](https://github.com/cloudposse/terraform-provider-utils) Terraform provider. Terraform executes the provider from the
component's folder (e.g. `components/terraform/vpc`), and we don't want to replicate `atmos.yaml` into every component's folder.

Both the `atmos` CLI and [terraform-provider-utils](https://github.com/cloudposse/terraform-provider-utils) Terraform provider use the same `Go` code,
which try to locate the [CLI config](/cli/configuration) `atmos.yaml` file before parsing and processing [Atmos stacks](/core-concepts/stacks).

This means that `atmos.yaml` file must be at a location in the file system where all processes can find it.

:::info

`atmos.yaml` is loaded from the following locations (from lowest to highest priority):

- System dir (`/usr/local/etc/atmos/atmos.yaml` on Linux, `%LOCALAPPDATA%/atmos/atmos.yaml` on Windows)
- Home dir (`~/.atmos/atmos.yaml`)
- Current directory
- ENV var `ATMOS_CLI_CONFIG_PATH`

:::

:::note

Initial Atmos configuration can be controlled by these ENV vars:

- `ATMOS_CLI_CONFIG_PATH` - where to find `atmos.yaml`. Path to a folder where the `atmos.yaml` CLI config file is located (just the folder without
  the file name)

- `ATMOS_BASE_PATH` - base path to `components` and `stacks` folders

:::

For this to work for both the `atmos` CLI and the Terraform provider, we recommend doing one of the following:

- Put `atmos.yaml` at `/usr/local/etc/atmos/atmos.yaml` on local host and set the ENV var `ATMOS_BASE_PATH` to point to the absolute path of the root
  of the repo

- Put `atmos.yaml` into the home directory (`~/.atmos/atmos.yaml`) and set the ENV var `ATMOS_BASE_PATH` pointing to the absolute path of the root of
  the repo

- Put `atmos.yaml` at a location in the file system and then set the ENV var `ATMOS_CLI_CONFIG_PATH` to point to that location. The ENV var must
  point to a folder without the `atmos.yaml` file name. For example, if `atmos.yaml` is at `/atmos/config/atmos.yaml`,
  set `ATMOS_CLI_CONFIG_PATH=/atmos/config`. Then set the ENV var `ATMOS_BASE_PATH` pointing to the absolute path of the root of the repo

- When working in a Docker container, place `atmos.yaml` in the `rootfs` directory
  at [/rootfs/usr/local/etc/atmos/atmos.yaml](https://github.com/cloudposse/atmos/blob/main/examples/quick-start-advanced/rootfs/usr/local/etc/atmos/atmos.yaml)
  and then copy it into the container's file system in the [Dockerfile](https://github.com/cloudposse/atmos/blob/main/examples/quick-start-advanced/Dockerfile)
  by executing the `COPY rootfs/ /` Docker command. Then in the Dockerfile, set the ENV var `ATMOS_BASE_PATH` pointing to the absolute path of the
  root of the repo. Note that the [Atmos Quick Start example](https://github.com/cloudposse/atmos/blob/main/examples/quick-start)
  uses [Geodesic](https://github.com/cloudposse/geodesic) as the base Docker image. [Geodesic](https://github.com/cloudposse/geodesic) sets the ENV
  var `ATMOS_BASE_PATH` automatically to the absolute path of the root of the repo on the local host

## Final Filesystem Layout

Taking into account all the above, we can place `atmos.yaml` at `/usr/local/etc/atmos/atmos.yaml` on the local host and use the following filesystem
layout:

```console
   │   # Centralized stacks configuration
   ├── stacks/
   │   ├── <stack_1>.yaml
   │   ├── <stack_2>.yaml
   │   └── <stack_3>.yaml
   │  
   │   # Centralized components configuration. Components are broken down by tool
   └── components/
       └── terraform/   # Terraform components (Terraform root modules)
           ├── vpc/
           └── vpc-flow-logs-bucket/
```

:::tip

For a Quick Start example, refer to [Atmos Quick Start](https://github.com/cloudposse/atmos/tree/main/examples/quick-start-advanced)

:::

---

## Configure Terraform Backend

import File from '@site/src/components/File'

In the previous steps, we've configured the `vpc-flow-logs-bucket` and `vpc` Terraform components to be provisioned into three AWS accounts
(`dev`, `staging`, `prod`) in the two AWS regions (`us-east-2` and `us-west-2`).

By default, Terraform will use a backend called [local](https://developer.hashicorp.com/terraform/language/settings/backends/local), which stores
Terraform state on the local filesystem, locks that state using system APIs, and performs operations locally. For any scenario beyond a basic
playground setup, such as staging or production environments, we'll provision and
configure the Terraform [s3](https://developer.hashicorp.com/terraform/language/settings/backends/s3) backend.

## Terraform Local Backend

Terraform's local backend is designed for development and testing purposes and is generally not recommended for production use. There are several
reasons why using the local backend in a production environment may not be suitable:

- **State Management**: The local backend stores the Terraform state file on the local file system. In a production environment, it's crucial to have
  a robust and scalable solution for managing the Terraform state. Storing state locally can lead to issues with collaboration, concurrency, and
  consistency.

- **Concurrency and Locking**: When multiple users or automation processes are working with Terraform concurrently, it's essential to ensure that only
  one process can modify the infrastructure at a time. The local backend lacks built-in support for locking mechanisms that prevent multiple Terraform
  instances from modifying the state simultaneously. This can lead to race conditions and conflicting changes.

- **Collaboration**: In a production environment with multiple team members, it's important to have a centralized and shared state. The local backend
  does not provide a way to easily share the state across different team members or systems. A remote backend, such as Amazon S3, Azure Storage, or
  HashiCorp Consul, is more suitable for collaboration.

- **Durability and Backup**: The local backend does not provide durability or backup features. If the machine where Terraform is run experiences
  issues, there's a risk of losing the state file, leading to potential data loss. Remote backends offer better durability and often provide features
  for versioning and backup.

- **Remote Execution and Automation**: In production, it's common to use Terraform in automated workflows, such as continuous integration/continuous
  deployment (CI/CD) pipelines. Remote backends are better suited for these scenarios, allowing for seamless integration with automation tools and
  supporting the deployment of infrastructure as code in a reliable and controlled manner.

To address these concerns, it's recommended to use one of the supported remote backends, such as Amazon S3, Azure Storage, Google Cloud Storage,
HashiCorp Consul, or Terraform Cloud, for production environments. Remote backends provide better scalability, collaboration support, and durability,
making them more suitable for managing infrastructure at scale in production environments.

## Terraform S3 Backend

Terraform's S3 backend is a popular remote backend for storing Terraform state files in an Amazon Simple Storage Service (S3) bucket. Using S3 as a
backend offers several advantages over local backends, particularly in production environments. Here's an overview of the key features and benefits of
using the Terraform S3 backend:

- **Remote State Storage**: The Terraform state file is stored remotely in an S3 bucket. This allows multiple users and Terraform instances to access
  and manage the same state file, promoting collaboration and consistency across deployments.

- **Concurrency and Locking**: S3 backend supports state file locking, which prevents multiple Terraform instances from modifying the state file
  simultaneously. This helps avoid conflicts and ensures that changes are applied in a coordinated manner, especially in multi-user or automated
  environments.

- **Durability and Versioning**: S3 provides high durability for object storage, and it automatically replicates data across multiple availability
  zones. Additionally, versioning can be enabled on the S3 bucket, allowing you to track changes to the state file over time. This enhances data
  integrity and provides a safety net in case of accidental changes or deletions.

- **Access Control and Security**: S3 supports fine-grained access control policies, allowing you to restrict access to the state file based on AWS
  Identity and Access Management (IAM) roles and policies. This helps ensure that only authorized users or processes can read or modify the Terraform
  state.

- **Integration with AWS Features**: The S3 backend integrates well with other AWS services. For example, you can use AWS Key Management Service (KMS)
  for server-side encryption of the state file, and you can leverage AWS Identity and Access Management (IAM) roles for secure access to the S3
  bucket.

- **Terraform Remote Operations**: The S3 backend can be used in conjunction with Terraform Remote Operations, allowing you to run Terraform
  commands remotely while keeping the state in S3. This is useful for scenarios where the Terraform client and the infrastructure being managed are
  separated.

To configure Terraform to use an S3 backend, you typically provide the S3 bucket name and an optional key prefix in your Terraform configuration.
Here's a simplified example:

<File title="backend.tf" type="hcl">
```hcl
terraform {
  backend "s3" {
    acl            = "bucket-owner-full-control"
    bucket         = "your-s3-bucket-name"
    key            = "path/to/terraform.tfstate"
    region         = "your-aws-region"
    encrypt        = true
    dynamodb_table = "terraform_locks"
  }
}
```
</File>

In the example, `terraform_locks` is a DynamoDB table used for state locking. DynamoDB is recommended for locking when using the S3 backend to ensure
safe concurrent access.

## Provision Terraform S3 Backend

Before using Terraform S3 backend, a backend S3 bucket and DynamoDB table need to be provisioned.

You can provision them using the [tfstate-backend](https://github.com/cloudposse/terraform-aws-tfstate-backend) Terraform module and
[tfstate-backend](https://github.com/cloudposse/terraform-aws-components/tree/main/modules/tfstate-backend) Terraform component (root module).

Note that the [tfstate-backend](https://github.com/cloudposse/terraform-aws-components/tree/main/modules/tfstate-backend) Terraform component
can be added to the `components/terraform` folder, the configuration for the component can be added to the `stacks`, and the component itself
can be provisioned with Atmos.

Here's an example of an Atmos manifest to configure the `tfstate-backend` Terraform component:

<File title="stacks/catalog/tfstate-backend/defaults.yaml" type="stack">
```yaml
components:
  terraform:
    tfstate-backend:
      vars:
        enable_server_side_encryption: true
        enabled: true
        force_destroy: false
        name: tfstate
        prevent_unencrypted_uploads: true
```
</File>

## Configure Terraform S3 Backend

Once the S3 bucket and DynamoDB table are provisioned, you can start using them to store Terraform state for the Terraform components.
There are two ways of doing this:

- Manually create `backend.tf` file in each component's folder with the following content:
  <File title="backend.tf" type="hcl">
  ```hcl
  terraform {
    backend "s3" {
      acl                  = "bucket-owner-full-control"
      bucket               = "your-s3-bucket-name"
      dynamodb_table       = "your-dynamodb-table-name"
      encrypt              = true
      key                  = "terraform.tfstate"
      region               = "your-aws-region"
      role_arn             = "arn:aws:iam::<your account ID>:role/<IAM Role with permissions to access the Terraform backend>"
      workspace_key_prefix = "<component name, e.g. `vpc` or `vpc-flow-logs-bucket`>"
    }
  }
  ```
  </File>

- Configure Terraform S3 backend with Atmos to automatically generate a backend file for each Atmos component. This is the recommended way
  of configuring Terraform state backend since it offers many advantages and will save you from manually creating a backend configuration file for
  each component

### Configure Terraform S3 Backend with Atmos

Configuring Terraform S3 backend with Atmos consists of the three steps:

- Set `auto_generate_backend_file` to `true` in the `atmos.yaml` CLI config file in the `components.terraform` section:
  <File title="atmos.yaml" type="config">
  ```yaml
  components:
    terraform:
      # Can also be set using 'ATMOS_COMPONENTS_TERRAFORM_AUTO_GENERATE_BACKEND_FILE' ENV var, or '--auto-generate-backend-file' command-line argument
      auto_generate_backend_file: true
  ```
  </File>

  Refer to [Quick Start: Configure CLI](/quick-start/advanced/configure-cli) and [CLI Configuration](/cli/configuration) for more details.

- Configure the S3 backend in one of the `_defaults.yaml` manifests. You can configure it for the entire Organization, or per OU/tenant, or per
  region, or per account.

  :::note
  The `_defaults.yaml`stack manifests contain the default settings for Organizations, Organizational Units, and accounts.
  :::

  :::info
  The `_defaults.yaml` stack manifests are not imported into other Atmos manifests automatically.
  You need to explicitly import them using [imports](/core-concepts/stacks/imports).
  :::

  To configure the S3 backend for the entire Organization, add the following config in `stacks/orgs/acme/_defaults.yaml`:

  <File title="stacks/orgs/acme/_defaults.yaml" type="stack">
  ```yaml
  terraform:
    backend_type: s3
    backend:
      s3:
        acl: "bucket-owner-full-control"
        encrypt: true
        bucket: "your-s3-bucket-name"
        dynamodb_table: "your-dynamodb-table-name"
        key: "terraform.tfstate"
        region: "your-aws-region"
        role_arn: "arn:aws:iam::<your account ID>:role/<IAM Role with permissions to access the Terraform backend>"
  ```
  </File>

- (This step is optional) For each component, you can add `workspace_key_prefix` similar to the following:
  <File title="stacks/catalog/vpc.yaml" type="stack">
  ```yaml
  components:
    terraform:
      # `vpc` is the Atmos component name
      vpc:
        # Optional backend configuration for the component
        backend:
          s3:
            workspace_key_prefix: vpc
        metadata:
          # Point to the Terraform component
          component: vpc
        settings: {}
        vars: {}
        env: {}
  ```
  </File>

  Note that this is optional. If you don’t add `backend.s3.workspace_key_prefix` to the component manifest, the Atmos component name will be used
  automatically (which, in this example, is `vpc`). `/` (slash) in the Atmos component name will be replaced with `-` (dash).

  We usually don’t specify `workspace_key_prefix` for each component and let Atmos use the component name as `workspace_key_prefix`.

Once all the above is configured, when you run the commands `atmos terraform plan vpc -s <stack>`
or `atmos terraform apply vpc -s <stack>`, before executing the Terraform commands, Atmos will deep-merge the backend configurations from
the `_defaults.yaml` manifest and from the component itself, and will generate a backend config JSON file `backend.tf.json` in the component's folder,
similar to the following example:

<File title="backend.tf.json" type="json">
```json
{
  "terraform": {
    "backend": {
      "s3": {
        "acl": "bucket-owner-full-control",
        "bucket": "your-s3-bucket-name",
        "dynamodb_table": "your-dynamodb-table-name",
        "encrypt": true,
        "key": "terraform.tfstate",
        "region": "your-aws-region",
        "role_arn": "arn:aws:iam::<your account ID>:role/<IAM Role with permissions to access the Terraform backend>",
        "workspace_key_prefix": "vpc"
      }
    }
  }
}
```
</File>

You can also generate the backend configuration file for a component in a stack by executing the
command [atmos terraform generate backend](/cli/commands/terraform/generate-backend). Or generate the backend configuration files for all components
by executing the command [atmos terraform generate backends](/cli/commands/terraform/generate-backends).

## Terraform Backend Inheritance

In the previous section, we configured the S3 backend for the entire Organization by adding the `terraform.backend.s3` section to
the `stacks/orgs/acme/_defaults.yaml` stack manifest. The same backend configuration (S3 bucket, DynamoDB table, and IAM role) will be used for all
OUs, accounts and regions.

Suppose that for security and audit reasons, you want to use different Terraform backends for the `dev`, `staging` and `prod` accounts. Each account
needs to have a separate S3 bucket, DynamoDB table, and IAM role with different permissions (for example, the `development` Team should be able to
access the Terraform backend only in the `dev` account, but not in `staging` and `prod`).

Atmos supports this use-case by using deep-merging of stack manifests, [Imports](/core-concepts/stacks/imports)
and [Inheritance](/core-concepts/stacks/inheritance), which makes the backend configuration reusable and DRY.

We'll split the backend config between the Organization and the accounts.

Add the following config to the Organization stack manifest in `stacks/orgs/acme/_defaults.yaml`:
  <File title="stacks/orgs/acme/_defaults.yaml" type="stack">
  ```yaml
  terraform:
    backend_type: s3
    backend:
      s3:
        acl: "bucket-owner-full-control"
        encrypt: true
        key: "terraform.tfstate"
        region: "your-aws-region"
  ```
  </File>

Add the following config to the `dev` stack manifest in `stacks/orgs/acme/plat/dev/_defaults.yaml`:

  <File title="stacks/orgs/acme/plat/dev/_defaults.yaml" type="stack">
  ```yaml
  terraform:
    backend_type: s3
    backend:
      s3:
        bucket: "your-dev-s3-bucket-name"
        dynamodb_table: "your-dev-dynamodb-table-name"
        role_arn: "<IAM Role with permissions to access the `dev` Terraform backend>"
  ```
  </File>

Add the following config to the `staging` stack manifest in `stacks/orgs/acme/plat/staging/_defaults.yaml`:
  <File title="stacks/orgs/acme/plat/staging/_defaults.yaml" type="stack">
  ```yaml
  terraform:
    backend_type: s3
    backend:
      s3:
        bucket: "your-staging-s3-bucket-name"
        dynamodb_table: "your-staging-dynamodb-table-name"
        role_arn: "<IAM Role with permissions to access the `staging` Terraform backend>"
  ```
  </File>

Add the following config to the `prod` stack manifest in `stacks/orgs/acme/plat/prod/_defaults.yaml`:

  <File title="stacks/orgs/acme/plat/prod/_defaults.yaml" type="stack">
  ```yaml
  terraform:
    backend_type: s3
    backend:
      s3:
        bucket: "your-prod-s3-bucket-name"
        dynamodb_table: "your-prod-dynamodb-table-name"
        role_arn: "<IAM Role with permissions to access the `prod` Terraform backend>"
  ```
  </File>

When you provision the `vpc` component into the `dev` account (by executing the command `atmos terraform apply vpc -s plat-ue2-dev`), Atmos will
deep-merge the backend configuration from the Organization-level manifest with the configuration from the `dev` manifest, and will automatically
add `workspace_key_prefix` for the component, generating the following final deep-merged backend config for the `vpc` component in the `dev` account:

<File title="backend.tf.json" type="json">
```json
{
  "terraform": {
    "backend": {
      "s3": {
        "acl": "bucket-owner-full-control",
        "bucket": "your-dev-s3-bucket-name",
        "dynamodb_table": "your-dev-dynamodb-table-name",
        "encrypt": true,
        "key": "terraform.tfstate",
        "region": "your-aws-region",
        "role_arn": "<IAM Role with permissions to access the `dev` Terraform backend>",
        "workspace_key_prefix": "vpc"
      }
    }
  }
}
```
</File>

In the same way, you can create different Terraform backends per Organizational Unit, per region, per account (or a group of accounts, e.g. `prod`
and `non-prod`), or even per component or a set of components (e.g. root-level components like `account` and IAM roles can have a separate backend),
and then configure parts of the backend config in the corresponding Atmos stack manifests. Atmos will deep-merge all the parts from the
different scopes and generate the final backend config for the components in the stacks.

## Terraform Backend with Multiple Component Instances

We mentioned before that you can configure the Terraform backend for the components manually (by creating a file `backend.tf` in each Terraform
component's folder), or you can set up Atmos to generate the backend configuration for each component in the stacks automatically. While
auto-generating the backend config file is helpful and saves you from creating the backend files for each component, it becomes a requirement
when you provision multiple instances of a Terraform component into the same environment (same account and region).

You can provision more than one instance of the same Terraform component (with the same or different settings) into the same environment by defining
many Atmos components that provide configuration for the Terraform component.

:::tip
For more information on configuring and provision multiple instances of a Terraform component,
refer to [Multiple Component Instances Atmos Design Patterns](/design-patterns/multiple-component-instances)
:::

For example, the following config shows how to define two Atmos components, `vpc/1` and `vpc/2`, which both point to
the same Terraform component `vpc`:

<File title="stack.yaml" type="stack">
```yaml
import:
  # Import the defaults for all VPC components
  - catalog/vpc/defaults

components:
  terraform:
    # Atmos component `vpc/1`
    vpc/1:
      metadata:
        # Point to the Terraform component in `components/terraform/vpc`
        component: vpc
        # Inherit the defaults for all VPC components
        inherits:
          - vpc/defaults
      # Define variables specific to this `vpc/1` component
      vars:
        name: vpc-1
        ipv4_primary_cidr_block: 10.9.0.0/18
      # Optional backend configuration for the component
      # If not specified, the Atmos component name `vpc/1` will be used (`/` will be replaced with `-`)
      backend:
        s3:
          workspace_key_prefix: vpc-1

    # Atmos component `vpc/2`
    vpc/2:
      metadata:
        # Point to the Terraform component in `components/terraform/vpc`
        component: vpc
        # Inherit the defaults for all VPC components
        inherits:
          - vpc/defaults
      # Define variables specific to this `vpc/2` component
      vars:
        name: vpc-2
        ipv4_primary_cidr_block: 10.10.0.0/18
      # Optional backend configuration for the component
      # If not specified, the Atmos component name `vpc/2` will be used (`/` will be replaced with `-`)
      backend:
        s3:
          workspace_key_prefix: vpc-2
```
</File>

If we manually create a `backend.tf` file for the `vpc` Terraform component in the `components/terraform/vpc` folder
using `workspace_key_prefix: "vpc"`, then both `vpc/1` and `vpc/2` Atmos components will use the same `workspace_key_prefix`, and they will
not function correctly.

On the other hand, if we configure Atmos to auto-generate the backend config file, then each component will have a different `workspace_key_prefix`
auto-generated by Atmos by using the Atmos component name (or you can override this behavior by specifying `workspace_key_prefix` for each component
in the component manifest in the `backend.s3.workspace_key_prefix` section).

For example, when the command `atmos terraform apply vpc/1 -s plat-ue2-dev` is executed, the following `backend.tf.json` file is generated in the
`components/terraform/vpc` folder:

<File title="backend.tf.json" type="json">
```json
{
  "terraform": {
    "backend": {
      "s3": {
        "acl": "bucket-owner-full-control",
        "bucket": "your-dev-s3-bucket-name",
        "dynamodb_table": "your-dev-dynamodb-table-name",
        "encrypt": true,
        "key": "terraform.tfstate",
        "region": "your-aws-region",
        "role_arn": "<IAM Role with permissions to access the `dev` Terraform backend>",
        "workspace_key_prefix": "vpc-1"
      }
    }
  }
}
```
</File>

Similarly, when the command `atmos terraform apply vpc/2 -s plat-ue2-dev` is executed, the following `backend.tf.json` file is generated in the
`components/terraform/vpc` folder:

<File title="backend.tf.json" type="json">
```json
{
  "terraform": {
    "backend": {
      "s3": {
        "acl": "bucket-owner-full-control",
        "bucket": "your-dev-s3-bucket-name",
        "dynamodb_table": "your-dev-dynamodb-table-name",
        "encrypt": true,
        "key": "terraform.tfstate",
        "region": "your-aws-region",
        "role_arn": "<IAM Role with permissions to access the `dev` Terraform backend>",
        "workspace_key_prefix": "vpc-2"
      }
    }
  }
}
```
</File>

The generated files will have different `workspace_key_prefix` attribute auto-generated by Atmos.

For this reason, configuring Atmos to auto-generate the backend configuration for the components in the stacks is recommended.

---

## Validate Configurations

Atmos supports [Stack Schema Validation](/cli/schemas) and [Custom Policy Validation](/core-concepts/validate)
using [JSON Schema](https://json-schema.org/) and [OPA Policies](https://www.openpolicyagent.org/).

:::tip

This Quick Start guide describes the steps to configure and provision the infrastructure
from the [Quick Start](https://github.com/cloudposse/atmos/tree/main/examples/quick-start-advanced) repository.

You can clone the repository and modify to your own needs. The repository will help you understand the validation configurations for
Atmos manifests and components.

:::

Configuring validation for Atmos manifests and components consists of the three steps:

- Configure validation schemas in `atmos.yaml`
- Configure Atmos manifests validation
- Configure Atmos components validation

## Configure Validation Schemas in `atmos.yaml`

In `atmos.yaml` CLI config file, add the `schemas` section as shown below:

```yaml title="atmos.yaml"
# Validation schemas (for validating atmos stacks and components)
schemas:
  # https://json-schema.org
  jsonschema:
    # Can also be set using 'ATMOS_SCHEMAS_JSONSCHEMA_BASE_PATH' ENV var, or '--schemas-jsonschema-dir' command-line arguments
    # Supports both absolute and relative paths
    base_path: "stacks/schemas/jsonschema"
  # https://www.openpolicyagent.org
  opa:
    # Can also be set using 'ATMOS_SCHEMAS_OPA_BASE_PATH' ENV var, or '--schemas-opa-dir' command-line arguments
    # Supports both absolute and relative paths
    base_path: "stacks/schemas/opa"
  # JSON Schema to validate Atmos manifests
  # https://atmos.tools/cli/schemas/
  # https://atmos.tools/cli/commands/validate/stacks/
  # https://atmos.tools/quick-start/advanced/configure-validation/
  # https://atmos.tools/schemas/atmos/atmos-manifest/1.0/atmos-manifest.json
  # https://json-schema.org/draft/2020-12/release-notes
  atmos:
    # Can also be set using 'ATMOS_SCHEMAS_ATMOS_MANIFEST' ENV var, or '--schemas-atmos-manifest' command-line arguments
    # Supports both absolute and relative paths (relative to the `base_path` setting in `atmos.yaml`)
    manifest: "stacks/schemas/atmos/atmos-manifest/1.0/atmos-manifest.json"
```

:::tip

For more information, refer to:

- [Quick-Start: Configure CLI](/quick-start/advanced/configure-cli)
- [Atmos CLI Configuration](/cli/configuration)

:::

## Configure Atmos Manifests Validation

[Atmos Manifest JSON Schema](pathname:///schemas/atmos/atmos-manifest/1.0/atmos-manifest.json) can be used to validate Atmos stack manifests and provide
auto-completion in IDEs and editors.

Complete the following steps to configure Atmos manifest validation:

- Add the [Atmos Manifest JSON Schema](pathname:///schemas/atmos/atmos-manifest/1.0/atmos-manifest.json) to your repository, for example
  in  [`stacks/schemas/atmos/atmos-manifest/1.0/atmos-manifest.json`](https://github.com/cloudposse/atmos/blob/main/examples/quick-start-advanced/stacks/schemas/atmos/atmos-manifest/1.0/atmos-manifest.json)

- Configure the `schemas.atmos.manifest` section in the `atmos.yaml` [CLI config file](/cli/configuration) as described
  in [Atmos Manifests Validation using JSON Schema](/cli/schemas)

  ```yaml title="atmos.yaml"
  # Validation schemas (for validating atmos stacks and components)
  schemas:
    atmos:
      # Can also be set using 'ATMOS_SCHEMAS_ATMOS_MANIFEST' ENV var, or '--schemas-atmos-manifest' command-line arguments
      # Supports both absolute and relative paths (relative to the `base_path` setting in `atmos.yaml`)
      manifest: "stacks/schemas/atmos/atmos-manifest/1.0/atmos-manifest.json"
  ```

- Execute the command [`atmos validate stacks`](/cli/commands/validate/stacks)

- Instead of configuring the `schemas.atmos.manifest` section in `atmos.yaml`, you can provide the path to
  the [Atmos Manifest JSON Schema](pathname:///schemas/atmos/atmos-manifest/1.0/atmos-manifest.json) file by using the ENV
  variable `ATMOS_SCHEMAS_ATMOS_MANIFEST` or the `--schemas-atmos-manifest` command line argument:

  ```shell
  ATMOS_SCHEMAS_ATMOS_MANIFEST=stacks/schemas/atmos/atmos-manifest/1.0/atmos-manifest.json atmos validate stacks
  atmos validate stacks --schemas-atmos-manifest stacks/schemas/atmos/atmos-manifest/1.0/atmos-manifest.json
  ```

:::tip

For more information, refer to:

- [Atmos Manifests Validation using JSON Schema](/cli/schemas)
- [atmos validate stacks](/cli/commands/validate/stacks)

:::

## Configure Atmos Components Validation

Atmos component validation allows:

* Validate component config (`vars`, `settings`, `backend`, `env`, `overrides` and other sections) using [JSON Schema](https://json-schema.org/)

* Check if the component config (including relations between different component variables) is correct to allow or deny component provisioning using
  [OPA Policies](https://www.openpolicyagent.org/)

To configure Atmos components validation, complete the steps described in [Custom Policy Validation](/core-concepts/validate).

:::tip

For more information, refer to:

- [Custom Policy Validation](/core-concepts/validate)
- Command: [`atmos validate component`](/cli/commands/validate/component)

:::

---

## Create Atmos Stacks

import File from '@site/src/components/File'

In the previous step, we've configured the Terraform components and described how they can be vendored into the repository.

Next step is to create and configure [Atmos stacks](/core-concepts/stacks).

## Create Catalog for Components

Atmos supports the [Catalog](/core-concepts/stacks/catalogs) pattern to configure default settings for Atmos components.
All the common default settings for each Atmos component should be in a separate file in the `stacks/catalog` directory.
The file then gets imported into the parent Atmos stacks.
This makes the stack configurations DRY by reusing the component's config that is common for all environments.

Refer to [Stack Imports](/core-concepts/stacks/imports) for more details on Atmos imports.

In the `stacks/catalog/vpc-flow-logs-bucket/defaults.yaml` file, add the following manifest for the `vpc-flow-logs-bucket` Atmos component:

<File title="stacks/catalog/vpc-flow-logs-bucket/defaults.yaml">
```yaml
components:
  terraform:
    vpc-flow-logs-bucket:
      metadata:
        # Point to the Terraform component
        component: vpc-flow-logs-bucket
      vars:
        enabled: true
        name: "vpc-flow-logs"
        traffic_type: "ALL"
        force_destroy: true
        lifecycle_rule_enabled: false
```
</File>

In the `stacks/catalog/vpc/defaults.yaml` file, add the following manifest for the `vpc` Atmos component:

<File title="stacks/catalog/vpc.yaml">
```yaml
components:
  terraform:
    vpc:
      metadata:
        # Point to the Terraform component
        component: vpc
      settings:
        # Validation
        # Supports JSON Schema and OPA policies
        # All validation steps must succeed to allow the component to be provisioned
        validation:
          validate-vpc-component-with-jsonschema:
            schema_type: jsonschema
            # 'schema_path' can be an absolute path or a path relative to 'schemas.jsonschema.base_path' defined in `atmos.yaml`
            schema_path: "vpc/validate-vpc-component.json"
            description: Validate 'vpc' component variables using JSON Schema
          check-vpc-component-config-with-opa-policy:
            schema_type: opa
            # 'schema_path' can be an absolute path or a path relative to 'schemas.opa.base_path' defined in `atmos.yaml`
            schema_path: "vpc/validate-vpc-component.rego"
            # An array of filesystem paths (folders or individual files) to the additional modules for schema validation
            # Each path can be an absolute path or a path relative to `schemas.opa.base_path` defined in `atmos.yaml`
            # In this example, we have the additional Rego modules in `stacks/schemas/opa/catalog/constants`
            module_paths:
              - "catalog/constants"
            description: Check 'vpc' component configuration using OPA policy
            # Set `disabled` to `true` to skip the validation step
            # `disabled` is set to `false` by default, the step is allowed if `disabled` is not declared
            disabled: false
            # Validation timeout in seconds
            timeout: 10
      vars:
        enabled: true
        name: "common"
        max_subnet_count: 3
        map_public_ip_on_launch: true
        assign_generated_ipv6_cidr_block: false
        nat_gateway_enabled: true
        nat_instance_enabled: false
        vpc_flow_logs_enabled: true
        vpc_flow_logs_traffic_type: "ALL"
        vpc_flow_logs_log_destination_type: "s3"
        nat_eip_aws_shield_protection_enabled: false
        subnet_type_tag_key: "acme/subnet/type"
        ipv4_primary_cidr_block: 10.9.0.0/18
```
</File>

In the `stacks/catalog/vpc/ue2.yaml` file, add the following manifest for the `vpc` Atmos component:

<File title="stacks/catalog/vpc/ue2.yaml">
```yaml
import:
  - catalog/vpc/defaults

components:
  terraform:
    vpc:
      vars:
        availability_zones:
          - us-east-2a
          - us-east-2b
          - us-east-2c
```
</File>

In the `stacks/catalog/vpc/uw2.yaml` file, add the following manifest for the `vpc` Atmos component:

<File title="stacks/catalog/vpc/uw2.yaml">
```yaml
import:
  - catalog/vpc/defaults

components:
  terraform:
    vpc:
      vars:
        availability_zones:
          - us-west-2a
          - us-west-2b
          - us-west-2c
```
</File>

In the `stacks/catalog/vpc/dev.yaml` file, add the following manifest for the `vpc` Atmos component:

<File title="stacks/catalog/vpc/dev.yaml">
```yaml
components:
  terraform:
    vpc:
      vars:
        ipv4_primary_cidr_block: 10.7.0.0/18
        vpc_flow_logs_enabled: false
```
</File>

In the `stacks/catalog/vpc/staging.yaml` file, add the following manifest for the `vpc` Atmos component:

<File title="stacks/catalog/vpc/staging.yaml">
```yaml
components:
  terraform:
    vpc:
      vars:
        ipv4_primary_cidr_block: 10.9.0.0/18
        vpc_flow_logs_enabled: false
```
</File>

In the `stacks/catalog/vpc/prod.yaml` file, add the following manifest for the `vpc` Atmos component:

<File title="stacks/catalog/vpc/prod.yaml">
```yaml
components:
  terraform:
    vpc:
      vars:
        ipv4_primary_cidr_block: 10.8.0.0/18
        # In `prod`, don't map public IPs on launch
        # Override `map_public_ip_on_launch` from the defaults
        map_public_ip_on_launch: false
```
</File>

These Atmos component manifests will be imported into the top-level Atmos stacks. The default variables (in the `vars` sections)
can be overridden in the derived Atmos components by using [Atmos Component Inheritance](/core-concepts/stacks/inheritance).

## Atmos Top-level Stacks

When executing the [CLI commands](/cheatsheets/commands), Atmos does not use the stack file names and their filesystem locations to search for the stack
where the component is defined. Instead, Atmos uses the context variables (`namespace`, `tenant`, `environment`, `stage`) to search for the stack. The
stack config file names (stack manifest names) can be anything, and they can be in any folder in any sub-folder in the `stacks` directory.

For example, when executing the `atmos terraform apply vpc -s plat-ue2-dev`
command, the Atmos stack `plat-ue2-dev` is specified by the `-s` flag. By looking at `name_pattern: "{tenant}-{environment}-{stage}"`
(see [Configure CLI](/quick-start/advanced/configure-cli)) and processing the tokens, Atmos knows that the first part of the stack name is `tenant`, the second
part is `environment`, and the third part is `stage`. Then Atmos searches for the top-level stack manifest (in the `stacks` directory)
where `tenant: plat`, `environment: ue2` and `stage: dev` are defined (inline or via imports).

Atmos top-level stacks can be configured using a Basic Layout or a Hierarchical Layout.

The Basic Layout can be used when you have a very simple configuration using just a few accounts and regions.
The Hierarchical Layout should be used when you have a very complex organization, for example, with many AWS Organizational Units (which Atmos
refers to as tenants) and dozens of AWS accounts and regions.

### Basic Layout

A basic form of stack organization is to follow the pattern of naming where each `$environment-$stage.yaml` is a file. This works well until you have
so many environments and stages.

For example, `$environment` might be `ue2` (for `us-east-2`) and `$stage` might be `prod` which would result in `stacks/ue2-prod.yaml`

Some resources, however, are global in scope. For example, Route53 and IAM might not make sense to tie to a region. These are what we call "global
resources". You might want to put these into a file like `stacks/global-region.yaml` to connote that they are not tied to any particular region.

In our example, the filesystem layout for the stacks Basic Layout using `dev`, `staging` and `prod` accounts and `us-east-2` and `us-west-2` regions
would look like this:

<File title="infra-live/" icon="fa-solid fa-folder">
```console
   │   # Centralized stacks configuration
   ├── stacks
   │   ├── catalog
   │   │    ├── vpc.yaml
   │   │    └── vpc-flow-logs-bucket.yaml
   │   ├── ue2-dev.yaml
   │   ├── ue2-staging.yaml
   │   ├── ue2-prod.yaml
   │   ├── uw2-dev.yaml
   │   ├── uw2-staging.yaml
   │   └── uw2-prod.yaml
   │  
   │   # Centralized components configuration. Components are broken down by tool
   └── components
       └── terraform   # Terraform components (Terraform root modules)
           ├── vpc
           └── vpc-flow-logs-bucket
```
</File>

### Hierarchical Layout

We recommend using a hierarchical layout that follows the way AWS thinks about infrastructure. This works very well when you may have dozens or
hundreds of accounts and regions that you operate in.

## Create Top-level Stacks

Although in this Quick Start guide we use just a few Terraform components which we want to provision into three AWS accounts in just two AWS regions
(which could be considered basic), we will use the Hierarchical Layout to show how the Atmos stacks can be configured for very complex organizations
and infrastructures.

We will assume we are using just one Organization `acme` and just one AWS Organizational Unit (OU) `plat`. But as you will notice, the layout
can be easily extended to support many AWS Organizations and Organizational Units.

Create the following filesystem layout (which will be the final layout for this Quick Start guide):

<File title="infra-live/" icon="fa-solid fa-folder">
```console
   │   # Centralized stacks configuration
   ├── stacks
   │   ├── catalog
   │   │    ├── vpc
   │   │    │   ├── defaults.yaml
   │   │    │   ├── disabled.yaml
   │   │    │   ├── prod.yaml
   │   │    │   ├── ue2.yaml
   │   │    │   └── uw2.yaml
   │   │    └── vpc-flow-logs-bucket
   │   │        ├── defaults.yaml
   │   │        └── disabled.yaml
   │   ├── mixins
   │   │    ├── tenant
   │   │    │   ├── core.yaml
   │   │    │   └── plat.yaml
   │   │    ├── region
   │   │    │   ├── us-east-2.yaml
   │   │    │   └── us-west-2.yaml
   │   │    └── stage
   │   │        ├── dev.yaml
   │   │        ├── prod.yaml
   │   │        └── staging.yaml
   │   └── orgs
   │        └── acme
   │            ├── _defaults.yaml
   │            └── plat
   │                 ├── _defaults.yaml
   │                 ├── dev
   │                 │   ├── _defaults.yaml
   │                 │   ├── us-east-2.yaml
   │                 │   └── us-west-2.yaml
   │                 ├── prod
   │                 │   ├── _defaults.yaml
   │                 │   ├── us-east-2.yaml
   │                 │   └── us-west-2.yaml
   │                 └── staging
   │                     ├── _defaults.yaml
   │                     ├── us-east-2.yaml
   │                     └── us-west-2.yaml
   │  
   │   # Centralized components configuration. Components are broken down by tool
   └── components
       └── terraform   # Terraform components (Terraform root modules)
           ├── vpc
           └── vpc-flow-logs-bucket
```
</File>

### Configure Region and Stage Mixins

[Mixins](/core-concepts/stacks/inheritance/mixins) are a special kind of "[import](/core-concepts/stacks/imports)".
It's simply a convention we recommend to distribute reusable snippets of configuration that alter the behavior in some deliberate way.
Mixins are not handled in any special way. They are technically identical to all other imports.

In `stacks/mixins/tenant/core.yaml`, add the following config:

<File title="stacks/mixins/tenant/core.yaml">
```yaml
vars:
  tenant: core

# Other defaults for the `core` tenant/OU
```
</File>

In `stacks/mixins/tenant/plat.yaml`, add the following config:

<File title="stacks/mixins/tenant/plat.yaml">
```yaml
vars:
  tenant: plat

# Other defaults for the `plat` tenant/OU
```
</File>

In `stacks/mixins/region/us-east-2.yaml`, add the following config:

<File title="stacks/mixins/region/us-east-2.yaml">
```yaml
import:
  # Import the `ue2` manifest with `vpc` configuration for `us-east-2` region
  - catalog/vpc/ue2
  # All accounts (stages) in `us-east-2` region will have the `vpc-flow-logs-bucket` component
  - catalog/vpc-flow-logs-bucket/defaults

vars:
  region: us-east-2
  environment: ue2

# Other defaults for the `us-east-2` region
```
</File>

In `stacks/mixins/region/us-west-2.yaml`, add the following config:

<File title="stacks/mixins/region/us-west-2.yaml">
```yaml
import:
  # Import the `uw2` manifest with `vpc` configuration for `us-west-2` region
  - catalog/vpc/uw2
  # All accounts (stages) in `us-west-2` region will have the `vpc-flow-logs-bucket` component
  - catalog/vpc-flow-logs-bucket/defaults

vars:
  region: us-west-2
  environment: uw2

# Other defaults for the `us-west-2` region
```
</File>

In `stacks/mixins/stage/dev.yaml`, add the following config:

<File title="stacks/mixins/stage/dev.yaml">
```yaml
vars:
  stage: dev

# Other defaults for the `dev` stage/account
```
</File>

In `stacks/mixins/stage/prod.yaml`, add the following config:

<File title="stacks/mixins/stage/prod.yaml">
```yaml
vars:
  stage: prod

# Other defaults for the `prod` stage/account
```
</File>

In `stacks/mixins/stage/staging.yaml`, add the following config:

<File title="stacks/mixins/stage/staging.yaml">
```yaml
vars:
  stage: staging

# Other defaults for the `staging` stage/account
```
</File>

As we can see, in the tenant, region and stage mixins, besides some other common variables, we are defining the global context
variables `tenant`, `environment` and `stage`, which Atmos uses when searching for a component in a stack. These mixins then get imported into the
parent Atmos stacks without defining the context variables in each top-level stack, making the configuration DRY.

### Configure Defaults for Organization, OU and accounts

The `_defaults.yaml` stack manifests contain the default settings for the Organization(s), Organizational Units, and accounts.

:::info
The `_defaults.yaml` stack manifests are not imported into other Atmos manifests automatically.
You need to explicitly import them using [imports](/core-concepts/stacks/imports).
:::

In `stacks/orgs/acme/_defaults.yaml`, add the following config:

<File title="stacks/orgs/acme/_defaults.yaml">
```yaml
vars:
  namespace: acme
```
</File>

The file defines the context variable `namespace` for the entire `acme` Organization.

In `stacks/orgs/acme/core/_defaults.yaml`, add the following config for the `core` OU (tenant):

<File title="stacks/orgs/acme/core/_defaults.yaml">
```yaml
import:
  - orgs/acme/_defaults
  - mixins/tenant/core
```
</File>

In `stacks/orgs/acme/plat/_defaults.yaml`, add the following config for the `plat` OU (tenant):

<File title="stacks/orgs/acme/plat/_defaults.yaml">
```yaml
import:
  - orgs/acme/_defaults
  - mixins/tenant/plat
```
</File>

In the `stacks/orgs/acme/plat/_defaults.yaml` file, we import the defaults for the Organization and for the `plat` tenant (which
corresponds to the `plat` Organizational Unit). When Atmos processes this stack config, it will import and deep-merge all the variables defined in the
imported files and inline. All imports are processed in the order they are defined.

In `stacks/orgs/acme/plat/dev/_defaults.yaml`, add the following config for the `dev` account:

<File title="stacks/orgs/acme/plat/dev/_defaults.yaml">
```yaml
import:
  - orgs/acme/plat/_defaults
  - mixins/stage/dev
```
</File>

In the file, we import the mixin for the `plat` tenant (which, as was described above, imports the defaults for the Organization), and then the mixin
for the `dev` account (which defines `stage: dev` variable). After processing all these imports, Atmos determines the values for the three context
variables `namespace`, `tenant` and `stage`, which it then sends to the Terraform components as Terraform variables. We are using hierarchical imports
here.

Similar to the `dev` account, add the following configs for the `prod` and `staging` accounts:

<File title="stacks/orgs/acme/plat/prod/_defaults.yaml">
```yaml
import:
  - orgs/acme/plat/_defaults
  - mixins/stage/prod
```
</File>

<File title="stacks/orgs/acme/plat/staging/_defaults.yaml">
```yaml
import:
  - orgs/acme/plat/_defaults
  - mixins/stage/staging
```
</File>

### Configure Top-level Stacks

After we've configured the catalog for the components, the mixins for the tenants, regions and stages, and the defaults for the Organization, OU and
accounts, the final step is to configure the Atmos root (top-level) stacks and the Atmos components in the stacks.

In `stacks/orgs/acme/plat/dev/us-east-2.yaml`, add the following config:

<File title="stacks/orgs/acme/plat/dev/us-east-2.yaml">
```yaml
# `import` supports POSIX-style Globs for file names/paths (double-star `**` is supported).
# File extensions are optional (if not specified, `.yaml` is used by default).
import:
  - orgs/acme/plat/dev/_defaults
  - mixins/region/us-east-2
  # Override the `vpc` component configuration for `dev` by importing the `catalog/vpc/dev` manifest
  - catalog/vpc/dev
```
</File>

In the file, we import the region mixin and the defaults for the Organization, OU and account (using hierarchical imports).

Similarly, create the top-level Atmos stack for the `dev` account in `us-west-2` region:

<File title="stacks/orgs/acme/plat/dev/us-west-2.yaml">
```yaml
import:
  - orgs/acme/plat/dev/_defaults
  - mixins/region/us-west-2
  # Override the `vpc` component configuration for `dev` by importing the `catalog/vpc/dev` manifest
  - catalog/vpc/dev
```
</File>

In `stacks/orgs/acme/plat/staging/us-east-2.yaml`, add the following config:

<File title="stacks/orgs/acme/plat/staging/us-east-2.yaml">
```yaml
import:
  - orgs/acme/plat/staging/_defaults
  - mixins/region/us-east-2
  # Override the `vpc` component configuration for `staging` by importing the `catalog/vpc/staging` manifest
  - catalog/vpc/staging
```
</File>

Similarly, create the top-level Atmos stack for the `staging` account in `us-west-2` region:

<File title="stacks/orgs/acme/plat/staging/us-west-2.yaml">
```yaml
import:
  - orgs/acme/plat/staging/_defaults
  - mixins/region/us-west-2
  # Override the `vpc` component configuration for `staging` by importing the `catalog/vpc/staging` manifest
  - catalog/vpc/staging
```
</File>

In `stacks/orgs/acme/plat/prod/us-east-2.yaml`, add the following config:

<File title="stacks/orgs/acme/plat/prod/us-east-2.yaml">
```yaml
# Import the tenant and region mixins, and the defaults for the components from the `catalog`.
# `import` supports POSIX-style Globs for file names/paths (double-star `**` is supported).
# File extensions are optional (if not specified, `.yaml` is used by default).
import:
  - orgs/acme/plat/prod/_defaults
  - mixins/region/us-east-2
  # Override the `vpc` component configuration for `prod` by importing the `catalog/vpc/prod` manifest
  - catalog/vpc/prod
```
</File>

In the file, we import the region mixin and the defaults for the Organization, OU and account (using hierarchical imports).

Similarly, create the top-level Atmos stack for the `prod` account in `us-west-2` region:

<File title="stacks/orgs/acme/plat/prod/us-west-2.yaml">
```yaml
import:
  - orgs/acme/plat/prod/_defaults
  - mixins/region/us-west-2
  # Override the `vpc` component configuration for `prod` by importing the `catalog/vpc/prod` manifest
  - catalog/vpc/prod
```
</File>

---

## Automate Common Workflows

import Terminal from '@site/src/components/Terminal'

Atmos workflows are a way of combining multiple commands into executable units of work.

:::tip
Refer to [Atmos Workflows](/core-concepts/workflows) for more information about configuring workflows
:::

:::note
You can use [Atmos Custom Commands](/core-concepts/custom-commands) in [Atmos Workflows](/core-concepts/workflows),
and [Atmos Workflows](/core-concepts/workflows)
in [Atmos Custom Commands](/core-concepts/custom-commands)
:::

To define workflows, add the following configurations:

- In `atmos.yaml`, add the `workflows` section and configure the base path to the workflows:

```yaml
workflows:
  # Can also be set using 'ATMOS_WORKFLOWS_BASE_PATH' ENV var, or '--workflows-dir' command-line arguments
  # Supports both absolute and relative paths
  base_path: "stacks/workflows"
```

- Add workflow manifests in the `stacks/workflows` folder. In this Quick Start example, we will define Atmos workflows
  in the `networking.yaml` and `validation.yaml` workflow manifests:

```console
   │   # Centralized stacks configuration
   ├── stacks
   │   └── workflows
   │       ├── networking.yaml
   │       └── validation.yaml
```

- Add the following Atmos workflows to the `stacks/workflows/networking.yaml` file:

```yaml
name: Networking & Logging
description: Atmos workflows for managing VPCs and VPC Flow Logs

workflows:

  plan-all-vpc-flow-logs:
    description: |
      Run 'terraform plan' on all 'vpc-flow-logs-bucket' components in all stacks
    steps:
      - command: terraform plan vpc-flow-logs-bucket -s plat-ue2-dev
      - command: terraform plan vpc-flow-logs-bucket -s plat-uw2-dev
      - command: terraform plan vpc-flow-logs-bucket -s plat-ue2-staging
      - command: terraform plan vpc-flow-logs-bucket -s plat-uw2-staging
      - command: terraform plan vpc-flow-logs-bucket -s plat-ue2-prod
      - command: terraform plan vpc-flow-logs-bucket -s plat-uw2-prod

  plan-all-vpc:
    description: |
      Run 'terraform plan' on all 'vpc' components in all stacks
    steps:
      - command: terraform plan vpc -s plat-ue2-dev
      - command: terraform plan vpc -s plat-uw2-dev
      - command: terraform plan vpc -s plat-ue2-staging
      - command: terraform plan vpc -s plat-uw2-staging
      - command: terraform plan vpc -s plat-ue2-prod
      - command: terraform plan vpc -s plat-uw2-prod

  apply-all-components:
    description: |
      Run 'terraform apply' on all components in all stacks
    steps:
      - command: terraform apply vpc-flow-logs-bucket -s plat-ue2-dev -auto-approve
      - command: terraform apply vpc -s plat-ue2-dev -auto-approve
      - command: terraform apply vpc-flow-logs-bucket -s plat-uw2-dev -auto-approve
      - command: terraform apply vpc -s plat-uw2-dev -auto-approve
      - command: terraform apply vpc-flow-logs-bucket -s plat-ue2-staging -auto-approve
      - command: terraform apply vpc -s plat-ue2-staging -auto-approve
      - command: terraform apply vpc-flow-logs-bucket -s plat-uw2-staging -auto-approve
      - command: terraform apply vpc -s plat-uw2-staging -auto-approve
      - command: terraform apply vpc-flow-logs-bucket -s plat-ue2-prod -auto-approve
      - command: terraform apply vpc -s plat-ue2-prod -auto-approve
      - command: terraform apply vpc-flow-logs-bucket -s plat-uw2-prod -auto-approve
      - command: terraform apply vpc -s plat-uw2-prod -auto-approve
```

- Add the following Atmos workflows to the `stacks/workflows/validation.yaml` file:

```yaml
name: Validation
description: Atmos workflows for VPCs and VPC Flow Logs validation

workflows:

  validate-all-vpc-flow-logs:
    description: "Validate all VPC Flow Logs bucket components in all stacks"
    steps:
      - command: validate component vpc-flow-logs-bucket -s plat-ue2-dev
      - command: validate component vpc-flow-logs-bucket -s plat-uw2-dev
      - command: validate component vpc-flow-logs-bucket -s plat-ue2-staging
      - command: validate component vpc-flow-logs-bucket -s plat-uw2-staging
      - command: validate component vpc-flow-logs-bucket -s plat-ue2-prod
      - command: validate component vpc-flow-logs-bucket -s plat-uw2-prod

  validate-all-vpc:
    description: "Validate all VPC components in all stacks"
    steps:
      - command: validate component vpc -s plat-ue2-dev
      - command: validate component vpc -s plat-uw2-dev
      - command: validate component vpc -s plat-ue2-staging
      - command: validate component vpc -s plat-uw2-staging
      - command: validate component vpc -s plat-ue2-prod
      - command: validate component vpc -s plat-uw2-prod
```

- Run the following Atmos commands to execute the workflows:

```shell
# Execute the workflow `plan-all-vpc-flow-logs` from the workflow manifest `networking.yaml`
atmos workflow plan-all-vpc-flow-logs -f networking

# Execute the workflow `plan-all-vpc` from the workflow manifest `networking.yaml`
atmos workflow plan-all-vpc -f networking

# Execute the workflow `apply-all-components` from the workflow manifest `networking.yaml`
atmos workflow apply-all-components -f networking

# Execute the workflow `validate-all-vpc-flow-logs` from the workflow manifest `validation.yaml`
atmos workflow validate-all-vpc-flow-logs -f validation

# Execute the workflow `validate-all-vpc` from the workflow manifest `validation.yaml`
atmos workflow validate-all-vpc -f validation
```

:::tip
Refer to [atmos workflow](/cli/commands/workflow) for more information on the `atmos workflow` CLI command
:::

The `atmos workflow` CLI command supports the `--dry-run` flag. If passed, the command will just print information about
the executed workflow steps without executing them. For example:

<Terminal title="atmos workflow plan-all-vpc -f networking --dry-run">
```console
Executing the workflow 'plan-all-vpc' from 'stacks/workflows/networking.yaml'

Executing workflow step: terraform plan vpc -s plat-ue2-dev
Executing workflow step: terraform plan vpc -s plat-uw2-dev
Executing workflow step: terraform plan vpc -s plat-ue2-staging
Executing workflow step: terraform plan vpc -s plat-uw2-staging
Executing workflow step: terraform plan vpc -s plat-ue2-prod
Executing workflow step: terraform plan vpc -s plat-uw2-prod
```
</Terminal>

---

## Final Notes

Atmos provides unlimited flexibility in defining and configuring stacks and components in the stacks.

- Terraform components can be in different sub-folders in the `components/terraform` directory. The sub-folders can be organized by type, by teams
  that are responsible for the components, by operations that are performed on the components, or by any other category

- Atmos stack manifests can have arbitrary names and can be located in any sub-folder in the `stacks` directory. Atmos stack filesystem layout is for
  people to better organize the stacks and make the configurations DRY. Atmos (the CLI) does not care about the filesystem layout, all it cares about
  is how to find the stacks and the components in the stacks by using the context variables `namespace`, `tenant`, `environment` and `stage`

- An Atmos component can have any name that can be different from the Terraform component name. For example, two different Atmos components `vpc/1`
  and `vpc/2` can provide configuration for the same Terraform component `vpc`

- We can provision more than one instance of the same Terraform component (with the same or different settings) into the same environment by defining
  many Atmos components that provide configuration for the Terraform component. For example, the following config shows how to define two Atmos
  components, `vpc/1` and `vpc/2`, which both point to the same Terraform component `vpc`:

  ```yaml
  import:
    - orgs/acme/plat/dev/_defaults
    - mixins/region/us-east-2
    # Import the defaults for all VPC components
    - catalog/vpc/defaults

  components:
    terraform:
      # Atmos component `vpc/1`
      vpc/1:
        metadata:
          # Point to the Terraform component in `components/terraform/vpc`
          component: vpc
          # Inherit the defaults for all VPC components
          inherits:
            - vpc/defaults
        # Define/override variables specific to this `vpc/1` component
        vars:
          name: vpc-1
          ipv4_primary_cidr_block: 10.9.0.0/18

      # Atmos component `vpc/2`
      vpc/2:
        metadata:
          # Point to the Terraform component in `components/terraform/vpc`
          component: vpc
          # Inherit the defaults for all VPC components
          inherits:
            - vpc/defaults
        # Define/override variables specific to this `vpc/2` component
        vars:
          name: vpc-2
          ipv4_primary_cidr_block: 10.10.0.0/18
  ```

  Then we can execute the following `atmos` commands to provision the two VPCs into the `dev` account in the `us-east-2` region:

  ```shell
  atmos terraform apply vpc/1 -s plat-ue2-dev
  atmos terraform apply vpc/2 --stack plat-ue2-dev
  ```

All the above makes Atmos an ideal framework to organize infrastructure, to design for organizational complexity, and to provision multi-account
environments for very complex organizations.

---

## Next Steps

You have just learned the **essentials of Atmos**.

Atmos is a powerful enterprise-grade workflow automation tool with so **much more to offer**!

## What's next?

Here are some of the major differentiators of Atmos and topics worth exploring in greater depth:

* [Atmos Design Patterns](/design-patterns)
* [Third-party Integrations](/integrations)
* [Atmos Vendoring](/core-concepts/vendor)
* [Component Vendoring](/core-concepts/vendor/component-manifest)
* [Imports](/core-concepts/stacks/imports) (mixins, catalogs)
* [Workflow Automation](/core-concepts/workflows)
* [Custom Commands](/core-concepts/custom-commands)
* [Component Inheritance & Multiple Inheritance](/core-concepts/stacks/inheritance)
* [JSON Schema & OPA Policy Enforcement](/core-concepts/validate)
* [Atmos Manifest JSON Schema](/cli/schemas)

---

## Provision

Having configured the Terraform components, the Atmos components catalog, all the mixins and defaults, and the Atmos top-level stacks, we can now
provision the components in the stacks.

The `vpc` Atmos components use the remote state from the `vpc-flow-logs-bucket` components, therefore the `vpc-flow-logs-bucket` components must
be provisioned first.

## Provision Atmos Components into all Stacks

Provision the `vpc-flow-logs-bucket` Atmos component into the stacks:

```shell
# `plat` OU, `dev` account, `us-east-2` and `us-west-2` regions
atmos terraform apply vpc-flow-logs-bucket -s plat-ue2-dev
atmos terraform apply vpc-flow-logs-bucket -s plat-uw2-dev

# `plat` OU, `staging` account, `us-east-2` and `us-west-2` regions
atmos terraform apply vpc-flow-logs-bucket -s plat-ue2-staging
atmos terraform apply vpc-flow-logs-bucket -s plat-uw2-staging

# `plat` OU, `prod` account, `us-east-2` and `us-west-2` regions
atmos terraform apply vpc-flow-logs-bucket -s plat-ue2-prod
atmos terraform apply vpc-flow-logs-bucket -s plat-uw2-prod
```

Provision the `vpc` Atmos component into the stacks:

```shell
# `plat` OU, `dev` account, `us-east-2` and `us-west-2` regions
atmos terraform apply vpc -s plat-ue2-dev
atmos terraform apply vpc -s plat-uw2-dev

# `plat` OU, `staging` account, `us-east-2` and `us-west-2` regions
atmos terraform apply vpc -s plat-ue2-staging
atmos terraform apply vpc -s plat-uw2-staging

# `plat` OU, `prod` account, `us-east-2` and `us-west-2` regions
atmos terraform apply vpc -s plat-ue2-prod
atmos terraform apply vpc -s plat-uw2-prod
```

Alternatively, you can execute the configured [Atmos workflow](/quick-start/advanced/create-workflows) to provision all the components in all the stacks:

```shell
# Execute the workflow `apply-all-components` from the workflow manifest `networking`
atmos workflow apply-all-components -f networking
```

## Stack Search Algorithm

Looking at the commands above, you might have a question "How does Atmos find the component in the stack and all the variables?"

Let's consider what Atmos does when executing the command `atmos terraform apply vpc -s plat-ue2-prod`:

- Atmos uses the [CLI config](/quick-start/advanced/configure-cli) `stacks.name_pattern: "{tenant}-{environment}-{stage}"` to figure out that the first part of
  the stack name is `tenant`, the second part is `environment`, and the third part is `stage`

- Atmos searches for the stack configuration file (in the `stacks` folder and all sub-folders) where `tenant: plat`, `environment: ue2`
  and `stage: prod` are defined (inline or via imports). During the search, Atmos processes all parent (top-level) config files and compares the
  context variables specified in the command (`-s` flag) with the context variables defined in the stack configurations, finally finding the matching
  stack

- Atmos finds the component `vpc` in the stack, processing all the inline configs and all the configs from the imports

- Atmos deep-merges all the catalog imports for the `vpc` component and then deep-merges all the variables for the component defined in all
  sections (global `vars`, terraform `vars`, base components `vars`, component `vars`), producing the final variables for the `vpc` component in
  the `plat-ue2-prod` stack

- And lastly, Atmos writes the final deep-merged variables into a `.tfvar` file in the component directory and then
  executes `terraform apply -var-file ...` command

---

## Vendor Components

import Terminal from '@site/src/components/Terminal'
import File from '@site/src/components/File'
import Step from '@site/src/components/Step'
import StepNumber from '@site/src/components/StepNumber'

In the previous steps, we've configured the repository and decided to provision the `vpc-flow-logs-bucket` and `vpc` Terraform
components into three AWS accounts (`dev`, `staging`, `prod`) in the two AWS regions (`us-east-2` and `us-west-2`).
We've also configured the Atmos CLI in the `atmos.yaml` CLI config file to search for the Terraform components in
the `components/terraform` directory.

Next step is to create the Terraform components `vpc-flow-logs-bucket` and `vpc`.

One way to create the Terraform components is to copy them into the corresponding folders in your repo:

- Copy the `vpc-flow-logs-bucket` component from the open-source component repository
  [vpc-flow-logs-bucket](https://github.com/cloudposse/terraform-aws-components/tree/main/modules/vpc-flow-logs-bucket)
  into the `components/terraform/vpc-flow-logs-bucket` folder

- Copy the `vpc` component from the open-source component repository
  [vpc](https://github.com/cloudposse/terraform-aws-components/tree/main/modules/vpc)
  into the `components/terraform/vpc` folder

:::note
The recommended way to vendor the components is to execute the `atmos vendor pull` CLI command.
:::

:::tip

For more information about Atmos Vendoring and the `atmos vendor pull` CLI command, refer to:

- [Atmos Vendoring](/core-concepts/vendor)
- [atmos vendor pull](/cli/commands/vendor/pull)

:::

To vendor the components from the open-source component repository [terraform-aws-components](https://github.com/cloudposse/terraform-aws-components),
perform the following steps:

<Step>

## <StepNumber/> Create a `vendor.yaml` config file

Create a `vendor.yaml` Atmos vendor config file in the root of the repo with the following content:

<File title="vendor.yaml">
```yaml
apiVersion: atmos/v1
kind: AtmosVendorConfig
metadata:
  name: example-vendor-config
  description: Atmos vendoring manifest
spec:
  # `imports` or `sources` (or both) must be defined in a vendoring manifest
  imports: []

  sources:
    # `source` supports the following protocols: local paths (absolute and relative), OCI (https://opencontainers.org),
    # Git, Mercurial, HTTP, HTTPS, Amazon S3, Google GCP,
    # and all URL and archive formats as described in https://github.com/hashicorp/go-getter.
    # In 'source', Golang templates are supported  https://pkg.go.dev/text/template.
    # If 'version' is provided, '{{.Version}}' will be replaced with the 'version' value before pulling the files from 'source'.
    - component: "vpc"
      source: "github.com/cloudposse/terraform-aws-components.git//modules/vpc?ref={{.Version}}"
      version: "1.398.0"
      targets:
        - "components/terraform/vpc"
      # Only include the files that match the 'included_paths' patterns.
      # If 'included_paths' is not specified, all files will be matched except those that match the patterns from 'excluded_paths'.
      # 'included_paths' support POSIX-style Globs for file names/paths (double-star `**` is supported).
      # https://en.wikipedia.org/wiki/Glob_(programming)
      # https://github.com/bmatcuk/doublestar#patterns
      included_paths:
        - "**/*.tf"
      excluded_paths:
        - "**/providers.tf"
      # Tags can be used to vendor component that have the specific tags
      # `atmos vendor pull --tags networking`
      # Refer to https://atmos.tools/cli/commands/vendor/pull
      tags:
        - networking
    - component: "vpc-flow-logs-bucket"
      source: "github.com/cloudposse/terraform-aws-components.git//modules/vpc-flow-logs-bucket?ref={{.Version}}"
      version: "1.398.0"
      targets:
        - "components/terraform/vpc-flow-logs-bucket"
      included_paths:
        - "**/*.tf"
      excluded_paths:
        - "**/providers.tf"
      # Tags can be used to vendor component that have the specific tags
      # `atmos vendor pull --tags networking,storage`
      # Refer to https://atmos.tools/cli/commands/vendor/pull
      tags:
        - storage
```
</File>

<details>
<summary>Advanced Configuration</summary>
:::warning

The `glob` library that Atmos uses to download remote artifacts does not treat the double-star `**` as including sub-folders.
If the component's folder has sub-folders, and you need to vendor them, they have to be explicitly defined as in the following example.

:::

<File title="vendor.yaml">
```yaml
spec:
  sources:
    - component: "vpc-flow-logs-bucket"
      source: "github.com/cloudposse/terraform-aws-components.git//modules/vpc-flow-logs-bucket?ref={{.Version}}"
      version: "1.398.0"
      targets:
        - "components/terraform/vpc-flow-logs-bucket"
      included_paths:
        - "**/*.tf"
        # If the component's folder has the `modules` sub-folder, it needs to be explicitly defined
        - "**/modules/**"
```
</File>
</details>
</Step>

<Step>
## <StepNumber/> Vendor the dependencies
Execute the command `atmos vendor pull` from the root of the repo.

<Terminal title="atmos vendor pull">
```shell
Processing vendor config file 'vendor.yaml'

Pulling sources for the component 'vpc'
from 'github.com/cloudposse/terraform-aws-components.git//modules/vpc?ref=1.343.1'
into 'components/terraform/vpc'

Pulling sources for the component 'vpc-flow-logs-bucket'
from 'github.com/cloudposse/terraform-aws-components.git//modules/vpc-flow-logs-bucket?ref=1.343.1'
into 'components/terraform/vpc-flow-logs-bucket/1.343.1'

```
</Terminal>

After the command is executed, the filesystem layout should look like this:

```console
   │   # Centralized stacks configuration
   ├── stacks
   │  
   │   # Centralized components configuration. Components are broken down by tool
   └── components
       └── terraform   # Terraform components (Terraform root modules)
           ├── vpc
           │   ├── context.tf
           │   ├── main.tf
           │   ├── outputs.tf
           │   ├── providers.tf
           │   ├── remote-state.tf
           │   ├── variables.tf
           │   ├── versions.tf
           │   └── vpc-flow-logs.tf
           └── vpc-flow-logs-bucket
               ├── context.tf
               ├── main.tf
               ├── outputs.tf
               ├── providers.tf
               ├── variables.tf
               └── versions.tf
```

</Step>

Each component follows the [Standard Module Structure](https://developer.hashicorp.com/terraform/language/modules/develop/structure) that Terraform
recommends. There are a few additions:

- `context.tf` - this file contains all the common variables that Terraform modules and components consume (to make the component's `variables.tf`
  file DRY). This is a standard file that is copied into each component. The file also defines the context
  variables (`namespace`, `tenant`, `environment`, `stage`) which are used by Atmos to search for Atmos stacks when executing
  the [CLI commands](/cheatsheets/commands)

- `remote-state.tf` in the `vpc` component - this file configures the
  [remote-state](https://github.com/cloudposse/terraform-yaml-stack-config/tree/main/modules/remote-state) Terraform module to obtain the remote state
  for the `vpc-flow-logs-bucket` component. The `vpc` Terraform component needs the outputs from the `vpc-flow-logs-bucket` Terraform component to
  configure [VPC Flow Logs](https://docs.aws.amazon.com/vpc/latest/userguide/flow-logs.html) and store them in the S3 bucket

<File title="components/terraform/vpc/remote-state.tf">
```hcl
module "vpc_flow_logs_bucket" {
  count = local.vpc_flow_logs_enabled ? 1 : 0

  source  = "cloudposse/stack-config/yaml//modules/remote-state"
  version = "1.5.0"

  # Specify the Atmos component name (defined in YAML stack config files)
  # for which to get the remote state outputs
  component = "vpc-flow-logs-bucket"

  # Override the context variables to point to a different Atmos stack if the
  # `vpc-flow-logs-bucket` Atmos component is provisioned in another AWS account, OU or region
  stage       = try(coalesce(var.vpc_flow_logs_bucket_stage_name, module.this.stage), null)
  tenant      = try(coalesce(var.vpc_flow_logs_bucket_tenant_name, module.this.tenant), null)
  environment = try(coalesce(var.vpc_flow_logs_bucket_environment_name, module.this.environment), null)

  # `context` input is a way to provide the information about the stack (using the context
  # variables `namespace`, `tenant`, `environment`, `stage` defined in the stack config)
  context = module.this.context
}
```
</File>

## Remote State Notes

The [remote-state](https://github.com/cloudposse/terraform-yaml-stack-config/tree/main/modules/remote-state) Terraform module uses the [terraform-provider-utils](https://github.com/cloudposse/terraform-provider-utils)
Terraform provider to read Atmos configuration and obtain the remote state for Atmos components.

Both the `atmos` CLI and [terraform-provider-utils](https://github.com/cloudposse/terraform-provider-utils) Terraform provider use the same `Go` code,
which try to locate the [CLI config](/cli/configuration) `atmos.yaml` file before parsing and processing [Atmos stacks](/core-concepts/stacks).

This means that `atmos.yaml` file must be at a location in the file system where all processes can find it.

While placing `atmos.yaml` at the root of the repository will work for Atmos, it will not work for
the [terraform-provider-utils](https://github.com/cloudposse/terraform-provider-utils) Terraform provider because the provider gets executed from the
component's directory (e.g. `components/terraform/vpc`), and we don't want to replicate `atmos.yaml` into every component's folder.

:::info

`atmos.yaml` is loaded from the following locations (from lowest to highest priority):

- System dir (`/usr/local/etc/atmos/atmos.yaml` on Linux, `%LOCALAPPDATA%/atmos/atmos.yaml` on Windows)
- Home dir (`~/.atmos/atmos.yaml`)
- Current directory
- ENV variables `ATMOS_CLI_CONFIG_PATH` and `ATMOS_BASE_PATH`

:::

For this to work for both the `atmos` CLI and the Terraform `utils` provider, we recommend doing one of the following:

- Put `atmos.yaml` at `/usr/local/etc/atmos/atmos.yaml` on local host and set the ENV var `ATMOS_BASE_PATH` to point to the absolute path of the root
  of the repo

- Put `atmos.yaml` into the home directory (`~/.atmos/atmos.yaml`) and set the ENV var `ATMOS_BASE_PATH` pointing to the absolute path of the root of
  the repo

- Put `atmos.yaml` at a location in the file system and then set the ENV var `ATMOS_CLI_CONFIG_PATH` to point to that location. The ENV var must
  point to a folder without the `atmos.yaml` file name. For example, if `atmos.yaml` is at `/atmos/config/atmos.yaml`,
  set `ATMOS_CLI_CONFIG_PATH=/atmos/config`. Then set the ENV var `ATMOS_BASE_PATH` pointing to the absolute path of the root of the repo

- When working in a Docker container, place `atmos.yaml` in the `rootfs` directory
  at [/rootfs/usr/local/etc/atmos/atmos.yaml](https://github.com/cloudposse/atmos/blob/main/examples/quick-start-advanced/rootfs/usr/local/etc/atmos/atmos.yaml)
  and then copy it into the container's file system in the [Dockerfile](https://github.com/cloudposse/atmos/blob/main/examples/quick-start-advanced/Dockerfile)
  by executing the `COPY rootfs/ /` Docker command. Then in the Dockerfile, set the ENV var `ATMOS_BASE_PATH` pointing to the absolute path of the
  root of the repo. Note that the [Atmos example](https://github.com/cloudposse/atmos/blob/main/examples/quick-start)
  uses [Geodesic](https://github.com/cloudposse/geodesic) as the base Docker image. [Geodesic](https://github.com/cloudposse/geodesic) sets the ENV
  var `ATMOS_BASE_PATH` automatically to the absolute path of the root of the repo on local host

For a complete description of how Atmos components use remote state, refer to [Component Remote State](/core-concepts/share-data/remote-state).

---

## Install Atmos

import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';
import Link from '@docusaurus/Link';
import Intro from '@site/src/components/Intro'
import LatestRelease from '@site/src/components/LatestRelease'
import File from '@site/src/components/File'
import CodeBlock from '@theme/CodeBlock';

<Intro>
There are many ways to install Atmos. Choose the method that works best for you!
</Intro>

To check what version of Atmos you have installed, just run `atmos version`. The latest version of Atmos is <LatestRelease />.

To find the latest available version of Atmos, visit the [Releases Page](https://github.com/cloudposse/atmos/releases). The latest version will always be available for download here.

## Using OS Package Managers

Atmos has native packages for macOS and every major Linux distribution. We also supply binary releases for Windows.

<Tabs>
  <TabItem value="macos" label="macOS" default>
    #### macOS (OSX)

    From Homebrew, install directly by running:

    ```shell
    brew install atmos
    ```

    #### Pro tip! Use a `Brewfile`

    Create a `Brewfile` in your [Atmos project](/core-concepts/projects), and add `brew "atmos"`. This way, you can ensure that everyone on your team is using the same version of Atmos.

    <File title="Brewfile">
    <pre><code>brew "atmos", "<LatestRelease />"</code></pre>
    </File>

    Then just run `brew install` in the same directory as the `Brewfile`.
  </TabItem>

  <TabItem value="deb" label="Debian/Ubuntu">

    #### Debian Linux (DEB)

    On Debian, use the Cloud Posse package repository provided by Cloudsmith:

    ```shell
    # Add the Cloud Posse package repository hosted by Cloudsmith
    apt-get update && apt-get install -y apt-utils curl
    curl -1sLf 'https://dl.cloudsmith.io/public/cloudposse/packages/cfg/setup/bash.deb.sh' │ bash

    # Install atmos
    apt-get install atmos@="${ATMOS_VERSION}-*"
    ```
  </TabItem>

  <TabItem value="rpm" label="RedHat/CentOS">
    #### RedHat/CentOS Linux (RPM)

    On RedHat or CentOS, use the Cloud Posse package repository provided by Cloudsmith:

    ```shell
    curl -1sLf 'https://dl.cloudsmith.io/public/cloudposse/packages/setup.rpm.sh' │ sudo -E bash

    # Install atmos
    sudo yum install atmos-${ATMOS_VERSION}.x86_64
    ```
  </TabItem>

  <TabItem value="alpine" label="Alpine Linux">

    #### Alpine Linux (APK)
    On Alpine, use the Cloud Posse package repository provided by Cloudsmith:

    ```shell
    # Install the Cloud Posse package repository hosted by Cloudsmith
    curl -fsSL 'https://dl.cloudsmith.io/public/cloudposse/packages/setup.alpine.sh' │ bash

    # Install atmos
    apk add atmos@cloudposse
    ```
  </TabItem>

  <TabItem value="nixos" label="NixOS">

    #### NixOS
    On NixOS, run the following command to install:

    ```shell
    nix-env -iA nixpkgs.atmos
    ```

    To get the latest version, you may need to update the Nix config to add "unstable" packages.

    For example, in `~/.config/nixpkgs/config.nix` add the following line to the file:

    ```console
    nixpkgs.url = "github:nixos/nixpkgs/nixos-unstable";
    ```

    Then run the following to install:

    ```shell
    nix-shell -p atmos
    ```
  </TabItem>

  <TabItem value="scoop" label="Windows">

    #### Windows via scoop.sh

    On Windows, run the following command to install:

    ```shell
    scoop install atmos
    ```

    __NOTE:__ Don't have `scoop`? Install it first from https://scoop.sh/

  </TabItem>
</Tabs>

## Other Ways to Install

Atmos has a few other ways to install, including using Go, asdf, mise, aqua, building from source, or using the automatic installer.

<Tabs>
  <TabItem value="go" label="Go">
    #### Install with Go

    Install the latest version:

    ```shell
    go install github.com/cloudposse/atmos
    ```

    Grab a specific version:

    <CodeBlock language="shell">go install github.com/cloudposse/atmos@<LatestRelease /></CodeBlock>

    Or specifically request the latest version.

    ```shell
    go install github.com/cloudposse/atmos@latest
    ```

    __NOTE:__ Since the version is passed in via `-ldflags` during the build, when running `go install` without using `-ldflags`, the CLI will return `0.0.1` when running `atmos version`.
  </TabItem>

  <TabItem value="asdf" label="asdf">
    #### Install with asdf

    Install plugin dependencies as listed in [asdf-atmos repository](https://github.com/cloudposse/asdf-atmos#dependencies):

    ```shell
    apt-get update && apt-get install -y bash curl tar
    ```

    Install the plugin:

    ```shell
    asdf plugin add atmos https://github.com/cloudposse/asdf-atmos.git
    ```

    Install a specified version:

    <CodeBlock language="shell">asdf install atmos <LatestRelease /></CodeBlock>

    Alternatively, create a `.tool-versions` file in your project to specify a consistent version for the users:
    <File title=".tool-versions">
      <pre>
        <code>
          atmos <LatestRelease />
        </code>
      </pre>
    </File>

    Then, run `asdf install` in the same directory.

    __NOTE:__ Don't have `asdf`? Install it first from [here](https://asdf-vm.com/guide/getting-started.html)
  </TabItem>

  <TabItem value="mise" label="Mise">
    #### Install with Mise

    Install a specified version:
    <CodeBlock language="shell">mise use atmos@<LatestRelease /></CodeBlock>

    Alternatively, create a `.mise.toml` file in your repository to specify a consistent version for the users:
    <File title=".mise.toml">
      <pre>
        <code>
          [tools]
          atmos = '<LatestRelease />'
        </code>
      </pre>
    </File>

    Then, run `mise install` in the same directory.

    __NOTE:__ Don't have `mise`? Install it first from [here](https://mise.jdx.dev/getting-started.html)
  </TabItem>

  <TabItem value="aqua" label="aqua">
    #### Install with aqua

    [aqua](https://aquaproj.github.io/) is a CLI Version Manager.
    aqua allows you to manage multiple versions of CLI tools, making it easy to switch between different versions of Atmos and other tools in your projects.

    Create `aqua.yaml` by `aqua init`:

    ```shell
    aqua init
    ```

    Add atmos to aqua.yaml:

    ```shell
    aqua g -i cloudposse/atmos
    ```

    Then, run `aqua install` in the same directory.

    __NOTE:__ Don't have `aqua`? Install it first from [here](https://aquaproj.github.io/docs/install)
  </TabItem>

  <TabItem value="source" label="From Source">
    #### Build from Source

    ```shell
    make build
    ```

    or run this and replace `$version` with the version that should be returned with `atmos version`.

    ```shell
    go build -o build/atmos -v -ldflags "-X 'github.com/cloudposse/atmos/pkg/version.Version=$version'"
    ```
  </TabItem>

  <TabItem value="installer" label="Automatic Installer">
    #### Automatic Installer

    If you're not sure which method to use, you can always use the automatic installer. It will figure out which
    of the mechanisms above to use, and perform it.

    Paste the following into a macOS Terminal or Linux shell prompt.

    ```shell
    curl -fsSL https://atmos.tools/install.sh | bash
    ```
  </TabItem>
</Tabs>

:::note
The latest version of Atmos (<LatestRelease />) might not be available with third-party package managers.
:::

## Download Binaries from Releases Page

- Go to [Releases Page](https://github.com/cloudposse/atmos/releases)
- Download the binary for your operating system and architecture. Replace `${version}` with the desired version

  - e.g. If you’re on a Mac (M1/M2/M3), download the `atmos_${version}_darwin_arm64` binary
  - e.g. If you’re on an Intel Mac, download the `atmos_${version}_darwin_amd64` binary
  - e.g. If you’re on Windows, download `atmos_${version}_windows_amd64.exe`, etc.

- Rename the downloaded file to `atmos` (optional)
- Add the execution bit to the binary (e.g. on Linux and Mac, run `chmod u+x atmos`)
- Place the binary somewhere on your `PATH` (e.g. on Linux and Mac: `mv atmos /usr/local/bin/`)

## Set Up Your Terminal

Atmos is a modern CLI with a Text-based UI (TUI), as such, for the best experience we recommend ensuring you have
a decent terminal and modern fonts installed and configured in your terminal.

### TERM Environment Variable

Atmos uses ANSI color codes. These should work in every modern terminal, including the default terminal shipped with
macOS. You *may* have to set the `TERM` environment variable to `xterm-256color` for it to work.
This can be persisted in your `~/.bashrc` or `~/.zshrc` file (or the respective RC file of whatever shell is in use).

```shell
export TERM=xterm-256color
```

If you're having any troule, try [iTerm2](https://iterm2.com/) or any other modern day terminal that supports
ANSI characters and fonts.

:::tip Install NerdFonts
To display all icons used by `atmos`, we recommend the use of a Nerd Font, like *Fira Code*.
:::

### NerdFonts

 Nerd Fonts are popular fonts that are patched to include icons.

The exact process will depend on your terminal and operating system, but the general idea is to install a font that
includes the necessary glyphs and then configure your terminal.

Go to https://www.nerdfonts.com/ for more information.

:::tip
We recommend the "Fira Code" NerdFont version, which is what all our screenshots are based on.
:::

<Tabs>
  <TabItem value="homebrew-cask-fonts" label="Homebrew">
    #### Homebrew

    If you're using Homebrew, you can tap the `homebrew-cask-fonts` repository to install Nerd Fonts.

    Paste the following into a macOS Terminal window.

    ```shell
    brew tap homebrew/cask-fonts     # You only need to do this once!
    brew search nerd-font            # Search for font packages

    # Install the NerdFont version of Fira Code
    brew install --cask font-fira-code-nerd-font
    ```
  </TabItem>
</Tabs>

## Next Steps

Now, try one of our Quick Start guides to get started with Atmos

<Link to="/quick-start/simple" className="button button--lg button--primary">Simple Tutorial</Link>
<Link to="/quick-start/advanced" className="button button--lg button--outline button--primary ml20">Advanced Tutorial</Link>

---

## Quick Start Introduction

import Link from '@docusaurus/Link'
import Intro from '@site/src/components/Intro'
import PrimaryCTA from '@site/src/components/PrimaryCTA'
import SecondaryCTA from '@site/src/components/SecondaryCTA'

<Intro>
Atmos is a CLI and a powerful enterprise-grade workflow automation tool for DevOps. It's also a framework that prescribes patterns and best practices to structure and organize components and stacks to design for organizational complexity and provision multi-account environments for complex organizations.
</Intro>

It allows you to very quickly configure and provision infrastructure into many environments (e.g. AWS accounts and regions), and make those configurations extremely [DRY](https://en.wikipedia.org/wiki/Don%27t_repeat_yourself).

One of the main principles of Atmos is the separation of configuration from code, so the code is generic and can be deployed anywhere (in any Organization, Organizational Unit, region, account). This design principle is called [separation of concerns](https://en.wikipedia.org/wiki/Separation_of_concerns).

Atmos separates the components (logic) and stacks (configuration of the components for different environments) so they can be independently managed and evolved. In the case of using Terraform, the components are [Terraform root modules](https://developer.hashicorp.com/terraform/language/modules#the-root-module).

In many cases, with enterprise-grade infrastructures (multi-org, multi-tenant, multi-account, multi-region, multi-team), the configuration is much more complicated than the code. That's what Atmos is trying to solve - to make the configuration manageable, reusable (by using [Imports](/core-concepts/stacks/imports), [Inheritance](/core-concepts/stacks/inheritance), and other Atmos features) and [DRY](https://en.wikipedia.org/wiki/Don%27t_repeat_yourself), and to make the code completely generic.

In this Quick Start guide, we describe how to provision infrastructure managed by Terraform into different AWS environments. Atmos manages the configurations for the environments.

<PrimaryCTA to="/quick-start/simple">
    Try our Simple Tutorial
</PrimaryCTA>

<SecondaryCTA to="/quick-start/advanced">
    Try our Advanced Tutorial
</SecondaryCTA>

---

## Thinking Like Atmos

import Link from '@docusaurus/Link'
import KeyPoints from '@site/src/components/KeyPoints'
import Screengrab from '@site/src/components/Screengrab';
import Step from '@site/src/components/Step'
import StepNumber from '@site/src/components/StepNumber'
import Intro from '@site/src/components/Intro'
import ActionCard from '@site/src/components/ActionCard'
import PrimaryCTA from '@site/src/components/PrimaryCTA'

<Intro>
    Atmos can change how you think about the Terraform code you write to build your infrastructure.

    When you design cloud architectures with Atmos, you will first break them apart into pieces called components. Then, you will implement Terraform "root modules" for each of those components. Finally, compose your components in any way you like using stacks, without the need to write any code or messy templates for code generation.
</Intro>

In this tutorial, we’ll guide you through the thought process of building Terraform "root modules" that are suitable for use as components. To ensure your components are highly reusable, parameterize them with variables.
Design them to serve a single purpose, making them the smallest possible unit of infrastructure in a typical
software development lifecycle (SDLC). Group pieces that usually change together, and separate those that change independently.

You're about to discover a new way to think about terraform...

<Link
    to="/quick-start/simple"
    className="button button--lg button--primary">
    Try our Simple Tutorial
</Link>

<Link
    to="/core-concepts"
    className="button button--lg button--outline button--primary ml20">
    Learn Atmos
</Link>

 Your Architecture is Made up of Components

Start by thinking about your architecture in terms of its logical components.

For example, let’s consider the architecture of a typical modern website. Its components might include a network, a cluster, some applications running on it, a database to store data, a cache, and maybe some object storage.

```mermaid
---
title: Infrastructure Components of a Typical Website
---

graph TD

    subgraph Network
        style Network fill:none
        subgraph Cluster
            style Cluster fill:#ffffff15
            App1(Application 1)
            App2(Application 2)
        end
        LB[[Load Balancer]]
        DB[(Database)]
        Cache[(Cache)]
        ObjectStorage[(Object Storage)]
    end
    LB --> App1
    LB --> App2
    App1 --> DB
    App1 --> Cache
    App2 --> ObjectStorage
```

Each of these logical pieces is a component and typically changes independently.

They are loosely related and this relationship forms a stack. A stack logically combines components without tightly coupling them,
ensuring each component’s state is separate. This approach minimizes the blast radius of changes, keeps the state separate, and
allows teams to own components. Plus, it encourages reuse since there is a finite number of ways to express infrastructure.

Then, to implement this architecture with Atmos, you will usually follow these same five steps every time.

<details>
<summary>Why don't we just put everything into a single "Root module"?</summary>

The goal should be to build a loosely coupled [service-oriented architecture (SOA)](https://en.wikipedia.org/wiki/Service-oriented_architecture). Like with modern app design, we should avoid monolithic infrastructure code. Decoupling our infrastructure into [components](/core-concepts/components) ensures resilience and maintainability.

When we place all the infrastructure into a single "root module," we end up with a [Terralith](/terms/terralith), which is roughly [Stage-2 of the Terraform maturity path](/introduction/why-atmos/stage-2). All your eggs are in one basket.

- Massive blast radius for every change
- Brittle and prolonged plan/apply cycles
- All your state is in one place

Large root modules are less reusable across teams due to varying requirements, resulting in many specialized root modules without significant benefits. As these root modules grow, they become increasingly time-consuming to plan and apply. This can even lead to hitting cloud provider rate limits, rendering your infrastructure code undeployable.
</details>

## The Five Steps of Atmos

<Step>
    ### <StepNumber/> Start Your Project

    Create a solid foundation with a well-structured folder layout, embracing best practices and conventions for a consistently organized project. By convention, we recommend keeping all configurations separate from your components, which helps ensure your components' reusability across teams.

    <details>
    <summary>See Example</summary>
    <Screengrab title="Start your Project" command="# here's an example of what your folder structure will like..." slug="demo-stacks/start-your-project" />
    </details>

    <ActionCard>
        Projects are organized into a well-structured folder layout, embracing best practices and conventions for a consistently organized project.
        <PrimaryCTA to="/core-concepts/stacks">Learn About Projects</PrimaryCTA>
    </ActionCard>

</Step>

<Step>
    ### <StepNumber/> Write Your Components (e.g. Terraform Root Modules)

    For each component in your architecture, write a Terraform root module.

    Use your existing Terraform root modules or create new ones. As your component library grows, you’ll need to write fewer new components, enabling faster development cycles by reusing infrastructure components across projects.

    Try to reuse 80% of your components across projects, and only write new ones when necessary.

    ```mermaid
    ---
    title: Example of Typical Root Modules
    ---
    flowchart TB
        subgraph Network ["Network"]
            style Network fill:none
            VPC[VPC Network Root Module]
            LB[Load Balancer Root Module]
        end

        subgraph Platform ["App Platform"]
            style Platform fill:none
            Cluster[Cluster Root Module]
            App["Application Root Module(s)"]
        end

        subgraph Services ["Backing Services"]
            style Services fill:none
            DB[Database Root Module]
            Cache[Cache Root Module]
            ObjectStorage[Object Storage Root Module]
        end

    ```
    Each component stands alone as its own “root module.” Think of them as layers if it helps.

    What you should end up with are "root modules" for each of these pieces. Something like this...
    <details>
    <summary>See Example</summary>
    <Screengrab title="Write your Components" command="# Then write your terraform root modules..." slug="demo-stacks/write-your-components" />
    </details>

    <ActionCard>
        Components form the essential building blocks of your architecture. They are the smallest unit of infrastructure in a typical software development lifecycle (SDLC).
        <PrimaryCTA to="/core-concepts/components">Learn Atmos Components</PrimaryCTA>
    </ActionCard>
</Step>

<Step>
    ### <StepNumber/> Pass Values Between Components

    Use Terraform's native ability to read the remote state or configuration of any other component, for a "loosely coupled" architecture. Atmos provides [methods that make this easy](/core-concepts/share-data).

    ```mermaid
    classDiagram
        class Network["VPC (root module)"] {
            outputs: vpc_id
        }

        class TerraformStateBackend["Terraform State Backend (e.g S3)"] {
            vpc_id: 12345
        }

        class Cluster["Kubernetes Cluster (root module)"] {
            needs: vpc_id
        }

        Network --> TerraformStateBackend : Stores Output
        Cluster --> TerraformStateBackend : Reads Remote State
        Cluster ..> Network : Loosely Depends on

    ```

    <ActionCard>
        Passing state between components is how you build a loosely coupled architecture.
        <PrimaryCTA to="/core-concepts/share-data">Learn How</PrimaryCTA>
    </ActionCard>
</Step>

<Step>
    ### <StepNumber/> Configure Your Components with Stacks

    Configure your environments such as development, staging, and production—each tailored to different stages of the lifecycle, ensuring smooth transitions and robust deployment strategies. Use a combination imports and inheritance for a template-free way to keep your configurations DRY and enforce consistency across your stacks.

    <details>
    <summary>See Example</summary>
    <Screengrab title="Define your Stacks" command="# Configure your stacks using YAML... easily import and inherit settings" slug="demo-stacks/define-your-stacks" />
    </details>
    <ActionCard>
        Components are configured with Stacks, which are defined om YAML. Stacks are separate from Components (terraform root modules) to ensure root modules remain highly reusable across teams and promote testing.
        <PrimaryCTA to="/core-concepts/stacks">Learn Configuration</PrimaryCTA>
    </ActionCard>
</Step>

<Step>
    ### <StepNumber/> Deploy Stacks with Atmos 🚀

    Execute deployments with precision using Terraform's `plan` and `apply` commands, fully integrated with [native GitOps workflows](/integrations) through [GitHub Actions](/integrations/github-actions) for seamless deployment automation. Leverage [workflows](/core-concepts/workflows) to orchestrate more complicated deployments that involve multiple steps.

    ```mermaid
    graph LR
        Dev[Development]
        Stage[Staging]
        Prod[Production]

        Dev --> Stage
        Stage --> Prod
    ```

    <details>
    <summary>See Example</summary>
    <Screengrab title="Atmos Stacks" command="# Deploy your stacks with the console UI or using GitHub Actions" slug="demo-stacks/deploy" />
    </details>
    <ActionCard>
        Deploy Components with Atmos, fully integrated with GitOps workflows through GitHub Actions for seamless deployment automation.
        <PrimaryCTA to="/core-concepts/deploy">Deploying with Atmos</PrimaryCTA>
    </ActionCard>
</Step>

## Where to Go From Here

This brief introduction covered the essentials of designing cloud architectures that can be used with Atmos.

Now you can start with your first Atmos project! Try out our [Simple](/quick-start/simple)/[Advanced](/quick-start/advanced) Quick Start,
or delve deeper into the [syntax and concepts](/core-concepts) used in this tutorial.

<Link
    to="/quick-start/simple"
    className="button button--lg button--primary">
    Try our Simple Tutorial
</Link>

<Link
    to="/core-concepts"
    className="button button--lg button--outline button--primary ml20">
    Learn Atmos
</Link>

---

## Configure Atmos CLI(Simple)

import EmbedFile from '@site/src/components/EmbedFile'
import KeyPoints from '@site/src/components/KeyPoints'
import Screengrab from '@site/src/components/Screengrab'
import LatestRelease from '@site/src/components/LatestRelease'
import Step from '@site/src/components/Step'
import StepNumber from '@site/src/components/StepNumber'
import Intro from '@site/src/components/Intro'
import ActionCard from '@site/src/components/ActionCard'
import PrimaryCTA from '@site/src/components/PrimaryCTA'
import SecondaryCTA from '@site/src/components/SecondaryCTA'
import Note from '@site/src/components/Note'

<Intro>
The `atmos.yaml` configuration file controls the behavior of the `atmos` CLI and how Atmos will work with your project.
</Intro>

Therefore, this file should exist in the project repository alongside your Terraform components and Atmos stacks. It's also where you can [configure integrations](/integrations), like with our [GitHub Actions](/integrations/github-actions).

<KeyPoints>
- How install `atmos` and make sure you're on a current version
- How to configure `atmos.yaml` for your project's filesystem layout
- How Atmos identifies stack configurations using context variables and naming patterns
</KeyPoints>

<Step>
## <StepNumber/> Install Atmos
Let's ensure you've [properly installed Atmos](/install) by running the following command.

```bash
atmos version
```

You should see something like this...
<Screengrab title="atmos help" slug="atmos-version" />
<Note>the current release of Atmos is **<LatestRelease />**</Note>
</Step>

<Step>
## <StepNumber/> Configure `atmos.yaml` for your project
To configure Atmos to work with your project, we'll create a file called `atmos.yaml` to tell Atmos where to find the
Terraform components and Atmos stacks. Almost everything in Atmos is configurable via this file.

Below is the minimum recommended configuration for Atmos to work with Terraform and to configure [Atmos components](/core-concepts/components)
and [Atmos stacks](/core-concepts/stacks). Copy this YAML config below into your `atmos.yaml` file.

<EmbedFile filePath="examples/demo-stacks/atmos.yaml" />

<Note>For the description of all the sections, refer to [CLI Configuration](/cli/configuration).</Note>
</Step>

<Step>
## <StepNumber/> Understand what it's doing

And here's what all that means...

### Basic Settings

<dl>
  <dt>`stacks.name_pattern`</dt>
  <dd>
    Atmos uses “slugs” to refer to stacks, which are defined by the `name_pattern` setting.
    Instead of relying on file names and locations, which can change, Atmos uses context variables (`namespace`, `tenant`, `environment`, `stage`)
    to identify the stack. For example, with the command `atmos terraform apply myapp -s dev`,
    Atmos interprets the slug `dev` using the pattern `{stage}` to locate the correct stack configuration
    in the stacks directory.
  </dd>

  <dt>`logs.level`</dt>
  <dd>
    Set to `Info` to see the most helpful logs. You can also set it to `Trace` to see all the logs, which is helpful for debugging.
  </dd>

  <dt>`logs.file`</dt>
  <dd>
    Set to `/dev/stderr` to send all of Atmos output to the standard error stream. This is useful when running Atmos in a CI/CD pipeline.
  </dd>
</dl>

### Path Configuration

Well-known paths are how Atmos finds all your stack configurations, components, and workflows. Here are the essential paths that you need to configure:

<dl>
  <dt>`base_path`</dt>
  <dd>The base path for components, stacks, and workflow configurations. We set it to `./` so it will use the current working directory. Alternatively, we can override this behavior by setting the ENV var `ATMOS_BASE_PATH` to point to another directory location.</dd>

  <dt>`components.terraform.base_path`</dt>
  <dd>The base path to the Terraform components (Terraform root modules). As described in [Configure Repository](/quick-start/advanced/configure-repository), we've decided to put the Terraform components into the `components/terraform` directory, and this setting tells Atmos where to find them. Atmos will join the base path (set in the `ATMOS_BASE_PATH` ENV var) with `components.terraform.base_path` to calculate the final path to the Terraform components</dd>

  <dt>`stacks.base_path`</dt>
  <dd>The base path to the Atmos stacks. As described in [Configure Repository](/quick-start/advanced/configure-repository), we've decided to put the stack configurations into the `stacks` directory, and this setting tells Atmos where to find them. Atmos will join the base path (set in the `ATMOS_BASE_PATH` ENV var) with `stacks.base_path` to calculate the final path to the stacks</dd>

  <dt>`stacks.included_paths`</dt>
  <dd>List of file paths to the top-level stacks in the `stacks` directory to include in search when Atmos searches for the stack where the component is defined when executing `atmos` commands</dd>

  <dt>`stacks.excluded_paths`</dt>
  <dd>List of file paths to the top-level stacks in the `stacks` directory to exclude from search when Atmos searches for the stack where the component is defined when executing `atmos` commands</dd>

  <dt>`workflows.base_path`</dt>
  <dd>The base path to Atmos [Workflows](/core-concepts/workflows) files</dd>
</dl>

<details>
<summary>Advanced Options</summary>
<dl>
  <dt>`components.terraform.apply_auto_approve`</dt>
  <dd>if set to `true`, Atmos automatically adds the `-auto-approve` option to instruct Terraform to apply the plan without
  asking for confirmation when executing `terraform apply` command</dd>

  <dt>`components.terraform.deploy_run_init`</dt>
  <dd>if set to `true`, Atmos runs `terraform init` before executing [`atmos terraform deploy`](/cli/commands/terraform/deploy) command</dd>

  <dt>`components.terraform.init_run_reconfigure`</dt>
  <dd>if set to `true`, Atmos automatically adds the `-reconfigure` option to update the backend configuration when executing `terraform init` command</dd>

  <dt>`components.terraform.auto_generate_backend_file`</dt>
  <dd>if set to `true`, Atmos automatically generates the Terraform backend file from the component configuration when executing `terraform plan` and `terraform apply` commands</dd>

  <dt>`commands`</dt>
  <dd>configuration for [Atmos Custom Commands](/core-concepts/custom-commands)</dd>

  <dt>`schemas`</dt>
  <dd>
  [JSON Schema](https://json-schema.org/) and [OPA Policy](https://www.openpolicyagent.org/) configurations for:
  - [Atmos Manifests Validation](/cli/schemas)
  - [Atmos Stack Validation](/core-concepts/validate)
  </dd>
</dl>

</details>

### Config File Location

While placing `atmos.yaml` at the root of the repository will work for the `atmos` CLI, it will not work
for [Component Remote State](/core-concepts/share-data/remote-state) because it uses
the [terraform-provider-utils](https://github.com/cloudposse/terraform-provider-utils) Terraform provider. Terraform executes the provider from the
component's folder (e.g. `components/terraform/vpc`), and we don't want to replicate `atmos.yaml` into every component's folder.

Both the `atmos` CLI and [terraform-provider-utils](https://github.com/cloudposse/terraform-provider-utils) Terraform provider use the same `Go` code,
which try to locate the [CLI config](/cli/configuration) `atmos.yaml` file before parsing and processing [Atmos stacks](/core-concepts/stacks).

This means that `atmos.yaml` file must be at a location in the file system where all processes can find it.

<details>
<summary>How is the `atmos.yaml` file located?</summary>

`atmos.yaml` is loaded from the following locations (from lowest to highest priority):

- System dir (`/usr/local/etc/atmos/atmos.yaml` on Linux, `%LOCALAPPDATA%/atmos/atmos.yaml` on Windows)
- Home dir (`~/.atmos/atmos.yaml`)
- Current directory
- ENV var `ATMOS_CLI_CONFIG_PATH`

:::note

Initial Atmos configuration can be controlled by these ENV vars:

<dl>
  <dt>`ATMOS_CLI_CONFIG_PATH`</dt>
  <dd>Directory that contains the `atmos.yaml` (just the folder without the file name). It's not possible to change the filename at this time.</dd>

  <dt>`ATMOS_BASE_PATH`</dt>
  <dd>Base path to the `components/` and `stacks/` folders.</dd>
</dl>
:::
</details>
</Step>

<ActionCard title="Ready to take the next step?">
    Now you're ready to learn how to write your first Atmos component using Terraform.

      <PrimaryCTA to="/quick-start/simple/write-components">Next Step</PrimaryCTA>
      <SecondaryCTA to="/cli/configuration">Deep Dive</SecondaryCTA>

</ActionCard>

---

## Start Your Project

import KeyPoints from '@site/src/components/KeyPoints'
import Intro from '@site/src/components/Intro'
import ActionCard from '@site/src/components/ActionCard'
import PrimaryCTA from '@site/src/components/PrimaryCTA'
import SecondaryCTA from '@site/src/components/SecondaryCTA'

<Intro>
The folder structure for an Atmos project is designed to organize your infrastructure effectively on the file system. It separates configuration from code, ensuring your Terraform root modules are distinct and reusable.
</Intro>

While you can customize the structure using the `atmos.yaml` configuration, we will start with a simple layout to get you started.

<KeyPoints>
- Why Atmos works best with monorepos
- How to structure your project repository on the filesystem
- Where to put your Terraform "root modules" and Stack configurations
</KeyPoints>

Atmos works best with [monorepo](https://en.wikipedia.org/wiki/Monorepo) infrastructure repositories when managing the configurations for components
and stacks. For example, multiple monorepos can also be used for different teams or products. Monorepos are recommended because they provide a single source of truth for all configurations and simplify infrastructure management.

<details>
<summary>What is a monorepo?</summary>

A "monorepo" is a version-controlled repository that stores all the code, configurations and scripts for the entire infrastructure composed of individual components with independent lifecycles. Monorepos usually improve collaboration, CI/CD build speed, and overall productivity. A monorepo should not be confused with a [monolith](https://en.wikipedia.org/wiki/Monolithic_application), which is a single, often large, codebase for an application.

Polyrepo architectures consist of several version-controlled repositories for code, configurations and scripts for different parts of the infrastructure. For example, depending on various requirements (including security, lifecycle management, access control, audit, etc.), separate repositories can be used to manage infrastructure per account (e.g. `dev`, `staging`, `prod`), per service, or per team.
</details>

## Filesystem Layout

Here's what it will look like on the filesystem:

```console
   │   # Atmos CLI configuration
   ├── atmos.yaml
   │  
   │   # Centralized stacks configuration
   ├── stacks/
   │   ├── <stack_1>.yaml
   │   ├── <stack_2>.yaml
   │   └── <stack_3>.yaml
   │  
   │   # Centralized components configuration. Components are broken down by tool
   └── components/
       └── terraform/   # Terraform root modules
           └── myapp/
```

Ultimately, the paths are all configurable via the `atmos.yaml` CLI Configuration file, which we'll cover in the next chapter.

## Common Directories and Files

Atmos requires a few common directories and files, which need to be configured in the infrastructure repo:

<dl>
  <dt>`components/` directory (required)</dt>
  <dd>contains centralized component configurations</dd>

  <dt>`stacks/` directory (required)</dt>
  <dd>contains centralized stack configurations</dd>

  <dt>`atmos.yaml` (required)</dt>
  <dd>Atmos CLI config file</dd>

</dl>

:::tip

The source code for this Quick Start guide can be found in the [Atmos repo](https://github.com/cloudposse/atmos/tree/main/examples/quick-start-simple) demo repository. (It's similar to the demo you see on the Atmos landing page)

You can clone it and tweak it to your own needs. The example should be a good start for getting familiar with Atmos.

:::

Atmos separates code from configuration ([separation of concerns](https://en.wikipedia.org/wiki/Separation_of_concerns)). All the code is stored in the `components` directories, and the configurations for different environments are stored in the `stacks` directory. This allows the code (Terraform root modules and Helmfile components) to be environment-agnostic, meaning the components don't know and don't care how and where they will be provisioned. They can be provisioned into many accounts and regions - the configurations for different environments are defined in the `stacks` directory.

:::note
While it's recommended to use the directory names as shown above, the `stacks` and `components` directory names and filesystem locations are configurable in the `atmos.yaml` CLI config file. Refer to [Configure CLI](/quick-start/advanced/configure-cli) for more details.
:::

The following example provides the simplest filesystem layout that Atmos can work with:

```console
   │   # Centralized stacks configuration
   ├── stacks/
   │   ├── <stack_1>.yaml
   │   ├── <stack_2>.yaml
   │   └── <stack_3>.yaml
   │  
   │   # Centralized components configuration. Components are broken down by tool
   ├── components/
   │   └── terraform/   # Terraform root modules
   │       ├── <terraform_root_module_1>/
   │       ├── <terraform_root_module_2>/
   │       └── <terraform_root_module_3>/
   │
   │   # Atmos CLI configuration
   ├── atmos.yaml
   │
   │   # Atmos vendoring configuration
   └── vendor.yaml
```

<ActionCard title="Ready to take the next step?">
    Now you're ready to learn how to configure the Atmos CLI to work with your project.

      <PrimaryCTA to="/quick-start/simple/configure-cli">Next Step</PrimaryCTA>
      <SecondaryCTA to="/core-concepts/projects">Deep Dive</SecondaryCTA>

</ActionCard>

---

## Use Your Component in a Stack

import File from '@site/src/components/File'
import KeyPoints from '@site/src/components/KeyPoints'
import Intro from '@site/src/components/Intro'
import EmbedFile from '@site/src/components/EmbedFile'
import ActionCard from '@site/src/components/ActionCard'
import PrimaryCTA from '@site/src/components/PrimaryCTA'
import Note from '@site/src/components/Note'

<Intro>
[Atmos Stacks](/core-concepts/stacks) are the configuration for your components. It's how you can combine multiple reusable, stateful
components into your "Stack" that you depend on.
</Intro>

If you think of your "components" as applications, then Stacks are simply which components you depend on and the settings that you want
to pass to them. If you make your components highly reusable, it usually means they will need to accept a lot of configuration.

**This is what makes Atmos so powerful:** you can import and inherit configurations in a logical way to keep your configurations DRY and consistent.

<KeyPoints>
- How to specify the configuration for your Terraform "root modules" using Atmos Stacks
- How to organize Atmos Stacks into a Service Catalog
- How to use imports and inheritance for DRY configurations
- How Atmos identifies components using context variables and naming patterns
</KeyPoints>

Stack configurations are merely all the settings for your components. They can be organized in any way you like, but we recommend a hierarchical layout. We share some [different ways to organize your stacks in the Catalog](/core-concepts/stacks/catalogs), but for this example, we will use a simple layout.

## Create Catalog for Reusable Configurations

Atmos supports the concept of a [Service Catalog](/core-concepts/stacks/catalogs), which is where you can define all
of your default configurations.

All the common default settings for each Atmos component should be in a separate file in the `stacks/catalog` directory.
The file then gets imported into the parent Atmos stacks. This makes the stack configurations DRY by reusing the component's config that is common for all environments.

Refer to [Stack Imports](/core-concepts/stacks/imports) for more details on Atmos imports.

These Atmos component manifests will be imported into the top-level Atmos stacks. The default variables (in the `vars` sections)
can be overridden in the derived Atmos components by using [Atmos Component Inheritance](/core-concepts/stacks/inheritance).

## Atmos Top-level Stacks

When executing the [CLI commands](/cheatsheets/commands), Atmos does not use the stack file names and their filesystem locations to search for the stack where the component is defined. Instead, Atmos uses the context variables (`namespace`, `tenant`, `environment`, `stage`) to search for the stack. The stack config file names (stack manifest names) can be anything, and they can be in any folder in any sub-folder in the `stacks` directory.

For example, when executing the `atmos terraform apply station -s dev` command, the Atmos stack `dev` is specified by the `-s` flag. By looking at `name_pattern: "{stage}"` (see [Configure CLI](/quick-start/advanced/configure-cli)) and processing the tokens, Atmos knows that the `stage` is `dev`. This `name_pattern` also supports `{namespace}`, `{tenant}`, and `{environment}`.

<dl>
  <dt>`{namespace}`</dt>
  <dd>Corresponds to `var.namespace` in the stack configuration.</dd>

  <dt>`{tenant}`</dt>
  <dd>Corresponds to `var.tenant` in the stack configuration.</dd>

  <dt>`{environment}`</dt>
  <dd>Corresponds to `var.environment` in the stack configuration.</dd>

  <dt>`{stage}`</dt>
  <dd>Corresponds to `var.stage` in the stack configuration.</dd>
</dl>

Atmos top-level stacks can be configured using a Basic Layout or a Hierarchical Layout.

The Basic Layout can be used when you have a very simple configuration using just a few stages. A more Hierarchical Layout should be used when you have a very complex organization, for example, with many AWS Organizational Units (which Atmos refers to as tenants) and dozens of AWS accounts and regions.

### Basic Layout

A basic form of stack organization is to follow the pattern of naming where each `$environment-$stage.yaml` is a file. This works well until you have so many environments and stages.

For example, `$stage` might be `prod`, which would result in `stacks/deploy/prod.yaml`

In our example, the filesystem layout for the stacks uses a Basic Layout with `dev`, `staging` and `prod` and
would look like this:

<File title="infra-live/" icon="fa-solid fa-folder">
```console
├── atmos.yaml
├── components/
│   └── terraform/
│       └── weather/
│           ├── README.md
│           ├── main.tf
│           ├── outputs.tf
│           ├── variables.tf
│           └── versions.tf
│
│   # Centralized stacks configuration
└── stacks/
    ├── catalog/
    │   └── station.yaml
    └── deploy/
        ├── dev.yaml
        ├── prod.yaml
        └── staging.yaml
```
</File>

## Create Stack Configurations for Deployment

Since this is a simple Quick Start, we will just use a single Terraform component (`components/terraform/weather`); in a real-world scenario, you may have dozens of components that comprise your stack. Rest assured, the process is the same for all components.

### Define the Baseline Configuration
We’ll start by defining the baseline configuration of our Terraform root module to gather weather data. The name of our component’s configuration doesn’t need to match the name of the component folder. This allows us to deploy multiple instances of our component using different names.

```yaml
components:
  terraform:
    <name-of-component>:
```

To specify which component to use, set the `metadata.component` property to the path of the component's directory, relative to the `components.base_path` as defined in the `atmos.yaml`. In our case, the `components.base_path` is `components/terraform`, so we can simply specify `weather` as the path.

```yaml
components:
  terraform:
    station:
      metadata:
        component: weather
```
<Note title="Pro Tip">To support multiple versions of a component, simply place each component version in a subfolder, which is the typical convention for versioning in a monorepo. For example, `components/terraform/weather/v1`. The goal for maintainable infrastructure as code is to have as few versions as possible and to have all environments converge on the same version.</Note>

So the complete baseline definition for our weather station configuration might look something like the following.

<EmbedFile filePath="examples/quick-start-simple/stacks/catalog/station.yaml" />

<Note>It's important to quote strings that start with zeros (e.g., `0001`) in YAML to prevent them from being interpreted as integers. This is how YAML processes it, and not something special about Atmos.</Note>

### Define the Environment-specific Configurations

Next, we’ll define the environment-specific configurations for our Terraform root module. We’ll create a separate file for each environment and stage. In our case, we have three environments: `dev`, `staging`, and `prod`.

When Atmos processes this stack config, it will first import and deep-merge all the variables from the imported files, then overlay the inline configuration. While the order of keys in a YAML map doesn’t affect behavior, lists are strictly ordered. Therefore, the order of `imports` is important.

#### Define the `dev` Environment Configuration

In the `dev` stack configuration, Atmos first processes the `imports` in the order defined. Then, it applies the globals `vars` defined in the top-level section. Only include `vars` in the globals that are true for every single component in the stack. If that's not the case, define them on a per-component basis.

For example, by setting `var.stage` to `dev` at a global level, we assume that every component in this stack will have a stage variable.

Finally, in the component-specific configuration for the `station`, we set the fine-tuned parameters for this environment. Everything else gets inherited from its baseline configuration. There are no strict rules about where to place configurations. Organize them in a way that makes logical sense for your infrastructure’s data model.

<EmbedFile filePath="examples/quick-start-simple/stacks/deploy/dev.yaml" />

#### Define the `staging` Environment Configuration

The staging stack configuration is almost identical to the dev stack. The only changes are the location and language settings. Everything else stays the same as the baseline configuration, ensuring that the staging and dev environments are very similar.

<EmbedFile filePath="examples/quick-start-simple/stacks/deploy/staging.yaml" />

#### Define the `prod` Environment Configuration

And finally, we have the production (`prod`) stack. Much like the `staging` and `dev` stacks, it’s very similar to the baseline configuration but with some parameter changes. In this setup, all three stacks are separate, share the same component, and vary the station's parameters as needed.

<EmbedFile filePath="examples/quick-start-simple/stacks/deploy/prod.yaml" />

Now, we're finally ready to deploy our components. We'll show you how to do this in the next step.

<ActionCard title="Want to go deeper on this topic?">
    You can do so much more with stacks! We're just scratching the surface. If you want [learn about imports, inheritance, templating, etc](/core-concepts/stacks), check out the Core Concepts.
    <PrimaryCTA to="/core-concepts/stacks">Learn More</PrimaryCTA>
</ActionCard>

---

## Deploy Another App

import File from '@site/src/components/File'
import Intro from '@site/src/components/Intro'
import ActionCard from '@site/src/components/ActionCard'
import PrimaryCTA from '@site/src/components/PrimaryCTA'
import Note from '@site/src/components/Note'

<Intro>
We can provision more than one instance of the same Terraform component (with the same or different settings)
into the same environment by defining many Atmos components that provide configuration for the Terraform component.
</Intro>

For example, the following config shows how to define two Atmos components, `station/1` and `station/2`, which both point to the same Terraform component `components/terraform/weather`.

<Note>The `station/1` and `station/2` naming convention doesn't have any inherent meaning. Atmos simply interprets them as two different strings. Feel free to use whatever format you prefer.</Note>

<File title="stacks/deploy/dev.yaml">
```yaml
vars:
  stage: dev

import:
  # Import the baseline for all station components
  - catalog/station

components:
  terraform:
    # Atmos component `station/1`
    station/1:
      metadata:
        # Point to the Terraform component in `components/terraform/weather`
        component: weather
        # Inherit the defaults for all station components
        inherits:
          - station
      # Define/override variables specific to this `station/1` component
      vars:
        name: station-1

    # Atmos component `station/2`
    station/2:
      metadata:
        # Point to the Terraform component in `components/terraform/weather`
        component: weather
        # Inherit the defaults for all station components
        inherits:
          - station
      # Define/override variables specific to this `station/2` component
      vars:
        name: station-2
```
</File>

In this example, we've included more information than necessary to demonstrate the concept. For instance, we explicitly added `inherits` to show how you can use multiple inheritance to merge multiple baseline configurations. We also specified the component path in both instances, even though it's already defined in the baseline configuration. This redundancy is just to emphasize that both are pointing to the same component.

Then we can execute the following `atmos` commands to provision the two stations into the `dev` environment:

```shell
# Provision the first weather station
atmos terraform apply station/1 -s dev
# Provision the second weather station
atmos terraform apply station/2 --stack dev
```
<Note>You can use the shorthand `-s` flag to specify the stack name, or the long form `--stack`. They both do the same thing</Note>

Sweet! You’ve just finished your first Atmos QuickStart tutorial. Now you're at a crossroads with two options: you can continue to the bonus materials for some extra credit, or dive deeper into the core concepts and learn the Atmos Fundamentals.

---

## Add Custom Commands(Extra-credit)

import Terminal from '@site/src/components/Terminal'
import File from '@site/src/components/File'
import Intro from '@site/src/components/Intro'
import ActionCard from '@site/src/components/ActionCard'
import PrimaryCTA from '@site/src/components/PrimaryCTA'
import Note from '@site/src/components/Note'

<Intro>
Atmos can be easily extended to support any number of custom CLI commands. For example, imagine you wanted to add a command
like `atmos reload-database`, you can do that with custom commands.
</Intro>

Custom commands are defined in the `commands` section in `atmos.yaml` CLI configuration file.

<Note title="TIP">
Refer to [Atmos Custom Commands](/core-concepts/custom-commands) for more information about Atmos Custom Commands
</Note>

In this Quick Start guide, we'll define two custom commands to list the Atmos stacks in the infrastructure and the components in the stacks.

<File title="atmos.yaml">
```yaml
# Custom CLI commands
commands:
- name: ip
  description: Return my current IP
  steps:
    - curl -s https://ifconfig.me
    - echo

# Use Nested Custom Commands
- name: "github"
  commands:
  - name: "status"
    description: This command returns the number of stargazers for a GitHub repository
    steps:
      - curl -s https://www.githubstatus.com/api/v2/status.json | jq -r .status.description
```
</File>

Run the following Atmos command to get the current GitHub status.

<Terminal command="atmos github status">
```console
All Systems Operational
```
</Terminal>

Run the following Atmos command to retrieve your current public IP address.

<Terminal command="atmos ip">
```console
13.37.13.37
```
</Terminal>

<ActionCard title="Want to go deeper on this topic?">
    Custom commands can accept flags, arguments, and even advanced templating.
    <PrimaryCTA to="/core-concepts/custom-commands">Learn More</PrimaryCTA>
</ActionCard>

---

## Automate Common Workflows(Extra-credit)

import Terminal from '@site/src/components/Terminal'
import Step from '@site/src/components/Step'
import StepNumber from '@site/src/components/StepNumber'
import Intro from '@site/src/components/Intro'
import ActionCard from '@site/src/components/ActionCard'
import PrimaryCTA from '@site/src/components/PrimaryCTA'

<Intro>
[Atmos Workflows](/core-concepts/workflows) combine multiple commands into executable units of work, ideal for automating common
tasks and orchestrating “cold starts,” where you bring an environment up from scratch.
</Intro>

Workflows can call other workflows and be combined with [Custom Commands](/core-concepts/custom-commands). Usually, we use
workflows to provision some combinations of [Terraform](/core-concepts/components/terraform) components.

Defining workflows is entirely optional; use them if they are helpful for your project.

<Step>
## <StepNumber/> Configure Your Project to Support Workflows

To define workflows, update your `atmos.yaml` to tell it where to find your workflows.

Add the following `workflows` section and configure the base path to the workflows:

```yaml
workflows:
  # Can also be set using 'ATMOS_WORKFLOWS_BASE_PATH' ENV var, or '--workflows-dir' command-line arguments
  # Supports both absolute and relative paths
  base_path: "stacks/workflows"
```
</Step>

<Step>
## <StepNumber/> Create an Atmos Workflow

Now, let's create a workflow in the `stacks/workflows` directory.

```console
   │   # Centralized stacks configuration
   └── stacks/
       └── workflows/
           └── weather.yaml
```

Add the following Atmos workflows to the `stacks/workflows/weather.yaml` file:

```yaml
name: Workflows for Weather Station
description: Atmos workflows for managing Weather Station

workflows:

  plan-all:
    description: |
      Run 'terraform plan' on all 'station' components in all stacks
    steps:
      - command: terraform plan station -s dev
      - command: terraform plan station -s staging
      - command: terraform plan station -s prod

  apply-all:
    description: |
      Run 'terraform apply' on all 'station' components in all stacks
    steps:
      - command: terraform apply station -auto-approve -s dev
      - command: terraform apply station -auto-approve -s staging
      - command: terraform apply station -auto-approve -s prod
```
</Step>

<Step>

## <StepNumber/> Run the Atmos Workflow

Run the following Atmos commands to execute the workflows:

```shell
# Execute the workflow `plan-all-vpc-flow-logs` from the workflow manifest `weather.yaml`
atmos workflow plan-all -f weather

# Execute the workflow `apply-all-components` from the workflow manifest `weather.yaml`
atmos workflow apply-all -f weather
```

The `atmos workflow` CLI command supports the `--dry-run` flag. If passed, the command will just print information about
the executed workflow steps without executing them. For example:

<Terminal title="atmos workflow plan-all -f weather --dry-run">
```console
Executing the workflow 'plan-all' from 'stacks/workflows/weather.yaml'

Executing workflow step: terraform plan station -s dev
Executing workflow step: terraform plan station -s staging
Executing workflow step: terraform plan station -s prod
```
</Terminal>
</Step>

:::tip
Refer to [atmos workflow](/cli/commands/workflow) for more information on the `atmos workflow` CLI command
:::

<ActionCard title="Want to go deeper on this topic?">
    Workflows can do a lot more than we're showing here. Workflows support templates, and can call other workflows.
    <PrimaryCTA to="/core-concepts/workflows">Learn More</PrimaryCTA>
</ActionCard>

---

## Extra Credit!

import KeyPoints from '@site/src/components/KeyPoints'
import DocCardList from '@theme/DocCardList'
import Intro from '@site/src/components/Intro'

<Intro>
Now we want to provide you with some bonus material because we didn’t want to overwhelm you when you were just getting started.
These are common ways we use Atmos daily in the infrastructures we manage.
</Intro>

<KeyPoints>
- How to deploy additional instances of your components.
- How to automate repetitive operations using workflows.
- How to extend the Atmos CLI by adding your own custom commands, making it easy for developers to rely on a single command.
- How to vendor all your external dependencies, which can include other components, stack configurations, and really just anything!
</KeyPoints>

## What's Included

<DocCardList/>

---

## Vendor Dependencies

import Terminal from '@site/src/components/Terminal'
import File from '@site/src/components/File'
import Intro from '@site/src/components/Intro'
import Note from '@site/src/components/Note'
import ActionCard from '@site/src/components/ActionCard'
import PrimaryCTA from '@site/src/components/PrimaryCTA'
import Step from '@site/src/components/Step'
import StepNumber from '@site/src/components/StepNumber'

<Intro>
In the previous steps, we wrote our own component from scratch. In practice, you'll often want to reuse components that are already available in the open-source community, or within your organization. To do that, you'll leverage vendoring.
</Intro>

Vendoring is driven by the `atmos vendor pull` command. It allows you to pull in components from remote sources, like Git repositories, and place them in your project's filesystem. This way, you can reuse components across multiple projects. The vendoring process is defined in the `vendor.yaml` file, which specifies the components to pull in, where to place them, and how to filter them.

<Note>
For more information about Atmos Vendoring and the `atmos vendor pull` CLI command, refer to:
- [Atmos Vendoring](/core-concepts/vendor)
- [atmos vendor pull](/cli/commands/vendor/pull)
</Note>

Let's kick the tires on this, by vendoring some more components from the [`demo-library`](https://github.com/cloudposse/atmos/tree/main/examples/demo-library).

<Step>
## <StepNumber/> Create Vendor Manifest

Create a `vendor.yaml` Atmos vendor config file in the root of the repo with the following content:

<File title="vendor.yaml">
```yaml
apiVersion: atmos/v1
kind: AtmosVendorConfig
metadata:
  name: my-vendor-config
  description: Example Atmos vendoring manifest
spec:
  sources:
    - component: "ipinfo"
      source: "github.com/cloudposse/atmos.git//examples/demo-library/ipinfo?ref={{.Version}}"
      version: "main"
      targets:
        - "components/terraform/{{.Component}}"
      included_paths:
        - "**/*.tf"
        # If the component's folder has the `modules` sub-folder, it needs to be explicitly defined
        - "**/modules/**"
```
</File>

Let's unpack what's going on here. The `sources` section defines a list of components to pull in. Each component has the following attributes:
<dl>
  <dt>`component`</dt>
  <dd>The name of the component to pull in.</dd>

  <dt>`source`</dt>
  <dd>The source of the component. It can be a Git repository URL with an optional reference to a specific version. The `{{.Version}}` token is a placeholder for the version of the component. In this example, we're pulling in the `ipinfo` component from the `atmos` repository from the `examples/demo-library` folder.</dd>

  <dt>`version`</dt>
  <dd>The version (git ref) of the component to pull in. In this example, we're pulling in the `main` branch, but in practice you will usually pin to a tag or major release.</dd>

  <dt>`targets`</dt>
  <dd>The target directories where the component will be placed. In this example, we're placing the `ipinfo` component in the `components/terraform/ipinfo` directory, because we reference the `{{ .Component }}`</dd>

  <dt>`included_paths`</dt>
  <dd>
  A list of file paths to include in the vendored component. In this example, we're including all `.tf` files and any files in the `modules` sub-folder.
    <Note>
  The `glob` library that Atmos uses to download remote artifacts does not treat the double-star `**` as including sub-folders.
  If the component's folder has sub-folders, and you need to vendor them, they have to be explicitly defined as in the following example.
  </Note>
  </dd>

  <dt>`excluded_paths`</dt>
  <dd>A list of file paths to exclude from the vendored component.</dd>

</dl>

If you have a lot of dependencies, it can be helpful to break them down into smaller, more manageable pieces. You can do this by importing other vendoring configurations. Just define the `imports` section in the `vendor.yaml` file.

<File title="vendor.yaml">
```yaml
apiVersion: atmos/v1
kind: AtmosVendorConfig
metadata:
  name: my-vendor-config
  description: Example Atmos vendoring manifest
spec:
  imports: vendor/**.yaml
```
</File>

In this example, could add additional vendoring configurations in the `vendor/` directory, and they will be imported into the main vendoring configuration.

</Step>

<Step>
  ## <StepNumber/> Pull the Components
  Execute the command `atmos vendor pull` from the root of the repo

  <Terminal command="atmos vendor pull">
  ```shell
  Processing vendor config file 'vendor.yaml'

  Pulling sources for the component 'ipinfo'
  from 'github.com/cloudposse/atmos.git//examples/demo-library/ipinfo?ref=main'
  into 'components/terraform/ipinfo'
  ```
  </Terminal>

  After the command completes, the filesystem layout should look like this (in addition to any other files you already had):

  ```console
     │
     ├── stacks/
     │
     └── components/
         └── terraform/   # Terraform root modules
             └── ipinfo/  # Vendored component
                 ├── main.tf
                 ├── outputs.tf
                 ├── providers.tf
                 ├── variables.tf
                 └── versions.tf
  ```
</Step>

<Step>
  ## <StepNumber/> Configure Component in Stack

  Now that the component is vendored, you can use it in your stack configurations. For example, you can include the `ipinfo` component in your stack configuration like this:

  <File title="stacks/catalog/ipinfo.yaml">
  ```yaml
  terraform:
    components:
      ipinfo:
        metadata:
          component: ipinfo
        vars: {}
  ```
  </File>
</Step>

<ActionCard title="Want to go deeper on this topic?">
    Vendoring is a critical concept to master, if you want to reuse components across multiple projects.
    <PrimaryCTA to="/core-concepts/vendor">Learn More</PrimaryCTA>
</ActionCard>

---

## Deploy Everything

import KeyPoints from '@site/src/components/KeyPoints'
import Intro from '@site/src/components/Intro'
import ActionCard from '@site/src/components/ActionCard'
import PrimaryCTA from '@site/src/components/PrimaryCTA'
import SecondaryCTA from '@site/src/components/SecondaryCTA'

<Intro>
After you've written your components and configured your stacks, now we're ready to deploy them!
</Intro>

<KeyPoints>
- How Atmos identifies components using context variables and naming patterns
- How to preview your configurations and changes before applying them
- How to deploy your components, one at a time
</KeyPoints>

## Provision Atmos Components into all Stacks

Provision the `station` Atmos component into the stacks:

```shell
atmos terraform apply station -s dev

atmos terraform apply station -s staging

atmos terraform apply station -s prod
```

Alternatively, you can execute the configured [Atmos workflow](/quick-start/advanced/create-workflows) to provision all the components in all the stacks:

```shell
# Execute the workflow `apply-all-components` from the workflow manifest `networking`
atmos workflow apply-all-components -f networking
```

## Stack Search Algorithm

Looking at the commands above, you might have a question "How does Atmos find the component in the stack and all the variables?"

Let's consider what Atmos does when executing the command `atmos terraform apply station -s prod`:

- Atmos uses the `stacks.name_pattern` defined in the [CLI config](/quick-start/advanced/configure-cli). In this example, we have defined a simple name based on `stacks.name_pattern: "{stage}"`. This means that the stack name is just the `stage` part of the stack name.

- Atmos searches for the stack configuration file (in the `stacks` folder and all sub-folders) where `stage: prod` is defined (inline or via imports). During the search, Atmos processes all parent (top-level) config files and compares the context variables specified in the command (`-s` flag) with the context variables defined in the stack configurations, finally finding the matching stack

- Atmos finds the component `station` in the stack, processing all the inline configs and all the imports

- Atmos deep-merges all the catalog imports for the `station` component and then deep-merges all the variables for the component defined in all sections (global `vars`, terraform `vars`, base components `vars`, component `vars`), producing the final variables for the `station` component in the `prod` stack

- And lastly, Atmos writes the final deep-merged variables into a `.tfvar.json` file in the component directory and then
  executes `terraform apply -var-file ...` command

<ActionCard title="Ready to take the next step?">
     Now that you’ve seen Atmos in action, you can explore the extra credit!

     Or take a moment to explore its core concepts. You have only just scratched the surface of Atmos. Atmos is a powerful enterprise-grade framework with so **much more to offer**!


      <PrimaryCTA to="/quick-start/simple/extra-credit">Extra Credit</PrimaryCTA>
      <SecondaryCTA to="/core-concepts">Deep Dive</SecondaryCTA>

</ActionCard>

---

## Simple Atmos Tutorial

import PillBox from '@site/src/components/PillBox'
import DocCardList from '@theme/DocCardList'
import KeyPoints from '@site/src/components/KeyPoints'
import Intro from '@site/src/components/Intro'
import ActionCard from '@site/src/components/ActionCard'
import PrimaryCTA from '@site/src/components/PrimaryCTA'

<PillBox>Easy</PillBox>

<Intro>
In this simple example, we will show you, how you can Terraform the weather. We won’t focus on more advanced concepts like configuring state backends or integrating with specific cloud providers,
allowing us to concentrate on the core concepts of Atmos.
</Intro>

You're about to discover [a new way to think about terraform...](/quick-start/mindset)

<KeyPoints>
1. How to set up a project repository to organize your Terraform code and configurations
2. How to configure the Atmos CLI to work with the project
3. Write a simple Terraform root module
4. How to create stacks and configure the root module as a component of a stack
5. Finally, how to deploy everything using Atmos
</KeyPoints>

**Spoiler alert** You can’t actually change the weather with Terraform, but you can certainly ask for it.

We’ll [use this example](https://github.com/cloudposse/atmos/tree/main/examples/quick-start-simple) to avoid relying on complicated cloud credentials, which can vary based on your organizational context and cloud provider. Instead, we want to focus on Terraform and how to utilize it effectively as a component within Atmos. Once you understand this, you’ll see how to tailor your Terraform "root modules" to work seamlessly with Atmos.

__NOTE:__ This tutorial is for those who prefer hands-on learning and want to create something tangible quickly.
If you prefer to learn theory and concepts first, start with the [Core Concepts](/core-concepts).

## Prerequisites

Here's what you'll need to get started with this tutorial.

- [Install Atmos](/install)
- [Install Terraform](https://learn.hashicorp.com/tutorials/terraform/install-cli)

:::tip Use GitHub Codespaces

**You can try out `atmos` directly in your browser using GitHub Codespaces**

We bundle all the examples and tools you need to get started with Atmos in a GitHub Codespace. Click the button below to start a new Codespace with the Atmos repository (it's FREE).

[![Open in GitHub Codespaces](https://github.com/codespaces/badge.svg)](https://github.com/codespaces/new?hide_repo_select=true&ref=main&repo=cloudposse/atmos&skip_quickstart=true)
<sub>Already start one? Find it [here](https://github.com/codespaces).</sub>

:::

Then, when you're ready for a deeper understanding of using Atmos with Terraform, handling state management and working with
AWS authentication, please refer to our [advanced quick start guide](/quick-start/advanced).

## Extra Credit

After you finish mastering the basics, maybe check out the "Extra Credit" exercises to learn how to...

- [Deploy additional instances of your components](/quick-start/simple/extra-credit/add-another-component)
- [Automate common operations with workflows](/quick-start/simple/extra-credit/create-workflows)
- [Customize the Atmos CLI by adding your own commands](/quick-start/simple/extra-credit/add-custom-commands)
- [Vendor your dependencies for proper Immutable Infrastructure](/quick-start/simple/extra-credit/vendor-components)

## What's Included

Start your first project in 5 easy steps. After this, you can configure and provision infrastructure with Terraform using Atmos.

<DocCardList/>

<ActionCard>
    Start with the first step, which is configuring your project for Atmos.
    <PrimaryCTA to="/quick-start/simple/configure-project">Configure Project</PrimaryCTA>
</ActionCard>

---

## Where to go from here...

import KeyPoints from '@site/src/components/KeyPoints'
import Intro from '@site/src/components/Intro'
import ActionCard from '@site/src/components/ActionCard'
import PrimaryCTA from '@site/src/components/PrimaryCTA'

<Intro>
Atmos provides unlimited flexibility in defining and configuring stacks and components in the stacks.
</Intro>

<KeyPoints title="What you have learned">
- Terraform "root modules" are deployed as components that are organized in sub-folders within `components/terraform` by type, team, operations, or any other category.
- Stacks define the configuration of your "root modules", and can have arbitrary names and located anywhere in the `stacks/` directory; Atmos finds stacks using a "slug".
- Terraform components can be reused any number of times by giving them unique names in the stack configurations
  and pointing them to the same Terraform root module.
</KeyPoints>

All the above makes Atmos an ideal framework for organizing infrastructure, designing for organizational complexity, and provisioning multi-account environments for very complex organizations.

As you continue to poke around, here are some suggestions for what to do next:

1. [Review Atmos Design Patterns](/design-patterns)
2. [Use GitHub Actions](/integrations/github-actions)
3. [Try the Advanced Tutorial](/quick-start/advanced)

<ActionCard title="Next Steps">
    Now that you’ve seen Atmos in action, take a moment to explore its core concepts.

    You have only just scratched the surface of Atmos. Atmos is a powerful enterprise-grade framework with so **much more to offer**!
    <PrimaryCTA to="/core-concepts">Learn Core Concepts</PrimaryCTA>
</ActionCard>

---

## Write Some Components

import EmbedFile from '@site/src/components/EmbedFile'
import KeyPoints from '@site/src/components/KeyPoints'
import Screengrab from '@site/src/components/Screengrab'
import LatestRelease from '@site/src/components/LatestRelease'
import Step from '@site/src/components/Step'
import StepNumber from '@site/src/components/StepNumber'
import Intro from '@site/src/components/Intro'
import ActionCard from '@site/src/components/ActionCard'
import PrimaryCTA from '@site/src/components/PrimaryCTA'
import SecondaryCTA from '@site/src/components/SecondaryCTA'
import Note from '@site/src/components/Note'

<Intro>
When you [design cloud architectures with Atmos](/quick-start/mindset), you will first break them apart into pieces called [components](/core-concepts/components). Then, you will implement [Terraform "root modules"](/core-concepts/components/terraform) for each of those components.
</Intro>

After we're done with this, we'll show you [how you connect your components using stacks](/quick-start/simple/configure-stacks), so that everything comes together.

<KeyPoints>
- Where to place your components
- How to write a suitable Terraform root module to use as a component
- How to make your components reusable and other considerations
</KeyPoints>

<Step>
## <StepNumber/> Decide what your component should do

Once we consider our infrastructure in terms of components, we need to determine the specific functions each component must perform.

We recommend following the [Single Responsibility Principle (SRP)](https://en.wikipedia.org/wiki/Single-responsibility_principle). That is, design components to serve a single purpose, making them the smallest possible unit of infrastructure in a typical software development lifecycle (SDLC). Group pieces usually change together as one component and separate those that change independently (or seldom).

We discuss some of the [best practices for components](/best-practices/components) in our documentation.

In this case, since our goal is to retrieve the current weather, let's create a component that will do just that.

</Step>

<Step>
## <StepNumber/> Create a Directory for Your Component

The implementation for the component (e.g. HCL code) will be stored in a directory under the `components/` folder corresponding to the toolchain used (e.g. `terraform/`). For Terraform, place your component in the `components/terraform` directory. Inside this directory, create a folder to store your component’s Terraform code, which serves as the root module. Since our component is a simple Terraform root module, we will create a directory called `weather` to store the Terraform code for our weather component.

```bash
mkdir -p components/terraform/weather
```

</Step>

<Step>
## <StepNumber/> Write a Terraform Root Module

In the following descriptions, you’ll see that everything is just plain Terraform (HCL) with nothing specific to Atmos. That’s intentional: we want to demonstrate that Atmos works seamlessly with plain Terraform. Atmos introduces conventions around how you use Terraform with its framework, which will become more evident in the subsequent lessons.

To write our Terraform root module, we’ll follow Terraform's standard conventions:
<dl>
  <dt>`variables.tf`</dt>
  <dd>Defines variables</dd>

  <dt>`outputs.tf`</dt>
  <dd>Defines outputs</dd>

  <dt>`versions.tf`</dt>
  <dd>Specifies required provider versions</dd>

  <dt>`providers.tf`</dt>
  <dd>Specifies providers</dd>

  <dt>`main.tf`</dt>
  <dd>Implements main functionality</dd>
</dl>

So, let’s start by defining our root module as usual.

### Implement `variables.tf`

To make the best use of Atmos, ensure your root modules are highly reusable by accepting parameters, allowing them to be deployed multiple times without conflicts. This also usually means provisioning resources with unique names.

<EmbedFile filePath="examples/quick-start-simple/components/terraform/weather/variables.tf" />

### Implement `main.tf`

The `main.tf` file is where the main implementation of your component resides. This is where you define all the business logic for what you’re trying to achieve—the core functionality of your root module. If this file becomes too large or complex, you can break it into multiple files in a way that makes sense. However, sometimes that is also a red flag, indicating that the component is trying to do too much and should be broken down into smaller components.

In this example, we define a local variable that creates a URL using the variable inputs we receive. We also set up a data source to perform an HTTP request to that endpoint and retrieve the current weather. Additionally, we write this output to a file to demonstrate a stateful resource.

<EmbedFile filePath="examples/quick-start-simple/components/terraform/weather/main.tf" />

<details>
<summary>How do we recommend ensuring unique resource names?</summary>

Cloud Posse's [`terraform-null-label`](https://github.com/cloudposse/terraform-null-label) module makes this easier by turning naming into a standardized, programmatically enforced convention. Setting the null label parameters ensures unique resource names and enforces consistency across your team and organization.

<Note>In our example, we will keep it simple to focus on the basics, so we will not use this module.</Note>
</details>

For the purpose of this quickstart, we’ll assume you have minimal experience with Terraform. Some of this information might be obvious to more experienced users, but we’re including it here to ensure a smooth learning experience for everyone.

### Implement `versions.tf`

The `versions.tf` file is where provider pinning is typically defined. Provider pinning increases the stability of your components and ensures consistency between deployments in multiple environments.

<EmbedFile filePath="examples/quick-start-simple/components/terraform/weather/versions.tf" />

### Implement `outputs.tf`

The `outputs.tf` file is where, by convention in Terraform, you place any outputs you want to expose from your root module. Outputs are essential for passing state between your root modules and can be used in conjunction with [remote state](/core-concepts/share-data/remote-state) or the [Atmos function to retrieve the state](/functions/template) of any other component.
In object-oriented parlance, think of your outputs as the “public” attributes of the module that are intended to be accessed by other modules. This convention helps maintain clarity and organization within your Terraform configurations.

<EmbedFile filePath="examples/quick-start-simple/components/terraform/weather/outputs.tf" />
<Note>Technically outputs can be placed in any file, but the standard convention in Terraform is to place them usually in `outputs.tf`.</Note>
</Step>

<ActionCard title="Ready to take the next step?">
    Now that you have a Terraform "root module", you can use it as a component in your Atmos stacks, and deploy it any number of times, tweaking the configuration as needed.

    We will show you [how to do that in the next step](/quick-start/simple/configure-stacks).

      <PrimaryCTA to="/quick-start/simple/configure-stacks">Next Step</PrimaryCTA>
      <SecondaryCTA to="/core-concepts/components/terraform">Deep Dive</SecondaryCTA>

</ActionCard>

---

## Atmos Alternatives & Inspirations

import Intro from '@site/src/components/Intro'

<Intro>
To better understand where Atmos fits in, it may be helpful to understand some of the tooling that has inspired its design or serve as possible alternatives to its approach.
</Intro>

## Conceptual Inspiration

Atmos is inspired by the follow frameworks or tooling.

### React for JavaScript

https://react.dev/

[React’s](https://react.dev/) component-based architecture serves as a key inspiration for Atmos. By breaking down UIs into reusable components, React simplifies the development of complex applications. Similarly, Atmos promotes modularity in infrastructure as code, allowing components to be reused across different environments and projects. For example, in Atmos, any Terraform "root module" may be used as a component.

### Kustomize for Kubernetes

https://kustomize.io/

[Kustomize](https://kustomize.io/) introduces a template-free way to customize Kubernetes configurations, focusing on overlays and inheritance to manage configuration variations. Atmos adopts a similar approach, enabling users to import, overlay, and override configurations efficiently, thereby simplifying the management of complex infrastructure setups, all without relying on templating.

However, due to popular demand, Atmos now supports advanced templating and data sources in addition to the original template-free configurations. Templating complicates configurations and should be used solely as an escape hatch for when the first-class concepts of imports and inheritance are insufficient.

### Helmfile for Helm Charts

https://helmfile.com

[Helmfile](https://helmfile.com) manages collections of Helm charts with declarative syntax, combining them into a "stack" for deployment to Kubernetes. It handles environmental configuration, deep merging it, and evaluating templates with a Go template engine before passing the values files to Helm.

Atmos draws from Helmfile’s ability to orchestrate multiple Helm charts, applying the concept to Terraform root modules to manage interdependencies and deployment order. It supports environmental configuration through stack configurations that define all the environmental configurations for Terraform root modules. Atmos generates the necessary Terraform `.tfvar` files, much like Helmfile generates Helm values files, ensuring consistent and efficient deployment of Terraform infrastructure.

### Helm Charts for Configuration

https://helm.sh/

[Helm Charts](https://helm.sh/) provide a packaging format for deploying applications on Kubernetes, simplifying the processes of defining, installing, and upgrading even the most complex applications. Similarly, Atmos organizes Terraform configurations into reusable, versioned modules, facilitating consistent and repeatable infrastructure deployments.

The concept is that if your root modules are sufficiently parameterized, they function much like Helm charts. You only need to supply the appropriate values to achieve the desired configuration.

### Vendir by Tanzu

https://github.com/carvel-dev/vendir

Atmos Vendoring was heavily inspired by [Vendir from VMWare Tanzu](https://github.com/carvel-dev/vendir), which served as the basis for our initial implementation. However, after using it, we realized we only needed a simpler subset of Vendir’s full functionality. Therefore, we implemented our own version using [HashiCorp’s GoGetter (MPL-2.0) library](https://github.com/hashicorp/go-getter). Additionally, we’ve added support for OCI, allowing Vendoring to pull configurations from anywhere. This advanced feature enables consistent and declarative pulling of configurations not just for components, but also for stack configurations or any dependencies you have.

## General Alternatives

There are many tools in the general category of "task runners" or "workflow automation".
Here are some of the alternatives to Atmos, many of which inspired core functionality in Atmos.

### Make (Makefile) by Free Software Foundation

https://www.gnu.org/software/make/

Many companies (including Cloud Posse) started by leveraging `make` with `Makefile` and targets to call `terraform`. Using `make` is a popular method of orchestrating tools, but it has trouble scaling up to support large projects. We know this because [Cloud Posse](https://cloudposse.com/) used it for 3+ years. The problem we ran into is that `make` targets do not support "natural" parameterization, which leads to a proliferation of environment variables that are difficult to validate or hacks like overloading make-targets and parsing them (e.g. `make apply/prod`). Makefiles are unintuitive for newcomers because they are first evaluated as a template and then executed as a script where each line of a target runs in a separate process space. Spaces matter too, and it's made worse with inconsistent rules using tabs in some places and spaces in others.

### Mage (Magefile)

https://magefile.org/

Mage is a make/rake-like build tool using native Golang and plain-old `Go` functions. Mage then automatically provides a CLI to call them as
Makefile-like runnable targets.

### Task (Taskfile)

https://github.com/go-task/task

Task is a task runner and build tool that aims to be simpler and easier to use than GNU Make.

:::info
Atmos supports native [workflows](/core-concepts/workflows) that have very similar schema to "Taskfile", only they can be defined together
with [Stacks](/core-concepts/stacks) or as standalone workflow files.
:::

### Variant

https://github.com/mumoshu/variant
https://github.com/mumoshu/variant2 (second generation)

Variant lets you wrap all your scripts and CLIs into a modern CLI and a single-executable that can run anywhere.

:::info
The earliest versions of `atmos` were built on top of [`variant2`](https://github.com/mumoshu/variant2) until we decided to rewrite it from the ground
up in pure Go. Atmos supports native [workflows](/core-concepts/workflows) which provide similar benefits.
:::

### AppBuilder by Choria

https://github.com/choria-io/appbuilder

AppBuilder is a tool built in Golang to create a friendly CLI command that wraps your operational tools.

:::info
Atmos is heavily inspired by the excellent schema provided by AppBuilder and has implemented a similar interface as part of
our [Custom Commands](/core-concepts/custom-commands).
:::

## Terraform-specific Tooling Alternatives

There are many tools explicitly designed around how to deploy with Terraform.

The following is a list of tools that only support Terraform.

:::tip Atmos Differentiators
Atmos supports Terraform and can also be used to manage any command-line tool. For example, by combining [Custom commands](/core-concepts/custom-commands) and [workflows](/core-concepts/workflows), it's possible to support any CLI tool (even the ones listed below) or even reimplement the core functionality of Atmos. That's how extensible it is.
:::

### Terragrunt by Gruntwork

https://github.com/gruntwork-io/terragrunt

Terragrunt is a tool built in Golang that is a thin wrapper for Terraform that provides extra tools for working with multiple Terraform modules.

### Terramate by Mineros

https://github.com/mineiros-io/terramate

Terramate is a tool built in Golang for managing multiple Terraform stacks with support for change detection and code generation.

### Terraspace (Terrafile) by Bolt Ops

https://github.com/boltops-tools/terraspace

Terrapsace is a tool built in Ruby that provides an opinionated framework for working with Terraform.

### Terraplate by Verifa

https://github.com/verifa/terraplate

Terraplate is a tool built in Golang that is a lightweight wrapper for Terraform the focused on code generation.

### Astro by Uber (abandoned)

https://github.com/uber/astro

Astro is a tool built in Golang that provides a YAML DSL for defining all your terraform projects and then running them.

### Opta by Run X

https://github.com/run-x/opta

Opta is a tool built in Python that makes Terraform easier by providing high-level constructs and not getting stuck with low-level cloud configurations.

### pterradactyl by Nike

https://github.com/Nike-Inc/pterradactyl

Pterradactyl is a library developed to abstract Terraform configuration from the Terraform environment setup.

### Leverage by Binbash

The Leverage CLI is written in Python and intended to orchestrate Leverage Reference Architecture for AWS

https://github.com/binbashar/leverage

---

## Reference

import DocCardList from '@theme/DocCardList'

<DocCardList/>

---

## Slides

import Slides from "@site/src/components/Slides";

<Slides/>

---

## Debugging

:::note
TODO
:::

---

## Common Errors

## Common Mistakes

* Running out of date version of `atmos` with newer configuration parameters
* An `atmos.yaml` with incorrect `stacks.stack_name` pattern (often due to copy pasta)

## Common Errors

Here are some common errors that can come up.

### Error: `stack name pattern must be provided`

```console
stack name pattern must be provided in 'stacks.name_pattern' config or 'ATMOS_STACKS_NAME_PATTERN' ENV variable
```

This means that you are probably missing a section like this in your `atmos.yml`. See the instructions on CLI Configuration for more details.

```yaml
stacks:
  name_pattern: "{tenant}-{environment}-{stage}"
```

### Error: `The stack name pattern ... does not have a tenant defined`

```console
The stack name pattern '{tenant}-{environment}-{stage}' specifies 'tenant', but the stack ue1-prod does not have a tenant defined
```

This means that your `name_pattern` declares a `tenant` is required, but not specified in the Stack configurations. Either specify a `tenant` in
your `vars` for the Stack configuration, or remove the `{tenant}` from the `name_pattern`.

```yaml
stacks:
  name_pattern: "{tenant}-{environment}-{stage}"
```

### Error: `depends_on expected a map, got slice`

```console
decoding: depends_on expected a map, got slice
```

The `depends_on` functionality originally existed only under `settings.spacelift.depends_on` and was a list of other components that the current
component depends on.

We have since updated `depends_on` to be more generic and be directly under `settings.depends_on` (so it can also be used in GitHub Actions and other
tools).

The updated key is now a map (rather than a list). If you see this error, it means that someone put a `depends_on`
block directly under `settings` but added it as a list (rather than a map as the new config requires).

The solution is to move it under `settings.spacelift.depends_on` (legacy, deprecated, not recommended) or update the dependencies to be a map.

```yaml
components:
  terraform:
    top-level-component2:
      metadata:
        component: "top-level-component1"
      settings:
        spacelift:
          workspace_enabled: false
        depends_on:
          1:
            # If the `context` (namespace, tenant, environment, stage) is not provided,
            # the `component` is from the same Atmos stack as this component
            component: "test/test-component"
          2:
            # If the `context` (namespace, tenant, environment, stage) is not provided,
            # the `component` is from the same Atmos stack as this component
            component: "test/test2/test-component-2"
          3:
            file: "tests/fixtures/scenarios/complete/components/terraform/mixins/introspection.mixin.tf"
          4:
            folder: "tests/fixtures/scenarios/complete/components/helmfile/infra/infra-server"
      vars:
        enabled: true
```

:::tip

For more information, refer to:

- [`atmos describe affected` CLI command](/cli/commands/describe/affected)
- [`atmos describe dependents` CLI command](/cli/commands/describe/dependents)

:::

### Error: `A change in the backend configuration has been detected, which may require migrating existing state`

```console
A change in the backend configuration has been detected, which may require migrating existing state
```

The Terraform/OpenTofu error means Terraform noticed that your backend configuration (the section in your code that
defines where and how Terraform stores state) has changed compared to what’s currently in use..

#### Why it happens

Terraform keeps its state in a backend (for example, S3, local, remote, Terraform Cloud, etc.).
If you update the backend configuration in your code — for example:

- Switching from local to s3
- Changing the bucket, key, or region for an S3 backend
- Modifying a prefix or workspace config in Terraform Cloud
- Adjusting encryption or locking settings
- Terraform detects that the state might need to be migrated from the old backend to the new one.

#### What Terraform expects

When this happens, Terraform doesn’t automatically move state — it prompts you to confirm migration.
Typically, you’ll see a message like:

```console
Do you want to copy existing state to the new backend?
Enter "yes" to copy and "no" to start with an empty state.
```

If you type `yes`, Terraform migrates the state (so you keep your existing resources).
If you type `no`, Terraform initializes a fresh, empty state in the new backend (dangerous unless intentional).

#### How to resolve

In `atmos.yaml`, set `components.terraform.init_run_reconfigure` to `true`:

```yaml
components:
  terraform:
    # Can also be set using 'ATMOS_COMPONENTS_TERRAFORM_INIT_RUN_RECONFIGURE' ENV var, or '--init-run-reconfigure' command-line argument
    init_run_reconfigure: true
```

This will tell Atmos to run `terraform init -reconfigure` when executing Terraform commands.
The `-reconfigure` option disregards any existing backend configuration and configures a new backend,
preventing migration of any existing state.

:::tip

For more information, refer to:

- [`terraform init` command](https://developer.hashicorp.com/terraform/cli/commands/init)
- [Terraform Backend Initialization](https://developer.hashicorp.com/terraform/cli/commands/init#backend-initialization)
- [Atmos CLI configuration](/cli/configuration/)
- [Atmos Terraform/OpenTofu commands](/cli/commands/terraform/usage/)

:::

---

## Performance Profiling

import Terminal from '@site/src/components/Terminal'
import File from '@site/src/components/File'
import Intro from '@site/src/components/Intro'

<Intro>
    Atmos includes built-in support for performance profiling using Go's standard `pprof` tool.
    Profiling helps identify performance bottlenecks, memory usage patterns, and CPU hotspots to optimize Atmos
    operations.
</Intro>

## What is pprof?

[pprof](https://pkg.go.dev/net/http/pprof) is Go's built-in profiling tool that collects runtime performance data from Go applications.
It can capture several types of profiles:

- **CPU Profile**: Shows where your program spends CPU time
- **Heap Profile**: Shows current heap memory allocation patterns
- **Allocs Profile**: Shows all memory allocations since program start
- **Goroutine Profile**: Shows active goroutines and their call stacks
- **Block Profile**: Shows operations that led to blocking on synchronization primitives
- **Mutex Profile**: Shows lock contention patterns
- **Thread Create Profile**: Shows stack traces that led to thread creation
- **Trace Profile**: Shows detailed execution traces for performance analysis

## Atmos Profiling Integration

Atmos integrates pprof in two modes:

### File-Based Profiling

File-based profiling captures profiles directly to a file, which is ideal for CLI tools that exit quickly.
This mode automatically enables profiling when a file path is specified. You can specify the type of profile to collect using the `--profile-type` flag.

```shell
# CPU profiling (default)
atmos terraform plan vpc -s plat-ue2-dev --profile-file=cpu.prof

# Memory heap profiling
atmos terraform plan vpc -s plat-ue2-dev --profile-file=heap.prof --profile-type=heap

# Execution trace profiling
atmos terraform plan vpc -s plat-ue2-dev --profile-file=trace.out --profile-type=trace

# Goroutine profiling
atmos terraform plan vpc -s plat-ue2-dev --profile-file=goroutine.prof --profile-type=goroutine
```

<Terminal title="File-based profiling output">
    ```
    INFO Profiling started type=cpu file=cpu.prof
    INFO Profiling completed type=cpu file=cpu.prof
    ```
</Terminal>

### Server-Based Profiling

Server-based profiling starts an HTTP server that serves pprof endpoints.
This mode is useful for long-running operations or when you need access to multiple profile types.

```shell
atmos terraform plan vpc -s plat-ue2-dev --profiler-enabled
```

<Terminal title="Server-based profiling output">
    ```
    INFO Profiler server available at: url=http://localhost:6060/debug/pprof/
    ```
</Terminal>

## Configuration

### CLI Flags

<dl>
    <dt>`--profile-file`</dt>
    <dd>Write profiling data to the specified file (enables profiling automatically)</dd>

    <dt>`--profile-type`</dt>
    <dd>Type of profile to collect when using `--profile-file`. Options: `cpu`, `heap`, `allocs`, `goroutine`, `block`, `mutex`, `threadcreate`, `trace` (default: `cpu`)</dd>

    <dt>`--profiler-enabled`</dt>
    <dd>Enable pprof profiling server (default: false)</dd>

    <dt>`--profiler-host`</dt>
    <dd>Host address for profiling server (default: localhost)</dd>

    <dt>`--profiler-port`</dt>
    <dd>Port for profiling server (default: 6060)</dd>
</dl>

### Environment Variables

All profiling flags can be set using environment variables for easier CI/CD integration:

<dl>
    <dt>`ATMOS_PROFILER_ENABLED`</dt>
    <dd>Enable pprof profiling server (equivalent to `--profiler-enabled`)</dd>

    <dt>`ATMOS_PROFILER_HOST`</dt>
    <dd>Host address for profiling server (equivalent to `--profiler-host`)</dd>

    <dt>`ATMOS_PROFILER_PORT`</dt>
    <dd>Port for profiling server (equivalent to `--profiler-port`)</dd>

    <dt>`ATMOS_PROFILE_FILE`</dt>
    <dd>File path for file-based profiling (equivalent to `--profile-file`)</dd>

    <dt>`ATMOS_PROFILE_TYPE`</dt>
    <dd>Profile type for file-based profiling (equivalent to `--profile-type`)</dd>
</dl>

**Examples:**
```bash
# File-based profiling via environment variables
export ATMOS_PROFILE_FILE=cpu.prof
export ATMOS_PROFILE_TYPE=cpu
atmos terraform plan vpc -s prod

# Server-based profiling via environment variables
export ATMOS_PROFILER_ENABLED=true
export ATMOS_PROFILER_PORT=8080
atmos terraform apply vpc -s prod

# Combined environment and CLI usage
export ATMOS_PROFILER_HOST=0.0.0.0
atmos terraform plan vpc -s prod --profiler-enabled --profiler-port=9090
```

### Configuration File

You can also enable profiling through your `atmos.yaml` configuration:

<File title="atmos.yaml">
    ```yaml
    profiler:
      enabled: true
      host: "localhost"
      port: 6060
      file: "profile.out"           # Optional: file-based profiling
      profile_type: "cpu"           # Optional: profile type for file-based profiling
    ```
</File>

**Configuration Precedence:**
1. Command-line flags (highest priority)
2. Environment variables
3. Configuration file (`atmos.yaml`)
4. Default values (lowest priority)

## Analyzing Profile Data

### File-Based Profiles

After capturing a profile file, you can analyze it using Go's pprof tool. The analysis method depends on the profile type:

**CPU and Memory Profiles:**
```shell
# Interactive text mode
go tool pprof cpu.prof
go tool pprof heap.prof

# Web interface (requires Graphviz: brew install graphviz)
go tool pprof -http=:8080 cpu.prof
go tool pprof -http=:8080 heap.prof

# Direct text output
go tool pprof -top cpu.prof
go tool pprof -top heap.prof
```

**Trace Profiles:**
```shell
# Use go tool trace for execution traces
go tool trace trace.out

# This opens a web interface showing:
# - Timeline view of goroutines
# - Network blocking profile
# - Synchronization blocking profile
# - System call blocking profile
```

<Terminal title="Sample pprof output">
    ```
    (pprof) top
    Showing nodes accounting for 230ms, 95.83% of 240ms total
    Dropped 15 nodes (cum &lt;= 1.20ms)
    flat flat% sum% cum cum%
    80ms 33.33% 33.33% 80ms 33.33% github.com/cloudposse/atmos/internal/exec.processStackConfig
    60ms 25.00% 58.33% 60ms 25.00% gopkg.in/yaml.v3.(*Decoder).Decode
    40ms 16.67% 75.00% 40ms 16.67% github.com/cloudposse/atmos/pkg/utils.ProcessTmplWithDatasources
    30ms 12.50% 87.50% 30ms 12.50% encoding/json.(*Decoder).Decode
    20ms 8.33% 95.83% 20ms 8.33% github.com/cloudposse/atmos/pkg/stack.ProcessStackConfig
    ```
</Terminal>

### Server-Based Profiles

When using server mode, you can access different profile types through HTTP endpoints:

```shell
# CPU profile (30-second sample)
go tool pprof http://localhost:6060/debug/pprof/profile

# Memory allocation profile
go tool pprof http://localhost:6060/debug/pprof/heap

# Goroutine profile
go tool pprof http://localhost:6060/debug/pprof/goroutine

# Mutex contention profile
go tool pprof http://localhost:6060/debug/pprof/mutex
```

You can also access the web interface directly:

```shell
open http://localhost:6060/debug/pprof/
```

## Common Profiling Scenarios

### Performance Optimization

Profile slow Atmos operations to identify bottlenecks:

```shell
# Profile CPU usage in a slow terraform plan operation
atmos terraform plan large-component -s prod --profile-file=slow-plan.prof --profile-type=cpu

# Profile memory usage
atmos terraform plan large-component -s prod --profile-file=memory.prof --profile-type=heap

# Profile detailed execution trace
atmos terraform plan large-component -s prod --profile-file=trace.out --profile-type=trace

# Analyze the results
go tool pprof -http=:8080 slow-plan.prof
go tool pprof -http=:8080 memory.prof
go tool trace trace.out
```

### Memory Usage Analysis

Identify memory-intensive operations using different approaches:

**File-based memory profiling:**
```shell
# Capture heap profile to file
atmos describe stacks --profile-file=heap.prof --profile-type=heap

# Capture allocation profile to file
atmos describe stacks --profile-file=allocs.prof --profile-type=allocs

# Analyze the results
go tool pprof -http=:8080 heap.prof
go tool pprof -http=:8080 allocs.prof
```

**Server-based memory profiling:**
```shell
# Start server mode for live memory profiling
atmos describe stacks --profiler-enabled

# In another terminal, capture memory profile
go tool pprof http://localhost:6060/debug/pprof/heap
go tool pprof http://localhost:6060/debug/pprof/allocs
```

### Custom Server Configuration

Run profiler on a different host/port for security or networking requirements:

```shell
atmos terraform apply vpc -s prod \
  --profiler-enabled \
  --profiler-host=0.0.0.0 \
  --profiler-port=8060
```

## Dependencies

### Graphviz (Optional)

The pprof web interface requires [Graphviz](https://graphviz.org/) to generate visual graphs:

**macOS:**
```shell
brew install graphviz
```

**Ubuntu/Debian:**
```shell
sudo apt-get install graphviz
```

**CentOS/RHEL:**
```shell
sudo yum install graphviz
```

Without Graphviz, you can still use the text-based analysis modes.

## Best Practices

### File-Based vs Server-Based

- **Use file-based profiling** for most CLI operations and performance analysis. Supports all profile types: `cpu`, `heap`, `allocs`, `goroutine`, `block`, `mutex`, `threadcreate`, `trace`
- **Use server-based profiling** for long-running operations or when you need to collect multiple profile types interactively

### Profile Regularly

- Profile before and after performance optimizations to measure impact
- Establish baseline profiles for typical operations
- Profile different stack sizes and complexity levels

### Security Considerations

- Server-based profiling exposes runtime information through HTTP
- Use `localhost` binding in production environments
- Disable profiling in production unless actively debugging

## Troubleshooting

### Common Issues

**Profile file creation errors:**
```shell
# Ensure the directory exists
mkdir -p /path/to/profile/
atmos command --profile-file=/path/to/profile/cpu.prof --profile-type=cpu
```

**Invalid profile type:**
```shell
# Check supported profile types
echo "Supported types: cpu, heap, allocs, goroutine, block, mutex, threadcreate, trace"
atmos command --profile-file=profile.out --profile-type=heap
```

**Graphviz not found:**
```shell
# Use text-based analysis instead
go tool pprof -top cpu.prof
go tool pprof -list=functionName cpu.prof
```

**Server already running:**
```shell
# Use a different port
atmos command --profiler-enabled --profiler-port=7070
```

**Environment variable usage:**
```shell
# CI/CD environment setup for profiling
export ATMOS_PROFILE_FILE=/var/log/atmos-cpu.prof
export ATMOS_PROFILE_TYPE=cpu
atmos terraform apply --auto-approve

# Different profile types via environment variables
export ATMOS_PROFILE_TYPE=heap
export ATMOS_PROFILE_FILE=memory-analysis.prof
atmos describe stacks

# Server mode via environment variables
export ATMOS_PROFILER_ENABLED=true
export ATMOS_PROFILER_HOST=0.0.0.0
export ATMOS_PROFILER_PORT=6060
atmos terraform plan vpc -s prod
```

### Understanding Profile Data

- **flat**: Time spent in the function itself
- **cum**: Cumulative time spent in the function and its callees
- **flat%**: Percentage of total execution time spent in the function
- **sum%**: Cumulative percentage up to this function

Focus optimization efforts on functions with high **flat** time and high **flat%** percentages.

---

## Atmos component migration in YAML config

## Migrate component from using deprecated key `component` to use `metadata.inherits`

1. Identify the component to move, we'll use `vpc` for this example.

    Here is an example of the older method of using the `component` key

    ```yaml
    # stacks/catalog/vpc.yaml
    components:
      terraform:
        vpc:
          # This cannot be enabled here in the older method due to the global
          # AZ fragment importation
          settings:
            spacelift:
              workspace_enabled: false
          vars:
            enabled: true
            # ...etc...

    # stacks/gbl-ue2.yaml
    components:
      terraform:
        vpc:
          # This is the AZ fragment importation that's imported across all stacks
          # even stacks without a vpc.
          availability_zones:
            - us-east-1a
            - us-east-1b
            - us-east-1c

    # stacks/plat-ue2-dev.yaml
    import:
      - gbl-ue2
      - catalog/vpc

    components:
      terraform:
        vpc:
          component: vpc
          # This is where spacelift has to be enabled
          settings:
            spacelift:
              workspace_enabled: true
          vars:
            name: vpc
            # ...etc...

        vpc1:
          component: vpc
          # This is where spacelift has to be enabled
          settings:
            spacelift:
              workspace_enabled: true
          vars:
            name: vpc1
            # ...etc...
    ```

1. Verify terraform plan for `vpc` component is a `no change`. If not, configure it and apply it until it shows `no change`.

    ```sh
    ⨠ atmos terraform plan vpc --stack plat-ue2-dev
    ```

1. Pull down the latest `vpc` component and repeat the previous step (optional)

    ```sh
    ⨠ wget https://raw.githubusercontent.com/cloudposse/atmos/main/examples/quick-start-advanced/components/terraform/vpc/component.yaml -O components/terraform/vpc/component.yaml
    ⨠ sed -i 's,infra/vpc-flow-logs-bucket,vpc,g' components/terraform/vpc/component.yaml
    ⨠ atmos vendor pull -c vpc
    ```

1. Verify the current `workspace` name

    ```sh
    ⨠ atmos describe component vpc --stack plat-ue2-dev | grep ^workspace
    workspace: plat-ue2-dev
    ```

1. Count the current root stacks where the `vpc` component is defined

    ```sh
    ⨠ atmos describe stacks --components=vpc --sections=components | grep -oP '^[a-z0-9-]+' | wc -l
    17
    ```

1. In the `vpc.yaml` stack catalog, rename `vpc` to be `vpc/defaults`

    Add the `metadata` block

    ```yaml
    components:
      terraform:
        vpc/defaults:
          metadata:
            type: abstract
          # spacelift can now be enabled in the catalog
          settings:
            spacelift:
              workspace_enabled: true
          vars:
            enabled: true
            # ...etc...
    ```

1. Create a `vpc` component that inherits from `vpc/defaults`

    ```yaml
        vpc:
          metadata:
            component: vpc
            inherits:
              - vpc/defaults
          vars:
            name: vpc
    ```

1. Verify the `workspace` is the same

    ```sh
    ⨠ atmos describe component vpc --stack plat-ue2-dev | grep ^workspace
    workspace: plat-ue2-dev
    ```

    NOTE: Since the `vpc` component name shares the same name as its base component, the `workspace` will remain the same.

    NOTE: If the `workspace` has the `component` suffixed to it e.g. `plat-ue2-dev-vpc1` then the tfstate will have to be migrated. See component renaming and subsequent tfstate rename below before continuing.

1. Add `availability_zones` to `vpc/defaults` and only add the letters and omit the region.

1. Move any `vpc` component global overrides for `availability_zones` to `vpc/defaults`.

    The `vpc` configuration may have a fragment in a global region file

    ```yaml
        vpc:
          availability_zones:
            - us-east-1a
            - us-east-1b
            - us-east-1c
    ```

    It can now be set as this in the `vpc.yaml` catalog

    ```yaml
          availability_zones:
            - a
            - b
            - c
    ```

1. Count the root stacks again and verify the number is lower than previous

    ```sh
    ⨠ atmos describe stacks --components=vpc --sections=components | grep -oP '^[a-z0-9-]+' | wc -l
    9
    ```

1. Rerun the plan and verify `no changes` again

    ```sh
    ⨠ atmos terraform plan vpc --stack plat-ue2-dev
    ```

## Renaming components

If renaming a component is desired, for example, from `vpc` to `vpc1`, the workspace will change.

Follow these steps to ensure your state is properly managed.

1. Identify all the stacks affected by the component rename

1. Select a root stack and the component. We will use `plat-ue2-dev` root stack and `vpc` component for this example.

1. Verify terraform plan for `vpc` component is a `no change`. If not, configure and apply it.

1. Verify the old and new workspaces.

    Before renaming

    ```sh
    ⨠ atmos describe component vpc --stack plat-ue2-dev | grep ^workspace
    workspace: plat-ue2-dev
    ```

    After renaming

    ```sh
    ⨠ atmos describe component vpc1 --stack plat-ue2-dev | grep ^workspace
    workspace: plat-ue2-dev-vpc1
    ```

1. Follow one of the guides below for migrating the state or overriding the workspace

1. After you have completed the previous step, you will also need to rename any `component` keys in `remote-state.tf` files since the original reference to the component has been removed from the tfstate

### Migrating state manually

This approach is recommended because this will allow the catalog to be imported without additional overrides.

1. Navigate to the component and list the workspaces

    ```sh
    ⨠ cd components/terraform/vpc
    ⨠ terraform init
    ⨠ terraform workspace list
    default
    * plat-gbl-sandbox
      plat-ue2-dev
    ```

1. Select the original component's workspace

    ```sh
    ⨠ terraform workspace select plat-ue2-dev
    ```

1. Dump the terraform state into a new file

    ```sh
    ⨠ terraform state pull > plat-ue2-dev.tfstate
    ```

1. Select or create the new workspace if it does not exist

    ```sh
    ⨠ terraform workspace select plat-ue2-dev-vpc1
    ⨠ terraform workspace new plat-ue2-dev-vpc1
    Created and switched to workspace "plat-ue2-dev-vpc1"!
    ```

1. Push up the original workspace's state to the new workspace's state

    ```sh
    ⨠ terraform state push plat-ue2-dev.tfstate
    Releasing state lock. This may take a few moments...
    ```

1. Verify terraform plan for the new component is a `no change`.

    ```sh
    ⨠ atmos terraform plan vpc1 --stack plat-ue2-dev
    ```

### Overriding the workspace name

The fastest approach would be to override the workspace name but is not recommended since this will need to be done for every root stack where the catalog is imported.

1. In the root stack affected, instantiate the component and override the `terraform_workspace` to match the original.

    ```yaml
    # root stack: plat-ue2-dev
    import:
      - catalog/vpc

    components:
      terraform:
        vpc1:
          metadata:
            terraform_workspace: plat-ue2-dev
            component: vpc
            inherits:
              - vpc/defaults
          vars:
            name: vpc
    ```

1. Verify the `workspace` is correct

    ```
    ⨠ atmos describe component vpc1 --stack plat-ue2-dev | grep ^workspace
    workspace: plat-ue2-dev
    ```

1. Repeat the above steps for all the root stacks where the component is imported

---

## Atmos Example Infrastructure

The [example](https://github.com/cloudposse/atmos/tree/main/examples/quick-start-advanced) folder contains a complete solution that shows how to:

- Structure the Terraform components
- Configure the CLI
- Add [stack configurations](https://github.com/cloudposse/atmos/tree/main/examples/quick-start-advanced/stacks) for the Terraform and helmfile components (to
  provision them to different environments and stages)

## Example Filesystem Layout

This example provides a simple filesystem layout that looks like this:

```console
   │  
   │   # Centralized stacks configuration
   ├── stacks/
   │   │
   │   └── <stack_1>.yaml
   │   └── <stack_2>.yaml
   │   └── <stack_3>.yaml
   │  
   │   # Centralized components configuration. Components are broken down by tool
   ├── components/
   │   │
   │   ├── terraform/   # Terraform components (Terraform root modules)
   │   │   ├── infra/
   │   │   ├── mixins/
   │   │   ├── test/test-component/
   │   │   └── top-level-component1/
   │   │
   │   └── helmfile/  # Helmfile components are organized by Helm chart
   │       ├── echo-server/
   │       └── infra/infra-server
   │  
   │   # Root filesystem for the Docker image (see `Dockerfile`)
   ├── rootfs/
   │
   │   # Makefile for building the CLI
   ├── Makefile
   │   # Atmos CLI configuration
   ├── atmos.yaml
   │  
   │   # Docker image for shipping the CLI and all dependencies
   └── Dockerfile (optional)
```

## Stack Configuration

Atmos provides separation of configuration and code, allowing you to provision the same code into different regions, environments and stages.

In our example, all the code (Terraform and helmfiles) is in
the [components](https://github.com/cloudposse/atmos/tree/main/examples/quick-start-advanced/components) folder.

The centralized stack configurations (variables for the Terraform and helmfile components) are in
the [stacks](https://github.com/cloudposse/atmos/tree/main/examples/quick-start-advanced/stacks) folder.

In the example, all stack configuration files are broken down by environments and stages and use the predefined format `$environment-$stage.yaml`.

Environments are abbreviations of AWS regions, e.g. `ue2` stands for `us-east-2`, whereas `uw2` would stand for `us-west-2`.

`$environment-globals.yaml` is where you define the global values for all stages for a particular environment.
The global values get merged with the `$environment-$stage.yaml` configuration for a specific environment/stage by the CLI.

In the example, we defined a few config files:

- [stacks/orgs/cp/tenant1/dev/us-east-2.yaml](https://github.com/cloudposse/atmos/tree/main/examples/quick-start-advanced/stacks/orgs/cp/tenant1/dev/us-east-2.yaml)
  - stack configuration (Terraform and helmfile variables) for the environment `ue2` and stage `dev`
- [stacks/orgs/cp/tenant1/staging/us-east-2.yaml](https://github.com/cloudposse/atmos/tree/main/examples/quick-start-advanced/stacks/orgs/cp/tenant1/staging/us-east-2.yaml)
  - stack configuration (Terraform and helmfile variables) for the environment `ue2` and stage `staging`
- [stacks/orgs/cp/tenant1/prod/us-east-2.yaml](https://github.com/cloudposse/atmos/tree/main/examples/quick-start-advanced/stacks/orgs/cp/tenant1/prod/us-east-2.yaml)
  - stack configuration (Terraform and helmfile variables) for the environment `ue2` and stage `prod`
- [stacks/orgs/cp/tenant1/dev/global-region.yaml](https://github.com/cloudposse/atmos/tree/main/examples/quick-start-advanced/stacks/ue2-globals.yaml) - global
  settings for the environment `ue2` (e.g. `region`, `environment`)
- [stacks/orgs/cp/_defaults.yaml](https://github.com/cloudposse/atmos/blob/main/examples/quick-start-advanced/stacks/orgs/cp/_defaults.yaml) - global settings
  for the entire solution

:::note
The stack configuration structure and the file names described above are just an example of how to name and structure the config files.
You can choose any file name for a stack. You can also include other configuration files (e.g., globals for the environment, and globals for the entire
solution) in a stack config using the `import` directive.
:::

:::info
The `_defaults.yaml` stack manifests are not imported into other Atmos manifests automatically.
You need to explicitly import them using [imports](/core-concepts/stacks/imports).
:::

Stack configuration files have a predefined format:

```yaml
import:
  - orgs/cp/tenant1/staging/_defaults

vars:
  stage: dev

terraform:
  vars: {}

helmfile:
  vars: {}

components:
  terraform:
    vpc:
      command: "/usr/bin/terraform-0.15"
      backend:
        s3:
          workspace_key_prefix: "vpc"
      vars:
        cidr_block: "10.102.0.0/18"
    eks:
      backend:
        s3:
          workspace_key_prefix: "eks"
      vars: {}

  helmfile:
    nginx-ingress:
      vars:
        installed: true
```

It has the following main sections:

- `import` - (optional) a list of stack configuration files to import and combine with the current configuration file
- `vars` - (optional) a map of variables for all components (Terraform and helmfile) in the stack
- `terraform` - (optional) configuration (variables) for all Terraform components
- `helmfile` - (optional) configuration (variables) for all helmfile components
- `components` - (required) configuration for the Terraform and helmfile components

The `components` section consists of the following:

- `terraform` - defines variables, the binary to execute, and the backend for each Terraform component.
  Terraform component names correspond to the Terraform components in
  the [terraform](https://github.com/cloudposse/atmos/tree/main/examples/quick-start-advanced/components/terraform) folder

- `helmfile` - defines variables and the binary to execute for each helmfile component.
  Helmfile component names correspond to the helmfile components in
  the [helmfile](https://github.com/cloudposse/atmos/tree/main/examples/quick-start-advanced/components/helmfile) folder

## Run the Example

To run the example, execute the following commands in a terminal:

- `cd example`
- `make all` - it will build the Docker image, build the CLI tool inside the image, and then start the container

---

## Getting Started with Atmos(Tutorials)

## Introduction

Atmos is part of the SweetOps toolchain and was built to make DevOps and Cloud automation easier across multiple tools. It has direct support for
automating Terraform, Helm, Helmfile, and Istio. By natively utilizing [stacks](/core-concepts/stacks), `atmos` enables you to effortlessly manage
your Terraform and Helmfile [components](/core-concepts/components) from your local machine or in your CI/CD pipelines.

In this tutorial we'll be looking at a simple (albeit contrived) example of automating multiple Terraform components together into a workflow. This
will give you some understanding of what `atmos` can do while also giving you some experience with using it at the command line.

## Prerequisites

### System Requirements

To accomplish this tutorial, you'll need to have [Git](https://git-scm.com/book/en/v2/Getting-Started-Installing-Git)
and [Docker](https://docs.docker.com/get-docker/) installed on your local machine.

**That's it**.

### Understanding

Prior to starting this tutorial, you should be sure you understand [our various concepts and terminology](/core-concepts)) and have gone
through our [Getting started with Geodesic](https://docs.cloudposse.com/tutorials/geodesic-getting-started/) tutorial because we'll be using Geodesic
as our means to run `atmos`.

## Tutorial

### 1. Clone the tutorials repository, then run the `tutorials` image

As part of this tutorial (and others following in our tutorial series), we will utilize [our tutorials](https://github.com/cloudposse/tutorials)
repository](https://github.com/cloudposse/tutorials). The repository includes code and relevant materials for you to use alongside this tutorial
walkthrough.

Let's clone it to your local machine and `cd` into it:

```bash
git clone git@github.com:cloudposse/tutorials.git

cd tutorials
```

Now that we've got our code, we'll want to interact with it using our standard set of tools. Following the SweetOps methodology, we will use a docker
toolbox, a Docker image built on top of Geodesic. This entire `tutorials` repository is actually Dockerized to make that part easy, so let's run
our `cloudposse/tutorials` image:

```bash
# Run our docker image
docker run -it \
          --rm \
          --volume "$HOME":/localhost \
          --volume "$PWD":/tutorials \
          --name sweetops-tutorials \
          cloudposse/tutorials:latest;
```

This command will pull the `tutorials` image to your local machine, run a new container from that image, and mount the various tutorial folders so you
can edit them on your host machine or in the container and the changes will propagate either direction.

![Tutorial Shell](/img/tutorials-3-tutorials-shell.png)

Now that we're running inside our container, let's get into our specific tutorial directory:

```bash
cd /tutorials/02-atmos
```

This `02-atmos/` directory should look like the following:

```
.
├── README.md
├── components/
└── stacks/
```

### 2. Confirm our tools are working

Now that we have an interactive bash login shell open into our `cloudposse/tutorials` image with our home folder, `stacks/`, and `components/`
directories all mounted into it, let's check that all is working correctly by invoking a couple commands to make sure things are install correctly:

```bash
terraform -v # Should return: Terraform vX.X.X

atmos version # Should return a simple Semver number.
```

Awesome! We've now successfully seen our first `atmos` command, and we're ready to start using it!

### 3. Terraform plan and apply a component

Now that we've got access to `atmos`, let's do something simple like execute `plan` and `apply` on some terraform code! To do that, we need two
things:

1. Components -- We've provided 3 small example components in our `components/terraform/` directory, which is mounted to `/tutorials/02-atmos/` inside
   your running container.
1. A Stack configuration -- We've provided a simple example stack located at `stacks/example.yaml`. This is similarly mounted
   to `/tutorials/02-atmos/` inside your running container.

For our example in this step, we'll check out the `components/terraform/fetch-location` component. To plan that component, let's execute the
following:

```bash
atmos terraform plan fetch-location --stack=example
```

If you correctly entered your command, you should see a successful plan which resulted in "Terraform will perform the following actions" followed by "
Changes to Outputs." You'll notice this first executes a `terraform init` before doing the plan. This is intentional to ensure `atmos` can be invoked
without prior project setup. Note, we'll discuss the currently unknown `--stack` parameter shortly.

So now that we've done a plan... let's get this project applied. We could invoke `atmos terraform apply ...`, but our best option at this point would
be to invoke `deploy` which will execute a terraform `init`, `plan`, and `apply` in sequential order:

```bash
atmos terraform deploy fetch-location --stack=example
```

Even though this component didn't have any resources, your deploy’s `apply` step will utilize
the [`http`](https://registry.terraform.io/providers/hashicorp/http/latest/docs/data-sources/http) data source to invoke a request
to `https://ipwhois.app/json/` and output your city, region, country, and latitude + longitude (found by your IP address).

Awesome, we've got a component applied, but that would've been pretty trivial to do without `atmos`, right? We consolidated down 3 commands into one
which is great, but we can do a lot better... Let's show you where `atmos` really provides value: Workflows.

### 5. Invoke an Atmos Workflow

The SweetOps methodology is built on small, composable components because through experience practitioners have found large root modules to become
cumbersome: They require long `plan` times, create large blast radiuses, and don't foster reuse. However, the tradeoff with smaller root
modules ([components](/core-concepts/components)) is that you then need to orchestrate them in an order that makes sense for what you're building.
That is where `atmos` workflows come in. Workflows enable you to describe the ordering of how you want to orchestrate your terraform or helmfile
components so that you can quickly invoke multiple components via one command. Let's look at an example in our `/stacks/example.yaml` file:

```yaml
vars: {}

terraform:
  vars: {}

helmfile:
  vars: {}

components:
  terraform:
    fetch-location:
      vars: {}

    fetch-weather:
      vars: {}

    output-results:
      vars:
        print_users_weather_enabled: true

  helmfile: {}

workflows:
  deploy-all:
    description: Deploy terraform projects in order
    steps:
      - command: terraform deploy fetch-location
      - command: terraform deploy fetch-weather
      - command: terraform deploy output-results
```

Here we can see our first stack, so let's break this file down to help understand what it is doing:

1. We've got a couple of empty elements at the top: `import` and `vars`. We'll address these in an upcoming tutorial.
1. We've got `terraform` and `helmfile` elements that have empty `vars` elements. These provide any shared configuration variables across components
   when dealing with more complicated stacks. We'll address these in an upcoming tutorial as well.
1. We've got our `components` element which has `terraform` and `helmfile` elements. This is where we describe our various components that make up our
   stack and the input configurations that we want to invoke them with (via their `vars` elements). You can see here we have our 3 terraform
   components from within our `components/terraform/` folder specified here and some configuration to go along with them.
1. Finally, we've got our `workflows` element. This is a `map` type element that accepts a workflow name as the key and then the description and steps
   as values. In the example `deploy-all` workflow, our steps are `job` items which describe to `atmos` that we want to run `atmos terraform deploy`
   on each component in our stack.

To sum it up, our stack represents an environment: It describes the components we need to deploy for that environment, the configuration we want to
supply to those components, and finally the ordering of how to orchestrate those components. This is immensely powerful as it enables us to provide
one source of truth for what goes into building an environment and how to make it happen.

Now that we know what is in our `example.yaml` stack configuration, let's invoke that workflow:

```bash
atmos workflow deploy-all -f example.yaml -s example
```

This will run our various steps through `atmos` and you should see the sequential `init`, `plan`, and `apply` of each component in the workflow to
output the current weather for your area. We hope it's sunny wherever you're at 😁 🌤

Let's move on to updating our code and getting a feel for working a bit more hands on with `atmos` and stacks.

### 6. Update our Stack

One of the critical philosophies that SweetOps embodies is a focus
on [improving Day 2+ operations](https://docs.cloudposse.com/fundamentals/philosophy/#optimize-for-day-2-operations) and with that in mind, it's
important to know how you would update this stack and utilize `atmos` to make those changes. Luckily, that's as simple as you might think. Let's try
it out and update the `stacks/example.yaml` file on our local machines to the following:

```yaml
vars: {}

terraform:
  vars: {}

helmfile:
  vars: {}

components:
  terraform:
    fetch-location:
      vars: {}

    fetch-weather:
      vars:
        # Let's get the weather for a particular day.
        # Feel free to update to a date more relevant to you!
        date: 2021/03/28

    output-results:
      vars:
        print_users_weather_enabled: false # We disable outputting via our Terraform local-exec.

  helmfile: {}

workflows:
  deploy-all:
    description: Deploy terraform projects in order
    steps:
      - command: terraform deploy fetch-location
      - command: terraform deploy fetch-weather
      - command: terraform deploy output-results
```

Above, we updated a couple of variables to change the behavior of our terraform code for this particular stack. Since we mounted our local `stacks/`
folder to our Atmos container via `--volume` argument, when you save the above stack file, Docker will update your container's
`/stacks/example.yaml` file as well.

Now to execute this again... we simply invoke our `deploy-all` workflow command.
This should run through our workflow similar to the way we did it before. Still, this time we'll see our temperature return from the weather API for
the date you specified instead of today's date. We'll skip over our terraform `local-exec`'s `echo` command for "pretty printing" our weather data.
Instead, we'll just get our updated weather information as one of our `Outputs`.

## Conclusion

Wrapping up, we've seen some critical aspects of SweetOps in action as part of this tutorial:

1. Another usage of Geodesic to easily provide a consistent environment where we have easy access to tools (like `atmos` and `terraform`).
1. An example stack along with the breakdown of what goes into a stack and why it is a powerful way to describe an environment.
1. Example components that require a specific workflow in order to execute correctly.
1. Usage of Atmos in executing against some terraform code and orchestrating a workflow from our stack.

With these tools, you can skip documenting the various steps of building an environment (aka WikiOps) and instead focus on just describing and
automating those steps! And there are many more Atmos features that can do beyond this brief intro, so keep looking around the docs for more
usage patterns!

Want to keep learning but with a more real-world
use-case? [Check out our next tutorial on deploying your first AWS environment with SweetOps](/tutorials/first-aws-environment).

---

## Your First Environment on AWS

## Intro

Up until now, our tutorial content has been been primarily using contrived examples to enable folks to get up to speed with some of the more fundamental SweetOps concepts and tools. This tutorial will be a bit different in that we'll actually get something deployed to your AWS account so you can see how SweetOps works in a more concrete way.

## Prerequisites

### System Requirements

To accomplish this tutorial, you'll need the following:

- [Git](https://git-scm.com/book/en/v2/Getting-Started-Installing-Git)
- [Docker](https://docs.docker.com/get-docker/)
- An AWS test account and credentials for that account added to [Leapp](https://docs.cloudposse.com/reference/tools#leapp) using the "default" profile so they're ready to be used in Geodesic.
  - We recommend Administrator privileges to make everything easy, but you can likely get away with credentials that only allow S3, CloudFront, and DynamoDB access.
  - If you haven't set this up before, check out [the Geodesic how-to on authenticating with Leapp](https://docs.cloudposse.com/howto/geodesic/authenticate-with-leapp/).

### Understanding

Prior to starting this tutorial, you should be sure that you understand [our various concepts and terminology](/core-concepts) and have gone through our [Getting started with Geodesic](https://docs.cloudposse.com/tutorials/geodesic-getting-started/) and [Getting started with Atmos tutorial](/tutorials/atmos-getting-started/) because we'll be building upon the knowledge in both of those tutorials.

## Tutorial

### 1. Clone the Tutorials Repository + Run the `tutorials` Image

As part of this tutorial (and others following in our tutorial series), we will utilize [our tutorials](https://github.com/cloudposse/tutorials) repository.
This repository includes code and relevant materials for you to use alongside this tutorial walkthrough.

Let's clone it to your local machine and `cd` into it:

```bash
git clone git@github.com:cloudposse/tutorials.git

cd tutorials
```

Now that we've got our code, we'll want to interact with it using our standard set of tools. Following the SweetOps methodology, we will use a docker toolbox, a Docker image built on top of Geodesic. This entire `tutorials` repository is actually Dockerized to make that part easy, so let's run our `cloudposse/tutorials` image:

```bash
# Run our docker image
docker run -it \
          --rm \
          --volume "$HOME":/localhost \
          --volume "$PWD":/tutorials \
          --name sweetops-tutorials \
          cloudposse/tutorials:latest;
```

This will pull the `tutorials` image to your local machine, run a new container from that image, and mount the various tutorial folders so you can edit them on your host machine or in the container and the changes will propagate either direction.

![Tutorial Shell](/img/tutorials-3-tutorials-shell.png)

Now that we're running inside of our container, let's start a new shell as your AWS profile and get into our specific tutorial directory:

```bash
cd /tutorials/03-first-aws-environment
```

> _**Troubleshooting tip**:_ If you are logged in with Leapp using profile "default", you should already
be authenticated in Geodesic and see a green "√" at the beginning of the command line prompt.
> If you are logged in with a different profile name, configure Geodesic to use it by
> running `export AWS_PROFILE=profile-name` where "profile-name" is the Named Profile in your
> Leapp session. If you have the profile names in sync but you see a red "✗" instead of the green "√",
> review the How-To on [authenticating with Leapp](https://docs.cloudposse.com/howto/geodesic/authenticate-with-leapp/) and get that working before proceeding further.

This `03-first-aws-environment/` directory that you're now in should be looking a little bit familiar from our `atmos` tutorial:

* We've got a `components/` directory which holds terraform components that we're looking to `apply` to our AWS Account.
* We've got a `stacks/` directory which holds the declarative descriptions of 3 environments:
  * `uw2-dev.yaml`: Our dev environment hosted in us-west-2.
  * `uw2-prod.yaml`: Our prod environment hosted in the same region.
  * `ue2-root.yaml`: Our global resources hosted in us-east-2.
* We've also got a `bin/random-pet.sh` file that we'll discuss shortly.

Cool, let's jump into our next step and actually do something with this small project.

### 2. Build and generate our `tfstate-backend` for this project

In our last tutorial, we showed `atmos` being used to apply some fairly simple terraform projects that didn't actually create any resources in the cloud. For this tutorial however, we're going to be making a simple static site in AWS and as part of that we should be sure to follow one of the most important best practices in terraform: **Use remote state**. Luckily, `atmos` and stacks make that easy on us by providing a simple abstraction layer to configure all our terraform components with a single backend config. In this example, we'll be using the S3 backend and [the tfstate-backend component](https://github.com/cloudposse/terraform-aws-components/tree/master/modules/tfstate-backend) to automate the resources we need for that backend, so let's dive into that:

1. To start off, we need to do something funky to generate unique resource names. This is due to this tutorial being used by many people, so it's not something a normal project setting would need. The reasoning is that this tutorial is creating S3 buckets and their names need to be globally unique across all AWS accounts. To make that easy on you, we've included a little helper script (`bin/random-pet.sh`) which was mentioned previously. This small script generates some random pet names and utilizes [`yq`](https://mikefarah.gitbook.io/yq/commands/evaluate) to update some of our stack files to include those random names. Let's run that now:

```bash
bin/random-pet.sh
```

This should update most of our stack and catalog files to now include uniquely generated names.

2. Next, we need to get our `tfstate-backend` component deployed using local state before we can actually utilize any backend. This is a good example of [the chicken or the egg problem](https://en.wikipedia.org/wiki/Chicken_or_the_egg), so it's a bit funky but luckily we only need to do this initial set of steps once for all components. To get started let's plan and apply our `tfstate-backend` component:

```bash
atmos terraform plan tfstate-backend --stack ue2-root

# Check the plan looks good and doesn't have 'TODO' anywhere if so, be sure to run `bin/random-pet.sh`)
atmos terraform apply tfstate-backend --stack ue2-root
```

This will provision our S3 bucket + Dynamo DB table for usage as our backend.

3. Next, let's generate the configuration for that backend:

```bash
atmos terraform generate backend tfstate-backend --stack ue2-root
```

This will look at our stack, find the imported `terraform.backend.s3` configuration and build a valid `backend.tf.json` file and put it in our component directory. Then going forward, whenever we `plan` or `apply` against that component it will be properly configured to use that S3 backend.

4. Now that we have our `backend.tf.json` file, we can change over our `tfstate-backend` component from using local state to its S3 backend:

```bash
atmos terraform plan tfstate-backend --stack ue2-root
```

This will prompt you with the following:

```txt
Do you want to migrate all workspaces to "s3"?
Both the existing "local" backend and the newly configured "s3" backend
support workspaces. When migrating between backends, Terraform will copy
all workspaces (with the same names). THIS WILL OVERWRITE any conflicting
states in the destination.

Terraform initialization doesn't currently migrate only select workspaces.
If you want to migrate a select number of workspaces, you must manually
pull and push those states.

If you answer "yes", Terraform will migrate all states. If you answer
"no", Terraform will abort.

Enter a value:
```

Enter "yes" and this will migrate our local state to our S3 backend. Success!

5. Now that we've migrated our single "chicken or the egg" component, we'll want to generate the `backend.tf.json` file for our other components as well considering we'll want them to use the S3 backend too. Luckily for us, there is only one, simple component as part of this tutorial: `components/terraform/static-site`. Let's go ahead and generate the backend config for that component:

```bash
atmos terraform generate backend static-site --stack uw2-dev
```

Exactly the same as for our `tfstate-backend` component, our `atmos` `backend generate` command will put a `backend.tf.json` file in our `components/terraform/static-site/` directory so that component will always utilize the correct backend. If this were a real project that we were working on and practicing proper GitOps, then we'd actually check those files into git so that our backend configuration would persist going forward, but since this is a tutorial we'll skip that step.

### 3. Apply our Static Site

Next, let's move onto applying our `static-site` component to actually build something real on AWS. Well that's pretty simple actually:

```bash
atmos terraform deploy static-site --stack uw2-dev
```

This command will plan and apply our `static-site` component with our configuration specified in our `stacks/uw2-dev.yaml` stack. This should output something similar to the following at the end:

```txt
Apply complete! Resources: 7 added, 0 changed, 0 destroyed.

Outputs:

domain_name = "CLOUDFRONT_ID.cloudfront.net"
s3_bucket_name = "acme-uw2-dev-static-site-RANDOM_PET-origin"
```

If you copy the `domain_name` value and paste it into your browser then you should see our simple static site!

![Static Site Example Image](/img/tutorial-3-static-site.png)

As you can see, this deploys our static site for our `dev` environment, but not our `prod` environment. This tutorial won't cover deploying the `prod` environment, but feel free to play around with the variables if you would like to mess around with it!

## Conclusion

In this tutorial, we've given you a bit more of taste of what it looks like to work on a real project while following the SweetOps methodology:

1. We've seen real world components: `tfstate-backend` and `static-site`. These are sharable root modules that you didn't need write yourself or touch to get up and running, you just need to supply the configuration. We have a whole library of components (which `tfstate-backend` is one of) over at [`cloudposse/terraform-aws-components`](https://github.com/cloudposse/terraform-aws-components).
1. We utilized `atmos` to deploy our terraform state backend for our various components and then also generate the `backend.tf.json` files to ensure our components are properly being built from a central configuration in our `stacks/` directory.
1. You now have an example of what it looks like to organize stack configurations **for different environments and regions**. This is immensely important and is why SweetOps is so powerful: Our stacks centrally define our environments, where they're deployed, and how they're configured; the components are completely agnostic to their environment and only responsible for business logic. This separation is critical to building reusable, composable infrastructure. Not to mention, we ensure that critical configurations are declared as code and not just committed to the memory of a few senior engineers.

With this knowledge, you're now ready to actually build projects on AWS using the SweetOps methodology!

---

## Tutorials


Here are some lessons on how to implement Atmos in a project.

import DocCardList from '@theme/DocCardList'

<DocCardList/>

---

## Welcome to Atmos
